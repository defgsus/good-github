# [<](2020-10-14.md) 2020-10-15 [>](2020-10-16.md)

195,435 events, 110,465 push events, 176,978 commit messages, 13,354,027 characters


## [mpv-player/mpv](https://github.com/mpv-player/mpv)@[57fbc9cd76...](https://github.com/mpv-player/mpv/commit/57fbc9cd76f7a78f1034c42dd3c453ff35123264)
#### Thursday 2020-10-15 22:05:40 by wm4

player: make repeated hr-seeks past EOF trigger EOF as expected

If you have a normal file with audio and video, and keep "spamming"
forward hr-seeks, the player just kept showing the last video frame
instead of exiting or playing the next file. This started happening
since commit 6bcda94cb. Although not a bug per se, it was odd, and very
user-noticable.

The main problem was that the pending seek command was processed before
the EOF was "noticed". Processing the command reset everything, so the
player did not terminate playback, but repeated the seek.

This commit restores the old behavior.

For one, it makes video return the correct status (video.c). The
parameter is a bit ugly, but better than duplicating the logic or having
another MPContext field. (As a minor detail, setting r=VD_EOF makes sure
have_new_frame() returns true, rather than going through another
iteration or whatever the hell will happen instead, which would clobber
logical_eof.)

Another thing is making the seek logic actually wait until the seek
outcome has been determined if audio is also active. Audio needs to wait
for video in order to get the video seek target position. (Which in turn
is because hr-seek still "snaps" to video frames. You can't seek in
between two frames, so audio can't just use the seek target, but always
has to wait on the timestamp of the video frame. This has other
disadvantages and is a misdesign, but not something I'll fix today.)

In theory, this might make hr-seeks less responsive, because it needs to
fully decode/filter the audio too, but in practice most time is spent on
video, which had to be fully decoded before this change. (In general,
hr-seek could probably just show a random frame when a queued hr-seek
overrides the current hr-seek, which would probably lead to a better
user experience, but that's out of scope.)

Fixes: #7206

---
## [Mu-L/openstack](https://github.com/Mu-L/openstack)@[dd6b5d5e11...](https://github.com/Mu-L/openstack/commit/dd6b5d5e11d43197492055ec62fef3e8d239c27e)
#### Thursday 2020-10-15 22:47:16 by Zuul

Update git submodules

* Update keystone from branch 'master'
  - Merge "Implement more robust connection handling for asynchronous LDAP calls"
  - Implement more robust connection handling for asynchronous LDAP calls
    
    Keystone's paging implementation contains a memory leak. The issue is
    noticeable if you integrate keystone with an LDAP server that supports
    paging and set `keystone.conf [ldap] page_size` to a low integer
    (e.g., 5).
    
    Keystone's LDAP backend uses `python-ldap` to interact with LDAP
    servers. For paged requests, it uses `search_ext()`, which is an
    asynchronous API [0]. The server responds with a message ID, which the
    client uses to retrieve all data for the request. In keystone's case,
    the `search_ext()` method is invoked with a page control that tells
    the server to deliver responses in increments according to the page
    size configured with `keystone.conf [ldap] page_size`. So long as the
    client has the original connection used to fetch the message ID, it
    can request the rest of the information associated to the request.
    
    Keystone's paging implementation loops continuously for paged
    requests. It takes the message ID it gets from `search_ext()` and
    calls `result3()`, asking the server for the data associated with that
    specific message. Keystone continues to do this until the server sends
    an indicator that it has no more data relevant to the query (via a
    cookie). The `search_ext()` and `result3()` methods must use the same
    LDAP connection.
    
    Given the above information, keystone uses context managers to provide
    connections. This is relevant when deploying connection pools, where
    certain connections are re-used from a pool. Keystone relies on Python
    context managers to handle connections, which is pretty typical
    use-case for context managers. Connection managers allow us to do the
    following (assuming pseudocode):
    
      with self.get_connection as conn:
          response = conn.search_s()
          return format(response)
    
    The above snippet assumes the `get_connection` method provides a
    connection object and a callable that implements `search_s`. Upon
    exiting the `with` statement, the connection is disconnected, or put
    back into the pool, or whatever the implementation of the context
    manager decides to do. Most connections in the LDAP backend are
    handled in this fashion.
    
    Unfortunately, the LDAP driver is somewhat oblivious to paging, it's
    control implementation, or the fact that it uses an asynchronous API.
    Instead, the driver leaves it up to the handler objects it uses for
    connections to determine if the request should be controlled via
    paging. This is an anti-pattern since the backend establishes the
    connection for the request but doesn't ensure that connection is
    safely handled for asynchronous APIs.
    
    This forces the `search_ext()` and `result3()` implementations in the
    PooledLDAPHandler to know how to handle connections and context
    managers, since it needs to ensure the same connection is used for
    paged requests. The current code tried to clean up the context
    manager responsible for connections after the results are collected
    from the server using the message ID. I believe it does this because
    it needs to get a new connection for each message in the paged
    results, even though it already operates from within a connection
    established via a context manager and the PooledLDAPHandler almost
    always returns the same connection object from the pool. The code
    tries to use a weak reference to create a callback that tears down the
    context manager when nothing else references it. At a high-level, the
    idea is similar to the following pseudocode:
    
      with self.get_connection as conn:
          while True:
    	ldap_data = []
    	context_manager = self.get_connection()
    	connection = context_manager.__enter__()
    	message_id = connection.search_ext()
    	results = connection.result3(message_id)
    	ldap_data.append(results)
    	context_manager.__exit__()
    
    I wasn't able to see the callback get invoked or work as described in
    comments, resulting in memory bloat, especially with low page sizes
    which results in more requests. A weak reference invokes the callback
    when the weak reference is called, but there are no other references
    to the original object [1]. In our case, I don't think we invoke that
    path because we don't actually do anything with the weak reference. We
    assume it's going to run the callback when the object is garbage
    collected.
    
    This commit attempts to address this issue by using the concept of a
    finalizer [2], which was designed for similar cases. It also attempts
    to hide the cleanup implementation in the AsynchronousMessage object,
    so that callers don't have to worry about making sure they invoke the
    finalizer.
    
    An alternative approach would be to push more of the paging logic and
    implementation up into the LDAP driver. This would make it easier to
    put the entire asynchronous API flow for paging into a `with`
    statement and relying on the normal behavior of context managers to
    clean up accordingly. This approach would remove the manual cleanup
    invocation, regardless of using weak references or finalizer objects.
    However, this approach would likely require a non-trivial amount of
    design work to refactor the entire LDAP backend. The LDAP backend has
    other issues that would complicate the re-design process:
    
      - Handlers and connection are generalized to mean the same thing
      - Method names don't follow a convention
      - Domain-specific language from python-ldap bleeds into keystone's
        implementation (e.g., get_all, _ldap_get_all, add_member) at
        different points in the backend (e.g., UserApi (BaseLdap), GroupApi
        (BaseLdap), KeystoneLDAPHandler, PooledLDAPHandler,
        PythonLDAPHandler)
      - Backend contains dead code from when keystone supported writeable
        LDAP backends
      - Responsibility for connections and connection handling is spread
        across objects (BaseLdap, LDAPHandler)
      - Handlers will invoke methods differently based on configuration at
        runtime, which is a sign that the relationship between the driver,
        handlers, and connection objects isn't truely polymorphic
    
    While keeping the logic for properly handling context managers and
    connections in the Handlers might not be ideal, it is a relatively
    minimal fix in comparison to a re-design or backend refactor. These
    issues can be considered during a refactor of the LDAP backend if or
    when the community decides to re-design the LDAP backend.
    
    [0] https://www.python-ldap.org/en/python-ldap-3.3.0/reference/ldap.html#ldap.LDAPObject.search_ext
    [1] https://docs.python.org/3/library/weakref.html#weakref.ref
    [2] https://docs.python.org/3/library/weakref.html#finalizer-objects
    
    Closes-Bug: 1896125
    Change-Id: Ia45a45ff852d0d4e3a713dae07a46d4ff8d370f3

---
## [OdSazib/halcyon_kernel_sdm660](https://github.com/OdSazib/halcyon_kernel_sdm660)@[0e484a8f51...](https://github.com/OdSazib/halcyon_kernel_sdm660/commit/0e484a8f519e05f43ea4a6f01a12a9dd347238f0)
#### Thursday 2020-10-15 23:18:04 by Michal Hocko

proc: oom: drop bogus task_lock and mm check

Series "Handle oom bypass more gracefully", V5

The following 10 patches should put some order to very rare cases of mm
shared between processes and make the paths which bypass the oom killer
oom reapable and therefore much more reliable finally.  Even though mm
shared outside of thread group is rare (either vforked tasks for a short
period, use_mm by kernel threads or exotic thread model of
clone(CLONE_VM) without CLONE_SIGHAND) it is better to cover them.  Not
only it makes the current oom killer logic quite hard to follow and
reason about it can lead to weird corner cases.  E.g.  it is possible to
select an oom victim which shares the mm with unkillable process or
bypass the oom killer even when other processes sharing the mm are still
alive and other weird cases.

Patch 1 drops bogus task_lock and mm check from oom_{score_}adj_write.
This can be considered a bug fix with a low impact as nobody has noticed
for years.

Patch 2 drops sighand lock because it is not needed anymore as pointed
by Oleg.

Patch 3 is a clean up of oom_score_adj handling and a preparatory work
for later patches.

Patch 4 enforces oom_adj_score to be consistent between processes
sharing the mm to behave consistently with the regular thread groups.
This can be considered a user visible behavior change because one thread
group updating oom_score_adj will affect others which share the same mm
via clone(CLONE_VM).  I argue that this should be acceptable because we
already have the same behavior for threads in the same thread group and
sharing the mm without signal struct is just a different model of
threading.  This is probably the most controversial part of the series,
I would like to find some consensus here.  There were some suggestions
to hook some counter/oom_score_adj into the mm_struct but I feel that
this is not necessary right now and we can rely on proc handler +
oom_kill_process to DTRT.  I can be convinced otherwise but I strongly
think that whatever we do the userspace has to have a way to see the
current oom priority as consistently as possible.

Patch 5 makes sure that no vforked task is selected if it is sharing the
mm with oom unkillable task.

Patch 6 ensures that all user tasks sharing the mm are killed which in
turn makes sure that all oom victims are oom reapable.

Patch 7 guarantees that task_will_free_mem will always imply reapable
bypass of the oom killer.

Patch 8 is new in this version and it addresses an issue pointed out by
0-day OOM report where an oom victim was reaped several times.

Patch 9 puts an upper bound on how many times oom_reaper tries to reap a
task and hides it from the oom killer to move on when no progress can be
made.  This will give an upper bound to how long an oom_reapable task
can block the oom killer from selecting another victim if the oom_reaper
is not able to reap the victim.

Patch 10 tries to plug the (hopefully) last hole when we can still lock
up when the oom victim is shared with oom unkillable tasks (kthreads and
global init).  We just try to be best effort in that case and rather
fallback to kill something else than risk a lockup.

This patch (of 10):

Both oom_adj_write and oom_score_adj_write are using task_lock, check for
task->mm and fail if it is NULL.  This is not needed because the
oom_score_adj is per signal struct so we do not need mm at all.  The code
has been introduced by 3d5992d2ac7d ("oom: add per-mm oom disable count")
but we do not do per-mm oom disable since c9f01245b6a7 ("oom: remove
oom_disable_count").

The task->mm check is even not correct because the current thread might
have exited but the thread group might be still alive - e.g.  thread group
leader would lead that echo $VAL > /proc/pid/oom_score_adj would always
fail with EINVAL while /proc/pid/task/$other_tid/oom_score_adj would
succeed.  This is unexpected at best.

Remove the lock along with the check to fix the unexpected behavior and
also because there is not real need for the lock in the first place.

Link: http://lkml.kernel.org/r/1466426628-15074-2-git-send-email-mhocko@kernel.org
Signed-off-by: Michal Hocko <mhocko@suse.com>
Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
Acked-by: Oleg Nesterov <oleg@redhat.com>
Cc: David Rientjes <rientjes@google.com>
Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Change-Id: Id5b29755e8dbc70fc104263e207bb54de5eeb6ee

---
## [pytorch/pytorch](https://github.com/pytorch/pytorch)@[dda95e6914...](https://github.com/pytorch/pytorch/commit/dda95e69142df17e638076b2a0b46837951144a5)
#### Thursday 2020-10-15 23:35:46 by Taylor Robie

More Timer refinement (#46023)

Summary:
This PR just adds more polish to the benchmark utils:

1) `common.py`, `timer.py`, and `valgrind_wrapper/timer_interface.py` are now MyPy strict compliant. (except for three violations due to external deps.) Compare and Fuzzer will be covered in a future PR.
2) `CallgrindStats` now uses `TaskSpec` rather than accepting the individual fields which brings it closer to `Measurement`.
3) Some `__repr__` logic has been moved into `TaskSpec` (which `Measurement` and `CallgrindStats` use in their own `__repr__`s) for a more unified feel and less horrible f-string hacking, and the repr's have been given a cleanup pass.
4) `Tuple[FunctionCount, ...]` has been formalized as the `FunctionCounts` class, which has a much nicer `__repr__` than just the raw tuple, as well as some convenience methods (`__add__`, `__sub__`, `filter`, `transform`) for easier DIY stat exploration. (I find myself using the latter two a lot now.) My personal experience is that manipulating `FunctionCounts` is massively more pleasant than the raw tuples of `FunctionCount`. (Though it's still possible to get at the raw data if you want.)
5) Better support for multi-line `stmt` and `setup`.
6) Compare now also supports rowwise coloring, which is often the more natural layout for A/B testing.
7) Limited support for `globals` in `collect_callgrind`. This should make it easier to benchmark JIT models. (CC ZolotukhinM)
8) More unit tests, including extensive tests for the Callgrind stats manipulation APIs.
9) Mitigate issue with `MKL_THREADING_LAYER` when run in Jupyter. (https://github.com/pytorch/pytorch/issues/37377)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46023

Test Plan: changes should be covered by existing and new unit tests.

Reviewed By: navahgar, malfet

Differential Revision: D24313911

Pulled By: robieta

fbshipit-source-id: 835d4b5cde336fb7ff0adef3c0fd614d64df0f77

---

# [<](2020-10-14.md) 2020-10-15 [>](2020-10-16.md)

