# [<](2020-04-13.md) 2020-04-14 [>](2020-04-15.md)

184,043 events, 100,556 push events, 158,142 commit messages, 11,346,616 characters


## [kokizzu/cockroach](https://github.com/kokizzu/cockroach)@[470c5f4234...](https://github.com/kokizzu/cockroach/commit/470c5f4234cd7c039d90f8890a1ecb8a1d1815a9)
#### Tuesday 2020-04-14 22:28:20 by craig[bot]

Merge #47431 #47465

47431: sql: fix renaming of columns involved in hash sharded indexes r=rohany a=ajwerner

Prior to this commit, renames in hash sharded indexes were busted in a few
different ways.

1) The column references were using `UnresolvedName` when it should have been
   using `*tree.ColumnItem`. This was going on both in the check constaint and
   in the computed expr.

2) The `ShardedDescriptor` was not being updated to refer to the new name.

Another rough edge in all of this is that we're in deep trouble if we can't
tie the shard column back to the index[es] which are responsible for its
existence. Today this relationship is annoyingly tenuous. In an ideal world
we'd populate the ColumnDescriptor with information to mark it as a sharded
column and, ideally, the sharded descriptor. Furthermore, it's something of a
bummer that we have multiple of the same `ShardedDescriptor`s which each imply
a column but are denormalized into the `IndexDescriptor`.

Given we are where we are however, we really want to keep the name of the
shard column in sync with its source columns. This commit makes sure that
happens when member columns are renamed, the shard column is accordingly
renamed. The commit also disallows the renaming of a shard column as that
would upset the careful name synchronization.

It's worth noting that this is a little bit weird as other auto-generated
names don't change when the members change. For example an index created
with a name in the `CREATE TABLE` statement of a `FAMILY` clause.

Fixes #47087.

Release note (bug fix): In earlier betas, columns which were members of
hash sharded indexes could not be renamed. Indexes created in prior
releases will need to be dropped and recreated to resolve this limitation.

47465: kv: release latches before semi-synchronous intent resolution r=nvanbenschoten a=nvanbenschoten

Fixes #47187.
Fixes #47186.

This commit addresses the deadlock described in #47187. In that issue, we saw that a single batch deadlocked while performing synchronous intent resolution while continuing to hold latches over the spans that it was trying to resolve. This cascaded into a full workload deadlock because all other requests piled up in the latch manager.

This deadlock was introduced in 0e4fac5. That change made it possible for semi-synchronous intent resolution to begin on the read-write path before a request had released its latches. Specifically, in that issue we saw an EndTxn that hit a TransactionAbortedError run into this bug. This case is now tested in `TestEndTxnWithErrorAndSyncIntentResolution`. Before this commit, the test successfully reproduced the deadlock.

It turns out that this has also been very close to a bug in the read-only path for years. In fact, I believe it was broken or almost broken from the very first commit that introduced this semi-synchronous intent resolution: 9858253. Even in that commit, we see that latches (at the time, the CommandQueue) are held during the call to (poorly named) `processIntentsAsync` in `addReadOnlyCmd`. I believe the saving grace here was that we only seem to populate the Intents field on the LocalResult for INCONSISTENT reads, which don't hold latches and therefore can't create the deadlock. But if that's really the case then I don't understand https://github.com/cockroachdb/cockroach/blob/29c0efdcc5edb5d100449a093b25df107f1df2d6/pkg/kv/kvserver/replica_read.go#L144. Either way, this prevented me from hitting a deadlock here or writing a test for this specific case.

I've tested this with 20 iterations of `cdc/tpcc-1000/rangefeed=true` and it has been stable throughout. I'll spin up another 30 tonight.

Release notes (bug fix): a bug that could cause a workload to stall under heavy load has been fixed. This stall was due to a deadlock that was introduced in an earlier v20.1 release.

Release justification: fixes a high-priority bug in existing functionality. The bug could result in full-workload deadlocks under heavy load.

Co-authored-by: Andrew Werner <ajwerner@cockroachlabs.com>
Co-authored-by: Nathan VanBenschoten <nvanbenschoten@gmail.com>

---
## [kokizzu/cockroach](https://github.com/kokizzu/cockroach)@[4a7c77d753...](https://github.com/kokizzu/cockroach/commit/4a7c77d753a72faff2c6041800b369abcb56f4d4)
#### Tuesday 2020-04-14 22:28:20 by Andrew Werner

sql: fix renaming of columns involved in hash sharded indexes

Prior to this commit, renames in hash sharded indexes were busted in a few
different ways.

1) The column references were using `UnresolvedName` when it should have been
   using `*tree.ColumnItem`. This was going on both in the check constaint and
   in the computed expr.

2) The `ShardedDescriptor` was not being updated to refer to the new name.

Another rough edge in all of this is that we're in deep trouble if we can't
tie the shard column back to the index[es] which are responsible for its
existence. Today this relationship is annoyingly tenuous. In an ideal world
we'd populate the ColumnDescriptor with information to mark it as a sharded
column and, ideally, the sharded descriptor. Furthermore, it's something of a
bummer that we have multiple of the same `ShardedDescriptor`s which each imply
a column but are denormalized into the `IndexDescriptor`.

Given we are where we are however, we really want to keep the name of the
shard column in sync with its source columns. This commit makes sure that
happens when member columns are renamed, the shard column is accordingly
renamed. The commit also disallows the renaming of a shard column as that
would upset the careful name synchronization.

It's worth noting that this is a little bit weird as other auto-generated
names don't change when the members change. For example an index created
with a name in the `CREATE TABLE` statement of a `FAMILY` clause.

Release note (bug fix): In earlier betas, columns which were members of
hash sharded indexes could not be renamed. Indexes created in prior
releases will need to be dropped and recreated to resolve this limitation.

---
## [nonomal/git](https://github.com/nonomal/git)@[c716fe4bd9...](https://github.com/nonomal/git/commit/c716fe4bd917e013bf376a678b3a924447777b2d)
#### Tuesday 2020-04-14 22:55:08 by Jeff King

credential: detect unrepresentable values when parsing urls

The credential protocol can't represent newlines in values, but URLs can
embed percent-encoded newlines in various components. A previous commit
taught the low-level writing routines to die() when encountering this,
but we can be a little friendlier to the user by detecting them earlier
and handling them gracefully.

This patch teaches credential_from_url() to notice such components,
issue a warning, and blank the credential (which will generally result
in prompting the user for a username and password). We blank the whole
credential in this case. Another option would be to blank only the
invalid component. However, we're probably better off not feeding a
partially-parsed URL result to a credential helper. We don't know how a
given helper would handle it, so we're better off to err on the side of
matching nothing rather than something unexpected.

The die() call in credential_write() is _probably_ impossible to reach
after this patch. Values should end up in credential structs only by URL
parsing (which is covered here), or by reading credential protocol input
(which by definition cannot read a newline into a value). But we should
definitely keep the low-level check, as it's our final and most accurate
line of defense against protocol injection attacks. Arguably it could
become a BUG(), but it probably doesn't matter much either way.

Note that the public interface of credential_from_url() grows a little
more than we need here. We'll use the extra flexibility in a future
patch to help fsck catch these cases.

---
## [microsoft/opensource-portal](https://github.com/microsoft/opensource-portal)@[7c891f6998...](https://github.com/microsoft/opensource-portal/commit/7c891f6998bfabd7d38056da89102763e2077ccd)
#### Tuesday 2020-04-14 23:29:05 by Jeff Wilcox

Latest monolith updates

Half a year of drift.

- Operations changes
    - Unlinked users are removed from repos they are a collaborator on directly *if* the new query cache sytem is used
    - Introduces new '/administration' endpoint for configuring GitHub Apps
- User experience improvements
    - Showing all corporate metadata and status on repo pages
    - Transparent audit log data, when available, is shown for repos and teams
    - Repo "undo" experience allows people to restore their own accidental repo permission failures in many situations (opt-in)
    - Forked repos display upstream information in more locations
    - Optional "news" display on the homepage to mention basic updates
- THANK YOU
  - Special thanks to @RomanFritsch and @JonasScholl for contributing important bug fixes. I must do better at immediately handling.
- Scalability
  - Introduces Query Cache approach to showing large lists
  - Additional reliance on cache providers
  - New jobs to get to an eventually consistent state
  - GitHub Apps are the preferred way of interfacing with your GitHub orgs
  - Dymamic settings allows configuring orgs without redeployments
- Additional providers
  - Cosmos DB session provider augments the existing Redis session store provider as a choice
  - New queue processor supports Azure Queues (storage) as a new alternative to Service Bus
  - Refactored REST API cache supports Redis, Cosmos DB, blob storage
  - More generic "corporate profile" provider type for retrieving associated contacts
  - Microsoft graph provider bug fixes to work with any tenant ID (not just MSFT)
  - Newer cache providers include the ability to store larger objects in blob
- New and updated jobs
  - Continued refactoring to a more straightforward await/async approach
  - Recent open source contributors code (FOSS Fund implementation similar to Indeed Starfish)
  - Early implementation of an import routine for the GitHub audit log
  - Blob cache cleanup job if using expiring blob cache provider
  - More generic job for updating user display information over time (name changes, etc.)
- Scripts folder
  - Theretical place for scripts that do not really need to be deployed or used
  - Adds a simple Postgres setup script for new installations or environments
- Newer entities
  - Audit log records (webhook events captured for key scenarios and undo candidate operations)
  - Public contribution events record store
  - Organization member cache as part of the Query Cache system
  - Campaign settings for allowing unsubscribe operations from targeted mail
- Updated APIs
  - API to link a user
- New feature flags
  - Newer code and capabilities are using feature flags that are opt-in based
  - Incoming repo transfer lockdown
  - Fork lockdown system
  - Undo system
- Updated core components
  - Introduces the ability to retrieve a user's public GitHub event log
  - Can retrieve the GitHub Pages configuration associated with a repo
  - Base URLs are more consistent inside the system for entities to self-report locations
- Chores
  - Continued refactoring
  - Latest @octokit/rest.js and associated libraries in use, including many breaking rename changes
  - Better use of TypeScript annotations whenever possible
  - Some GitHub endpoints try and use documented alternate by-ID paths when possible vs slugs/names
  - Strong removal of callback pyramids
  - APIs less likely to be limited by GitHub such as cross-org calls when using GitHub Apps are performed in parallel
  - Attempts at using more shared utility code
  - The 'Q' library and its related codepaths have been removed
  - Continued removal of more uses of the 'async' library
  - Many version updates including adoption of the newer generation of Azure libraries
  - Default cache times have been reduced
  - Microsoft-specific "review service" has been refactored in a way that should not impact building in other environments; this sub-component will be removed in time.
  - GitHub Apps can be configured through local private key files instead of having to use KeyVault or other systems
  - Imports used instead of requires in more places outside dynamic routes and specific modules
  - Background jobs will use any available GitHub App if you have not configured a background job-specific GitHub App for an org
  - Integrated a change from @JonasScholl to default to allowing HTTP, to make local development easier. Most load balancers keep HTTP traffic away anyway.

- Of note, internal work
  - This pull request does continue to bring in some company-specific wording and phrasing, we are working on improving
  - Starting in April 2020 we build and deploy the "open source" version of this site with every change to keep in sync
  - Removes more references to the 'witness redis' system that was used internally to lookup manager information for users
- Still unfortunate:
  - No useful tests
  - Broken old dead code such as the reports system

---
## [backwardn/mpv](https://github.com/backwardn/mpv)@[f6c81047fa...](https://github.com/backwardn/mpv/commit/f6c81047fa5a9199084fa92327c41c6d8a16b059)
#### Tuesday 2020-04-14 23:35:46 by wm4

player: do not fall back to a default track with explicit selections

Consider e.g. --aid=2 with a file that has only 1 track. Then it would
fall back to selecting track 1. Stop doing this. If no matching track is
found, this will not select any track now.

Note that the fingerprint stuff (track_layout_hash in the source)
prevents softens the impact of this change. Without the fingerprint,
playing a dual-audio file with the second track selected, and then a
single-audio file, would play the second file without audio. But the
fingerprint resets it due to differences in the track list.

Try to exhaustively document this and tricky interactions between the
other features. What a damn mess, I think it's simply cursed. Of course
it's still my fault.

See: #7608

---
## [ale8k/nodescape](https://github.com/ale8k/nodescape)@[35792fc608...](https://github.com/ale8k/nodescape/commit/35792fc608a371f2fdf3b77b4795c9516ff721b1)
#### Tuesday 2020-04-14 23:48:03 by Alexander Kilroy (ale8k)

Summarised P81

^ In prep for writing this shite out properly, it's fucking enormous lol. A lot of data in one packet...
So I'm gonna have to design it in a very particular way, our bit masks will be a callback I think...
I.e., P81(DATA, BITMASK(DATA)), I'm also gonna try (try being the keyword here lol) set it up so that
I can pass it an array of data and just spread it, then have the array be populated from a 'player' file
for testing...

Or maybe a big object? But if my data can check indices properly, an array will work fine.

---

# [<](2020-04-13.md) 2020-04-14 [>](2020-04-15.md)

