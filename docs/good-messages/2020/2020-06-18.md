# [<](2020-06-17.md) 2020-06-18 [>](2020-06-19.md)

164,407 events, 85,398 push events, 139,808 commit messages, 11,689,799 characters


## [StollD/linux-fedora](https://github.com/StollD/linux-fedora)@[e75a239a76...](https://github.com/StollD/linux-fedora/commit/e75a239a760a178d950797ae03319868903c6529)
#### Thursday 2020-06-18 22:05:31 by Jeremy Cline

kdump: add support for crashkernel=auto

Rebased for v5.3-rc1 because the documentation has moved.

    Message-id: <20180604013831.574215750@redhat.com>
    Patchwork-id: 8166
    O-Subject: [kernel team] [PATCH RHEL8.0 V2 2/2] kdump: add support for crashkernel=auto
    Bugzilla: 1507353
    RH-Acked-by: Don Zickus <dzickus@redhat.com>
    RH-Acked-by: Baoquan He <bhe@redhat.com>
    RH-Acked-by: Pingfan Liu <piliu@redhat.com>

    Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1507353
    Build: https://brewweb.engineering.redhat.com/brew/taskinfo?taskID=16534135
    Tested: ppc64le, x86_64 with several memory sizes.
            kdump qe tested 160M on various x86 machines in lab.

    We continue to provide crashkernel=auto like we did in RHEL6
    and RHEL7,  this will simplify the kdump deployment for common
    use cases that kdump just works with the auto reserved values.
    But this is still a best effort estimation, we can not know the
    exact memory requirement because it depends on a lot of different
    factors.

    The implementation of crashkernel=auto is simplified as a wrapper
    to use below kernel cmdline:
    x86_64: crashkernel=1G-64G:160M,64G-1T:256M,1T-:512M
    s390x:  crashkernel=4G-64G:160M,64G-1T:256M,1T-:512M
    arm64:  crashkernel=2G-:512M
    ppc64:  crashkernel=2G-4G:384M,4G-16G:512M,16G-64G:1G,64G-128G:2G,128G-:4G

    The difference between this way and the old implementation in
    RHEL6/7 is we do not scale the crash reserved memory size according
    to system memory size anymore.

    Latest effort to move upstream is below thread:
    https://lkml.org/lkml/2018/5/20/262
    But unfortunately it is still unlikely to be accepted, thus we
    will still use a RHEL only patch in RHEL8.

    Copied old patch description about the history reason see below:
    '''
        Non-upstream explanations:
        Besides "crashkenrel=X@Y" format, upstream also has advanced
        "crashkernel=range1:size1[,range2:size2,...][@offset]", and
        "crashkernel=X,high{low}" formats, but they need more careful
        manual configuration, and have different values for different
        architectures.

        Most of the distributions use the standard "crashkernel=X@Y"
        upstream format, and use crashkernel range format for advanced
        scenarios, heavily relying on the user's involvement.

        While "crashkernel=auto" is redhat's special feature, it exists
        and has been used as the default boot cmdline since 2008 rhel6.
        It does not require users to figure out how many crash memory
        size for their systems, also has been proved to be able to work
        pretty well for common scenarios.

        "crashkernel=auto" was tested/based on rhel-related products, as
        we have stable kernel configurations which means more or less
        stable memory consumption. In 2014 we tried to post them again to
        upstream but NACKed by people because they think it's not general
        and unnecessary, users can specify their own values or do that by
        scripts. However our customers insist on having it added to rhel.

        Also see one previous discussion related to this backport to Pegas:
        On 10/17/2016 at 10:15 PM, Don Zickus wrote:
        > On Fri, Oct 14, 2016 at 10:57:41AM +0800, Dave Young wrote:
        >> Don, agree with you we should evaluate them instead of just inherit
        >> them blindly. Below is what I think about kdump auto memory:
        >> There are two issues for crashkernel=auto in upstream:
        >> 1) It will be seen as a policy which should not go to kernel
        >> 2) It is hard to get a good number for the crash reserved size,
        >> considering various different kernel config options one can setups.
        >> In RHEL we are easier because our supported Kconfig is limited.
        >> I digged the upstream mail archive, but I'm not sure I got all the
        >> information, at least Michael Ellerman was objecting the series for
        >> 1).
        > Yes, I know.  Vivek and I have argued about this for years.  :-)
        >
        > I had hoped all the changes internally to the makedumpfile would allow
        > the memory configuration to stabilize at a number like 192M or 128M and
        > only in the rare cases extend beyond that.
        >
        > So I always treated that as a temporary hack until things were better.
        > With the hope of every new RHEL release we get smarter and better. :-)
        > Ideally it would be great if we could get the number down to 64M for most
        > cases and just turn it on in Fedora.  Maybe someday.... ;-)
        >
        > We can have this conversation when the patch gets reposted/refreshed
        > for upstream on rhkl?
        >
        > Cheers,
        > Don

        We had proposed to drop the historic crashkernel=auto code and move
        to use crashkernel=range:size format and pass them in anaconda.

        The initial reason is crashkernel=range:size works just fine because
        we do not need complex algorithm to scale crashkernel reserved size
        any more.  The old linear scaling is mainly for old makedumpfile
        requirements, now it is not necessary.

        But With the new approach, backward compatibility is potentially at risk.
        For e.g. let's consider the following cases:
        1) When we upgrade from an older distribution like rhel-alt-7.4(which
        uses crashkernel=auto) to rhel-alt-7.5 (which uses the crashkernel=xY
        format)
        In this case we can use anaconda scripts for checking
        'crashkernel=auto' in kernel spec and update to the new
        'crashkernel=range:size' format.
        2) When we upgrade from rhel-alt-7.5(which uses crashkernel=xY format)
        to rhel-alt-7.6(which uses crashkernel=xY format), but the x and/or Y
        values are changed in rhel-alt-7.6.
        For example from crashkernel=2G-:160M to crashkernel=2G-:192M, then we have
        no way to determine if the X and/or Y values were distribution
        provided or user specified ones.
        Since it is recommended to give precedence to user-specified values,
        so we cannot do an upgrade in such a case."

        Thus turn back to resolve it in kernel, and add a simpler version
        which just hacks to use the range:size style in code, and make
        rhel-only code easily to maintain.
    '''

    Signed-off-by: Dave Young <dyoung@redhat.com>
    Signed-off-by: Herton R. Krzesinski <herton@redhat.com>

Upstream Status: RHEL only
Signed-off-by: Jeremy Cline <jcline@redhat.com>

---
## [szeder/git-cooking-topics-for-travis-ci](https://github.com/szeder/git-cooking-topics-for-travis-ci)@[d2d7fbe129...](https://github.com/szeder/git-cooking-topics-for-travis-ci/commit/d2d7fbe12945dd9032c9c7c14a4563dbecc7e629)
#### Thursday 2020-06-18 22:08:22 by Jeff King

diff: discard blob data from stat-unmatched pairs

When performing a tree-level diff against the working tree, we may find
that our index stat information is dirty, so we queue a filepair to be
examined later. If the actual content hasn't changed, we call this a
stat-unmatch; the stat information was out of date, but there's no
actual diff.  Normally diffcore_std() would detect and remove these
identical filepairs via diffcore_skip_stat_unmatch().  However, when
"--quiet" is used, we want to stop the diff as soon as we see any
changes, so we check for stat-unmatches immediately in diff_change().

That check may require us to actually load the file contents into the
pair of diff_filespecs. If we find that the pair isn't a stat-unmatch,
then no big deal; we'd likely load the contents later anyway to generate
a patch, do rename detection, etc, so we want to hold on to it. But if
it is a stat-unmatch, then we have no more use for that data; the whole
point is that we're going discard the pair. However, we never free the
allocated diff_filespec data.

In most cases, keeping that data isn't a problem. We don't expect a lot
of stat-unmatch entries, and since we're using --quiet, we'd quit as
soon as we saw such a real change anyway. However, there are extreme
cases where it makes a big difference:

  1. We'd generally mmap() the working tree half of the pair. And since
     the OS may limit the total number of maps, we can run afoul of this
     in large repositories. E.g.:

       $ cd linux
       $ git ls-files | wc -l
       67959
       $ sysctl vm.max_map_count
       vm.max_map_count = 65530
       $ git ls-files | xargs touch ;# everything is stat-dirty!
       $ git diff --quiet
       fatal: mmap failed: Cannot allocate memory

     It should be unusual to have so many files stat-dirty, but it's
     possible if you've just run a script like "sed -i" or similar.

     After this patch, the above correctly exits with code 0.

  2. Even if you don't hit mmap limits, the index half of the pair will
     have been pulled from the object database into heap memory. Again
     in a clone of linux.git, running:

       $ git ls-files | head -n 10000 | xargs touch
       $ git diff --quiet

     peaks at 145MB heap before this patch, and 94MB after.

This patch solves the problem by freeing any diff_filespec data we
picked up during the "--quiet" stat-unmatch check in diff_changes.
Nobody is going to need that data later, so there's no point holding on
to it. There are a few things to note:

  - we could skip queueing the pair entirely, which could in theory save
    a little work. But there's not much to save, as we need a
    diff_filepair to feed to diff_filespec_check_stat_unmatch() anyway.
    And since we cache the result of the stat-unmatch checks, a later
    call to diffcore_skip_stat_unmatch() call will quickly skip over
    them. The diffcore code also counts up the number of stat-unmatched
    pairs as it removes them. It's doubtful any callers would care about
    that in combination with --quiet, but we'd have to reimplement the
    logic here to be on the safe side. So it's not really worth the
    trouble.

  - I didn't write a test, because we always produce the correct output
    unless we run up against system mmap limits, which are both
    unportable and expensive to test against. Measuring peak heap
    would be interesting, but our perf suite isn't yet capable of that.

  - note that diff without "--quiet" does not suffer from the same
    problem. In diffcore_skip_stat_unmatch(), we detect the stat-unmatch
    entries and drop them immediately, so we're not carrying their data
    around.

  - you _can_ still trigger the mmap limit problem if you truly have
    that many files with actual changes. But it's rather unlikely. The
    stat-unmatch check avoids loading the file contents if the sizes
    don't match, so you'd need a pretty trivial change in every single
    file. Likewise, inexact rename detection might load the data for
    many files all at once. But you'd need not just 64k changes, but
    that many deletions and additions. The most likely candidate is
    perhaps break-detection, which would load the data for all pairs and
    keep it around for the content-level diff. But again, you'd need 64k
    actually changed files in the first place.

    So it's still possible to trigger this case, but it seems like "I
    accidentally made all my files stat-dirty" is the most likely case
    in the real world.

Reported-by: Jan Christoph Uhde <Jan@UhdeJc.com>
Signed-off-by: Jeff King <peff@peff.net>
Signed-off-by: Junio C Hamano <gitster@pobox.com>

---
## [Maebh-Hughes/Red-DiscordBot](https://github.com/Maebh-Hughes/Red-DiscordBot)@[175fbebd73...](https://github.com/Maebh-Hughes/Red-DiscordBot/commit/175fbebd73873350a6b2db894ebc7819d0167d5c)
#### Thursday 2020-06-18 22:21:10 by Draper

Move logic for fetching latest Red version info to internal util (#3904)

* Only Send out of date message to Final builds available on PyPi

* Only Send out of date message to Final builds available on PyPi

* sorted the resulting list so that the newest build is first in the list

* forgot about this one

* well jack is a bitch but we love him.

* simplify logic

* Add the new function to `__all__`

Co-authored-by: jack1142 <6032823+jack1142@users.noreply.github.com>

---
## [saqib-ali/hivemined](https://github.com/saqib-ali/hivemined)@[75305b3721...](https://github.com/saqib-ali/hivemined/commit/75305b372117e513cc29b72d3ed4a6e75dcc75b3)
#### Thursday 2020-06-18 23:44:45 by Saqib Ali

Job @ uptodate Ventures (Munich): Data Analyst / D... | JOIN. Job @ uptodate Ventures (Munich): Data Analyst / D... | JOIN. “Laplace’s Demon: A Seminar Series about Bayesian Machine Learning at Scale” and my answers to their questions «  Statistical Modeling, Causal Inference, and Social Science. Data Analyst II – 1000ml – Train &amp; Verify. Actuarial Data Analyst – 1000ml – Train &amp; Verify. ateeca Business Data Analyst | SmartRecruiters. Data Analyst – Claims Data, Electronic Health Records, 0-3 years experience – 1000ml – Train &amp; Verify. Data Analysis Software Developer / Postdoc (m/f/d) - Machine learning / Artificial Intelligence, Garching bei München, Jülich Centre for Neutron Science (JCNS), auf dem Stellenmarkt von jobs.pro-physik.de. “Laplace’s Demon: A Seminar Series about Bayesian Machine Learning at Scale” and my answers to their questions «  Statistical Modeling, Causal Inference, and Social Science. East Tennessee State University Employment Site | Technical Data Analyst to TN State Agencies.

---

# [<](2020-06-17.md) 2020-06-18 [>](2020-06-19.md)

