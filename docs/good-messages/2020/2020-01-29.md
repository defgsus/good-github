# [<](2020-01-28.md) 2020-01-29 [>](2020-01-30.md)

2,055,513 events, 1,038,981 push events, 1,640,656 commit messages, 124,474,962 characters


## [austation/austation@dfb31d6554...](https://github.com/austation/austation/commit/dfb31d6554cf9d8fc55cd8096d31eb60480f8413)
##### 2020-01-29 00:18:51 by AuStation Bot

[MIRROR] [READY] Adds pineapple juice, creme de coconut, Pina Colada; adds Painkiller drink; adjusts Bahama Mama and pineapple snowcone (#1247)

* [READY] Adds pineapple juice, creme de coconut, Pina Colada; adds Painkiller drink; adjusts Bahama Mama and pineapple snowcone (#48783)

This adds two drink ingredients: creme de coconut (a coconut liqueur obtainable in the booze dispenser) and pineapple juice (obtainable in the soda dispenser or via juicing pineapples). It further adds the Pina Colada and Painkiller cocktails & corresponding sprites, and it adjusts the recipe for Bahama Mama. The recipe for pineapple snowcones was also adjusted to require pineapple juice rather than pineapple slices.

Finally, pineapple juice can be found in a carton or craftable juice box, and I've added sprites for this.
Why It's Good For The Game

First off, more drink ingredients equal more realistic drink mixes and more variety, which means more fun for bartenders.

Pineapple snowcones are better off using pineapple juice as opposed to pineapple slices. Previously the recipe called for two pineapple slices; you can now juice those two pineapple slices to produce pineapple juice sufficient for the snowcone.

Regarding the changes to the Bahama Mama recipe: With these changes, the recipe now more closely represents the classic Bahama Mama, which involves coconut, coffee, rum, and pineapple juice as its base ingredients. I removed orange juice to prevent conflict with the Painkiller recipe, but also feel justified in doing so because orange juice is not an essential ingredient of a Bahama Mama, nor is lime.
Changelog

🆑
add: pineapple juice, creme de coconut
add: Pina Colada cocktail
add: Painkiller cocktail
add: pineapples and pineapple slices can be juiced
tweak: Bahama Mama recipe, pineapple snowcone recipe
imageadd: pineapple juicebox sprite, pineapple juice carton sprite, Painkiller sprite, Pina Colada sprite
/🆑

* [READY] Adds pineapple juice, creme de coconut, Pina Colada; adds Painkiller drink; adjusts Bahama Mama and pineapple snowcone

Co-authored-by: OnlineGirlfriend <50865623+OnlineGirlfriend@users.noreply.github.com>

---
## [austation/austation@548a8223be...](https://github.com/austation/austation/commit/548a8223bed502cc236350819b7ddeac29264762)
##### 2020-01-29 00:20:48 by AuStation Bot

[MIRROR] New King Goat Lavaland Boss! (#1248)

* New King Goat Lavaland Boss! (#48823)

About The Pull Request

This pr adds in a new lavaland boss in a seperate z level arena also included are goat plushies that ram into people and goat skins which you get by butchering goats and can wear on your head, a goat gun that shoots goats and finally multiple tweaks/fixes generally involving possible ways to teleport out of noteleport area and or cheese the king goat.
Why It's Good For The Game

Lavaland has not seen any new bosses for a long bloody time and although the boss may be a bit silly I feel more hardcore players will enjoy the challenge it brings since this is meant to be a very hard boss also new goat related items are always neat.

🆑 Fluffe9911 for porting/making most of it, Monster and Sabiran for the King Goat!
add: A new king goat lavaland boss!
add: Goats now drop skin which you can wear on your head! (coder sprite)
add: Goat themed plushies that ram people! (realistic version sprite by identification code by karma)
/🆑

goat

This is ported over from my work on yogstation with minor changes mostly to make things work there is more goat content I made but didnt wanna do too much at once and dont know if tg would like it

* New King Goat Lavaland Boss!

Co-authored-by: fluffe9911 <dablank02@hotmail.com>

---
## [jkatz/postgres-operator@85651913ff...](https://github.com/jkatz/postgres-operator/commit/85651913ff7db9bc049d69a78ecd002f11cd0768)
##### 2020-01-29 04:43:06 by Jonathan S. Katz

Add support for PostgreSQL tablespaces in the HA containers

Tablespaces can be used to spread out PostgreSQL workloads across
multiple volumes, which can be used for a variety of use cases:

- Partitioning larger data sets
- Putting data onto archival systems
- Utilizing hardware (or a storage class) for a particular database
object, e.g. an index

and more.

Tablespaces can be created via the `pgo create cluster` command using
the `--tablespaces` flag. The arguments to `--tablespaces` can be passed
in via a command-separated list; each argument consister of two values:

- The name of the tablespace
- The type of storage to use for the tablespace

For example:

  pgo create cluster hacluster --tablespaces=ts=nfsstorage

All tablespaces are mounted in the `/tablespaces` directory. The
PostgreSQL Operator manages the mount points and persistent volume
claims (PVCs) for the tablespaces, and ensures they are available
throughout all of the PostgreSQL lifecycle operations, including:

- Provisioning
- Backup & Restore
- High-Availability, Failover, Healing
- Clone

etc.

Two additional values were added to the pgcluster CRD:

- TablespaceMounts: a map of the name of the tablespace and its
associated storage.

Tablespaces are automatically created in the PostgreSQL cluster. You can
access them as soon as the cluster is initialized. For example, using
the tablespace created above, you could create a table on the tablespace
`ts` with the following SQL:

  CREATE TABLE (id int) TABLESPACE ts;

Presently, tablespaces can only be added to a PostgreSQL cluster when
it is initialized. Based on usage, future work will look to making this
more flexible. Dropping tablespaces can be tricky as no objects must
exist on a tablespace in order for PostgreSQL to drop it (i.e. there is
no DROP TABLESPACE .. CASCADE command).

Co-authored-by: Brian Faherty <anothergenericuser@gmail.com>
Issue: [ch6495]

---
## [CalaMariGold/Rebirth-Of-The-Night@6e7102bdab...](https://github.com/CalaMariGold/Rebirth-Of-The-Night/commit/6e7102bdabf32d5d58b8403b3dc374a422a66e44)
##### 2020-01-29 06:14:49 by SandwichHorror

cool & good update

- Bosses don't show 2 healthbars
- part 1 of making sure all mob variants get Roughmobs raw stat attributes (follow range, knockback resistance for zombs, etc.)
Next up: skeletons and creepers
- spiders are 10% faster
- all wolves are beefier and faster so they don't die in 1 hit and make you sad
- all bears are tougher. have you ever knocked back a bear with a punch? I don't think so
- removed dungeon tactics rings and magic tether (it was just a more OP and cheaper recall potion)
- removed sausage-in-bread, beef wellington, and mob foods from harvestcraft (first two had way few ingredients and were the 2nd best level of food in the game. rest removed for balance/not infinite food from monster hordes. there's already so much fun food in this pack. mob soup will be added back later with a new ingredient)
- haha funny chef rat recipes

---
## [mrakgr/The-Spiral-Language@6cbc2685e4...](https://github.com/mrakgr/The-Spiral-Language/commit/6cbc2685e489836d8971bedc97660d33855ce3ab)
##### 2020-01-29 10:43:47 by Marko Grdinić

"9:45am. I am up. This time my sleep was better, but I am still quite stressed.

Yesterday, I suggested Deep CFR, but my contrary desires are at full force here. I do not trust deep anything when it comes to RL. And even if I make that agent, then come the interfaces.

Done properly, those interfaces could be the most interesting part of the whole journey, and I would be stuck doing nasty hacks instead.

Basically, Deep CFR is an invitation for doing sloppy work everywhere. Tabular RL is great, but that is as far as my trust goes.

9:50am. Last year, that whole theorem proving adventure was started by the my mistrust of real numbers in some way.

The most sensible continuation would be to go to spikes.

"> This is similar to how deep neural networks were before LeCunn applied backpropagation for their training.

Maybe SNNs having the restriction of not being able to use backprop directly will be an advantage in the long run. Backprop is like a black hole of understanding - it is a tiny piece of learning that prevents one from seeing the full picture. Being forced not to rely on differentiability might lead to fundamentally new insights of what learning really is."

I wrote this as a reply in that thread I posted yesterday.

I really do not feel motivated to work when I am full of doubt like now. What I really feel like doing is going down the path I believe in.

Spikes aren't necessary the higher data structures that would make ML makes sense. If I wanted to understand ML, it certainly would not occur to me use them personally.

But nature could do it. And rather than trying to figure this out ourselves, it might be better to swallow the ego and just learn this by imitating somebody smarter.

9:55am. Forget RL. Eventually the path will be there and one will be able to race continually closer to goal.

What I need to do is thus - let me start the review here. I might as well do that now that I am undecided.

"Last month was unusual for me as for the first time in the last 5 years I decided to do some vacationing. Back in December I was so disgusted at math that I decided to start work on [Spiral v0.2](https://github.com/mrakgr/The-Spiral-Language/tree/v0.2), but more like an reaction that planned move forward. For the first time, I was completely aimless and unsure how to make progress on my various goals.

I was really rough towards classical logic and its offspring like set theory, but in review, even if finitists had won the math foundation war of the early 20th century would I really have what I wanted right now? I was mad enough to blame all my ills on infinitists, but...the reality of the situation is that regardless of mathematical talent or aptitude nobody could have anticipated the progress in deep learning in the last decade. There are very many smart people who can write proofs about this or that thing, but the reality is that even for some very simple ML algorithms, nobody can really say that they understand them.

Last year, I just jumped to a conclusion that I need more math skill - because what else could I do at that point? It was the only thing left, so obviously learning type theory and how to hack Coq and Agda has to be the answer, right?

But now that this effort has been done, one thing I can really appreciate is just how little far logical thinking can get one. Logic is weak. And proofs aren't really understanding. The reality is that you need understanding to do a proof, but if you do not have understanding then type theory will not help you get it. And if you understand something, then the harsh reality is that you do not need to do a proof.

In this quest for AI, I can't trust myself and I can't trust others. That leaves only [nature](https://www.reddit.com/r/MachineLearning/comments/ev5p1m/d_spiking_neural_networks_a_primer_with_dr/) left.

Back in 2016 when I was working on my ML library in F# back which would eventually lead to Spiral, Theano was still a thing, Tensorflow had just come out and was Linux only, and neuromorphic chips were far off future that I did not need to think about. What used to be fiction in the past is getting closer to being reality by the day. And right now is the time when I should seriously start thinking about making the neuromorphic backend for Spiral.

My theorem proving adventure of 2019 was from the start spurned by wanting to look into how differentiability and floats interface with each other. Things like stability problems in RL and GAN training, and long term credit assignment issues cannot be solved from within the current framework of deep learning. I always had the opinion that it should be possible to add more structure that would reveal what learning is in starker contrast, but this basic research question is something I have no idea how to do.

And admittedly, back when I was forming those views it completely slipped my mind to consider the fact that actual brains use spikes for communication. So rather than elaborating the floats, the way to go forward might be to get rid of differentiability altogether. Haskell as a language is rather inspiring in this regard - it being pure is a large restriction that spurned a lot of innovation in programming technique. Spiking NNs should have the same effect on ML research.

To me the question is less about the spikes, and more about getting out of the dead end that is deep learning. Once learning without differentiability is understood, that will feedback into doing ML properly on regular architectures that use floats for communication between layers.

Looking back at the last 5 years, my stated goal was always to make a poker agent. But I am glad I took the long way around and tempered my enthusiasm with skepticism even if it was misdirected. And even though it was a huge drain on time, I did have fun discovering new programming techniques and getting better as I worked on Spiral. Pretty much everything I know about ML is of low value, but the actual programming that I've learned is definitely something that I hold in high regard. It would just be too bad to stop here before I've managed to share any of my insights about programming with the world.

I need to try again. I have too much experience to leave this work unfinished.

This time I can do Spiral properly. Neuromorphic chips are the proper target. Spiking NNs are the proper substrate for lifelike agents.

Once the pieces are in place, I will be able to do what I really wanted which is practical ML in the real world as opposed to this meme thing that exists right now."

11:10am. I am not done yet.

"Let me talk a bit about the future.

There is an enormous power waiting to be unleashed in ML. For me poker is merely the most sensible starting point. Supposing I had the agent, the next challenge after that is interesting in its own right.

I am talking about interfacing with the online gaming sites - since money is at stake, those sites are at this moment the most sophisticated Turing tests one can imagine..."

...Ah, no. Forget it. I do not feel like it. I am just bullshitting at this point. I thought I would talk about the Singularity for a bit, but it is not coming to me. I said all that I wanted to.

11:30am. Yeah, the reason why I failed is that I did not have the pieces ready. Somebody else might have been able to hack things through from what exists, but there are all sorts of programmers and I am not the one who can stand sloppy work.

11:35am. Maybe an artist is what you are rather than what you become?

Or maybe there is a reason why I decided not to become a programmer after high school. I really want to just be a specific kind of programmer. Doing it any other way does not hold my interest.

Now...let me take a break here.

I've been thinking about those closures a lot and I think I'll leave doing this for later. This piece is rather complex and I am not confident that I want to be doing it now before I have the language up and running.

Once I do the basic tests, it will be the first thing on the list.

Today I'll just do the basic join points and then move on to the rest after that.

I thought I would be done with the partial evaluator before the end of the month. I am certainly behind schedule.

11:40am. Tsk. Forget that hard stuff. I am really going to do it this time.

Without a doubt, I will get my hands on those chips and then wait with my net spread out."

---
## [mrakgr/The-Spiral-Language@a7875819c5...](https://github.com/mrakgr/The-Spiral-Language/commit/a7875819c57e7ccb826055a9f374cadee5e367ae)
##### 2020-01-29 14:37:37 by Marko Grdinić

"1:10pm. Done with breakfast. Next are chores and chilling.

2:05pm. Done with that.

2:10pm. Now it is time to start, but let me focus inwardly for a bit.

I am thinking how I would talk the guy into hiring me 1-2 years from now, but that is the wrong mindset to be in.

It does not really matter what happens to Spiral. What matters is that I use my full power to reach my goals.

2:15pm. I am so rattled. I really do not feel like programming.

I said that thing about wanting to try evolagos, and now I am aiming for neuromorphic chips.

Yeah, there is definitely pain of not being able to aim towards my goals directly.

But it just goes to show how weak the trust in my ideas were that once I saw something even remotely realistic, I immediately ditch them.

2:20pm. Instead of working today, why don't I turn off and step away from the screen for a while?

Yeah, that is what I will do.

I just came up with the neurochip goal yesterday and presented it today here.

But as a result, now I am changing my direction again. Yesterday I said I would do Deep CFR, and now I suddenly do not feel like it because it would be sloppy work.

2:25pm. I really, really wanted to figure it out, but I knew that it would be unrealistic from the start. Even though I knew, it does not stop it from hurting. I just had to show my determination. I just had to test myself at how far I can go. I just had to take the responsibility for doing it myself.

Patience - a viable strategy, but a painful word for me.

I feel like I have a responsibility to make the Singularity come as soon as possible. I took it upon myself.

But now that I know the task of understanding learning is beyond the capacity of humans, the smart thing to do would be to change strategies and just like I am letting the work of creating hardware to others, to let the neuroscientists figure out the secrets of spiking NNs.

2:30pm. I am absolutely sure. I've tried my best during 2018-2019 and I know for certain that I cannot beat nature's design. I am supposedly a good programmer, but I am nothing much when the big picture is considered. Those lofty heights of programming skill are surely reserved for the post-human entities of the future who I want to be one of.

2:35pm. It is ironic that human technology compared to nature is more primal. It is one that taps into the core virtues of reality. Maybe that is because humans are weak, so they have to do it properly in order to ensure that it works.

Two years...

Two years...

Two years...

What will happen in two years? These chips will be out...and there should be something to make good use of them. Yesterday in one of the videos the speaker said that bee brains were realistic and that human brains are too complex, but he is not being realistic.

If bee brains are here, then human brains won't be too far behind.

2:40pm. The truth is at this point the Singularity is making me sick. Here I am trying to squeeze some kind of a plan here into a timeframe where the long term does not give me a risk of death - nor do I want my family members to just slip through the cracks of time, but the real world is going at its own pace.

2:45pm. Just where are neuromorphic chips now?

Where will they be in 2030 for example?

Pretty much all ML advancements are just buzzing around the hardware. Hardware is what acts as a catalyst to everything else.

2:55pm. https://www.extremetech.com/computing/295043-intels-neuromorphic-loihi-processor-scales-to-8m-neurons-64-cores

8m neurons compared to the brains 100b. Also Loihi has 1k synapses for every neuron so I'll assume this applies here as well.

"“With the Loihi chip we’ve been able to demonstrate 109 times lower power consumption running a real-time deep learning benchmark compared to a GPU, and 5 times lower power consumption compared to specialized IoT inference hardware,” said Chris Eliasmith, co-CEO of Applied Brain Research and professor at University of Waterloo. “Even better, as we scale the network up by 50 times, Loihi maintains real-time performance results and uses only 30 percent more power, whereas the IoT hardware uses 500 percent more power and is no longer real-time.”"

109 times lower power consumption than a GPU.

Upgrade the process a little, figure out the memristors, and 100b neurons on a chip sounds doable. Of course the brain will have a lot more complex architecture, is analog, so it might be much more powerful pound for pound.

But still...

Let me just update the review. A bit. I need to highlight just how humiliated I am at not being able to take advantage of the great programmability advantages on present hardware.

3:10pm. "I feel shame very strongly over this state of affairs.

I've lost, but it is not to other programmers or mathematicians. Rather if I have lost to somebody, it would be nature. I have such strong beliefs and conviction and yet despite the huge advantages that being able to program the hardware directly provides I am not able to convert that potential into actual power. But that does not change that the potential is still there waiting to be tapped."

Yeah, I am mad at the mathematicians as well, but being mad at nature is where the benefits will be.

"If I am to be mad or to hate the lack of progress, the place I should direct that effort is definitely nature. It is there that this sort of dedication will have merit."

3:20pm. I definitely felt the hate when I wrote this.

3:25pm. Let me go back to what I was writing.

I need to focus on this vision. In 2030 and most likely even sooner, the bee brain tier of power and skill should definitely be here. This is the worst case scenario, and even in it I should have enough leeway to move properly with the insights derived from brain research initiatives.

3:30pm. This was actually my mindset back before 2014 when I was having fun with futurism.

I need to get back to it. Forget about causing the Singularity by some fixed date. The self improvement loop will swallow me when the time comes.

I do not owe it to anybody to do it in 2022 or whatever.

If it happens by 2030 then that is fine. If it happens by 2040 then that is fine too. I will do my utmost - not to hold out that long, but to have fun in the interim.

That is what matters the most.

3:30pm. I need to regain the spark towards the future. I need to have joy in making Spiral.

I need to feel like I am making real progress, not just so I can make some agent or whatever, but for its own sake.

3:35pm. Now as I said, I would let me step away from the screen and I will just take a nap for a few hours.

I want to dwell on this feeling. When it bursts, I will come back and resume work on the language.

I vow one thing no matter what. I will never again clown around by saying I want some agent, or get frustrated when results inevitably do not arrive at the schedule I would like. I am absolutely through with that.

There is only one reason to make Spiral - because I am the only one who can.

And because I am the best."

---
## [rschell/FreeBSD-src@8070b00339...](https://github.com/rschell/FreeBSD-src/commit/8070b003398e203618b8ec0414b6c05ad696c669)
##### 2020-01-29 15:01:08 by kevans

MFC r357103-r357104: unbreak local.lua, add a modules.loaded hook

r357103:
loader.lua: re-arrange to load local.lua *after* config loading

The major problem with the current ordering is that loader.conf may contain
all of the magic we need to actually setup the console, so loading local.lua
prior to that can make it excessively difficult and annoying to debug
(whoops, sorry Ravi & Warner).

The new ordering has some implications, but I suspect they are a non-issue.
The first is that it's no longer possible for the local module to inject any
logic prior to loading config -- I suspect no one has relied on this. The
second implication is that the config.loaded hook is now useless, as the
local module will always be included after that hook would have fired.

For config.loaded, I will opt to leave it in, just in case we add an early
point for local lua to get injected or in case one wants to schedule some
deferred logic in a custom loader.lua. The overhead of having it if no hooks
will be invoked is relatively minimal.

r357104:
lua: add modules.loaded hook

This may be used for the local module to hook in and load any additional
modules that it wants, since it can't modify the modules table internal to
config. We may consider adding API to do so at a later time, but I suspect
it will be more complicated to use with little return.

status is captured but ignored for the purpose of loading the hook. status
will be false if *any* module failed to load, but we typically don't let
that halt the boot so there's no reason to let it halt hooks. Some vendors
or setups may have expected fails that would be actively thwarted by
checking it.

We may, at a later date, consider adding an API for letting non-config
modules check which modules have successfully (or not) loaded in case an
unexpected failure *should* halt whatever they are doing.

---
## [junho85/TIL@dceccb5a70...](https://github.com/junho85/TIL/commit/dceccb5a7026c6a7423a2cf2045bf875d49b1ed7)
##### 2020-01-29 15:56:58 by junho85

cake
My best friend Ben and I have known each other for two and a half years.
To put that in perspective, Romeo and Juliet only knew each other for less than four days.
So, we're closer than those two ever were.
Ben and I used to hang out at least three times a week.
We would grab dinner, see movies, and watch hockey games together.
We had what some people call a 'bromance'.
At time passed, we slowly drifted apart.
Life became busy.
Free time became scarce.
Laughter became a memory.

---
## [halhex/ray-casting@868b9690da...](https://github.com/halhex/ray-casting/commit/868b9690da71af2008112ef02be468ee89aa002c)
##### 2020-01-29 16:11:26 by halhex

Abundance / In The Reeds · Kennebec

Form is everything. It is the secret of life. Find expression for a sorrow, and it will become dear to you. Find expression for a joy, and you intensify its ecstasy. Do you wish to love? Use Love's Litany, and the words will create the yerning from which the world fancies that they spring. Have you a grief that corrodes your heart? Steep yourself in the language of grief, learn its utterance from Prince Hamlet and Queen Constance, and ou will find that mere expression is a mode of consolation and that Form, which is the birth of passion, is also the death of pain.

---
## [mrakgr/The-Spiral-Language@be8637cf92...](https://github.com/mrakgr/The-Spiral-Language/commit/be8637cf92b7eefefaab5476293dac3405c8ff09)
##### 2020-01-29 17:24:34 by Marko Grdinić

"https://techcrunch.com/2018/03/15/the-red-hot-ai-chip-space-gets-even-hotter-with-56m-for-a-startup-called-sambanova/

Rather than stepping away I am reading some of these articles. Yeah, that space really is hot.

5:50pm. I am back. I did some thinking.

I am not going to give up. The brain is a very specific data structure with its own laws. Once I know what they are I will be able to do the kind of programming that I want. Without a doubt. I will never stop believing this. It is not some continuous function with nonlinearities inbetween but something else.

It will definitely be found. Absolutely.

SNNs are the only answer answer.

5:55pm. Just as that article says, there are a lot of players in the SNN space. Rather thinking of just Intel, with Spiral I should be targeting every one of them.

I need to start thinking about advertising myself and Spiral in this space.

Right now I do not have much to show, but I really should finish v0.2 in the next year and then start hunting for sponsors.

6pm. I will absolutely resume work tomorrow.

Work hard and then play hard, that is the way to live. I need to start making progress rather than just lying in bed.

That having said, I did have some insight.

```
            | TyPair(a,b) -> memo <| TyPair(f a, f b)
            | TyKeyword(a,b) -> memo <| TyKeyword(a, Array.map f b)
```

I need to stop memoizing pairs, keywords and records. Why?

```
inl q = a,b
inl w = a,b
join f q w
```

I would prefer if this got specialized the same way as...

```
inl q = a,b
join f q q
```

But because I am memoizing by reference here it won't.

What I need to do is memoize only the vars and the functions.

Also the vars are a problem.

```
| TyV of TyV
```

Ugh, note that they are only compared by their tag. Do not want to do that in the join points.

```
    // For use in join points, layout types and macros
    | TyRV of Ty
    | TyRR of int
```

Let me break everything like this. I'll fix this tomorrow.

6:05pm. Well, rather than not memoizing pairs and such outright, what I should do is not turn them into `TyRRs`. I just have to do that. But using reference memoization to just speed things up is fine for all of them.

6:10pm. So with this, I did do a bit of work today.

In the end I've resolved to make SNNs a channel for my ML urges and hopes, and decided to make Spiral a product viable for widespread consumption. When I have that, I am going to be an entrepreneur and get paying work for the first time in my life. One of those startups will bite. Spiral will definitely have a market, I just need to push it a bit.

6:15pm. Until then, all I need to do is keep a steady pace and do a bit of work every day. V0.2 will get finished, I will be better than ever having both perfected Spiral and mastered unification via constraint solving, neurochips will arrive and SNNs will bloom.

Everything will be hunky dory.

6:20pm. I need to give it my best. I can't just assume that I will have v0.2. The only way that will happen is if I do not take it for granted and actually put in the effort every day.

I am going to start this tomorrow.

First I will redo the renaming function from scratch and then do the join points. Then I'll aim to do the rest. I'll leave the closures for part two after I can see what is going on with them in the generated code.

I am not going to leave myself room to get stressed. Instead I am going to reduce everything to their limits and then strike.

One piece at a time, I will get v0.2 done, just like I had the previous version."

---

# [<](2020-01-28.md) 2020-01-29 [>](2020-01-30.md)

