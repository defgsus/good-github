# [<](2020-02-27.md) 2020-02-28 [>](2020-02-29.md)

2,200,886 events, 1,070,483 push events, 1,722,410 commit messages, 128,139,651 characters


## [msobkow/htdocs@d90ffc3605...](https://github.com/msobkow/htdocs/commit/d90ffc3605262ea2372570f667ee49db1a1870a5)
##### 2020-02-28 03:23:22 by Mark Stephen Sobkow

2020-02-27 2.13 Programmer's Notes

For now I'll continue to work on plugging memory leaks, and have already prepared for the
release by incrementing the build numbers and taking snapshots of the freshly versioned code.

I've spent two days thinking about how I want to structure the reworked C++ CFLib exceptions, and
I have a plan for dealing with my exception leaks.

On WSL2 And Eventual Debian debhelper Build Process

I've been waiting for WSL2 to come with the next big Windows 10 Home update.  That will finally
let me do proper .deb installers for data, amd64, and any targets (common data files used by both
Java and C++, the Ubuntu 18.04 LTS amd64 builds, and the Java installation (which will be modified
to actually install my jars to /usr/share/java/msobkow/2.0.13.)  I'll probably start with packaging
a Java installer for msscf_2_12, just to get back into the swing of using debheler.  It won't run
under WSL1 because WSL1 doesn't support virtual environments.

I'm going to have to learn how to write my own debhelper rule files.  I don't like the automated
process that builds a test version of a library before building the debian version of the code, and
then builds both static and dynamic debian link libraries.  I want to do build one dynamic link library
and be done with it.  I don't have the CPU horsepower to throw away to do three builds when one
will do.  I don't support nor encourage static linking for real projects.

I'll add scripts to PackageXxxTarballs213.bash which create compressed tarballs tarballs of the
source code for each libxxx and xxxramloader directory,
naming them com.github.msobkow-2.0.13.build-libxxx-src.tgz and
com.github.msobkow-2.0.13.build-xxxramloader-src.tgz.  Debian build processes like to take fresh
extracts of the code in a clean chrooted directory tree when building .debs.

I can then have a debian subdirectory in each project analagous to the cplus directory tree,
and manufacture the debian-specific debhelper files to that tree.

CleanDebian213.bash will completely wipe the debian directory tree for the schema/project of
.tgz files, build litter, and non-debian source code (i.e. the extracted .tgz contents.)

CopyToDebianTarballs213.bash will copy the tarballs to a cleaned debian tree for building.

BuildDebianLayer213.bash scripts will be analagous to the BuildXxx.bash scripts I already have,
but will get debian to do a build of the .debs for the library, and then install them instead
of doing builds in the cplus directory.

The various *cust* libraries and executable directories would also need to be built.  I may
formalize custom executables and libraries and with the new SchemaDef elements CustLibrary
and CustExecutable.  CustLibrary would automatically add on the "libschemacust" part of
the custom library name, and append the lowercase CustLibrary.Name to that, and hook
them into manufactured scripts to support builds for those libraries.

CustLibrary would need to be able specify CustDepends that can name any library it likes
using a raw shared library name as passed behind "-l" to the linker.  CustDepends can
optionally specify AppLayer to identify a particular application model and layer's library
is a requirement.  CustExecutable would be very similar, except it wouldn't automate the
directory name at all.  Obviously a CustExecutable depends on libraries, too.

Future MSS Code Factory 2.13 Features

I'm also thinking about adding a "-build build-version-string" option to the command
line for the factory itself, and use that string instead of the default 12345 that I have
in my current knowledge base.  I don't like this restriction to 5-digit-exactly build
identification in order for my ex.cmd magic in the BuildXxx.bash scripts for C++ to work.

The model would see the addition of SchamaDef elements CustExecutable, CustLibrary,
and CustDepends as described in the above notes.  I have realized I'd also want to
specify CustDepends on a per-library basis, with the ability to inherit and append
custom dependencies added on by each project schema in the system.  Similarly, the
executables and libraries would have to be accumulated, and I'd also need to automate
the packaging and installation of those dependencies as well.

My goal is a "smart system" that knows how to perform an entire installation operation
of the most common kinds: install, update-build-environment, clone-src, refresh-src,
build-src, package-src-build, install-src-build, uninstall-src-build,
clean-manufactured-src, manufacture-src, clean-src-build, distclean-src-build,
package-src, post-build, install-build, reinstall-build, and so on.
I want to completely automate the updating, rebuilding, packaging, and distribution
of a build so I can just run one command, use one core plus threads, and let the computer
chew for a few days until it encounters a problem and halts for manual intervention
and code repair.

Ideally, I want two penultimate commands: make-it-so-src-system and make-it-so-system
that will do all the appropriate run-around, build and install the code, and have
the system ready for database installation or upgrades.

I could use https access to specify referenced models via URL as a fallback for
local model file names, though I don't know how to write HTTPS clients with C++; I
presume there are Apache libraries for it and code samples to follow.  I don't
recall the last time I tested URL resolution with Java.  I don't think I've ever
done it over HTTPS.

Then you could configure a filtration set of model resolution locations in your
system's /usr/config/msobkow/.msscfrc and /usr/config/msobkow/.msscfclirc.  Those
would be searched by default, and must exist for any operations to be performed.

The home directory is searched for those two files first, allowing you to specify
account defaults in locked-down system profiles, or override the system defaults
in developer accounts and build accounts.  If those aren't found, then
$MSSCF_HOME/config/2.0.13/.msscf[cli]rc are to be searched for if MSSCF_HOME
was specified.  Next /usr/share/msobkow/2.0.13/config/.msscf[cli]rc are probed
for, and then /usr/config/msobkow/2.0.13/.msscf[cli]rc.  If all of those options
exhast to no file, then I'll point you to a default hard-coded system URL to
resolve the new-installation defaults.  They'll be copied to your local
/usr/config/msobkow/2.0.13/.msscf[cli]rc if no such exists.

I'd want to do builds on virtual machines that eventually can start out with
no MSS Code Factory libraries for the minor version installed or fetched,
using nothing more than a get command to fetch the initial .msscfrc and
.msscfclirc to your HOME directory.  That means there are no 2.0.13
subdirectories under /usr/share/msobkow, /usr/include/msobkow, or
/usr/bin/msobkow on a clean-build system.

I'll want to add GitServer[n] configuration variables to the .msscf[cli]rc files,
so you can specify where your git servers are located.  You can clone any
project to a local git server and have at it if you need to customize it;
all the code is available under Apache V2.  If local customization isn't found,
your the default git servers for MSS Code Factory will be used.

The virtual machine would have a specific set of build tools pre-installed,
a pre-installed release of the prior version of MSS Code Factory tools
using whatever search path you have configured for your tool chains,
and MSSCF_HOME set to a valid and existing potentially-empty directory
somewhere.

I want to modify the resolution of .msscfrc and .msscfclirc to default
to /usr/share/msobkow/2.0.13/config/.msscf[cli]rc, and to
/usr/share/config/.msscf[cli]rc if those aren't found.  You'll need
to identify MSSCF_HOME, MSSCF_DEFAULT_MODEL_URL, and
MSSCF_DEFAULT_CARTRIDGE_URL.  The latter two will be optional, and
will default to hard-coded initialization values by default.
I'm not sure how to do that in standard C++2a, as I've never written an HTTP client
in C++ before.  Hopefully Apache has handy-dandy libraries available as I will need
them someday regardless.)

Thus I could have a server of the most recently released shared models to be used
by projects in the field, you could install system-level overrides of those
shared models, or just search project, project-url, department, department-url,
location, location-url, tenant-url, cluster-url, or top-domain-url locations.
Each would be searched in turn for target files, allowing you to override them
at any level.  The same applies to cartridges and models in the existing versions
of the configuration files.

On Report Generation

Someday I'll also write a report generation tool with MSS Code Factory.  The idea is to
instantiate an MssCFEngine and load it with the report definitions through a separate
rule cartridge of reports, and then specifies GenReport specifications based on GenFile,
but which work with passed-in stream handles instead of creating files.  That way you
could define any supporting rules common to your reports that you like.

I'd need to greatly enhance GEL to provide proper support for calculations and formatting,
of course, so this won't happen any time soon.

The initialization time on the server for the MssCFEngine would be substantial, so you'd want
to create a pool of MssCFEngine instances that have been pre-loaded with the report schema
definitions.

---
## [rafaelespinoza/godfish@8232ab7582...](https://github.com/rafaelespinoza/godfish/commit/8232ab7582ce499ccfe148f90ebf2667b0bd5d6e)
##### 2020-02-28 06:53:05 by Rafael Espinoza

fix: Filename interpretation, break Migration interface

Being more flexible is an advantage since different migration tools
produce their versions a little differently, and it'd be cool if godfish
can work with migration files produced by those other tools. If the
timestamp is too long, then the extra numbers would just spill over into
the migration name.

feat: Add Version interface, break Migration interface

- Store version label for timestamp b/c it fixes filename interpretation
where the version is too short. Also remove a superfluous return value
in parseVersion,
- Simplify Version implementation. A time.Time is a little more
complicated than what's needed in this tool. By reducing the comparable
value to the lowest common denominator (works w/ unix epoch values) we
can be more flexible with intepreting filenames. We can rely on the
Version interface to do the comparing. By default keep the datetime
implications, but this is only for formatting. Update comments.

feat: Add Direction alias type, break Migration interface

- Choose direction label during migration filtering in reverse

fix: List migrations file bugs

- From the listMigrationsToApply function, the list of applied versions
will only be in the forward direction. But when using that list in
selectMigrationsToApply, need to manually reset the direction. In this
context, we're more concerned with how versions relate to each other.
- Update makeFilename to be more lenient w/ direction
- Hoping to leverage globbing when searching for migrations to apply. When
you only have the version to go off of, the direction alias is unclear.

fix: connection bugs

- Did not realize that using the Query method on sql.Connection struct
would return a *sql.Rows, which need to be closed. There is no need to
do something with the Rows in these Driver methods. The code was
ignoring that variable anyways, but the reference was probably still
there. An alternative is to use the Exec method on sql.Connection. This
returns a sql.Result, which doesn't need to be closed.
- This error has been prevalant in the mysql driver tests. Here is a brief
account of the error:
  When running the mysql Driver tests, frequently run into error 1040,
  max connections. It seems that every time the *sql.Connection is
  referenced, it creates a new connection? The sql.Connection struct is
  actually a connection pool, and you rarely, if ever need to close it
  yourself. It might close itself on its own, I don't know. Whatever it
  may be, I ran one test while in debug mode while monitoring the number
  of open connections. To my surprise, one call to the godfish.Migrate
  function opened up 6 connections!

refactor: rename Migration method Name to Label

- Name in the Migration interface has no functional effects, it's just a
convenience label.

refactor: clean up migration parsing functions

- Using "base" as a variable name is a little confusing. Does it mean a
numeric base (confusing when considering version numbers)? Remove unused
parameter from parseTimestamp function.

refactor(postgres): Use error code for schema migrations does not exist

refactor: Add migrationFinder type

- This type is syntactic sugar for organizing the functions that read
migration files and select the appropriate ones. There was only one
entry point and the remaining methods were used amongst themselves. Now
that entry point and the subsequent dependencies are easier to spot.
Removed the direction not found errors, those are just annoying.

refactor: remove some unnecessary abstraction

- The scanAppliedVersions function was set up using a callback pattern
so that it could make use of a closure. It turned out that there was
not much needed and it could simply be passed in. Also eliminate a
helper function b/c there are too many functions used in just one
place!

test: Extract stub driver into own package

- Need a package-level boundary to test parts of the exported API without
having to rely on a working database system. This is especially useful
for dealing with migration files. Many of, but not all, the tests in
versions_test.go are obsolete because the functionality is covered by
internal/test.go. A few edge cases would have to be moved over, adapted.

test: Reorganize setup and teardown

- Refactor driver tests so setup and teardown is better available. It's
easier to recreate bugs and test specific functions now. The exported
test function has less boilerplate definitions.
- The test organization is not perfect, but overall there are less
assumptions, less magic.

test: Update tests

- Add workaround to stub test driver
- remove annoying filename tests
- Add Driver tests for alternate directions, filenames
- Pass DB_USER to internal driver test
- Update test variables and types to be more self-descriptive.

docs: Update README

build: Update Makfile

---
## [mrakgr/The-Spiral-Language@d3c2487aa9...](https://github.com/mrakgr/The-Spiral-Language/commit/d3c2487aa90d7c025a61ebc62ccfcfe16de3b9a7)
##### 2020-02-28 10:58:47 by Marko Grdinić

"9:45am. Today I got up late it seems. Let me take the time to make that reply to the PM and then I will start with the book.

```
> Have you found some areas where inlining and join wouldn't be automatically infereable (or at least very hard)?

> It seems to me that join points are trivial to locate.

The easy way of doing it would be to just put them at the end of every `let` function statement and have the optimizer decide when to inline them. The point of Spiral though is that it allows the programmer to decide where to put them.

Also the difficulty in join points is not just where to put them, but whether to convert functions to heap allcoated closures, and whether to specialize to constants or push them to runtime. This is a lot harder problem. In the case of closure conversion, the v0.09 partial evaluator does not even have the information necessary to do this and so has to be done by hand.

> Is inlining powerful enough to replace any syntactic macros?

No, but conversely to get the same power that Spiral's partial evaluation offers, you'd actually have to reimplement those features via macros. Macros do not memoize the results of their evaluation - this is what join points actually are. Make macros lexically scoped (hygienic) and allow them to do that memoization, and you have the core of Spiral's functionality.

> What is the difference between specialization and inlining?

In the v0.09 documentation, I kind of mushed the meaning of the two together even when they aren't. Back when I was writing it, I was putting in the effort to make up my own explanation for what Spiral does because no other language has its feature set, but let me see if I can get my vocabulary in order.

Inlining is just when you inline the body of the function, and specialization is what happens in join points. Also it is what happens when you instantiate reified join points (layout types in v0.09).

```
inl add a b = a + b
inl _ = add 1 2, add 1.1 2.2 // inlining - the bodies will get inlined
```

```
inl add a b = join a + b
inl _ = add 1 2, add 1.1 2.2 // specialization - join points will get compiled to two function calls during code gen
```

> Did you try to replace type inference with partial evaluation?

Yeah, in v0.09 type inference is done via partial evaluation. In effect, the way it happens is similar to in dynamic languages in that you have to run the program to get the type errors, except it happens at compile time. I call it bottom-up type inference as opposed to top-down.

After programming like that for a good while back in 2018, I've concluded that it is too hard after all. One of the difficulties is that unlike in dynamic languages, you can't just put anything into an array, you have to specify its type somehow. So when doing generic functions (think of `Array.map`) that means having to run them at the type level and then once you have the array type at the value level. This is just the start of it, I had to do some gross hacks to infer the types for some functions. If you look at v0.09 you will see that it has `type_raise` and `type_catch` which allow me to do type inference by throwing exceptions. And I did even worse things in some cases.

It was interesting at the time, but good programming should be easy. Looking back, it is obvious to me now that I feel in love with experimentation a bit too much.

> Can you do runtime specialization?

No. It would be harder and not serve the same purpose. There are systems like Lancet which can, but in Spiral I only wanted to build a bridge from high level F# code to Cuda GPU kernels in order to make a ML library that does not rely on frameworks. This was my main goal, and I did not want to distract myself my making what is essentially a JIT compiler.

> How do you deallocate memory?

On the F# side, the GC takes care of that. In the ML library you do have to do it yourself for GPU memory, but I consider that a mistake. I should have used weak arrays in the GPU allocator and had the .NET GC take care of that as well. But I wasn't sure whether that sort of scheme would be reliable enough. Python is really lucky that it does reference counting and has reliable GC as a result.

I am going to have to do more research here - I definitely want full GC for my next ML library. Maybe it won't be needed though. For the sake of performance, in the next gen hardware effort is being made to localize memory to the cores rather than having them allocate from global pool. Meaning [neurochips](https://www.youtube.com/watch?v=jhQgElvtb1s) and [AI accelerators](https://www.youtube.com/watch?v=otoCxbZel1o) will (in programming terms) have static allocation (similar to shared and local Cuda memory).

Spiral is really well suited for this kind of thing by the way - if you look at how the ML code gets compiled you will see that all the tensors have their dimensions specialized in the GPU kernels. In some cases this is actually necessary in order to properly allocate shared memory. It also leads to less register use.

> How do you mutate memory?

Similarly to how the MLs and F# does it. References and arrays can be mutated directly. In GPU kernels the same philosophy is followed except they are statically allocated in registers.

There are also records with mutable fields, but I need to clean those up for v0.2.

I really like this explicit reference approach to mutation pioneered by the ML family of languages. Right now I am learning Typescript and everything is a huge mess as all the references are implicit.

I don't like how Haskell does it either as it is too rigid. Monadic code leads to coupling which is bad. I really made the right choice to write Spiral in F# as opposed to Haskell.
```

10:50am. The above is what I had been writing for the past hour. At my discretion, I'll paste it here in this journal since none of the questions are sensitive. If they were I'd have kept it private.

10:55am. Ok...I am thinking of whether to take a break here.

How about I give the Angular chapter a try for an hour and then I'll do that. I want to get this started.

11:45am. I've skimmed the rest of the book.

I really thought I would get a lot of out of the framework parts, but as it turns out I do want to leave this for when I actually need it.

With this I am finally done with the Essential TS book. Great.

Next up: the basics of web development. I am going to build my skills bottom up.

https://svelte.dev/

If I had to use a framework, I'd rather go for something which is a language. So maybe something like Elm or this.

11:55am. Let me take that break. I think I'll do the review today. Given that reply, and finishing the book ahead of schedule, today seems the ideal time to do it before I start on the next part. After that, I'll go through the video and go through those links.

It should not me too long to figure out the basics.

But ASP.NET after that should be more challenging. Still, I am looking forward to mastering this.

Just this thing and I will be able to do anything."

---
## [NetBSD/pkgsrc@b3a39145ac...](https://github.com/NetBSD/pkgsrc/commit/b3a39145acafbe4326a2a6b6a3bcf6722741f5fe)
##### 2020-02-28 12:36:20 by micha

doom2-pwad-eviternity: Imported version 1.0

Eviternity is a megawad comprised of six 5-map
episodes (called Chapters) plus two secret maps.
This project exclusively uses OTEX, a brand new
high quality texture pack by ukiro.

Eviternity's six chapters explore a series of
unique and varied themes, each featuring classic
gameplay with an interest in making each map hold
its own unique identity and personality. The
themes are "Medieval", "Techbase", "Icy Castles",
"Industrial / Brutalism", "Hell / Gore / Alien" and
"Heaven".

This project was created as a birthday gift to
Doom, which is celebrating its 25th birthday the day
this was first released ("RC1", Released on December
10th, 2018. The texture pack used in this project,
OTEX, was also released on the same day - so please
do not use Eviternity as a base for your wads & mods.
While mostly being a "Dragonfly project", with 24
maps being made or heavily worked on by myself, I
present to you a mighty lineup of well-known guest
mappers who have crafted beautiful and fun levels.

---
## [newstools/2020-daily-dispatch@62828874ad...](https://github.com/newstools/2020-daily-dispatch/commit/62828874ad8c5070a164ddc2558ff58f3b88b299)
##### 2020-02-28 16:21:49 by NewsTools

Created Text For URL [www.dispatchlive.co.za/news/2020-02-28-ex-soldier-jailed-for-life-for-murder-of-girlfriends-sister-in-shooting-rampage/]

---
## [PixelsCamp/talks@7a94980bc6...](https://github.com/PixelsCamp/talks/commit/7a94980bc6423c5e96310a6112118b13ce25939a)
##### 2020-02-28 16:45:40 by João Dias

Hooking up with React Hooks
=================================================

* Speaker   : **João Dias** and **Mickael Costa**
* Available : **All days**
* Length    : **90 minutes**
* Language  : **English**

Description
-----------

In the last months, React Hooks changed how we write and think about reusable and straightforward code. This training workshop will cover the fundamentals of React Hooks, how to convert your codebase and also how to take advantage and create your own custom ones. All of this in simple, straightforward and understandable examples.

By the end of this workshop we will be able to do the following:

-   Use built-in React Hooks to create more reusable and straightforward code
-   Re-factor Class components into functional components using hooks
-   Create custom hooks and reuse them throughout a project
-   Know where React Hooks fall short and we should choose class components instead.

Speaker Bio
-----------

### João

João is a Frontend Engineer at Feedzai with over 4 years of developer experience and over 9 as an interface designer. Mostly, he just loves to design and build stuff for the Web. His designer/developer cross-breed skills are his main strength and help him to bring the best of both worlds to every project he gets involved into.

### Mickael

Mickael is Senior Frontend Engineer at Feedzai. He had the chance to work and experiment with a variety of languages and frameworks namely React. He was a teacher so he enjoys teaching and learning with a good conversation about technology. He has other tastes like sports (futebol and Krav Maga) and travel.

Links
-----

### João

-   Company: https://feedzai.com
-   GitHub: https://github.com/joaotmdias
-   Personal: https://joaodias.me
-   LinkedIn: https://www.linkedin.com/in/joaotmdias/

### Mickael

-   Company: https://feedzai.com
-   GitHub: https://github.com/Mickael24
-   LinkedIn: https://www.linkedin.com/in/dinomickael/

Extra Information
-----------------

### João

-   [Speaker at ENEMM 2019 with the keynote "State of the Web in 2019"](https://www.enemm.pt/speakers/joao-dias)
-   _Workshop: The User's Perception of Speed_ (2016)
-   _Keynote: State of Javascript 2018: Angular or Vue or React?_ (2018)
-   _Keynote: Let's talk about Accessibility_ (2019)

### Mickael

-   ["First React Application" workshop teacher at FEUP Code Week 2019"](https://www.linkedin.com/posts/dinomickael_feup-codeweek-feedzai-activity-6592090630468448256-k9Jk/)
-   Assistant Professor at the Polytechnic Institute of Cávado and Ave

---
## [mrakgr/The-Spiral-Language@22418a20aa...](https://github.com/mrakgr/The-Spiral-Language/commit/22418a20aa04657a8b29125a0ae6b5aa8fd6c28e)
##### 2020-02-28 17:29:20 by Marko Grdinić

"1:35pm. Time for chores. Then I'll quickly do the review and then go over the first part of the /wdg/ intro video.

2:05pm. Done with chores. Let me start the review.

```
During the first week, I managed to get the first half of [Spiral v0.2](https://github.com/mrakgr/The-Spiral-Language/tree/v0.2) into an usable state. If I only wanted a language that does partial evaluation that would be most of it done already, but I have quite a bit more left to go.

Surprisingly I am learning [webdev now](https://www.youtube.com/watch?v=0pThnRneDjw). Before I can begin work on the typechecker, I need to figure out how language servers work and as it turns out everything on the VS Code side has evolved from a direction I am not used to. I spent 10 days diving into the API jungle and looking into various .NET language plugins before realizing that I am not making much progress and switching gears. I need to do it from the ground up. So far I've been starting it from the other end, and it has lead to much wasted time and effort.

I've always thought that at the end of my journey I would have ended knowing everything programming-wise so I suspected I would have to touch this at some point, but the fact that I am starting to learn this now is definitely a surprise to me. I definitely did not think that giving my language editor support would lead to this. Still, even more than just learning servers, I feel like I can gain a lot from this.

In 2016, I put a lot of effort into learning how to make desktop GUIs, and even did a graphical poker game, but it was a huge disappointment because just how much effort GUIs take. Doing the game + RL agents was something like 500 LOC. The GUI actually added 1000 LOC to that which was ridiculous. In 2019 one of the reasons I gave Pharo a shot is to satisfy my craving for interactive experience, but I could not satisfy all my desires as I did not like the thought of having to implement Spiral in Pharo. Fundamentally Pharo did not sit right with me as I want to do programming in a statically typed functional language with great type inference much like F# rather than a primitive, dynamic OO one like Pharo. Having great UI support is one thing, but I really want static types with Hindley-Milner type inference when I am writing a 4.5k LOC language that I frequently modify.

It might turn out that where both desktop GUIs in .NET and Pharo failed me, the web will give me what I want. I'll write my language in F# as intended, have VS Code for editor support, and when I get back to having to chart progress for my RL agents or do a short game the web will be there as a separate service. Once Wasm gets GC I'll seriously consider changing my high level target from .NET to it. I can also take this as the first step in learning how to interface intelligent agents with the wider world.

Also somewhat ironically, learning webdev will definitely increase my employability. And just as good as they will be for just myself, being able to show other people graphical projects will definitely be a great advantage in garnering interest for them. The disadvantage of course is that I am going to have to spend a few months learning and exploring the field. But I wanted to be graphical from the start. That I got stuck to the command line with Spiral was an accident and a consequence of how poor desktop development is.

At this exact moment, I've just finished going through the **Essential Typescript** book (it is pretty good), and will move to learning HTML and CSS. After that I'll get back to studying ASP.NET. The short term goal is to learn how servers work from the ground up, and since their evolution is intimately tied with other parts of web development I can't really skip learning those other things. I've already tried that and got in trouble.
```

2:50pm. I think this is good enough. Let me start the video.

2:55pm. Focus me, focus.

3:10pm. How annoying. That Udemy course is paywalled. Is there something on Coursera?

3:20pm. There is really a whole bunch on Coursera. Everything on Udemy is always paywalled.

3:25pm. There seems to be stuff on Edx as well.

...Ok. Let me go for videos for the rest of the day.

https://www.youtube.com/results?search_query=html

I'll go with Youtube.

https://www.youtube.com/watch?v=UB1O30fR-EE
HTML Crash Course For Absolute Beginners

I guess I'll see what this guy has to say. He is the one that made that intro video.

4:10pm. Uf, there was a second round of the chores. I am still at the start of the video only. Let me resume.

4:30pm. https://youtu.be/UB1O30fR-EE?t=1314

The stuff he is showing here in the Chrome tools is pretty interesting. All this is quite useful for somebody wanting to do interfaces and such.

5:50pm. Done with that video. I've been focused throughout, but now I feel tired.

https://www.youtube.com/watch?v=pQN-pnXPaVg
HTML Full Course - Build a Website Tutorial

This is another thing on the same theme. Since I already covered the same material, let me see what this guy says. I'll save the CSS video for later.

6:20pm. Phew, done with lunch. Let me watch that vid for a bit.

6:25pm. It is so slow. Forget it. I am done for the day here.

It is time for some fun."

---
## [mifl/android_kernel_qcom_msm-3.18@6aee4badd8...](https://github.com/mifl/android_kernel_qcom_msm-3.18/commit/6aee4badd8126f3a2b6d31c5e2db2439d316374f)
##### 2020-02-28 19:08:22 by Linus Torvalds

Merge branch 'work.openat2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs

Pull openat2 support from Al Viro:
 "This is the openat2() series from Aleksa Sarai.

  I'm afraid that the rest of namei stuff will have to wait - it got
  zero review the last time I'd posted #work.namei, and there had been a
  leak in the posted series I'd caught only last weekend. I was going to
  repost it on Monday, but the window opened and the odds of getting any
  review during that... Oh, well.

  Anyway, openat2 part should be ready; that _did_ get sane amount of
  review and public testing, so here it comes"

From Aleksa's description of the series:
 "For a very long time, extending openat(2) with new features has been
  incredibly frustrating. This stems from the fact that openat(2) is
  possibly the most famous counter-example to the mantra "don't silently
  accept garbage from userspace" -- it doesn't check whether unknown
  flags are present[1].

  This means that (generally) the addition of new flags to openat(2) has
  been fraught with backwards-compatibility issues (O_TMPFILE has to be
  defined as __O_TMPFILE|O_DIRECTORY|[O_RDWR or O_WRONLY] to ensure old
  kernels gave errors, since it's insecure to silently ignore the
  flag[2]). All new security-related flags therefore have a tough road
  to being added to openat(2).

  Furthermore, the need for some sort of control over VFS's path
  resolution (to avoid malicious paths resulting in inadvertent
  breakouts) has been a very long-standing desire of many userspace
  applications.

  This patchset is a revival of Al Viro's old AT_NO_JUMPS[3] patchset
  (which was a variant of David Drysdale's O_BENEATH patchset[4] which
  was a spin-off of the Capsicum project[5]) with a few additions and
  changes made based on the previous discussion within [6] as well as
  others I felt were useful.

  In line with the conclusions of the original discussion of
  AT_NO_JUMPS, the flag has been split up into separate flags. However,
  instead of being an openat(2) flag it is provided through a new
  syscall openat2(2) which provides several other improvements to the
  openat(2) interface (see the patch description for more details). The
  following new LOOKUP_* flags are added:

  LOOKUP_NO_XDEV:

     Blocks all mountpoint crossings (upwards, downwards, or through
     absolute links). Absolute pathnames alone in openat(2) do not
     trigger this. Magic-link traversal which implies a vfsmount jump is
     also blocked (though magic-link jumps on the same vfsmount are
     permitted).

  LOOKUP_NO_MAGICLINKS:

     Blocks resolution through /proc/$pid/fd-style links. This is done
     by blocking the usage of nd_jump_link() during resolution in a
     filesystem. The term "magic-links" is used to match with the only
     reference to these links in Documentation/, but I'm happy to change
     the name.

     It should be noted that this is different to the scope of
     ~LOOKUP_FOLLOW in that it applies to all path components. However,
     you can do openat2(NO_FOLLOW|NO_MAGICLINKS) on a magic-link and it
     will *not* fail (assuming that no parent component was a
     magic-link), and you will have an fd for the magic-link.

     In order to correctly detect magic-links, the introduction of a new
     LOOKUP_MAGICLINK_JUMPED state flag was required.

  LOOKUP_BENEATH:

     Disallows escapes to outside the starting dirfd's
     tree, using techniques such as ".." or absolute links. Absolute
     paths in openat(2) are also disallowed.

     Conceptually this flag is to ensure you "stay below" a certain
     point in the filesystem tree -- but this requires some additional
     to protect against various races that would allow escape using
     "..".

     Currently LOOKUP_BENEATH implies LOOKUP_NO_MAGICLINKS, because it
     can trivially beam you around the filesystem (breaking the
     protection). In future, there might be similar safety checks done
     as in LOOKUP_IN_ROOT, but that requires more discussion.

  In addition, two new flags are added that expand on the above ideas:

  LOOKUP_NO_SYMLINKS:

     Does what it says on the tin. No symlink resolution is allowed at
     all, including magic-links. Just as with LOOKUP_NO_MAGICLINKS this
     can still be used with NOFOLLOW to open an fd for the symlink as
     long as no parent path had a symlink component.

  LOOKUP_IN_ROOT:

     This is an extension of LOOKUP_BENEATH that, rather than blocking
     attempts to move past the root, forces all such movements to be
     scoped to the starting point. This provides chroot(2)-like
     protection but without the cost of a chroot(2) for each filesystem
     operation, as well as being safe against race attacks that
     chroot(2) is not.

     If a race is detected (as with LOOKUP_BENEATH) then an error is
     generated, and similar to LOOKUP_BENEATH it is not permitted to
     cross magic-links with LOOKUP_IN_ROOT.

     The primary need for this is from container runtimes, which
     currently need to do symlink scoping in userspace[7] when opening
     paths in a potentially malicious container.

     There is a long list of CVEs that could have bene mitigated by
     having RESOLVE_THIS_ROOT (such as CVE-2017-1002101,
     CVE-2017-1002102, CVE-2018-15664, and CVE-2019-5736, just to name a
     few).

  In order to make all of the above more usable, I'm working on
  libpathrs[8] which is a C-friendly library for safe path resolution.
  It features a userspace-emulated backend if the kernel doesn't support
  openat2(2). Hopefully we can get userspace to switch to using it, and
  thus get openat2(2) support for free once it's ready.

  Future work would include implementing things like
  RESOLVE_NO_AUTOMOUNT and possibly a RESOLVE_NO_REMOTE (to allow
  programs to be sure they don't hit DoSes though stale NFS handles)"

* 'work.openat2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
  Documentation: path-lookup: include new LOOKUP flags
  selftests: add openat2(2) selftests
  open: introduce openat2(2) syscall
  namei: LOOKUP_{IN_ROOT,BENEATH}: permit limited ".." resolution
  namei: LOOKUP_IN_ROOT: chroot-like scoped resolution
  namei: LOOKUP_BENEATH: O_BENEATH-like scoped resolution
  namei: LOOKUP_NO_XDEV: block mountpoint crossing
  namei: LOOKUP_NO_MAGICLINKS: block magic-link resolution
  namei: LOOKUP_NO_SYMLINKS: block symlink resolution
  namei: allow set_root() to produce errors
  namei: allow nd_jump_link() to produce errors
  nsfs: clean-up ns_get_path() signature to return int
  namei: only return -ECHILD from follow_dotdot_rcu()

---
## [123stephend123/bye-drod@fa42fd5d87...](https://github.com/123stephend123/bye-drod/commit/fa42fd5d8720264d640c54d67e9d4a71d19b4a97)
##### 2020-02-28 19:19:03 by 123stephend123

Stephen-Rog

Dear David-Rod, 

From first Intrepid lunch to talking about sailing to slogging through NU SAIL together to a thousand small, interactions when passing each other in the hall...to our last lunch (well, last with you as an Intrepid employee), you've always put a smile on my face, warmed my heart and challenged me to be a better product person and leader. While I'm sad you're leaving, I'm happy you're bringing the amazing uniqueness that is you to a new place. They're lucky to have you. 

Yours truly, 

Stephen-Rog

---

# [<](2020-02-27.md) 2020-02-28 [>](2020-02-29.md)

