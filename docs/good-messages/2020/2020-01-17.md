# [<](2020-01-16.md) 2020-01-17 [>](2020-01-18.md)

1,919,457 events, 968,022 push events, 1,546,942 commit messages, 113,184,841 characters


## [newstools/2020-naija-news-agency](https://github.com/newstools/2020-naija-news-agency)@[ed4116ef91...](https://github.com/newstools/2020-naija-news-agency/commit/ed4116ef91edc21a6009754495d485ec0a3953a5)
#### Friday 2020-01-17 04:11:10 by NewsTools

Created Text For URL [naijanewsagency.com/oko-mi-ololufe-mi-i-love-you-forever-toyin-abraham-showers-praises-on-her-husband-kola-as-he-clocks-new-age/]

---
## [facebook/pyre-check](https://github.com/facebook/pyre-check)@[7a703fcfd6...](https://github.com/facebook/pyre-check/commit/7a703fcfd659bfd2be7447050944603751b2a49d)
#### Friday 2020-01-17 06:54:22 by Jia Chen

Introduce APIs for parallel scheduling policy

Summary:
We'll need to go through a fair amount of background before one can make sense out of my proposal here.

Context
======

The problem we are trying to address here is to figure out a reasonable way to schedule the list of workloads passed into `Scheduler.map_reduce` (or `Scheduler.iter`). Past experience has shown that the scheduling algorithm has critical impact on the performance profile of both `pyre` and `pysa` (examples where a simple decision change lead to hundreds of seconds of perf win: D19385381, D13330658).

Here's a little bit of background on how the underlying `MultiWorker` module in `hack_parallel` is coded up. Supposed that we have `M `tasks and `N` workers. Conceptually, the way it works is that we take the list of tasks, break it apart into smaller chunks (aka. `bucket` in the code), and put all the chunks into a queue. Then we repeatedly poll from a pool of workers -- if all workers are busy, we block and wait for one that becomes non-busy; otherwise, we pull a task out of the queue, assign it to a non-busy worker, and poll for the next non-busy worker.

As you can see from the description above, there's really not a lot we can fine-tune given this framework -- picking a scheduling algorithm, under this context, essentially boils down to deciding how the list of `M` input tasks can be chunked.

Although the underlying `MultiWorker` API makes it possible to implement more advanced chunking algorithm, I think to avoid over-engineering we'll just stick with the simplest and dumbest chunking method possible for now: splitting the `M` jobs into chunks of equal length. Given that the input size `M` can vary across different invocations of `Scheduler.map_reduce`, naturally there are two ways we can do the split:
- The Fixed-Bucket-Size policy keeps the size of each chunk (henceforth denoted as `B`) constant, and let the number of chunks vary as `M / B`.
- The Fixed-Bucket-Count policy keeps the number of chunks (henceforth denoted as `C`) constant, and let the size of each chunk vary as `M / C`. Note that if this kind of policy is used, it usually makes more sense to let `C` scale with the number of workers -- hence in the code you won't find  `C` directly because it is always derived by multiplying the worker count and a multiplier.

Choosing Policies
=============

I've thought a lot about how we should choose between the Fixed-Bucket-Size policy and the Fixed-Bucket-Count policy. My current understanding is that there's no clear winner and it's always about tradeoffs:
- The Fixed-Bucket-Size policy tends to have more predictable **latency**
  + It's easier to estimate how long it takes for a worker to complete a given chunk of work if the chunk size is known.
- The Fixed-Bucket-Count policy tends to have more predictable **throughput**
  + It's easier to estimate the amount of IPC communication overhead if we know how many chunks need to be processed in total.

Following this line of thoughts, the first principle we should follow seems to be that we should do Fixed-Bucket-Size whenever progress needs to be shown to the user (because humans are more latency sensitive), and do Fixed-Bucket-Count otherwise for better overhead control. Note that I'm simplifying the issues a lot -- there are more perf/memory concerns regarding the worker GC if we are willing to dig into more details. But I want to get the big picture right first, before we can dive into micro-optimizations.

Fallback Behaviors
==============

Unfortunately, I can't conclude the discussion of scheduling policy at this point, as we do have some devils that hide in the edge cases.

The simple Fixed-Bucket-Size and Fixed-Bucket-Count policies described above work great when the number of task `M` is sufficiently large. But when the number becomes small, we run into troubles fairly quickly:
- The Fixed-Bucket-Size policy is prone to the problem of *starvation*: when the number of work `M` is not sufficient enough to fill `N` buckets (with `N` being the number of workers) of size `B`, some worker is going to be left with little to none work to do, while other workers will keep getting `B` chunks of works.
- The Fixed-Bucket-Count policy is prone to the problem of *excessive overhead*: when the number of work `M` is not sufficient to the extent that each bucket only contains very few work to do, the IPC/synchronization overhead between master-worker starts to dominate the wall clock. I've seen in practice cases where for a small amount of work, doing everything in master takes milliseconds while sending the same workloads to workers would take tens of seconds.

To mitigate these two issues, I propose two simple adaptations to the two policies mentioned before:
- First, we introduce the notion of *fallback behavior*, which kicks in when we find that following the original Fixed-Bucket-Size or Fixed-Bucket-Count policy may lead to issues due to insufficient input size.
  + For Fixed-Bucket-Size policy, if we detect that the number of buckets would be smaller than a threshold `C`, we fall back to a Fixed-Bucket-Count policy of count `C` instead to avoid starvation. The other way of looking at this is that we always cap the number of buckets to be at least `C`.
  + For Fixed-Bucket-Count policy, if we detect that the bucket size would be smaller than a threshold `B`, we fall back to a Fixed-Bucket-Size policy of size `B` to avoid excessive overhead. The other way of looking at this is that we always cap the bucket size to be at least `B`.
- Second, we introduce the notion of `sequential_cutoff`. This is going to be a parameter of our policy, and it means that if the input size `M` is smaller than `sequential_cutoff`, we unconditionally run everything sequentially in the master process. The exact value of `sequential_cutoff` may vary: if each task takes long time to compute (e.g. Pysa fixpoint iteration), we should use a small number; if each task only takes miliseconds (e.g. most of the environment table update), we should use a larger number. Note that this number should always be larger than 1: if `M` is 1, it is unconditionally better to do it in master than in worker.
  + In the code, `sequential_cutoff` is implicitly computed by multiplying the minimum bucket size `C` and the minimum bucket count `B`.

Overview of This Diff
===============

This diff tries to provide implementations for both Fixed-Bucket-Size-WIth-Fallback policy and the Fixed-Bucket-Count-With-Fallback policy. It also preserves the two policies currently used in production (denoted as "legacy" policies in the code) -- the goal is to keep this particular diff semantic-preserving, and gradually phase out the legacy policies in followup diffs.

See `schduler.mli` first to get an idea of how the `Scheduler` API evolves. My change should be fairly straightforward: the `bucket_size` parameter (which used to switch between our legacy version of Fixed-Bucket-Size and Fixed-Bucket-Count policy) is replaced with a full-blown policy type `Policy.t`.

The type `Policy.t` is kept private to ensure that one can only create them via the provided APIs. Under the hood it is nothing but a function that computes the bucket count.

Rest of the changes are there to cope with the `Scheduler` API updates.

Reviewed By: sinancepel

Differential Revision: D19402492

fbshipit-source-id: a9b627cdf9af083b021fe51e825168dc3abfaa45

---
## [iamankypanky/git-gitHub](https://github.com/iamankypanky/git-gitHub)@[7c7fc6de84...](https://github.com/iamankypanky/git-gitHub/commit/7c7fc6de84ec0bb5497165e6cfe6592b4186145d)
#### Friday 2020-01-17 09:20:52 by iamankypanky

Update README.md

This is my first trial. Apologies for any stupidity. I love to ask stupid questions although.

---
## [esmero/webform_strawberryfield](https://github.com/esmero/webform_strawberryfield)@[fd06d69760...](https://github.com/esmero/webform_strawberryfield/commit/fd06d69760d4cdee9a1370d5524cd8ee4b4d0e48)
#### Friday 2020-01-17 15:36:02 by Diego Pino Navarro

Further Fixes for ISSUE-17

This was annoying but finally this should be fixed.

For some reason, When issuing a Display Mode Switch (during node edit/ingest) the ajax callback was validating the whole form, including the internals of the webform, forcing us (our fault of course) to delete the session information / private storage that kept data connected between Webform widget and the actual Webform multistep process. Result. If doing an ingest from scratch using the demo account which is heavy on ajax, and changing the display mode at the end, we ended with an empty JSON and a lot of shame. I had the bad luck to explore (explode!) this in public while showcasing Archipelago. This is new and is related to some internals of that switch field, still my fault. There are so many ways to avoid this while testing = ingest correctly,  but i always made the right ajax combinations to have this explode in my face!. Anyway. This brings some minor changes.
- First, we keep the initial widget state id consistent, without reassigning every time the form rebuilds. Not a big change but allows in a next pull to restore from private storage a draft (if you have to go an open the door for UPS and your computer ends without battery).
- Second, we don't explicitly empty the session private storage anymore. That is not needed, we let it expire, but that way we can also restore it.
- I also addded a humble attempt of #limit validation errors for the webform itself, so other elements in different trees don't affect this so much. Still can be improved.
- Finally, i added error logging in case private temp store fails (means fatal DB error, still good to alert people while black smoke comes out right?) during save but also during partial webform validation, to make sure that in the worst of the cases, people don't spend hours filling a form that will not be able to be saved anyway. I also dump the webform content to logs, so at least, for now, data is not lost and tbis can be further debugged by an admin.

/src/Plugin/Field/FieldWidget/StrawberryFieldWebFormWidget.php looks like it was changed a lot, but its mostly just wrong identation fixed, legacy of my first steps months ago. You can see without spaces when reviewing for the actual changes.

@giancarlobi second pair of eyes would be appreciated!

---
## [mmihaeladraghici/talks](https://github.com/mmihaeladraghici/talks)@[48c0f89b97...](https://github.com/mmihaeladraghici/talks/commit/48c0f89b97b492bb5032d2daa4df3c7909202a73)
#### Friday 2020-01-17 16:23:14 by mmihaeladraghici

Product Managers, to pair or not to pair?

Product Managers, to pair or not to pair?
Speaker : Mihaela Draghici
Available : first day, second day, third day
Length : 30 minutes
Language : English
Description
What does pairing mean for PMs? Why is it great? Why does it suck? When is it good to pair? And more than that, how to make the most of it when you do pair?

There are little resources available regarding pairing for Product Managers. That's because it's not too common.   I aim to share my lessons learned and tips & tricks about pairing for PMs (from own experience), to give you more insights on this topic.

Speaker Bio
A Romanian, with a British passport, living in Lisbon. Marketing girl, turned tech girl, building products that deliver happiness. Strong advocate for gender equality and inclusive work environments in tech industries.

Links
Company: https://www.linkedin.com/company/volkswagen-digital-solutions/
GitHub: https://github.com/mmihaeladraghici/
Photo: https://pbs.twimg.com/profile_images/1271157175/mihaela-draghici1_400x400.jpg
Extra Information
The purpose of this talk is to offer more information regarding pairing practices for Product Managers to to help:

those who have not heard of pairing for PMs, learn more about it;
those who have heard about it but have not considered pairing in their product teams: start considering it and weigh options around benefits for their products/business;
those who have tried it: open a conversation about advantages/disadvantages and exchange knowledge.
I have over 10 years of international work experience in digital marketing and product management. I have contributed to both small and large scale product initiatives to bring business value and worked closely with engineering teams, UX designers, localisation specialists as well as business stakeholders across regions.    I am passionate about what I do, the products I've built and the people I work with.  Since I have learned a lot throughout my career and benefited from great colleagues and mentors, I am looking to share my knowledge and give back to the community through talks, events & mentoring.

Some references to myself and my work:  

https://www.facebook.com/GirlsinTechLondon/
https://www.youtube.com/watch?v=VSHNdMvXlUY  
https://www.eventbrite.co.uk/e/getting-into-tech-tech-spectrum-panel-for-graduates-tickets-50451371410#
https://issuu.com/businesswomanclub/docs/april_digital_business_women_emagaz/60
https://www.eventbrite.co.uk/e/in-common-women-at-work-tickets-65778833261#
https://www.linkedin.com/pulse/10-things-i-love-being-product-manager-mihaela-draghici 
https://performancein.com/news/2019/06/18/awin-thinktankuk-2019-and-future-proofing-affiliate-industry/

---
## [Dermaed/Zmeka](https://github.com/Dermaed/Zmeka)@[b7502d4136...](https://github.com/Dermaed/Zmeka/commit/b7502d4136b52784f2f0773a42f0916ab6ad2fa7)
#### Friday 2020-01-17 16:43:26 by York

Added pictures for snake

dude this party sucks i fucking hate these people

---
## [Dermaed/Zmeka](https://github.com/Dermaed/Zmeka)@[4de43945ff...](https://github.com/Dermaed/Zmeka/commit/4de43945fff8568c58f7a37b647e92b5f7646ec2)
#### Friday 2020-01-17 16:56:57 by York

Logic head of snake

fuck this game, i hate this fucking shit!!!!
P.S. Added logic head of snake, depending on the direction of the head, this way it will be shown

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[a89cdbdc37...](https://github.com/mrakgr/The-Spiral-Language/commit/a89cdbdc37c0d2cc100ced92d1d472a068a2eb0d)
#### Friday 2020-01-17 17:22:17 by Marko GrdiniÄ‡

"9:30am. I am up.

10:50am. Done with vol 7. Let me have that breakfast.

12:40pm. I am nearly a third of the way into vol 8, but let me stop here. It is time for me to mentally start preparing for the programming day. It is also time I deal with the chores.

2:10pm. Today's chores took a bit more than I thought they would. Let me catch my breath for 10m and then I will start.

2:20pm. Let me start.

What I need to do now is really start the prepass.

I just need to do this thing. I feel lethargic just thinking about it.

It is not that any of the pieces are hard, but as is usual at times like these the main problem is gathering momentum.

Still if there is something that I've learned during my long years of programming that would be - if the pieces are there, then it is possible to complete the entire thing.

2:25pm. So focus me, focus. The pieces are all there.

Right now when I think of ML, the pathways that I use to create Spiral are the ones that respond.

This is not a ability that the average guy off the cubicle has. The ML guys certainly do not have it. So I have my own edge. I just need to draw it out.

Just one more time.

I've been saying this like a broken record for the past 5 years, but it is not like 5 years is that long of a time.

In order to win a game, you just need to start making the right moves.

Now that damn prepass.

Let me just focus on annotating the all the functions with their free variables and stack sizes. This is something that I've done before - this time what I need to do is also take note of the type variables.

This kind of simple task should serve me well for what will come after that.

2:30pm. Still...damn.

Yeah, free vars and stack sizes are not difficult by themselves. Neither is the prepass.

But what do I do about inlining?

Inlining is supposed to be done during the prepass (so I do not have to keep around the extra info during caching)

2:40pm. I have no idea.

Hrrrmmmm...Just like yesterday I am going to have to dedicate some time to thinking about this.

There are 3 pieces to the prepass:

1) The free vars and stack size calculations.
2) The renaming and conversion from raw expressions to the array friendly ones that the peval needs.
3) And now inlining of globals.

3:10pm. I am still thinking about it.

3:50pm. I am still thinking about it.

The whole inlining thing being in the last place in the order really throws a wrench in my plans.

4:45pm. I've actually been thinking about it all this time. I nearly have it.

I am going to switch to an alternative naming scheme that will make it unecessary to do things in two passes. This will simplify things.

But the typecase bindings and global inlining makes things complicated and I haven't been able to figure out how to push that through completely yet.

Of course I know ways of doing it, but I haven't settled on the right one exactly.

4:50pm. I just need to go a little bit further in my reasoning - just how do I pass globals around during the prepass?

5:25pm. Ok, I think I've pretty much 95% settled on the new architecture for the prepass.

In fact, I had this idea last time, but decided to go with the more complicated scheme that v0.1 has for some reason.

Now that it has come to this, I need to go all out.

5:30pm. Ok, I figured out one more detail. That is 98% where I am at.

The last 2% is just me wrapping up essentially.

5:35pm. Let me wax the idea for a while longer and then I am going to call it a day.

5:40pm. Ok, I am done.

Here is what I am going to do the basic idea is simple...

```
inl x -> let y = 2 in b + c + x + y
```

Given a function like this, I am going to tag free variables in the negative direction `b -> -1`, `c -> -2` while the bound variables will go in the positive direction `x -> 0`, `y -> 1`. This will make a second of renaming unecessary.

As for the general architecture....

```
let prepass (global_type : Map<string, ExprT>) (global_value : Map<string, Expr>) =
    let rec loop (free_vars_type : Dictionary<string, ExprT>) (free_vars_value : Dictionary<string, Expr>) (vars_type : Set<string>) (vars_value : Set<string>) (local_type : Map<string, ExprT>) (local_value : Map<string, Expr>) = ...
```

5:55pm. Writing this out like this is making me realize that it made more sense in my head.

But...an idea is occurring to me.

Maybe it would not be bad to get rid of maps completely.

Remember that one test in v0.09 which gave me compilation issues I could not get rid of?

```
//let speed3 = // Minus the startup, this takes 0.05s to peval versus 1.5s for the previous version of the compiler.
//    let code =
//        let var i = sprintf "var_%i" i
//        let bnd (a, b) = sprintf "inl %s = %s" a b
//        let vars = [|0..900|] |> Array.map var // Any more than this and it will stack overflow.
//        let bnds =
//            vars |> Array.pairwise |> Array.map (fun (a,b) -> b,a)
//            |> Array.map bnd |> String.concat "\n"
//        let adds = String.concat " + " vars
//        String.concat "\n" [|bnd (var 0, "dyn 0");bnds;adds|]

//    "speed3",([] : Module list),"Does the linear sequence of bindings get compiled in linear time?",code
```

It is difficult to understand what it does just by looking at it now, but as far as I remember, this is a simple program with like 900 different let statements in a row.

Propagating all the free variables takes a crazy amount of time and the main problem is that the immutable data structures are just slow to deal with this. I think the union operation was the main culprit.

```
let prepass (global_type : Map<string, ExprT>) (global_value : Map<string, Expr>) =
    let rec loop (free_vars_type : Dictionary<string, ExprT>) (free_vars_value : Dictionary<string, Expr>) (vars_type : Set<string>) (vars_value : Set<string>) (local_type : Map<string, ExprT>) (local_value : Map<string, Expr>) = ...
```

It seems complicated at first glance, but what if I used a dictionary and cps'd the program so that the adds get removed as soon as the let statements go out of scope.

That would resolve the issues with if statements.

```
(vars_type : Set<string>) (vars_value : Set<string>)
```

As for these two, well, I really need `vars_type` in order to insert the bindings into typecase.

Very well, I guess I'll just keep `vars_type` around for that then.

The missing variables error I can just calculate from the free variables in the global statements.

`(global_type : Map<string, ExprT>) (global_value : Map<string, Expr>)`

These two I will turn into dictionaries as well. No need for immutable maps here.

6:10pm. Let me stop here.

I think the dual naming scheme and the CPS'd dictionaries idea are what I was really looking for. I am going to absolutely crush the prepass this time and forge it to perfection. I admit, before now it never even occurred to me to consider removing things from a dictionary, but it is a great idea.

6:20pm. Oh, yes. I am going to need `vars_value` if nothing else, but just to avoid inlining the globals incorrectly.

Well, I'll figure it out tomorrow as I go along."

---
## [mfilenko/form3-payments](https://github.com/mfilenko/form3-payments)@[c001d55da0...](https://github.com/mfilenko/form3-payments/commit/c001d55da08a0a01a95fdd564715c24418e6cb65)
#### Friday 2020-01-17 20:52:08 by Max Filenko

In the beginning there was darkness

If you're reading this it gives me a hope that we share the same
passion about project history and extensive commit messages which are
an essential part of it ;-)

I personally believe that Git is the greatest thing that ever happened
to collaborative software development.

I think it is vital for developers to understand the importance of
maintaining clean Git history of a project in order to prosper in a
longer run.

This is just an initial commit, so nothing really important will be
here, but to honor the fact that you're still reading it here's a
small but funny [collection][1] of other people's initial commit
messages.

Cheers!

[1]:
https://blog.no-panic.at/2014/10/20/funny-initial-git-commit-messages/

---
## [ludwikson/CodingPractise](https://github.com/ludwikson/CodingPractise)@[760f0fe97a...](https://github.com/ludwikson/CodingPractise/commit/760f0fe97a1415aa7375a929633e2064cb628c53)
#### Friday 2020-01-17 21:37:09 by ludwikson

FINALLY! I did it! (We, kinda)

It's awful when you're sure it should work but it doesn't. With a little help of my friend, especially with "53" in ASCII decimal system.

---
## [colszowka/simplecov](https://github.com/colszowka/simplecov)@[151926b715...](https://github.com/colszowka/simplecov/commit/151926b7152355d050062e78fdcb9e8b3341d574)
#### Friday 2020-01-17 21:40:37 by Tobias Pfeiffer

Fix crash when running against old version of file

We do a light check on every result if it looks like it conforms
to our file format. If not, we ignore it and throw it out.

Initial plan was to just catch an error and return a known good
result, but sadly since we don't know what order the results come
in that doesn't really work (as the first could be the "bad" result).
Might be able to change this if we always knew we started with
a known good result.

Checking feels kinda clunky, but I honestly didn't know of
a better way right now.

---
## [icarus496/MAT157](https://github.com/icarus496/MAT157)@[70e06cbe10...](https://github.com/icarus496/MAT157/commit/70e06cbe10e8e119f2519c455f66f9836f58b9e8)
#### Friday 2020-01-17 22:21:34 by icarus496

Oh my god I think this is the answer but it's fucking gross

---
## [Finbarr-1/Expedition](https://github.com/Finbarr-1/Expedition)@[12ac9c029f...](https://github.com/Finbarr-1/Expedition/commit/12ac9c029f7ac0d0e03214b481157e6ae8cfab2f)
#### Friday 2020-01-17 22:56:36 by PepperLola

As of 2:12 on January 17th, I have decided to rename the "isfacebooktrackingme" app icon to "thezucc". This reflects society as it stands today, as an homage to the fact Mark "The Zucc" Zuckerberg is controlling our everyday lives, as well as the lives of our friends and family. I believe that "isfacebooktrackingme" is simply too precise of a name, only addressing the fact that Facebook is tracking literally everyone, whereas "thezucc" is much more broad, addressing all of Facebook, as well as the creator of it. "How does this affect the user, and overall provide an exceptionally fantastic user experience?" you might ask. The simple answer is, it doesn't. Because this icon isn't commonly used among the majority of the population, it provides no real improvement to the experience of most users. However, it does make the life of our more *important* users a slight bit easier. Let's face it. "thezucc" is much easier and faster to type than "isfacebooktrackingme". Expedition is focused primarily on ease of use, so why not make everything as easy to use as possible? What's the point in having an app icon name that both takes longer to type, and isn't as intuitive. Most people have said the phrase "the zucc" at least once in their lives. If not, it still provides for a memorable phrase they won't forget whey they inevitably decide one of their apps needs The Zucc's face on it. Overall, renaming this app icon will provide not only an acceptable user experience, but an absolutely *fantastic* user experience, and have side effects such as excessive happiness, or the need to brag to friends about their fancy new app icon. With side effects like that, who wouldn't want to have an app icon this much easier to access. All we ask is that you change your app icons with caution, lest you have *too* much fun.

---
## [spuder/nomad](https://github.com/spuder/nomad)@[3a51761406...](https://github.com/spuder/nomad/commit/3a5176140609a2e463760689b486a5331ab0ded4)
#### Friday 2020-01-17 23:02:40 by Mahmood Ali

dev: avoid codecgen code in downstream projects

This is an attempt to ease dependency management for external driver
plugins, by avoiding requiring them to compile ugorji/go generated
files.  Plugin developers reported some pain with the brittleness of
ugorji/go dependency in particular, specially when using go mod, the
default go mod manager in golang 1.13.

Context
--------

Nomad uses msgpack to persist and serialize internal structs, using
ugorji/go library.  As an optimization, we use ugorji/go code generation
to speedup process and aovid the relection-based slow path.

We commit these generated files in repository when we cut and tag the
release to ease reproducability and debugging old releases.  Thus,
downstream projects that depend on release tag, indirectly depends on
ugorji/go generated code.

Sadly, the generated code is brittle and specific to the version of
ugorji/go being used.  When go mod picks another version of ugorji/go
then nomad (go mod by default uses release according to semver),
downstream projects face compilation errors.

Interestingly, downstream projects don't commonly serialize nomad
internal structs.  Drivers and device plugins use grpc instead of
msgpack for the most part.  In the few cases where they use msgpag (e.g.
decoding task config), they do without codegen path as they run on
driver specific structs not the nomad internal structs.  Also, the
ugorji/go serialization through reflection is generally backward
compatible (mod some ugorji/go regression bugs that get introduced every
now and then :( ).

Proposal
---------

The proposal here is to keep committing ugorji/go codec generated files
for releases but to use a go tag for them.

All nomad development through the makefile, including releasing, CI and
dev flow, has the tag enabled.

Downstream plugin projects, by default, will skip these files and life
proceed as normal for them.

The downside is that nomad developers who use generated code but avoid
using make must start passing additional go tag argument.  Though this
is not a blessed configuration.

---
## [DanaL/AdventOfCode](https://github.com/DanaL/AdventOfCode)@[c6cada20c0...](https://github.com/DanaL/AdventOfCode/commit/c6cada20c015abc6aa43c3b833556ccf12c325f2)
#### Friday 2020-01-17 23:10:15 by Dana Larose

Almost have my goddamn priority queue/dijkstra's working, god I hate Rust

---
## [sellmeadog/automatic-app-landing-page](https://github.com/sellmeadog/automatic-app-landing-page)@[0b20d51f30...](https://github.com/sellmeadog/automatic-app-landing-page/commit/0b20d51f305628d7b63f05c342b7d7ccc13f549e)
#### Friday 2020-01-17 23:37:01 by Kennie J. Davis

change footer

* remove the made with love shit
* add the reddit link

---

# [<](2020-01-16.md) 2020-01-17 [>](2020-01-18.md)

