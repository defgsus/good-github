# [<](2020-01-27.md) 2020-01-28 [>](2020-01-29.md)

2,195,273 events, 1,063,163 push events, 1,688,458 commit messages, 135,096,191 characters


## [mjherich/Sprint-Challenge--Graphs](https://github.com/mjherich/Sprint-Challenge--Graphs)@[b491eaa02b...](https://github.com/mjherich/Sprint-Challenge--Graphs/commit/b491eaa02bde1ad26f5b53c458164e32d017ed13)
#### Tuesday 2020-01-28 00:43:35 by Matt Herich

Add screenshot to prove 947 traversal; remember to stash before you checkout boys and girls

---
## [Grickl3/sqglz](https://github.com/Grickl3/sqglz)@[d9392fc359...](https://github.com/Grickl3/sqglz/commit/d9392fc3591c5f213108bad305c94310b5d22e09)
#### Tuesday 2020-01-28 01:20:02 by DeaconRodda

haven't been in for a while.In the following session, I intend to fuck not around, to get the next iteration of this site the fuck up, and to gain some goddamn momentum in my motherfucking business and my whole goddamn life.

---
## [freebsd/freebsd](https://github.com/freebsd/freebsd)@[db5ca2d2bf...](https://github.com/freebsd/freebsd/commit/db5ca2d2bf12a80629c4a8cedc188f55ab6a20e7)
#### Tuesday 2020-01-28 03:01:49 by kevans

MFC r357103-r357104: unbreak local.lua, add a modules.loaded hook

r357103:
loader.lua: re-arrange to load local.lua *after* config loading

The major problem with the current ordering is that loader.conf may contain
all of the magic we need to actually setup the console, so loading local.lua
prior to that can make it excessively difficult and annoying to debug
(whoops, sorry Ravi & Warner).

The new ordering has some implications, but I suspect they are a non-issue.
The first is that it's no longer possible for the local module to inject any
logic prior to loading config -- I suspect no one has relied on this. The
second implication is that the config.loaded hook is now useless, as the
local module will always be included after that hook would have fired.

For config.loaded, I will opt to leave it in, just in case we add an early
point for local lua to get injected or in case one wants to schedule some
deferred logic in a custom loader.lua. The overhead of having it if no hooks
will be invoked is relatively minimal.

r357104:
lua: add modules.loaded hook

This may be used for the local module to hook in and load any additional
modules that it wants, since it can't modify the modules table internal to
config. We may consider adding API to do so at a later time, but I suspect
it will be more complicated to use with little return.

status is captured but ignored for the purpose of loading the hook. status
will be false if *any* module failed to load, but we typically don't let
that halt the boot so there's no reason to let it halt hooks. Some vendors
or setups may have expected fails that would be actively thwarted by
checking it.

We may, at a later date, consider adding an API for letting non-config
modules check which modules have successfully (or not) loaded in case an
unexpected failure *should* halt whatever they are doing.

---
## [kevansevans/HxDoom](https://github.com/kevansevans/HxDoom)@[16ae137499...](https://github.com/kevansevans/HxDoom/commit/16ae1374998873749fce2970b4a8cda2e39bf9d4)
#### Tuesday 2020-01-28 03:15:09 by Kaelan

Cheatcode macro builder

Fucking witchcraft but I love it

---
## [tristanbowler/PersonalSpace](https://github.com/tristanbowler/PersonalSpace)@[e0c203836f...](https://github.com/tristanbowler/PersonalSpace/commit/e0c203836f895723314091e3db3d891a54e18f31)
#### Tuesday 2020-01-28 05:21:55 by Chris Payne

First batch of changes

Hopefully the last time i fix those STUPID FUCKING HOUSE PARTY NIGHT STANDS.
Added basic light and particle effect to objectives
Fixed some textures

---
## [FallingStar-Games/outsiders-of-thunder-city](https://github.com/FallingStar-Games/outsiders-of-thunder-city)@[97882f267c...](https://github.com/FallingStar-Games/outsiders-of-thunder-city/commit/97882f267c3651406070bd2ef46da08d8d593065)
#### Tuesday 2020-01-28 06:19:27 by NyxAsteria

We've Received A Copyright Take-Down Notice

-Ray's initial moveset is fully made, his character mostly statted out
-Fixed the turn counter bug partially
-Crits are in!
-Fang now has sick new moves
-You can now talk to Seth to get the Stunned status

To-do: Rework My Heart Bleeds Red, tweak Ray, playtest the fuck out of this, work on Lucia's character now

---
## [len9780/hydrogen](https://github.com/len9780/hydrogen)@[5e1425919d...](https://github.com/len9780/hydrogen/commit/5e1425919d24b101205a39062709c1d8c6ecf315)
#### Tuesday 2020-01-28 07:07:44 by theGreatWhiteShark

Doc, preprocessor directives, bugs
As I keep diving into the code to understand how the audio engine works, I decided to write down all details I figured out to start some proper documentation of Hydrogen.

#Preprocessor directives:
Doxygen has some quite annoying shortcommings. It either tries to be smart when setting `ENABLE_PREPROCESSING = YES` in the Doxyfile.in and neglects all documentation not defined with the current compiler flags or is really dump when setting `ENABLE_PREPROCESSING = N
O` making it impossible to document and references to global definitions, like the state of the audio engine `STATE_PREPARED` in core/include/hydrogen/hydrogen.h. Unfortunately, the former does not work properly and neither JACK nor any over additional capabilities are recognized and their documentation is neglected. Therefore, I introduced the `PREDEFINED = _DOXYGEN_` line in Doxyfile.in and changed the preprocessor directives from, e.g., `#ifdef H2CORE_HAVE_OSC` to `#if defined(H2CORE_HAVE_OSC) || _DOXYGEN_`. This way both Hydrogen will work as expected and all special functions will be documented at the same time.

# Bugs:
- I removed the obsolete `hydrogenInstance` object in line 1734 of hydrogen.cpp
- removed the void input type of Hydrogen::initBeatcounter() since does not have/uses any inputs.
- Implemented the #FIXME in JackAudioDriver::disconnect() and introduced a new error type Hydrogen::JACK_CANNOT_CLOSE_CLIENT
- Renaming JackAudioDriver.h into jack_audio_driver.h. I know, renaming files is bad, bad, bad. But violating the naming style is really a pain in the ass since it prevents the editor from jumping from the header to the source file and vice versa.
- The `m_nCond` variable in JackAudioDriver was defined as a `bool` and not initialized. It controls if the take over of the time master in JACK should be done conditionally or not. The corresponding function of the JACK API expects an `int`. If it is greater than zero, the take over will happen regardless if there is already a time master present or not. I renamed the variable into `m_nJackConditionalTakeOver` and initialized it with 1.

---
## [MannesdeGroot/Periode3](https://github.com/MannesdeGroot/Periode3)@[685618766c...](https://github.com/MannesdeGroot/Periode3/commit/685618766c019ab9204e8030088aa499f133533b)
#### Tuesday 2020-01-28 09:36:55 by MannesdeGroot

Anime/Gaming subs are an endless source of good Copypasta Wait people aren't actually pissed off at this are they? Okay a couple of points here to make a lot of you look like idiots.  A fetish does not make you a creep. For instance, people can be turned on by incest porn, but not want to fuck their own sister. Furries don't often want to fuck animals. And PLENTY of people like porn that involves cheating or rape, but would NEVER do those. Just so, loli fans aren't just a bunch of pedos (though statistically maybe a few probably are, but you could say that about any fetish). For instance, I'm a loli fan myself, but I have an 8 year old daughter who regularly brings friends over, and not ONCE (I swear) have I seen a single one of them in a sexual light. I'm attracted to the cute designs and innocent attitudes of lolis, that doesn't mean I'm gonna go fuck children, that's obviously fucking disgusting. Not to mention, most lolis are legal, it's the design, NOT the age that makes a loli.  Anime style video games, and games from Japan always have the "cute" option when picking love interests. Usually some young, innocent girl with a shy attitude. And yes, I'm well aware Futaba kinda falls in that category too, but she can also fall under the "geeky girl" concept, which is also typically a separate option from the "cute" one. And since the twins are explained multiple times to be much older, that really does just make them cute, and nothing more. It's a very normal thing for a game like this to make them dateable.  THEY'RE FUCKING VIRTUAL. At the end of the day, if my first two points didn't make you feel stupid, this one should. If you can't accept that people can have a fetish without becoming a creep, or that these are normal, long time tropes in Japanese games, then at least accept the fact that there's no point attacking people for wanting VIRTUAL CHARACTERS to be dateable. After all, who the hell is that hurting? Sure as hell not any kids, that's for sure. So why even bother getting pissy? What, cuz YOU don't wanna date em? Then don't! Simple as that!

---
## [cverna-hackathons/sup](https://github.com/cverna-hackathons/sup)@[f037245c41...](https://github.com/cverna-hackathons/sup/commit/f037245c418b5d15c4c0f639f39f071bf80723c3)
#### Tuesday 2020-01-28 09:40:46 by jCobbSK

Saatchi probe (#38)

* Fuck you

* working ID extraction

* wip

* Download image data to json files

* example artwork record shape

Co-authored-by: Peter Berezny <spontain@gmail.com>

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[3da884a42d...](https://github.com/mrakgr/The-Spiral-Language/commit/3da884a42d260a5ac4f73341ba04ea528e06be9d)
#### Tuesday 2020-01-28 10:15:26 by Marko GrdiniÄ‡

"10:55am. Tonight was hell. I think I had 3-4 different nightmares back to back. What the hell?

It is all deep learning's fault. Today, ML can only be described as oppressive.

I have various ideas, but I know they can't work. I can't avert my eyes from the truth. Everything is wrong, and yet I can't fix anything. The way this eats at me is unbelievable.

December was just the final nail in the coffin. I can't really rely on math. I can't relly on mathematical thinking nor mathematicians.

11am. I am going to go with my evoalgo as science plan of course, because it is right, but still I feel deep regret that I do not even have the faintest idea what the principles of learning or better structures for it would look like.

I did have just a little bit of insight last night.

https://www.sciencedirect.com/science/article/pii/S0893608019302655
Spiking Neural Networks and online learning: An overview and perspectives

I've literally never even thought about SNNs up to now. In 2019 I did a lot of theorem proving in order to get a grasp on numbers, but it sort of slipped my mind that nature itself did figure out ways of doing credit assignment without relying on differentiability.

Still, knowing only that the networks spike is really not a lot for me to go for.

11:05am. Yet, I am pissed at having to rely on differentiability which I do not believe is the right tool for learning tasks.

I want something better. I really want something better.

I want just little bit of insight at what the Singularity tier models will look like. And what self improvement will look like.

11:10am. Forget ML. From here on out, my main focus will be to be a good scientist.

Trying to figure out ML issues is just causing me stress. If I really have to do it, from here on out I should consider using evolution to try and evolve something that works better. I'll probably fail, but at least I should be doing what I think I should.

For the time being let me read that review paper I've linked to."

---
## [xainag/xain-fl](https://github.com/xainag/xain-fl)@[8b9f74c91d...](https://github.com/xainag/xain-fl/commit/8b9f74c91dc0a4c68270bb544063d04d8976053c)
#### Tuesday 2020-01-28 11:56:02 by Corentin Henry

XP-456: replace CLI argument with a config file (#221)

Cc: finiteprods <kwok.doc@gmail.com>
Cc: Robert Steiner <robertt.debug@gmail.com>
Cc: janpetschexain <58227040+janpetschexain@users.noreply.github.com>

References
==========

https://xainag.atlassian.net/browse/XP-456
https://xainag.atlassian.net/browse/DO-58

Rationale
=========

The CLI is getting complex so it is worth loading the configuration
from a file instead.

Implementation details
======================

TOML
----

We decided to use TOML for the following reasons:

  - it is human friendly, ie easy to read and write
  - our configuration has a pretty flat structure which makes TOML
    quite adapted
  - it is well specified and has lots of implementation
  - it is well known

The other options we considered:

  - INI: it is quite frequent in the Python ecosystem to use INI for
    config files, and the standard library even provides support for
    this. However, INI is not as powerful as TOML and does not have a
    specification
  - JSON: it is very popular but is not human friendly. For instance,
    it does not support comments, is very verbose, and breaks
    easily (if a trailing comma is forgotten at the end of a list for
    instance)
  - YAML: another popular choice, but is in my opinion more complex
    than TOML.

Validation
----------

We use the third-party `schema` library to validate the
configuration. It provides a convenient way to:

- declare a schema to validate our config
- leverage third-party libraries to validate some inputs (we use the
  `idna` library to validate hostnames)
- define our own validators
- transform data after it has been validated: this can be useful to
  turn a relative path into an absolute one for example
- provide user friendly error message when the configuration is
  invalid

The `Config` class
------------------

By default, the `schema` library returns a dictionary containing a
valid configuration, but that is not convenient to manipulate in
Python. Therefore, we dynamically create a `Config` class from the
configuration schema, and instantiate a `Config` object from the data
returned by the `schema` validator.

Package re-organization
-----------------------

We moved the command line and config file logic into its own `config`
sub-package, and moved the former `xain_fl.cli.main` entrypoint into
the `xain_fl.__main__` module.

Docker infrastructure
---------------------

- Cache the xain_fl dependencies. This considerably reduces
  "edit->build-> debug" cycle, since installing the dependencies takes
  about 30 minutes.
- Move all the docker related files into the `docker/` directory

Current limitations and future work
-----------------------------------

1. The documentation generated for the `ServerConfig`,
   `AiConfig` and `StorageConfig`  classes is wrong. Each attribute is
   documented as "Alias for field number X". This can be fixed by
   having `create_class_from_schema()` setting the `__doc__` attribute
   for each attribute. However, we won't be able to automatically
   document the type of each attribute.

2. When the configuration contains an invalid value, the error message
   we generate does not contain the invalid value in question. I think
   it is possible to enable this in the future but haven't really
   looked into it.

---
## [fish2000/CLU](https://github.com/fish2000/CLU)@[e64bd9e707...](https://github.com/fish2000/CLU/commit/e64bd9e707bd87f0cbc1de5da8f98c98bbb9f606)
#### Tuesday 2020-01-28 12:21:07 by Alexander BÃ¶hn

Bringing all treatments of â€œ__qualname__â€ values up to snuff
... as in, no longer do we just alias it to â€œ__name__â€ when weâ€™re
    renaming or resetting things â€“ we specifically seek to preserve
    the parts of â€œ__qualname__â€ dotpath-ish strings with namespace-
    specific information, while altering only the sections relevant
    to whatever it is we are doing (i.e. renaming, or whatever).
... likewise, in functions like â€œdetermine_name(â€¦)â€, in the event
    (however unlikely) that the code branches to the consideration
    of a â€œ__qualname__â€ value, we deterministically cleave off only
    the bit we care about using â€œstr.rpartition(â€¦)â€ and slicing
... there are one or two extremely corner-iffic cases where some
    still-existing â€œ__qualname__â€ negligence could possibly, under
    thoroughly bizarre and freakish circumstances, transpire â€“ but
    I have marked them shits as TODO and will assuredly find time
    to procrastinate from whatever I should actually be doing in
    the near-to-middling future and address these, toot sweet.
... in conclusion: thank you for using CLU, doggie, yeah!~

---
## [Midek/gomuks](https://github.com/Midek/gomuks)@[bcf24c2b5d...](https://github.com/Midek/gomuks/commit/bcf24c2b5d3a65fa03ba99f739290be1275e63d2)
#### Tuesday 2020-01-28 12:44:08 by Midek

longer notifications (with hardcoded pic because fuck you)

---
## [indorgro/Janne-Rubin](https://github.com/indorgro/Janne-Rubin)@[cf8f7dd1ab...](https://github.com/indorgro/Janne-Rubin/commit/cf8f7dd1aba90269cb4c03d5802f1fb5352752b5)
#### Tuesday 2020-01-28 13:15:10 by indorgro

Update and rename README.md to 2020 - The Dawn of A New Era

My story starts in 2012 when my Dad had a neck (postural) surgery from the C3-C7 using titanium rods, plates, and screws. After being discharged from the hospital he was prescribed opioid medication to manage the pain. In February 2013, my Dad needed a revision surgery on his neck due to complications and continued to use opioid medication to manage his chronic pain. While he was recovering, he tore his meniscus in his left knee and required surgery in May 2013. In 2015, my Dad had an assessment at Allevio Pain Management Clinic with Dr. Kevin Smith and was introduced to the ketamine-lidocaine infusion. Eight months later, my Dad was no longer taking opioids as the pain was managed through this treatment every two months. My Dad continues to receive this treatment with Dr. Kevin Smith at the Silver Medical Group today and remains off opioids. In December 2016, my Dad had a lumber surgery and ten days later he had developed severe head pain in the postural position. A spinal fluid leak (CFS leak) was detected and required another surgery to repair the leak in January 2017. Once again opioids were the only medication prescribed to manage his chronic pain. In April 2017 my Dad was on his way to work when he was rear-ended by an SUV on highway 400. He was diagnosed with a spinal cord concussion which led to PTSD, anxiety, and depression, in addition to his chronic pain.


Medical cannabis has significantly improved my Dadâ€™s quality of life and, as a result, the rest of my familyâ€™s quality of life. An opportunity presented itself to transition into this emerging industry by helping my Dad as a registered patient under the Access to Cannabis for Medical Purposes Regulations (ACMPR) to understand the legal framework. The journey was not easy and disappointment was at every corner. Nonetheless, we did not let regulatory issues, further delays in production, people and money stop us. We remained focused and sought after a partner that operates for the local medical community and is backed up by science. It is important for us to work with a medical Licensed Producer that is committed to advancing science and research through clinical studies that the industry requires to reach its full potential. While we were interested in Green Reliefâ€™s philosophy, we knew we had to tour the facility to meet the team and see for ourselves if we had similar synergies. 


I was walking through the facility and my expectations were exceeded. In my opinion, Green Relief truly does represent the gold standard for medical cannabis. As a Canadian, I feel pride witnessing what I thought was the most incredible state-of-the-art facility and one of its kind, right here in Ontario. We stand by Green Relief because of the biotechnological research that they undertake. Their medical patient-focussed program involves more time spent with patients providing them with the support and care they need, as well as constantly improving their medicine. The CEO, Dr. Neilank K. Jha is a Neurosurgeon and International Leading Expert in Traumatic Brain Injury.[1] Dr. Jha is a major reason for which we are motivated to move forward with Green Relief as he would be perfect to collaborate with on the issue of chronic pain. In addition to hearing that Dr. Jha has trained and worked with my Dadâ€™s neurosurgeon, Dr. Michael Fehlings at Toronto Western Hospital.[2] I was pleasantly surprised to hear the neurosurgeons history and I became incredibly excited as their background is critical for breakthrough medicine for those with chronic pain as a result of spinal surgery. The technology Green Relief is using to extract cannabinoid compounds is extraordinary and currently are creating the purest quality of CBD in the market today.

The high level of organization and cleanliness was impeccable as we were walking through the corridors and into each room. Green Relief exercises the highest standards of hygiene and follows Health Canadaâ€™s guidelines so much so that Health Canada utilizes Green Reliefâ€™s facility for their own training purposes. I strongly believe in a brand that fulfills its social responsibly and for Green Relief, they are donating the fish used in their aquaponic process to a local shelter each month to feed those in need, in addition to exploring ways to integrate job positions for Canadians through the Ontario Disability Employment Network.[3]

I was fascinated to learn about Green Reliefâ€™s aquaponic process for medical cannabis and I have to admit, I was quite skeptical about how the system actually operated. Aquaponics blend aquaculture and hydroponics together and is the most sustainable form of agriculture: it uses 90% less water than conventional farming and highly efficient for developing effective medicine.[4] The future for sustainable agriculture is through Green Reliefâ€™s highly balanced ecosystem and its innovative technology. Together it combines to create a remarkable pillar that will help propel the medical cannabis industry. I would like to give a special thank you to Steve LeBlanc who we toured the facility with and the team at Green Relief for their hospitality.

[1] https://www.greenrelief.ca/team/

[2] https://www.uhn.ca/PatientsFamilies/Search_Doctors/Pages/doctor_detail.aspx?doctor=361

[3] https://www.greenrelief.ca/

[4] https://www.greenrelief.ca/

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[2d4a532522...](https://github.com/mrakgr/The-Spiral-Language/commit/2d4a5325220e17990d590d318ccd879346928b17)
#### Tuesday 2020-01-28 13:42:28 by Marko GrdiniÄ‡

"1:15pm. https://youtu.be/WStnSDZ3ygA?t=1245
> Terry: My own group over the last few years has put some effort into being able to do stochastic gradient descent which is the algorithm that is used for most deep learning networks, for being able to adjust the weights, but to do that with spiking units rather than graded units and we had some success. This is Ben Huff who is a former graduate student of mine who is now working at IBM, but that looked very promising in a sense that we are able to generalize the algorithm that it would be able to work near threshold. Neurons have these very sharp thresholds - if you blow it nothing happens and as soon as you go above you get a spike, all-or-none spike, but we shown that if you have the blurry region - a window in between those two states that you could use it in an analog way in order to be able to compute the gradients. So that was very exciting, to have that. It was a NeurIPS paper a few years ago.

https://youtu.be/WStnSDZ3ygA?t=2119
> Terry: And we will know very soon, sometime next year (the video is Dec 2019), Clay Reed who is at the Alan Institute for Brain Science is going to announce a connectomic reconstruction. This is a tour the force, this is something that requires a petabyte of data to be stitched together, but we will able to get a wiring diagram for a cubic millimeter of the cortex.

https://youtu.be/WStnSDZ3ygA?t=2162
> Terry: What makes the brain powerful is that is not the case like in deep learning networks where the units are more or less the same just with different parameters - but more or less the same input and output and so forth. But there is 100s and 1000s of different types of neurons, specialized neurons. Just for the inhibitory neurons there is about 20 different types that have separate functions and are integrated into a circuit with great precision. And so natures has had much more time to evolve and optimize a circuit for a particular function that is has, and that's different parts of the brain. The cortex has a different circuit from, for example, another visual area called the superic culicula which is used in lower invertebrates for their visual system. And there are 100s of brain areas each of which has different type of neuron and type of circuit. So this is a very highly evolved system.

> Terry: Right now we are at the very beginning. I think you could compare where we are right now in deep learning networks where the Wright brothers were back in 1903 when they had their first flight. Right, it was a proof of principle, but was far from where we ended up today.

What he is saying next made me pause. Let me transcribe it.

https://youtu.be/WStnSDZ3ygA?t=2243
> Sam: There are a couple of papers that I wanted to make sure we covered. One of those was 'A simple framework for constructing functional spiking RNNs'. I feel like we talked about some of that stuff. Did we talk about this paper, the key results of this paper? Was it one of the one of the ones that you've mentioned.

> Terry: That is a foundational paper for this new method of replacing graded units with spiking units. We went through millions and millions of simulations in order to pin down the robustness and how many networks when you put them together will work as advertised and that turns out to be a very high percentage. But also varying a lot of the parameters - this is something you do when you get something. You have to prove that it actually works. But now what we've done - this is the paper that we are just preparing, is to apply that to much more complex problems and compare it to the recording of neurons in different parts of the cortex. And what we are finding is not just in our lab, but in many other place in the world, like Jim Decarlu in MIT, is that when you train up one of these networks - either deep learning for the visual system or recurrent networks which is what we are studying here which is relevant for the prefrontal cortex. What you find is that the properties of the units in these networks are phenomenally similar to the recordings made of monkeys and other species. That are solving specific problems: visual recognition problems or memory problems.

> Terry: So this is very exciting. It means that there is this very close convergence that is occurring between neuroscience on one hand, and machine learning and deep learning on the other hand which I think is going to be very...the interaction, the synergies, the fantastic opportunities that are opening up now for the first time now in AI that are going to be sending information and talent back and forth.

1:55pm. I suppose I could paste the above tidbits on the ML sub.

Let me do it.

2:40pm. https://www.reddit.com/r/MachineLearning/comments/ev5p1m/d_spiking_neural_networks_a_primer_with_dr/

Here it is.

https://www.youtube.com/watch?v=jhQgElvtb1s
Mike Davies: Realizing the Promise of Spiking Neuromorphic Hardware

I want to watch this next.

Let me do it. Tonight was hell, so I do not feel like programming today. Maybe learning more about SNNs will motivate me."

---
## [KDE/krita](https://github.com/KDE/krita)@[2e8294becc...](https://github.com/KDE/krita/commit/2e8294becc66a548321d9274dfe4109963ea6577)
#### Tuesday 2020-01-28 13:56:40 by Dmitry Kazakov

Fix rendering of vector masks applied to group shapes

The patch does multiple things:

1) Refactors KoShapeManager::paint() to use KisForest implementation
   to render shapes hierarchy. It is needed because we need to be able
   only a part of the shapes, and their parents might have filtering or
   masks. Theoretically, it is possible to implement this algorithm
   without KisForest, but it this case populateRenderSubtree(),
   buildRenderTree() and renderShapes() will be merged into a single
   function (or some counter-intuitive templated strategy), which would
   make the code hard to debug and maintain.

2) KoViewConverter is gone! Gone! You hear it?! It is gone!!! No more
   KoViewConverter in any rendering! :)
   Well, it is still used in the tools, but we can live with it for now.
   The main point, there is no ugly hacks in KoShape::absoluteTransformation()
   to correct shape's offset to let view scaling transform be applied
   *after* the shape. Now view tranform in applied *before* the shapes,
   right in KisShapeLayerCanvas (or the like).

The code touches a lot of code, so some testing is needed.

Things that were deprecated:
 * SVG-filtering rendering code was removed. It used an ugly hack to
   let it work on groups. It couldn't be kept after refactoring. When
   we implement it correctly, it should be just placed into renderShapes().

 * some features of ODF text and raster-based vector patterns might
   have become broken. But we have no way to test that :(

BUG:392242

---
## [mfauzandelfani/proyek_sda](https://github.com/mfauzandelfani/proyek_sda)@[ad9b3f442e...](https://github.com/mfauzandelfani/proyek_sda/commit/ad9b3f442e27ae1463ce0d0dc0f724ac64a36366)
#### Tuesday 2020-01-28 14:42:57 by M. Fauzan Delfani

Add files via upload

ini programnya my friend, love you muaccchh <3

---
## [mavit/slimserver](https://github.com/mavit/slimserver)@[43bc2ab494...](https://github.com/mavit/slimserver/commit/43bc2ab494b1c2205cbde7d43b4c7e7c10f7839b)
#### Tuesday 2020-01-28 15:12:06 by Michael Herger

Logitech Media Server 8.0.0!

I certainly hope this release will deserve the new version number. Because in Logitech Media Server's history v8 was supposed to be something big. After SqueezeCenter 7 was released, there was an increasing number of users asking for more flexibility navigating their music. In particular Classical Music lovers seemed to have quite different approaches to organizing music. We promised more flexibility for them in SqueezeCenter 8 (or whatever the name of the day back then was supposed to be).

One guy in the team was tasked to create a new database schema and whatever should come with it. He did work hard. But before he was done priorities got shifted. A media server was strapped on to our beloved music server. But despite the major changes LMS 7 has seen over the years, we never dared to bump the version to 8. Because 8 was supposed to provide that new flexibility we still hadn't fully figured out how to do...

Time flew by. Squeezebox was killed (but remains undead). Online streaming services grew more and more important. I started to pick up an idea we had early on already: "universal" or "global" search across all of a user's music. No matter whether it's online or on his disk. Every now and then I'd give it a try, failing quickly and giving up over and over again. At some point it reminded me of our failed attempts at giving more browsing flexibility. And I decided to make the step to version 8 if ever I managed to do the improved online music integration.

Around Christmas 2019 I gave it another try. An approach I thought I had explored before. But suddenly it seemed to be working. Within a few days I had my Spotify album collection integrated with my local music collection. Quick search across all albums, library views etc. A few more sleepless nights later I now feel confident that I have something working. At least working for me, that is :-D. That's what I decided to branch off Logitech Media Server 8.0 today. I doubt it'll be ground breaking. But anyway. Here we go.

---
## [newstools/2020-naija-news-agency](https://github.com/newstools/2020-naija-news-agency)@[2251d07ea8...](https://github.com/newstools/2020-naija-news-agency/commit/2251d07ea8f4f4fba5cadbba01d0b1f4879156b9)
#### Tuesday 2020-01-28 15:21:03 by NewsTools

Created Text For URL [naijanewsagency.com/chris-brown-confuses-fans-after-revealing-he-is-still-in-love-with-one-of-his-ex-girlfriends/]

---
## [newstools/2020-the-guardian-uk](https://github.com/newstools/2020-the-guardian-uk)@[72b32a8a16...](https://github.com/newstools/2020-the-guardian-uk/commit/72b32a8a16bf1f260ecc247b1e5e1d2df3adea55)
#### Tuesday 2020-01-28 17:49:56 by NewsTools

Created Text For URL [www.theguardian.com/lifeandstyle/2020/jan/28/i-love-my-boyfriend-so-why-do-i-fantasise-about-my-workmate]

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[9e6dc8683b...](https://github.com/mrakgr/The-Spiral-Language/commit/9e6dc8683bbde644f2a66bf47ea412ad510f8812)
#### Tuesday 2020-01-28 18:29:24 by Marko GrdiniÄ‡

"2:45pm. https://youtu.be/jhQgElvtb1s?t=240

What he is saying here is interesting. I do not really understand why SNNs would be more general than ANNs.

> Mike: There is a lot of confusion for those who do not know much about the neuromorphic space. An assumption that what we are trying to do is build better deep learning accelerator chips or something like that, and that's really not the primary point of our research. It is an understandable point of confusion because the spiking NN model is in many ways a generalization of what an ANN does therefore you can generally map standard NNs into a spiking algorithm, but that is not necessarily the best place how to use spiking NNs. And there is all kinds of other properties, it is not all about the spikes by any means, there are many different architectural principles that are related here.

3pm. https://youtu.be/jhQgElvtb1s?t=719

He says that if I am interested I should mail him. Well, this is probably meant for actual academics, but after Spiral is good enough and there aren't any toys out there that I can get my hands on I'll consider giving it a try.

https://youtu.be/jhQgElvtb1s?t=760
The Challenge: SNN algorithm discovery.

Yeah, this is what I want. I find backpropagation just so oppressive. I really want to know what the alternatives to it would be.

https://youtu.be/jhQgElvtb1s?t=888
New ideas guided by neuroscience

This stuff is new to me.

https://youtu.be/jhQgElvtb1s?t=1304
Loihi can be used to do path planning

https://youtu.be/jhQgElvtb1s?t=1399
Loihi can be used to do constraint solving.

Everything is batch size 1 which is what I need for RL.

3:50pm. "If you are into this, let me also recommend: [Mike Davies: Realizing the Promise of Spiking Neuromorphic Hardware](https://www.youtube.com/watch?v=jhQgElvtb1s). This is a presentation on Loihi, the neuromorphic chip at development at Intel.

At the start of the video he mentions there being some [controversy](https://www.zdnet.com/article/intels-neuro-guru-slams-deep-learning-its-not-actually-learning/) regarding neuromorphic chips, and here he gives some justification for their development. I think he does a very good job hyping it.

He makes the claim that neuromorphic chips being DL accelerators is a misconception and that spiking NNs generalize ANNs. He then elaborates that through several examples as implemented on the Loihi. It can be used to do things like path planning and solving constraint satisfaction problems which are important for biological brains.

The main advantage of Loihi besides its energy efficiency is that unlike GPUs and CPUs it works great with batch size of 1 as it is asynchronous and latency focused. Personally, when I tried online RL on the GPU I found them to be a massive disappointment as without batching the benefits of parallelism could not be manifested.

He states that using backprop with SNNs is low hanging fruit, and the main challenge going forward is moving beyond that towards more neuroscience inspired approaches."

Let me post this as a comment.

3:50pm. https://www.youtube.com/watch?v=_Xcdm6alfCY
Spiking Neural Networks Deep Dive

https://www.youtube.com/watch?v=4lY-oAY0aQU
MIT 6.S191: Biologically Inspired Neural Networks (IBM)

Let me stop here for a bit so I can do the chores.

4:25pm. I am back.

I've been thinking. I've made up my mind - Mike Davies is essentially right about everything. Loihi (or something like it) is what I should be angling Spiral for.

4:30pm. 2022 is only two years away. I am seriously going to aim that Spiral v0.2 will have a neuromorphic backend rather than a CPU one.

In fact, since after I am done with v0.2 I won't really be able to upgrade the rig, I am going to have to look for a job and hope that Spiral looks good enough as the sole item on my resume. Rather than beat around the bush, I should just email the guy directly and ask him if they are interested in hiring a PL guy...well, remotely anyway.

4:35pm. I don't think backprop is not learning, but I certainly hate it at this point so I am definitely on the same page as Mike now.

4:40pm. Every long effort needs a certain hope. In 2018 that was that I could find some way of making backprop work well enough to create a stable RL agent.

Loihi might as well be my future. Algorithms are necessary of course, but hardware is the backbone of ML.

Despite the math talent in the world, literally nobody in the 90s could have foreseen deep learning. Proof is in having a program and running it.

4:50pm. Neuromorphic hardware was a far off future in 2015, but is near at hand now.

Forget all of that nonsense about reversibility and using evolutionary algos, and aim for this one thing. Literally nobody is smart enough to design ML algorithms from the ground up. The Inspired will be able to do it, but I am really far below their level of capability.

I am not alone. Intelligence is in my brain. Intelligence is everywhere in nature. It just needs to be minded and refined into something usable. Neuroscience has been useless so far, but it will get its time to shine. They will get their chance to earn their paycheck.

4:55pm. I can make Spiral, but I can't imagine myself doing any significant algorithmic breakthroughs in the ML space.

...Ah, shit. I really will have to get a job. Well whatever. I'll thinking about that when it comes to that.

5pm. Let me finish today off with one more video.

https://www.youtube.com/watch?v=4lY-oAY0aQU
MIT 6.S191: Biologically Inspired Neural Networks (IBM)

Let me go with this.

...No that is not on neuromorphic chips. Ok, let me go for the other one. It is nearly 2h though.

5:20pm. Went through the slides. It is just really basic and long. Skip.

https://www.youtube.com/watch?v=gX9NqDuwTnA
Neuromorphic computing with emerging memory devices

Let me give this a chance.

I do not care if SNNs do not work as ANNs. I just want to run away from ANNs. The desire to create new algorithms and my inability to do so is what is giving me nightmares. It would really be far more productive to focus my efforts where I can make the most impact.

7:05pm. Done with lunch. Let me close for the day here. Tomorrow I will get on those join points.

I guess when I finish the language I can try Deep CFR on the CPU as an exercise, but otherwise my aim will be to break into the neuro programming field. Forget evolution or anything like that. I need firm principles in order to do proper engineering. Without principles I might as well be guessing.

I am going to track down some of the reference that Davies mentioned on mathematical formalization of some of the things they are doing.

I need to gather my resolve. I need a firm vision in order to have a belief in the future.

There is just one way to do this, it is to have patience. There isn't a genius who can figure out learning. The community as a whole will be one to make the hardware and to infer the processes of the brain's functioning."

---
## [Promethaes/CappuccinoEngine](https://github.com/Promethaes/CappuccinoEngine)@[c2e1cdca41...](https://github.com/Promethaes/CappuccinoEngine/commit/c2e1cdca41d09337957f74dd0678e7858d54c329)
#### Tuesday 2020-01-28 20:13:58 by Daniel Presas

[Fix] Animation constructor can take in initializer lists

Please
for the love of God
use const if you're gonna use a reference
unless you specifically plan to change the parameter being passed in
ffs

---
## [tgstation/tgstation](https://github.com/tgstation/tgstation)@[7f081640a3...](https://github.com/tgstation/tgstation/commit/7f081640a34a80035cb612319126585caee31cdc)
#### Tuesday 2020-01-28 22:45:24 by OnlineGirlfriend

[READY] Adds pineapple juice, creme de coconut, Pina Colada; adds Painkiller drink; adjusts Bahama Mama and pineapple snowcone (#48783)

This adds two drink ingredients: creme de coconut (a coconut liqueur obtainable in the booze dispenser) and pineapple juice (obtainable in the soda dispenser or via juicing pineapples). It further adds the Pina Colada and Painkiller cocktails & corresponding sprites, and it adjusts the recipe for Bahama Mama. The recipe for pineapple snowcones was also adjusted to require pineapple juice rather than pineapple slices.

Finally, pineapple juice can be found in a carton or craftable juice box, and I've added sprites for this.
Why It's Good For The Game

First off, more drink ingredients equal more realistic drink mixes and more variety, which means more fun for bartenders.

Pineapple snowcones are better off using pineapple juice as opposed to pineapple slices. Previously the recipe called for two pineapple slices; you can now juice those two pineapple slices to produce pineapple juice sufficient for the snowcone.

Regarding the changes to the Bahama Mama recipe: With these changes, the recipe now more closely represents the classic Bahama Mama, which involves coconut, coffee, rum, and pineapple juice as its base ingredients. I removed orange juice to prevent conflict with the Painkiller recipe, but also feel justified in doing so because orange juice is not an essential ingredient of a Bahama Mama, nor is lime.
Changelog

ðŸ†‘
add: pineapple juice, creme de coconut
add: Pina Colada cocktail
add: Painkiller cocktail
add: pineapples and pineapple slices can be juiced
tweak: Bahama Mama recipe, pineapple snowcone recipe
imageadd: pineapple juicebox sprite, pineapple juice carton sprite, Painkiller sprite, Pina Colada sprite
/ðŸ†‘

---
## [snsvrno/breacherbros](https://github.com/snsvrno/breacherbros)@[18edcb3a7f...](https://github.com/snsvrno/breacherbros/commit/18edcb3a7f157d9fd7c602fe2e94be455168d322)
#### Tuesday 2020-01-28 23:39:24 by snsvrno

prototype(spline-lib): spine accomplished

wanted to get a sample loaded, and i did, but OMG it was a pain the ass.
need to probably rewrite the 'runtime' since it was horrible to use and
seems to not be rewritten in good lua. not sure how solid this would be
when using a complex game with complex sprites...

looks like a job for another prototype!

---
## [iaristotleg18/WhoWouldWinSuperAlpha](https://github.com/iaristotleg18/WhoWouldWinSuperAlpha)@[33f36adc84...](https://github.com/iaristotleg18/WhoWouldWinSuperAlpha/commit/33f36adc84747621bffc25055e044a956b525db5)
#### Tuesday 2020-01-28 23:48:55 by Isaiah Glick

made the world real. mitosis sucks. god help me. we also have informed consent which is even stupider. life is stupid. stupid stupid stupid stupid stupid stupid stupid stupid stupid.

---

# [<](2020-01-27.md) 2020-01-28 [>](2020-01-29.md)

