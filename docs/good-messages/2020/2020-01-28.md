# [<](2020-01-27.md) 2020-01-28 [>](2020-01-29.md)

2,195,273 events, 1,063,163 push events, 1,688,458 commit messages, 135,096,191 characters


## [Grickl3/sqglz@d9392fc359...](https://github.com/Grickl3/sqglz/commit/d9392fc3591c5f213108bad305c94310b5d22e09)
##### 2020-01-28 01:20:02 by DeaconRodda

haven't been in for a while.In the following session, I intend to fuck not around, to get the next iteration of this site the fuck up, and to gain some goddamn momentum in my motherfucking business and my whole goddamn life.

---
## [ted-dokos/inequality-bar@ad4328c6d6...](https://github.com/ted-dokos/inequality-bar/commit/ad4328c6d66e5cdfc7221180ec92ad5b6c4006e0)
##### 2020-01-28 02:41:02 by tdokos

Do the refactor I promised: now the main svg hierarchy looks like

  svg -> g.country -> g.bar -> rect, text.barSize

The code is enjoyably clean, as well. To make my life easier, the percentile
objects now contain their country string (makes it easy to omit text for the
'percentBar-...' type 'country').

I spent a lot of time struggling to make the new data join work, and uncovered a
critical gap in my knowledge of d3. Specifically (and this is pretty obvious in
hindsight), determining which data is new and which is old is not an automagic
process. You need to tell d3 how to compare objects with the optional key
argument to .data(). In my case, the really important parts of the percentile
object are the size and positioning aspects, so that's what I keyed on.

In the process of making this change, I nearly submitted broken code. It's
become very clear that I need to get my work under test. I think the correct
plan of action is adding the slider and then shifting focus to tests.

---
## [freebsd/freebsd@db5ca2d2bf...](https://github.com/freebsd/freebsd/commit/db5ca2d2bf12a80629c4a8cedc188f55ab6a20e7)
##### 2020-01-28 03:01:49 by kevans

MFC r357103-r357104: unbreak local.lua, add a modules.loaded hook

r357103:
loader.lua: re-arrange to load local.lua *after* config loading

The major problem with the current ordering is that loader.conf may contain
all of the magic we need to actually setup the console, so loading local.lua
prior to that can make it excessively difficult and annoying to debug
(whoops, sorry Ravi & Warner).

The new ordering has some implications, but I suspect they are a non-issue.
The first is that it's no longer possible for the local module to inject any
logic prior to loading config -- I suspect no one has relied on this. The
second implication is that the config.loaded hook is now useless, as the
local module will always be included after that hook would have fired.

For config.loaded, I will opt to leave it in, just in case we add an early
point for local lua to get injected or in case one wants to schedule some
deferred logic in a custom loader.lua. The overhead of having it if no hooks
will be invoked is relatively minimal.

r357104:
lua: add modules.loaded hook

This may be used for the local module to hook in and load any additional
modules that it wants, since it can't modify the modules table internal to
config. We may consider adding API to do so at a later time, but I suspect
it will be more complicated to use with little return.

status is captured but ignored for the purpose of loading the hook. status
will be false if *any* module failed to load, but we typically don't let
that halt the boot so there's no reason to let it halt hooks. Some vendors
or setups may have expected fails that would be actively thwarted by
checking it.

We may, at a later date, consider adding an API for letting non-config
modules check which modules have successfully (or not) loaded in case an
unexpected failure *should* halt whatever they are doing.

---
## [MannesdeGroot/Periode3@685618766c...](https://github.com/MannesdeGroot/Periode3/commit/685618766c019ab9204e8030088aa499f133533b)
##### 2020-01-28 09:36:55 by MannesdeGroot

Anime/Gaming subs are an endless source of good Copypasta Wait people aren't actually pissed off at this are they? Okay a couple of points here to make a lot of you look like idiots.  A fetish does not make you a creep. For instance, people can be turned on by incest porn, but not want to fuck their own sister. Furries don't often want to fuck animals. And PLENTY of people like porn that involves cheating or rape, but would NEVER do those. Just so, loli fans aren't just a bunch of pedos (though statistically maybe a few probably are, but you could say that about any fetish). For instance, I'm a loli fan myself, but I have an 8 year old daughter who regularly brings friends over, and not ONCE (I swear) have I seen a single one of them in a sexual light. I'm attracted to the cute designs and innocent attitudes of lolis, that doesn't mean I'm gonna go fuck children, that's obviously fucking disgusting. Not to mention, most lolis are legal, it's the design, NOT the age that makes a loli.  Anime style video games, and games from Japan always have the "cute" option when picking love interests. Usually some young, innocent girl with a shy attitude. And yes, I'm well aware Futaba kinda falls in that category too, but she can also fall under the "geeky girl" concept, which is also typically a separate option from the "cute" one. And since the twins are explained multiple times to be much older, that really does just make them cute, and nothing more. It's a very normal thing for a game like this to make them dateable.  THEY'RE FUCKING VIRTUAL. At the end of the day, if my first two points didn't make you feel stupid, this one should. If you can't accept that people can have a fetish without becoming a creep, or that these are normal, long time tropes in Japanese games, then at least accept the fact that there's no point attacking people for wanting VIRTUAL CHARACTERS to be dateable. After all, who the hell is that hurting? Sure as hell not any kids, that's for sure. So why even bother getting pissy? What, cuz YOU don't wanna date em? Then don't! Simple as that!

---
## [fish2000/CLU@e64bd9e707...](https://github.com/fish2000/CLU/commit/e64bd9e707bd87f0cbc1de5da8f98c98bbb9f606)
##### 2020-01-28 12:21:07 by Alexander Böhn

Bringing all treatments of “__qualname__” values up to snuff
... as in, no longer do we just alias it to “__name__” when we’re
    renaming or resetting things – we specifically seek to preserve
    the parts of “__qualname__” dotpath-ish strings with namespace-
    specific information, while altering only the sections relevant
    to whatever it is we are doing (i.e. renaming, or whatever).
... likewise, in functions like “determine_name(…)”, in the event
    (however unlikely) that the code branches to the consideration
    of a “__qualname__” value, we deterministically cleave off only
    the bit we care about using “str.rpartition(…)” and slicing
... there are one or two extremely corner-iffic cases where some
    still-existing “__qualname__” negligence could possibly, under
    thoroughly bizarre and freakish circumstances, transpire – but
    I have marked them shits as TODO and will assuredly find time
    to procrastinate from whatever I should actually be doing in
    the near-to-middling future and address these, toot sweet.
... in conclusion: thank you for using CLU, doggie, yeah!~

---
## [indorgro/Janne-Rubin@cf8f7dd1ab...](https://github.com/indorgro/Janne-Rubin/commit/cf8f7dd1aba90269cb4c03d5802f1fb5352752b5)
##### 2020-01-28 13:15:10 by indorgro

Update and rename README.md to 2020 - The Dawn of A New Era

My story starts in 2012 when my Dad had a neck (postural) surgery from the C3-C7 using titanium rods, plates, and screws. After being discharged from the hospital he was prescribed opioid medication to manage the pain. In February 2013, my Dad needed a revision surgery on his neck due to complications and continued to use opioid medication to manage his chronic pain. While he was recovering, he tore his meniscus in his left knee and required surgery in May 2013. In 2015, my Dad had an assessment at Allevio Pain Management Clinic with Dr. Kevin Smith and was introduced to the ketamine-lidocaine infusion. Eight months later, my Dad was no longer taking opioids as the pain was managed through this treatment every two months. My Dad continues to receive this treatment with Dr. Kevin Smith at the Silver Medical Group today and remains off opioids. In December 2016, my Dad had a lumber surgery and ten days later he had developed severe head pain in the postural position. A spinal fluid leak (CFS leak) was detected and required another surgery to repair the leak in January 2017. Once again opioids were the only medication prescribed to manage his chronic pain. In April 2017 my Dad was on his way to work when he was rear-ended by an SUV on highway 400. He was diagnosed with a spinal cord concussion which led to PTSD, anxiety, and depression, in addition to his chronic pain.


Medical cannabis has significantly improved my Dad’s quality of life and, as a result, the rest of my family’s quality of life. An opportunity presented itself to transition into this emerging industry by helping my Dad as a registered patient under the Access to Cannabis for Medical Purposes Regulations (ACMPR) to understand the legal framework. The journey was not easy and disappointment was at every corner. Nonetheless, we did not let regulatory issues, further delays in production, people and money stop us. We remained focused and sought after a partner that operates for the local medical community and is backed up by science. It is important for us to work with a medical Licensed Producer that is committed to advancing science and research through clinical studies that the industry requires to reach its full potential. While we were interested in Green Relief’s philosophy, we knew we had to tour the facility to meet the team and see for ourselves if we had similar synergies. 


I was walking through the facility and my expectations were exceeded. In my opinion, Green Relief truly does represent the gold standard for medical cannabis. As a Canadian, I feel pride witnessing what I thought was the most incredible state-of-the-art facility and one of its kind, right here in Ontario. We stand by Green Relief because of the biotechnological research that they undertake. Their medical patient-focussed program involves more time spent with patients providing them with the support and care they need, as well as constantly improving their medicine. The CEO, Dr. Neilank K. Jha is a Neurosurgeon and International Leading Expert in Traumatic Brain Injury.[1] Dr. Jha is a major reason for which we are motivated to move forward with Green Relief as he would be perfect to collaborate with on the issue of chronic pain. In addition to hearing that Dr. Jha has trained and worked with my Dad’s neurosurgeon, Dr. Michael Fehlings at Toronto Western Hospital.[2] I was pleasantly surprised to hear the neurosurgeons history and I became incredibly excited as their background is critical for breakthrough medicine for those with chronic pain as a result of spinal surgery. The technology Green Relief is using to extract cannabinoid compounds is extraordinary and currently are creating the purest quality of CBD in the market today.

The high level of organization and cleanliness was impeccable as we were walking through the corridors and into each room. Green Relief exercises the highest standards of hygiene and follows Health Canada’s guidelines so much so that Health Canada utilizes Green Relief’s facility for their own training purposes. I strongly believe in a brand that fulfills its social responsibly and for Green Relief, they are donating the fish used in their aquaponic process to a local shelter each month to feed those in need, in addition to exploring ways to integrate job positions for Canadians through the Ontario Disability Employment Network.[3]

I was fascinated to learn about Green Relief’s aquaponic process for medical cannabis and I have to admit, I was quite skeptical about how the system actually operated. Aquaponics blend aquaculture and hydroponics together and is the most sustainable form of agriculture: it uses 90% less water than conventional farming and highly efficient for developing effective medicine.[4] The future for sustainable agriculture is through Green Relief’s highly balanced ecosystem and its innovative technology. Together it combines to create a remarkable pillar that will help propel the medical cannabis industry. I would like to give a special thank you to Steve LeBlanc who we toured the facility with and the team at Green Relief for their hospitality.

[1] https://www.greenrelief.ca/team/

[2] https://www.uhn.ca/PatientsFamilies/Search_Doctors/Pages/doctor_detail.aspx?doctor=361

[3] https://www.greenrelief.ca/

[4] https://www.greenrelief.ca/

---
## [mrakgr/The-Spiral-Language@2d4a532522...](https://github.com/mrakgr/The-Spiral-Language/commit/2d4a5325220e17990d590d318ccd879346928b17)
##### 2020-01-28 13:42:28 by Marko Grdinić

"1:15pm. https://youtu.be/WStnSDZ3ygA?t=1245
> Terry: My own group over the last few years has put some effort into being able to do stochastic gradient descent which is the algorithm that is used for most deep learning networks, for being able to adjust the weights, but to do that with spiking units rather than graded units and we had some success. This is Ben Huff who is a former graduate student of mine who is now working at IBM, but that looked very promising in a sense that we are able to generalize the algorithm that it would be able to work near threshold. Neurons have these very sharp thresholds - if you blow it nothing happens and as soon as you go above you get a spike, all-or-none spike, but we shown that if you have the blurry region - a window in between those two states that you could use it in an analog way in order to be able to compute the gradients. So that was very exciting, to have that. It was a NeurIPS paper a few years ago.

https://youtu.be/WStnSDZ3ygA?t=2119
> Terry: And we will know very soon, sometime next year (the video is Dec 2019), Clay Reed who is at the Alan Institute for Brain Science is going to announce a connectomic reconstruction. This is a tour the force, this is something that requires a petabyte of data to be stitched together, but we will able to get a wiring diagram for a cubic millimeter of the cortex.

https://youtu.be/WStnSDZ3ygA?t=2162
> Terry: What makes the brain powerful is that is not the case like in deep learning networks where the units are more or less the same just with different parameters - but more or less the same input and output and so forth. But there is 100s and 1000s of different types of neurons, specialized neurons. Just for the inhibitory neurons there is about 20 different types that have separate functions and are integrated into a circuit with great precision. And so natures has had much more time to evolve and optimize a circuit for a particular function that is has, and that's different parts of the brain. The cortex has a different circuit from, for example, another visual area called the superic culicula which is used in lower invertebrates for their visual system. And there are 100s of brain areas each of which has different type of neuron and type of circuit. So this is a very highly evolved system.

> Terry: Right now we are at the very beginning. I think you could compare where we are right now in deep learning networks where the Wright brothers were back in 1903 when they had their first flight. Right, it was a proof of principle, but was far from where we ended up today.

What he is saying next made me pause. Let me transcribe it.

https://youtu.be/WStnSDZ3ygA?t=2243
> Sam: There are a couple of papers that I wanted to make sure we covered. One of those was 'A simple framework for constructing functional spiking RNNs'. I feel like we talked about some of that stuff. Did we talk about this paper, the key results of this paper? Was it one of the one of the ones that you've mentioned.

> Terry: That is a foundational paper for this new method of replacing graded units with spiking units. We went through millions and millions of simulations in order to pin down the robustness and how many networks when you put them together will work as advertised and that turns out to be a very high percentage. But also varying a lot of the parameters - this is something you do when you get something. You have to prove that it actually works. But now what we've done - this is the paper that we are just preparing, is to apply that to much more complex problems and compare it to the recording of neurons in different parts of the cortex. And what we are finding is not just in our lab, but in many other place in the world, like Jim Decarlu in MIT, is that when you train up one of these networks - either deep learning for the visual system or recurrent networks which is what we are studying here which is relevant for the prefrontal cortex. What you find is that the properties of the units in these networks are phenomenally similar to the recordings made of monkeys and other species. That are solving specific problems: visual recognition problems or memory problems.

> Terry: So this is very exciting. It means that there is this very close convergence that is occurring between neuroscience on one hand, and machine learning and deep learning on the other hand which I think is going to be very...the interaction, the synergies, the fantastic opportunities that are opening up now for the first time now in AI that are going to be sending information and talent back and forth.

1:55pm. I suppose I could paste the above tidbits on the ML sub.

Let me do it.

2:40pm. https://www.reddit.com/r/MachineLearning/comments/ev5p1m/d_spiking_neural_networks_a_primer_with_dr/

Here it is.

https://www.youtube.com/watch?v=jhQgElvtb1s
Mike Davies: Realizing the Promise of Spiking Neuromorphic Hardware

I want to watch this next.

Let me do it. Tonight was hell, so I do not feel like programming today. Maybe learning more about SNNs will motivate me."

---
## [thirdzcee/kernel_xiaomi_jasmine_sprout@7076f002ef...](https://github.com/thirdzcee/kernel_xiaomi_jasmine_sprout/commit/7076f002ef9cd3a86378a25379bb57eb9fcb0156)
##### 2020-01-28 14:17:03 by George Spelvin

lib/sort: make swap functions more generic

Patch series "lib/sort & lib/list_sort: faster and smaller", v2.

Because CONFIG_RETPOLINE has made indirect calls much more expensive, I
thought I'd try to reduce the number made by the library sort functions.

The first three patches apply to lib/sort.c.

Patch #1 is a simple optimization.  The built-in swap has special cases
for aligned 4- and 8-byte objects.  But those are almost never used;
most calls to sort() work on larger structures, which fall back to the
byte-at-a-time loop.  This generalizes them to aligned *multiples* of 4
and 8 bytes.  (If nothing else, it saves an awful lot of energy by not
thrashing the store buffers as much.)

Patch #2 grabs a juicy piece of low-hanging fruit.  I agree that nice
simple solid heapsort is preferable to more complex algorithms (sorry,
Andrey), but it's possible to implement heapsort with far fewer
comparisons (50% asymptotically, 25-40% reduction for realistic sizes)
than the way it's been done up to now.  And with some care, the code
ends up smaller, as well.  This is the "big win" patch.

Patch #3 adds the same sort of indirect call bypass that has been added
to the net code of late.  The great majority of the callers use the
builtin swap functions, so replace the indirect call to sort_func with a
(highly preditable) series of if() statements.  Rather surprisingly,
this decreased code size, as the swap functions were inlined and their
prologue & epilogue code eliminated.

lib/list_sort.c is a bit trickier, as merge sort is already close to
optimal, and we don't want to introduce triumphs of theory over
practicality like the Ford-Johnson merge-insertion sort.

Patch #4, without changing the algorithm, chops 32% off the code size
and removes the part[MAX_LIST_LENGTH+1] pointer array (and the
corresponding upper limit on efficiently sortable input size).

Patch #5 improves the algorithm.  The previous code is already optimal
for power-of-two (or slightly smaller) size inputs, but when the input
size is just over a power of 2, there's a very unbalanced final merge.

There are, in the literature, several algorithms which solve this, but
they all depend on the "breadth-first" merge order which was replaced by
commit 835cc0c8477f with a more cache-friendly "depth-first" order.
Some hard thinking came up with a depth-first algorithm which defers
merges as little as possible while avoiding bad merges.  This saves
0.2*n compares, averaged over all sizes.

The code size increase is minimal (64 bytes on x86-64, reducing the net
savings to 26%), but the comments expanded significantly to document the
clever algorithm.

TESTING NOTES: I have some ugly user-space benchmarking code which I
used for testing before moving this code into the kernel.  Shout if you
want a copy.

I'm running this code right now, with CONFIG_TEST_SORT and
CONFIG_TEST_LIST_SORT, but I confess I haven't rebooted since the last
round of minor edits to quell checkpatch.  I figure there will be at
least one round of comments and final testing.

This patch (of 5):

Rather than having special-case swap functions for 4- and 8-byte
objects, special-case aligned multiples of 4 or 8 bytes.  This speeds up
most users of sort() by avoiding fallback to the byte copy loop.

Despite what ca96ab859ab4 ("lib/sort: Add 64 bit swap function") claims,
very few users of sort() sort pointers (or pointer-sized objects); most
sort structures containing at least two words.  (E.g.
drivers/acpi/fan.c:acpi_fan_get_fps() sorts an array of 40-byte struct
acpi_fan_fps.)

The functions also got renamed to reflect the fact that they support
multiple words.  In the great tradition of bikeshedding, the names were
by far the most contentious issue during review of this patch series.

x86-64 code size 872 -> 886 bytes (+14)

With feedback from Andy Shevchenko, Rasmus Villemoes and Geert
Uytterhoeven.

Link: http://lkml.kernel.org/r/f24f932df3a7fa1973c1084154f1cea596bcf341.1552704200.git.lkml@sdf.org
Signed-off-by: George Spelvin <lkml@sdf.org>
Acked-by: Andrey Abramov <st5pub@yandex.ru>
Acked-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Reviewed-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Cc: Geert Uytterhoeven <geert@linux-m68k.org>
Cc: Daniel Wagner <daniel.wagner@siemens.com>
Cc: Don Mullis <don.mullis@gmail.com>
Cc: Dave Chinner <dchinner@redhat.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Dede Dindin Qudsy <xtrymind@gmail.com>

---
## [mavit/slimserver@43bc2ab494...](https://github.com/mavit/slimserver/commit/43bc2ab494b1c2205cbde7d43b4c7e7c10f7839b)
##### 2020-01-28 15:12:06 by Michael Herger

Logitech Media Server 8.0.0!

I certainly hope this release will deserve the new version number. Because in Logitech Media Server's history v8 was supposed to be something big. After SqueezeCenter 7 was released, there was an increasing number of users asking for more flexibility navigating their music. In particular Classical Music lovers seemed to have quite different approaches to organizing music. We promised more flexibility for them in SqueezeCenter 8 (or whatever the name of the day back then was supposed to be).

One guy in the team was tasked to create a new database schema and whatever should come with it. He did work hard. But before he was done priorities got shifted. A media server was strapped on to our beloved music server. But despite the major changes LMS 7 has seen over the years, we never dared to bump the version to 8. Because 8 was supposed to provide that new flexibility we still hadn't fully figured out how to do...

Time flew by. Squeezebox was killed (but remains undead). Online streaming services grew more and more important. I started to pick up an idea we had early on already: "universal" or "global" search across all of a user's music. No matter whether it's online or on his disk. Every now and then I'd give it a try, failing quickly and giving up over and over again. At some point it reminded me of our failed attempts at giving more browsing flexibility. And I decided to make the step to version 8 if ever I managed to do the improved online music integration.

Around Christmas 2019 I gave it another try. An approach I thought I had explored before. But suddenly it seemed to be working. Within a few days I had my Spotify album collection integrated with my local music collection. Quick search across all albums, library views etc. A few more sleepless nights later I now feel confident that I have something working. At least working for me, that is :-D. That's what I decided to branch off Logitech Media Server 8.0 today. I doubt it'll be ground breaking. But anyway. Here we go.

---
## [newstools/2020-naija-news-agency@2251d07ea8...](https://github.com/newstools/2020-naija-news-agency/commit/2251d07ea8f4f4fba5cadbba01d0b1f4879156b9)
##### 2020-01-28 15:21:03 by NewsTools

Created Text For URL [naijanewsagency.com/chris-brown-confuses-fans-after-revealing-he-is-still-in-love-with-one-of-his-ex-girlfriends/]

---
## [newstools/2020-the-guardian-uk@72b32a8a16...](https://github.com/newstools/2020-the-guardian-uk/commit/72b32a8a16bf1f260ecc247b1e5e1d2df3adea55)
##### 2020-01-28 17:49:56 by NewsTools

Created Text For URL [www.theguardian.com/lifeandstyle/2020/jan/28/i-love-my-boyfriend-so-why-do-i-fantasise-about-my-workmate]

---
## [mrakgr/The-Spiral-Language@9e6dc8683b...](https://github.com/mrakgr/The-Spiral-Language/commit/9e6dc8683bbde644f2a66bf47ea412ad510f8812)
##### 2020-01-28 18:29:24 by Marko Grdinić

"2:45pm. https://youtu.be/jhQgElvtb1s?t=240

What he is saying here is interesting. I do not really understand why SNNs would be more general than ANNs.

> Mike: There is a lot of confusion for those who do not know much about the neuromorphic space. An assumption that what we are trying to do is build better deep learning accelerator chips or something like that, and that's really not the primary point of our research. It is an understandable point of confusion because the spiking NN model is in many ways a generalization of what an ANN does therefore you can generally map standard NNs into a spiking algorithm, but that is not necessarily the best place how to use spiking NNs. And there is all kinds of other properties, it is not all about the spikes by any means, there are many different architectural principles that are related here.

3pm. https://youtu.be/jhQgElvtb1s?t=719

He says that if I am interested I should mail him. Well, this is probably meant for actual academics, but after Spiral is good enough and there aren't any toys out there that I can get my hands on I'll consider giving it a try.

https://youtu.be/jhQgElvtb1s?t=760
The Challenge: SNN algorithm discovery.

Yeah, this is what I want. I find backpropagation just so oppressive. I really want to know what the alternatives to it would be.

https://youtu.be/jhQgElvtb1s?t=888
New ideas guided by neuroscience

This stuff is new to me.

https://youtu.be/jhQgElvtb1s?t=1304
Loihi can be used to do path planning

https://youtu.be/jhQgElvtb1s?t=1399
Loihi can be used to do constraint solving.

Everything is batch size 1 which is what I need for RL.

3:50pm. "If you are into this, let me also recommend: [Mike Davies: Realizing the Promise of Spiking Neuromorphic Hardware](https://www.youtube.com/watch?v=jhQgElvtb1s). This is a presentation on Loihi, the neuromorphic chip at development at Intel.

At the start of the video he mentions there being some [controversy](https://www.zdnet.com/article/intels-neuro-guru-slams-deep-learning-its-not-actually-learning/) regarding neuromorphic chips, and here he gives some justification for their development. I think he does a very good job hyping it.

He makes the claim that neuromorphic chips being DL accelerators is a misconception and that spiking NNs generalize ANNs. He then elaborates that through several examples as implemented on the Loihi. It can be used to do things like path planning and solving constraint satisfaction problems which are important for biological brains.

The main advantage of Loihi besides its energy efficiency is that unlike GPUs and CPUs it works great with batch size of 1 as it is asynchronous and latency focused. Personally, when I tried online RL on the GPU I found them to be a massive disappointment as without batching the benefits of parallelism could not be manifested.

He states that using backprop with SNNs is low hanging fruit, and the main challenge going forward is moving beyond that towards more neuroscience inspired approaches."

Let me post this as a comment.

3:50pm. https://www.youtube.com/watch?v=_Xcdm6alfCY
Spiking Neural Networks Deep Dive

https://www.youtube.com/watch?v=4lY-oAY0aQU
MIT 6.S191: Biologically Inspired Neural Networks (IBM)

Let me stop here for a bit so I can do the chores.

4:25pm. I am back.

I've been thinking. I've made up my mind - Mike Davies is essentially right about everything. Loihi (or something like it) is what I should be angling Spiral for.

4:30pm. 2022 is only two years away. I am seriously going to aim that Spiral v0.2 will have a neuromorphic backend rather than a CPU one.

In fact, since after I am done with v0.2 I won't really be able to upgrade the rig, I am going to have to look for a job and hope that Spiral looks good enough as the sole item on my resume. Rather than beat around the bush, I should just email the guy directly and ask him if they are interested in hiring a PL guy...well, remotely anyway.

4:35pm. I don't think backprop is not learning, but I certainly hate it at this point so I am definitely on the same page as Mike now.

4:40pm. Every long effort needs a certain hope. In 2018 that was that I could find some way of making backprop work well enough to create a stable RL agent.

Loihi might as well be my future. Algorithms are necessary of course, but hardware is the backbone of ML.

Despite the math talent in the world, literally nobody in the 90s could have foreseen deep learning. Proof is in having a program and running it.

4:50pm. Neuromorphic hardware was a far off future in 2015, but is near at hand now.

Forget all of that nonsense about reversibility and using evolutionary algos, and aim for this one thing. Literally nobody is smart enough to design ML algorithms from the ground up. The Inspired will be able to do it, but I am really far below their level of capability.

I am not alone. Intelligence is in my brain. Intelligence is everywhere in nature. It just needs to be minded and refined into something usable. Neuroscience has been useless so far, but it will get its time to shine. They will get their chance to earn their paycheck.

4:55pm. I can make Spiral, but I can't imagine myself doing any significant algorithmic breakthroughs in the ML space.

...Ah, shit. I really will have to get a job. Well whatever. I'll thinking about that when it comes to that.

5pm. Let me finish today off with one more video.

https://www.youtube.com/watch?v=4lY-oAY0aQU
MIT 6.S191: Biologically Inspired Neural Networks (IBM)

Let me go with this.

...No that is not on neuromorphic chips. Ok, let me go for the other one. It is nearly 2h though.

5:20pm. Went through the slides. It is just really basic and long. Skip.

https://www.youtube.com/watch?v=gX9NqDuwTnA
Neuromorphic computing with emerging memory devices

Let me give this a chance.

I do not care if SNNs do not work as ANNs. I just want to run away from ANNs. The desire to create new algorithms and my inability to do so is what is giving me nightmares. It would really be far more productive to focus my efforts where I can make the most impact.

7:05pm. Done with lunch. Let me close for the day here. Tomorrow I will get on those join points.

I guess when I finish the language I can try Deep CFR on the CPU as an exercise, but otherwise my aim will be to break into the neuro programming field. Forget evolution or anything like that. I need firm principles in order to do proper engineering. Without principles I might as well be guessing.

I am going to track down some of the reference that Davies mentioned on mathematical formalization of some of the things they are doing.

I need to gather my resolve. I need a firm vision in order to have a belief in the future.

There is just one way to do this, it is to have patience. There isn't a genius who can figure out learning. The community as a whole will be one to make the hardware and to infer the processes of the brain's functioning."

---
## [tgstation/tgstation@00043330a0...](https://github.com/tgstation/tgstation/commit/00043330a0c70bff651e8e7f68fe0b6c52dd0be8)
##### 2020-01-28 22:50:30 by fluffe9911

New King Goat Lavaland Boss! (#48823)

About The Pull Request

This pr adds in a new lavaland boss in a seperate z level arena also included are goat plushies that ram into people and goat skins which you get by butchering goats and can wear on your head, a goat gun that shoots goats and finally multiple tweaks/fixes generally involving possible ways to teleport out of noteleport area and or cheese the king goat.
Why It's Good For The Game

Lavaland has not seen any new bosses for a long bloody time and although the boss may be a bit silly I feel more hardcore players will enjoy the challenge it brings since this is meant to be a very hard boss also new goat related items are always neat.

🆑 Fluffe9911 for porting/making most of it, Monster and Sabiran for the King Goat!
add: A new king goat lavaland boss!
add: Goats now drop skin which you can wear on your head! (coder sprite)
add: Goat themed plushies that ram people! (realistic version sprite by identification code by karma)
/🆑

goat

This is ported over from my work on yogstation with minor changes mostly to make things work there is more goat content I made but didnt wanna do too much at once and dont know if tg would like it

---

# [<](2020-01-27.md) 2020-01-28 [>](2020-01-29.md)

