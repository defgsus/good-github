# [<](2020-12-22.md) 2020-12-23 [>](2020-12-24.md)

2,316,832 events, 1,219,001 push events, 1,885,812 commit messages, 164,861,544 characters


## [KamiyaNya/todos](https://github.com/KamiyaNya/todos)@[bd8bb88f17...](https://github.com/KamiyaNya/todos/commit/bd8bb88f171887396335c08156fee7bef7b9100f)
#### Wednesday 2020-12-23 01:23:32 by KamiyaNya

this fucking shit dont working damn fuck me and this project

---
## [rwobig93/ServerRPGAdventure](https://github.com/rwobig93/ServerRPGAdventure)@[5a5631a4d5...](https://github.com/rwobig93/ServerRPGAdventure/commit/5a5631a4d5ca68046dc0321d605ccc0e2e3db1e1)
#### Wednesday 2020-12-23 01:27:13 by Mat McLovin

Holy fucking shit. God Damn. ;Test Check BackPack is ready for Mother Fucking QA.

---
## [0cjs/8bitdev](https://github.com/0cjs/8bitdev)@[e011eba234...](https://github.com/0cjs/8bitdev/commit/e011eba23465374aa3a9a5c24391bbc7df23efda)
#### Wednesday 2020-12-23 01:46:32 by Curt J. Sampson

src/mc68/pmon regscmd: Functionality to set saved registers

We change the command from `s` to `r` because the latter is just much more
instinctive and easier to remember. We deal with the loss of R)ead by
changing read, write and verify all to be under the T)ransfer command,
which should be fine because that command will have a ton of other options
anyway.

There's still a design problem with this, as showed up with some
frustrating debuggin during unit testing: since the args are always
adjacent, it's easy to type `a3b4` meaning to set A=3 and B=4, but that's
interpreted as A=3B 4= followed by a parse error. Currently, we're thinking
of allowing optional commas in to separate arguments to help mitigate this.

The code is a bit sloppy and has a good deal of duplication, and the tests
are showing issues similar to (but not the same as) the examine tests. But
this is going on master anyway because it's unlikely to have broken
anything working to this point and we want feedback on this sooner rather
than later.

---
## [noahrichards/advent-2020](https://github.com/noahrichards/advent-2020)@[fa53954fcd...](https://github.com/noahrichards/advent-2020/commit/fa53954fcdaeaa11c296d7c69c9a2778f9de21cf)
#### Wednesday 2020-12-23 03:07:42 by Noah Richards

Day 18!

This was a kinda fun one, also I cheated a bit.

I was trying to remember how to do expressions and was at a loss, so I
did two things:
1) I checked reddit to see which way people were going, which is where I
   saw a meme that Part 2 was going to change operator precedence, so I
   realized I should make the solution generic in that way (I originally
   was just going to blindly do it).
2) I remembered doing something like this in Scheme, where the trick was
   just to convert infix (which requires precedence rules) to
   S-expressions (which has no precedence). Then I remembered you could
   do the same with RPN, but I couldn't remember the algorithm. So I
   looked it up on Wikipedia (Shunting-yard) and implemented it from the
   psuedocode there.

Shunting-yard is actually really simple once you know it, you just need
the rules for when/how you move things from the operator queue to the
output queue. Once you have that, you plug in precedence for each part
and then write a simple evaluator for RPN, which looks a bit like
Shunting-yard but just uses a single stack where you push numbers and
then, when you encounter an operator, use it combine the last two
numbers. When you're done, you should be left with a stack with a single
number, which is your answer.

---
## [morgoth1145/advent-of-code](https://github.com/morgoth1145/advent-of-code)@[e9f5fed3af...](https://github.com/morgoth1145/advent-of-code/commit/e9f5fed3afbf8952a37c2f68f1c7849157889abd)
#### Wednesday 2020-12-23 07:27:44 by Patrick Hogg

2020 Day 23 Part 2

Well today was interesting. Today I learned that min(some_range) is way, *way* slower than directly using a variable for the range's lower bound. That seems bad! That singlehandedly destroyed my runtime, even after realizing that I needed a linked list and a direct lookup for nodes. Without knowing why my Python was slow I opted to instead implement the solution in C++. That solved my performance issue, but I foolishly used ideone.com instead of a real dev environment. That made tracking down errors a ridiculously huge pain!

Today was definitely an interesting problem. I just wish I'd known about the min(some_range) issue beforehand because I could have so easily leaderboarded without that insane performance hit.

---
## [ModSquad2020/SD_Matt](https://github.com/ModSquad2020/SD_Matt)@[5aa40abe59...](https://github.com/ModSquad2020/SD_Matt/commit/5aa40abe5938e320729b7614dcaa8e79c90d8d5d)
#### Wednesday 2020-12-23 08:34:17 by Matthew Kozubov

Finally filtered modomics data into a nice format

Holy shit this took forever. Modomics annoying as heck.

Everything you need is in VisualizingCSV jupyter notebook.
Simply make a filter and play with the data :)
For example:
	Currently found out that at position 13 it is mostly
	pseudouridine. But if you filter Y out at that position
	we see that in Lactococcus Lactis there is an m1A here!
	And in a lot of bacteria there is actually Cm and m1Gs

---
## [sharashak/Lessons](https://github.com/sharashak/Lessons)@[d3024e7586...](https://github.com/sharashak/Lessons/commit/d3024e7586e2cbe51fafeb738b29f409f73fbfd6)
#### Wednesday 2020-12-23 16:48:03 by ArsaevEvgeny83

Lost my shit while doing this fucking thiing.

This is fucking disgusting. Still yet not finished. I hate this bullshit, asshole my shit!

---
## [ilaflott/5p02TeV_ppJetAnalysis](https://github.com/ilaflott/5p02TeV_ppJetAnalysis)@[d1aa1872d5...](https://github.com/ilaflott/5p02TeV_ppJetAnalysis/commit/d1aa1872d53b4c57945cefaca3e10076dd46fe08)
#### Wednesday 2020-12-23 17:53:31 by ilaflott

long time no commit.

readForests_ppData_jetPlots now has HBHEIsoNoise filter and PFMET fraction < 0.3 cuts applied.

new event cuts account for at printPlots step for printPlots_evtCounts, at least.

the light in my life has dimmed many shades today. I love you mom. Fuck cancer.

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[33f7351d68...](https://github.com/mrakgr/The-Spiral-Language/commit/33f7351d685483f0d2a821b8352fb3cfd781dfdf)
#### Wednesday 2020-12-23 18:15:02 by Marko Grdinić

"10:50am. When I go to bed late I sleep better, but I also get up late as well. Based on how tired I was, I thought it would be 9-10am, but it is nearly 11.

10:55am. Agh, who is going to start now? Let me chill for a while, have breakfast, do the chores and then I will put in the work that is needed. The last thing I feel like doing now is rushing.

12:30pm. Let me start.

12:35pm. Let me gather my bearings.

```fs
    let buffer = Dictionary()
    let last_id = ref 0
    use __ = server.ReceiveReady.Subscribe(fun s ->
        let rec loop () = Utils.remove buffer !last_id (body(NetMQMessage 3)) id
        and body (msg : NetMQMessage) (address : NetMQFrame, x) =
```

Let me get rid of the message ordering. This made sense to me before - it would be the case that 1 or 2 out of 10 times during startup, the semantic token request would arrive before the file open and that would wreck things. But now this optimization is preventing the server from being usable by multiple instances. Let me deal with this.

I'll just remove it. Then I will fix the semantic token get so request that go out of range do not cause an out of bounds exception. That should do the trick.

12:40pm. Let me just do this. I'll get lost in thought at this rate.

```fs
let token_range (r_par : ParserRes) ((a,b) : VSCRange) =
    let from, near_to = min (r_par.lines.Length-1) a.line, min r_par.lines.Length (b.line+1)
    vscode_tokens from near_to r_par.lines
```

Removing that buffer was easy. This is where the token range is gotten.

12:50pm.

```fs
let token_range (r_par : ParserRes) ((a,b) : VSCRange) =
    let from = a.line |> min (r_par.lines.Length-1) |> max 0
    let near_to = b.line |> min r_par.lines.Length |> max 0
    vscode_tokens from near_to r_par.lines
```

I thought I did no affordances, but I think that what I had before was mostly right...

No, this is not right somehow...

```fs
let token_range (r_par : ParserRes) ((a,b) : VSCRange) =
    let from = a.line |> min r_par.lines.Length
    let near_to = b.line |> min r_par.lines.Length
    vscode_tokens from near_to r_par.lines
```

Yeah, this is the way to do it.

That -1 was not the right reasoning. When done like this, I'd get at minimum a range of 1. Instead ranges could be zero as well. Even worse, they could set the from to -1.

```fs
let token_range (r_par : ParserRes) ((a,b) : VSCRange) =
    let in_range x = min r_par.lines.Length x
    vscode_tokens (in_range a.line) (in_range b.line) r_par.lines
```

Let me do it like this.

12:55pm. Now what I need to do next is adjust the request code so it does not send the message id.

That will be easy.

```ts
const request = async (file: object): Promise<any> => {
    const sock = new zmq.Request()
    sock.connect(uriServer)
    await sock.send(JSON.stringify(file))
    const [x] = await sock.receive()
    return JSON.parse(x.toString())
}
```

Piece of cake. Let me try running this.

Can't forget to publish the new thing first...

1:05pm. Things work great now.

Let me implement heart beating next.

After that I'll be ready for publication on the VS Code marketplace, but one thing that makes me uncofmortable currently is how large the .NET compiled code is. All those .dlls take megabytes of size.

Would it be possible to bundle that down similarly to the JS code? There should be a way to do it.

1:10pm. Focus me. Let me deal with heart beating first.

```
const spiPingReq = async (): Promise<void> => request({ Ping: true })
```

Here is the request.

1:15pm.

```ts
    (async () => {

    })();
```

Right now I am thinking how to do this kind of loop.

1:25pm.

```ts
    const pingLater = (time : number) => {
        const f = () => {
            if (isProcessing) {spiPingReq(); pingLater(time)}
        }
        setTimeout(f, time)
    }
    pingLater(1000)
```

Let me go with this. Now let me implement things on the F# side.

1:30pm.

```fs
    let mutable time = DateTimeOffset.Now
    let timer = NetMQTimer(2000)
    poller.Add(timer)
    timer.EnableAndReset()
    use __ = timer.Elapsed.Subscribe(fun _ ->
        if TimeSpan.FromSeconds(2.0) < DateTimeOffset.Now - time then poller.Stop()
        )

    use __ = server.ReceiveReady.Subscribe(fun s ->
        let msg = server.ReceiveMultipartMessage(3)
        let address = msg.Pop()
        msg.Pop() |> ignore
        let x = Json.deserialize(Text.Encoding.Default.GetString(msg.Pop().Buffer))
        let push_back (x : obj) =
            match x with
            | :? Option<string> as x ->
                match x with
                | None -> msg.Push("null")
                | Some x -> msg.Push(sprintf "\"%s\"" x)
            | _ -> msg.Push(Json.serialize x)
            msg.PushEmptyFrame(); msg.Push(address)
        let send_back x = push_back x; server.SendMultipartMessage(msg)
        let send_back_via_queue x = push_back x; queue_server.Enqueue(msg)
        let job_null job = Hopac.start job; send_back null
        let job_val job = let res = IVar() in Hopac.start (job res >>=. IVar.read res >>- send_back_via_queue)
        match x with
        | ProjectFileOpen x -> job_null (supervisor *<+ SupervisorReq.ProjectFileOpen x)
        | ProjectFileChange x -> job_null (supervisor *<+ SupervisorReq.ProjectFileChange x)
        | ProjectFileDelete x -> job_null (supervisor *<+ SupervisorReq.ProjectFileDelete x)
        | ProjectCodeActionExecute x -> job_val (fun res -> supervisor *<+ SupervisorReq.ProjectCodeActionExecute(x,res))
        | ProjectFileLinks x -> job_val (fun res -> supervisor *<+ SupervisorReq.ProjectFileLinks(x,res))
        | ProjectCodeActions x -> job_val (fun res -> supervisor *<+ SupervisorReq.ProjectCodeActions(x,res))
        | FileOpen x -> job_null (supervisor *<+ SupervisorReq.FileOpen x)
        | FileChange x -> job_null (supervisor *<+ SupervisorReq.FileChange x)
        | FileDelete x -> job_null (supervisor *<+ SupervisorReq.FileDelete x)
        | FileTokenRange x -> job_val (fun res -> supervisor *<+ SupervisorReq.FileTokenRange(x,res))
        | HoverAt x -> job_val (fun res -> supervisor *<+ SupervisorReq.HoverAt(x,res))
        | BuildFile x -> job_null (supervisor *<+ SupervisorReq.BuildFile x)
        | Ping _ -> send_back null
        time <- DateTimeOffset.Now
        )
```

This should do the trick. Let me compile this and then I will try it out.

1:35pm. It works. I did it.

Everything is set. I only need to figure out how to compile Spiral a bit better now and I will be ready to push out version `2.0.0`.

What does that Pack under the build options do?

```
Build started...
1>------ Build started: Project: The Spiral Language 2, Configuration: Debug Any CPU ------
1>The Spiral Language 2 -> C:\Users\Marko\Source\Repos\The Spiral Language\The Spiral Language 2\bin\Debug\netcoreapp3.1\Spiral.dll
1>Successfully created package 'C:\Users\Marko\Source\Repos\The Spiral Language\The Spiral Language 2\bin\Debug\Spiral.1.0.0.nupkg'.
========== Build: 1 succeeded, 0 failed, 0 up-to-date, 0 skipped ==========
```

It creates a nuget package. I see.

And it is only 886kb.

Still, this is not something I can put into the plugin.

Unpacked the compiler is 12mb.

https://docs.microsoft.com/en-us/aspnet/mvc/overview/performance/bundling-and-minification

https://ianqvist.blogspot.com/2018/01/reducing-size-of-self-contained-net.html

Google is not giving me what I want directly.

Ok, I should at least try to do trimming.

1:50pm. https://128bit.io/post/ahead-of-time-compilation-with-net-core/

This is actually pretty interesting.

```
dotnet publish -r win-x64 -c release /p:PublishSingleFile=true /p:PublishTrimmed=true
```

This is a different command that the one used with the Trimming package. What should I use?

1:55pm. I am not sure what I should do here. I am thinking whether I should try the above. But just having the sized squeezed down for a single platform is no good for me. I want it to be portable like now.

I need to do more research on this. Let me watch the video in that link. Ideally I'd find a way to do AOT compilation for all the platforms. Excluding that, I want to figure out the right way to do trimming.

2pm. https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-publish

> We recommend that you specify this option in a publish profile rather than on the command line. For more information, see MSBuild.

There are a bunch of interesting options here.

2:05pm.

```
<?xml version="1.0" encoding="utf-8"?>
<!--
https://go.microsoft.com/fwlink/?LinkID=208121.
-->
<Project ToolsVersion="4.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <PropertyGroup>
    <Configuration>Release</Configuration>
    <Platform>Any CPU</Platform>
    <PublishDir>..\VS Code Plugin\compiler</PublishDir>
    <PublishProtocol>FileSystem</PublishProtocol>
    <TargetFramework>netcoreapp3.1</TargetFramework>
    <SelfContained>false</SelfContained>
  </PropertyGroup>
</Project>
```

I see the the publish command uses this profile that I created. I've decided that I do not need the Trimming package since the compilation options exists for that in the .NET Core 3.0 and up.

https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/visual-studio-publish-profiles?view=aspnetcore-5.0

```
/p:<NAME>=<VALUE>
```

Ah, so this is what it means. That `/:p` is literally setting the property to some value.

```
    <PublishSingleFile>true</PublishSingleFile>
    <PublishTrimmed>true</PublishTrimmed>
```

I've added this into the pubxml. Let me try publishing it again. But before that...

Ok, the compiler is 12.3mb now. I've erased it by hand. Let me see what comes out next.

```
Build started...
1>------ Publish started: Project: The Spiral Language 2, Configuration: Release Any CPU ------
1>It is not supported to publish an application to a single-file without specifying a RuntimeIdentifier. You must either specify a RuntimeIdentifier or set PublishSingleFile to false.
========== Build: 0 succeeded, 0 failed, 1 up-to-date, 0 skipped ==========
========== Publish: 0 succeeded, 1 failed, 0 skipped ==========
```

Ah, I see.

https://docs.microsoft.com/en-us/dotnet/core/deploying/ready-to-run

What is R2R? Let me read that and then I will try that instead.

> However, R2R binaries are larger because they contain both intermediate language (IL) code, which is still needed for some scenarios, and the native version of the same code.

No this is not good. Forget it. Let me just do trimming then. I'll have to be satisfied with that.

```
Build started...
1>------ Publish started: Project: The Spiral Language 2, Configuration: Release Any CPU ------
1>The Spiral Language 2 -> C:\Users\Marko\Source\Repos\The Spiral Language\The Spiral Language 2\bin\Release\netcoreapp3.1\Spiral.dll
1>Optimizing assemblies for size is not supported for the selected publish configuration. Please ensure that you are publishing a self-contained app.
========== Build: 0 succeeded, 0 failed, 1 up-to-date, 0 skipped ==========
========== Publish: 0 succeeded, 1 failed, 0 skipped ==========
```

Ok, let me try it self contained then.

Ah, I see, once I turn on self contained, I do get the options to optimize in the IDE.

But now I cannot compile things protably anymore.

I give up. I can either chose size or portability, not both. I'll go with the later then.

2:20pm. And it is time to publish on the marketplace. Let me turn off the shell.

Why is taking so long to run the extension host now all of a sudden?

2:25pm.

```ts
    const compiler_path = path.join(__dirname,"../compiler/Spiral.exe")
    const compiler_path_for_shell = `"${compiler_path}"`
    const p = cp.spawn(compiler_path,{shell: false, detached: true})
```

How complicated. The shell and the regular spawn require different paths.

Ok, let me package this. Then I will publish it.

...Hmmm, no, I need to get rid of the source maps. They just waste 100kb.

I have no idea. Disabling them in the tsconfig file just saves 30kb. It still produces the source maps.

```
devtool: 'source-map',
```

Let me comment out this in the webpack config file.

Yeah, that does the trick. Ok.

2:30pm. Let me publish it. Right now the plugin is 4.3mb large.

...No I am not happy with this. Let me bring in the trim package after all.

https://ianqvist.blogspot.com/2018/01/reducing-size-of-self-contained-net.html

From this post I mean.

```
Build started...
1>------ Build started: Project: The Spiral Language 2, Configuration: Release Any CPU ------
1>The Spiral Language 2 -> C:\Users\Marko\Source\Repos\The Spiral Language\The Spiral Language 2\bin\Release\netcoreapp3.1\Spiral.dll
2>------ Publish started: Project: The Spiral Language 2, Configuration: Release Any CPU ------
2>Trimmed 1 out of 16 files for a saving of 0 MB
2>Final app size is 7.48 MB
2>The Spiral Language 2 -> C:\Users\Marko\Source\Repos\The Spiral Language\The Spiral Language 2\bin\Release\netcoreapp3.1\Spiral.dll
2>Trimmed 0 out of 1 files for a saving of 0 MB
2>Final app size is 2.63 MB
2>The Spiral Language 2 -> C:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin\compiler\
========== Build: 1 succeeded, 0 failed, 0 up-to-date, 0 skipped ==========
========== Publish: 1 succeeded, 0 failed, 0 skipped ==========
```

Useless. Nevermind.

Actually, this thing here is pretty much the ideal use case for why you want a language with partial evaluation. If this were Spiral, the end executable would be miniscule in comparison.

I haven't even thought about compilation sizes before, but seeing what .NET is giving me is making me realize I should highlight this benefit of Spiral in the future.

Let me get rid of that package. It is not useful to me.

Let me move to publication.

2:40pm. Done. Right now it is doing verification. That will take a while, but I'll get an email when it is done.

2:45pm. Let me take a break here. Oh, it has been verified. Let me try installing the plugin.

...Done. I see that the title is messed up. Also it does not point to the front page. Nevermind that for now. Let me try running some files.

2:50pm. It is not working. Nothing is showing up. I fear that the plugin is not being activated.

Ahhhhhhhhhh...Note how now I am compiling into `dist`. But the package still has the main as being in out. Damn.

I need to think about this.

Focus me, put the Iron Ladies aside. I'll get back into the fray.

First, let me take this opportunity to fix the auxilaries first. The title of the plugin shows up as `spiral-lang-vscode`.

```json
 "name": "spiral-lang-vscode",
 "displayName": "The Spiral Language",
 "readme": "../readme.md",
```

I am not sure that this is how to fix the readme. Let me take a look at the publishing the plugin page again.

2:55pm.

> A README.md file at the root of your extension will be used to populate the extension's Marketplace page's contents. vsce will modify README links for you in two different ways:

How awkward.

3:05pm.

```
Activating extension 'mrakgr.spiral-lang-vscode' failed: No native build was found for platform=win32 arch=x64 runtime=electron abi=80 uv=1 libc=glibc.
```

I get this error when I try to run the extension. Nevermind this. First of all, let me see if the changes I did to the package file did anything.

```
 "readme": "../readme.md",
 "license": "../LICENSE",
```

I doubt the packager will smart enough to fetch there, but it could work.

3:05pm. Let me try uploading it without the compiler. I just want to see if the readme trick works.

While I wait for it to get verified, let me investigate why I can't run the thing in dist.

3:15pm. Damn it, this is some weird webpack related error.

https://stackoverflow.com/questions/59275743/local-native-node-module-causes-error-uncaught-error-no-native-build-was-found

Not this...

> Since you are using webpack in the repository you provided. I've been doing research into this issue, it appears that webpack can cause problems with __dirname. Some native node modules require __dirname to work properly inorder for them to use the native code. (In my case leveldown is the module in question). I am actively looking into a fix on the webpack side of things. I will post the results to my findings once completed.

https://github.com/webpack/webpack/issues/

There is stuff in issue 1599.

```
// the webpack config just works
target: 'node',
node: {
  __dirname: false,
  __filename: false,
}
```

Weird hacks are what I need to do here. Let me try it.

```
Activating extension 'mrakgr.spiral-lang-vscode' failed: No native build was found for platform=win32 arch=x64 runtime=electron abi=80 uv=1 libc=glibc.
```

No, it fails.

...Am I sure that the problem is the `__dirname` for me? Let me just try it out with a direct path.

```
    // const compiler_path = path.join(__dirname,"../compiler/Spiral.exe")
    // const compiler_path_for_shell = `"${compiler_path}"`
    // const p = cp.spawn(compiler_path,{shell: false, detached: true})
```

Actually, let me comment this out.

The same error. This is not it. I am barking up the wrong tree here.

```
 "readme": "../readme.md",
 "license": "../LICENSE",
```

I checked the uploaded plugin. It is useless. I am going to have to configure MSbuild so it copies the readme and the license into the plugin directory.

In fact, why don't I just put a link into the readme to the online repo instead of copying it?

3:30pm. I decided to go with that. That is the easiest way. I do not want multiple copies of the same readme floating around. It is better to use a link instead.

As for the license, it is not like that will be changing from here on out, so I can afford to keep a copy.

Ok, now that leave only the issue of Webpack not working correctly.

I only have to resolve this and then I will be done. Let me take a short break.

4pm. The short break took a while.

But I did come to some conclusions.

```
 "scripts": {
  "vscode:prepublish": "webpack --mode production",
  "webpack": "webpack --mode development",
  "webpack-dev": "webpack --mode development --watch",
  "test-compile": "tsc -p ./"
 },
```

Most likely what vsce is using is `"vscode:prepublish": "webpack --mode production"` in order to run the script. What I should try right now are some of Webpack's competitors rather than try to debug the crummy thing.

https://code.visualstudio.com/api/working-with-extensions/bundling-extension

This mentions Parcel and Rollup. Let me take a look at them.

https://rollupjs.org/guide/en/

Ok, let me aim for this. How do I use this with TS?

https://www.npmjs.com/package/rollup-plugin-typescript2

What a pain in the ass. Well, let me give it a try.

4:15pm.

```
C:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin>rollup -c

src/index.ts → dist/index.js...
[!] Error: Incompatible tsconfig option. Module resolves to 'CommonJS'. This is incompatible with rollup, please use 'module: "ES2015"' or 'module: "ESNext"'.
Error: Incompatible tsconfig option. Module resolves to 'CommonJS'. This is incompatible with rollup, please use 'module: "ES2015"' or 'module: "ESNext"'.
    at checkTsConfig (C:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin\node_modules\rollup-plugin-typescript2\dist\rollup-plugin-typescript2.cjs.js:25094:15)
    at parseTsConfig (C:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin\node_modules\rollup-plugin-typescript2\dist\rollup-plugin-typescript2.cjs.js:25125:5)
    at Object.options (C:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin\node_modules\rollup-plugin-typescript2\dist\rollup-plugin-typescript2.cjs.js:29177:73)
    at C:\Users\Marko\AppData\Roaming\npm\node_modules\rollup\dist\shared\rollup.js:19997:36
```

At least I am getting sane errors here. Will changing the module type break anything?

I remember having trouble because of that in the past, but the details escape me.

```
import * as zmq from "zeromq"
```

Now this is giving me an error.

4:25pm. This goes away when I set the module resolution to node. Great. Now I can in fact use the newer and more elegant module system.

But rollup is not actually doing any minification so far. How to do I get it to do that.

```
import { terser } from 'rollup-plugin-terser';
```

Ohhhh, now it is refusing to find this module!

Uahhh....

This is two days wasted on this shit - it is roughly what I expected of JS.

...I accidentally installed it globally. Nevermind. Now it finally does its thing.

4:40pm. Let me get rid of the lodash dependency since I am not using it.

4:45pm. Ok, I cleaned things up. Now everything should work. For some reason though the extension host takes forever to start.

4:50pm.

```
  {
   "type": "typescript",
   "tsconfig": "tsconfig.json",
   "problemMatcher": [
    "$tsc"
   ],
   "group": "build",
   "label": "tsc: build - tsconfig.json"
  },
```

Damn, in order to make a rollup launcher I need to run it somehow. But I do not know how to do the task for it.

Aghhhh...

What the hell is a problem matcher?

```
  {
   "type": "shell",
   "command": "npm run vscode:prepublish",
   "group": "build",
   "label": "rollup build"
  }
```

Maybe like this. I am letting the autocomplete guide me here. And I am getting errors when I make a mistake. Otherwise these scripts would be completely inaccessible for me.

Let me give this a try.

5pm.

```
"main": "dist/index.js",
```

This thing is giving me trouble. I have no idea how to use tsc watch and rollup side by side.

5pm. Now I can't use the tsc compiled plugins properly. It is telling me that it cannot use the import statements outside of a module. I remember getting similar kinds of errors before. The rollup version works fine though.

I guess I'll move to using TSC + rollup somehow. I don't really need the incremental compilation and the watching capability of TSC.

https://github.com/rollup/rollup-starter-app/blob/master/package.json

Rollup however does have the watching uption.

```
  "vscode:prepublish": "rollup -c",
  "build": "rollup -c",
  "watch": "rollup -c -w"
```

I'd want to find a way to pass in the flag so I can avoid doing the minifaction when I am not prepublishing.

```
// rollup.config.js
import typescript from 'rollup-plugin-typescript2';
import { terser } from 'rollup-plugin-terser';

const production = !process.env.ROLLUP_WATCH;

export default {
    input: 'src/index.ts',
    output: {
        file: 'dist/index.js',
        format: 'cjs'
    },

    plugins: [
        typescript(),
        production && terser()
    ]
}
```

Ah, so it is like this. Yeah, this makes sense.

```
  {
   "type": "typescript",
   "tsconfig": "tsconfig.json",
   "option": "watch",
   "problemMatcher": [
    "$tsc-watch"
   ],
   "group": "build",
   "label": "tsc: watch - tsconfig.json"
  },
  {
   "type": "typescript",
   "tsconfig": "tsconfig.json",
   "problemMatcher": [
    "$tsc"
   ],
   "group": "build",
   "label": "tsc: build - tsconfig.json"
  },
```

Let me take these tasks out.

The only thing I am worried about is that this might start multiple rollup watches.

```
{
 "version": "2.0.0",
 "tasks": [
  {
   "type": "shell",
   "command": "npm run vscode:prepublish",
   "group": "build",
   "label": "rollup build"
  },
  {
   "type": "shell",
   "command": "npm run watch",
   "group": "build",
   "label": "rollup watch"
  }
 ]
}
```

I have no idea. Let me just try first.

...Running rollup watch from the shell is blocking the extension host from starting. Not good.

```
       {
            "label": "Build extension",
            "type": "npm",
            "script": "build",
            "presentation": {
                "reveal": "silent"
            }
        }
```

There is an `npm` type?

5:20pm.

```
external: ["vscode","path","child_process"],
```

This gets rid of the external dependency warnings. Good.

I gave up on getting the pre launch task to work. From here on out, whenever I am working on the plugin I'll have to remember to `npm run watch` first. What a pain in the ass.

But at the very least, I have TS with the latest module system working now. Plus rollup's minification does work nicely.

It squeezes it down to 8kb from about 12mb. I really need this, I can't have the users download the unminified thing. I wish I could get the compiler down further, but there is no helping it now. One option would be to provide multiple versions for each of the OS as separate packages, but that would be too much of a burden for me.

5:30pm. Let me put the compiler in the right place so I can see if this works.

```
> spiral-lang-vscode@2.0.2 vscode:prepublish C:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin
> rollup -c

src/index.ts → dist/index.js...
(node:5780) ExperimentalWarning: Conditional exports is an experimental feature. This feature could change at any time
(!) Unresolved dependencies
https://rollupjs.org/guide/en/#warning-treating-module-as-external-dependency
zeromq (imported by src\index.ts)
created dist/index.js in 1.9s
Publishing mrakgr.spiral-lang-vscode@2.0.2...
 INFO  Extension URL (might take a few minutes): https://marketplace.visualstudio.com/items?itemName=mrakgr.spiral-lang-vscode
 INFO  Hub URL: https://marketplace.visualstudio.com/manage/publishers/mrakgr/extensions/spiral-lang-vscode/hub
 DONE  Published mrakgr.spiral-lang-vscode@2.0.2.
```

I decided to publish, but why am I getting an unresolved dependency error here? This is not supposed to happen.

https://github.com/rollup/plugins/tree/master/packages/node-resolve

It seems I need to resolve it. It is recommending I use this.

```
src/index.ts → dist/index.js...
(node:6544) ExperimentalWarning: Conditional exports is an experimental feature. This feature could change at any time
[!] (plugin rpt2) Error: C:/Users/Marko/Source/Repos/The Spiral Language/VS Code Plugin/src/index.ts(128,20): semantic error TS2354: This syntax requires an imported helper but module 'tslib' cannot be found.
```

Seriously - fuck Javascript. That are my honest feelings right now.

5:40pm. I got tslib as a dev dependency.

https://rollupjs.org/guide/en/#error-name-is-not-exported-by-module

Now I get this error in several places.

> It can be solved by using the namedExports option, which allows you to manually fill in the information gaps.

There is a link to `namedExports` in there, except it is dead and points to nothing.

5:45pm. I give up. I am throwing in the towel on minimization. Each user will simply have to get 12 extra megabytes of unused libraries every time he downloads the plugin.

I can't deal with this shit. I already wasted an entire day. Let me restore the tasks to their original setting.

6pm. Right now I am waiting for 2.0.3 to finish verifying. Then I'll download it and try it out. Assuming it works I will be done for the day. I expect it to work.

6:10pm. Yeah, it works.

Back while I was redoing the semantic token ranges I accidentally intro'd a bug.

```fs
let token_range (r_par : ParserRes) ((a,b) : VSCRange) =
    let in_range x = min r_par.lines.Length x
    vscode_tokens (in_range a.line) (in_range (b.line+1)) r_par.lines
```

This should be +1 like this.

6:15pm. Now it works.

The 2.0.4 works as it should. Now the plugin is installed and I can start random Spiral projects in it.

The whole sordid affair with Webpack and Rollup I will just sweep under the rug. To a lesser extent, the same goes for the .NET compiler. I'll forget my grudges here and just go with the sytem I've established today.

It is lunch time.

6:55pm. Done with lunch. I am so emotionally drained by this point. I am definitely going to close here, but let me put in the last few words for the day.

Thanks to today's work, the Spiral language is now published on the VS Code marketplace. I really brought up the schedule for this to a surprising extent. I even amazed myself.

And on the plus side, with this I am absolutely done with editor support. Now the plugin can start the compiler on its own in the background and the process has the ability to close on its own. And the language is published and the compilation pipeline is established. I will never have to go through the hastle of the last two days again.

7:05pm. The next thing to do for the next few weeks is to just keep filling out the documentation.

Hmmm...a thought comes to me. My writting is always filled with typos and grammar errors. I wonder in DL technology is good enough to serve as an editor? I should look into that. I need to put my paragraphs through stringent tests. It would be bad if I write 200 pages only to give the impression of illiteracy.

The important thing is to keep going. Doing the documentation will also give me the chance to more thoroughly test the language as I go. I maybe tested 30% of the surface area of the language so far. There is plenty of room for errors to lurk in. The work on the documentation will force me to systematically cover all of the language's features.

7:10pm. Soon I will be in business. The next review will definitely have something out of the ordinary."

---
## [frank078/ColorShifter](https://github.com/frank078/ColorShifter)@[112b0406dc...](https://github.com/frank078/ColorShifter/commit/112b0406dc3fdafd885a35040fffe16d56b40c97)
#### Wednesday 2020-12-23 18:25:30 by frank078

Fuck you Particles

Fixed bugs with the invincibility particle effects

---
## [yttrian/deja](https://github.com/yttrian/deja)@[b769b68939...](https://github.com/yttrian/deja/commit/b769b68939d7a7909290294b1eadf74f081be908)
#### Wednesday 2020-12-23 19:45:46 by Ian Moore

Create AnimationScreen abstract class

Helps reduce the size and complexity of FlashbackPlayer.

On a different note, it seems the odd animation reversal happens only
after you see a flashback for the first time, which is very odd.

Wait a second, oh my god, I think I figured out what is happening!

The player is literally seeing a memory of the flashback, that is so
funny! How did I not notice this until I started typing this out! At
least I know now how to fix it!

---
## [pytorch/pytorch](https://github.com/pytorch/pytorch)@[a3c66f7403...](https://github.com/pytorch/pytorch/commit/a3c66f7403264787f202e0e5f12cacae5883079b)
#### Wednesday 2020-12-23 20:42:50 by Brian Hirsh

Update on "Plumbing TLS keys through the dispatcher"

Table of contents because this description is too long:
- Overview
- High-level changes
- What order to look at things in
- What I don't like about this PR / things I want to change
- Benchmarking results
- Takeaways

### Overview
Benchmarks have shown that reading from TLS (thread-local scope) in the dispatcher is a noticeable source of framework overhead. The TLS read comes from the fact that some dispatch keys are stored using TLS, and these keys must be read from and aggregated to calculate a single dispatch key, which the dispatcher uses to determine which kernel to dispatch to. The logic for that lives [here](https://github.com/pytorch/pytorch/blob/55b431b17aba504ae7b75f6f97b4437101e50f38/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h#L50). The overhead mainly comes from the fact that calling an operator often results in multiple trips through the dispatcher, and the dispatch keys are read and re-aggregated on every trip.


### High-level changes
This PR attempts to reduce that overhead by "plumbing" the set of keys that the dispatcher computed through (opted-in) kernels. The only kernels currently opted in are the codegen'd autograd + tracing kernels. Kernels can opt into this plumbing by:
- Accepting a DispatchKeySet as their first argument, and manually calculating the new dispatch key set to pass back to the dispatcher
- registering themselves to the dispatcher with a slightly different API

This is accomplished by changing the calling convention inside the dispatcher:
- The dispatcher now expects all kernels registered to have a first argument of type DispatchKeySet.
- There are now two parallel registration paths in the dispatcher
- There are two versions of the functor in `make_boxed_from_unboxed_functor.cpp`. Both versions now also take in a DispatchKeySet argument. One does nothing with it, and the other passes it to the registered kernel.


### What order to look at things in
- In `make_boxed_from_unboxed_functor.h`, look at `make_boxed_from_unboxed_functor_withKeys` and `wrap_kernel_functor_unboxed_withKeys`. This is the core part of the updated calling convention in the dispatcher for unboxed kernels - one functor passes the keys through to the kernel and the other does not. I also added static asserts to make sure that you register your kernel with the right API (only use the new API if your kernel has a first argument of type DispatchKeySet)
- The equivalent version of that for boxed kernels is `KernelFunction_impl.h:make_boxed_function_withKeys()`. This isn't actually used anywhere, and won't be unless we write/update a boxed fallback to plumb TLS keys.
- `KernelFunction.h`, `KernelFunction_impl.h`, `KernelFunction.cpp` and `library.h` all contain the alternate registration path, starting with `Library::impl_withKeys()` and ending in `make_boxed_from_unboxed_functor.h`.
- `Dispatcher.h`, `KernelFunction_impl.h` and `boxing.h` also have changes for the new calling convention: versions of `callWithDispatchKey()` and `redispatch()` that take in a DispatchKeySet. they pass it along to `KernelFunction::call()`, which now always takes in a DispatchKeySet.
- `gen_variable_type.py` and `gen_trace_type.py` are where the change is actually used- the tracing and autograd kernels now accept DispatchKeySet as their first argument, remove the autograd keys from it and forward it back to the dispatcher. They register themselves using the new registration path
  - I also modified the autograd kernels to use `redispatch()` rather than `call()` - originally to avoid making another API, but it turned out to also have perf benefits. See the benchmarking section.
- `op_registration.h`, `CppSignature.h`, `Metaprogramming.h` changes are due to some hoops I had to jump through in the new registration path. They generate a version of CppSignature and FunctionSchema that ignore the first argument to the passed-in function pointer. Reasoning:
  - FunctionSchema expects all of its arguments to be convertible to ivalues, which isn't the case for DispatchKeySet (we probably want to totally hide it from jit-land)
  - OperatorEntry expects the CppSignature of all of its kernels to match, which is technically no longer the case. I could break that invariant, but instead I chose to hide the DispatchKeySet argument so they would all match (I'm open to either though).
- `gen_unboxing_wrappers.py`, `generated_unboxing_wrappers.cpp`, `OperatorEntry.h/cpp` required changes because the unboxing wrapper that we generate now has a different calling convention depending on whether or not the corresponding unboxed kernel takes in a DispatchKeySet. This is a hack on top of a hack, and should go away when we're totally c10-full. I manually hardcoded the fact that only autograd/tracing use the new calling convention, inside of `OperatorEntry.cpp`.


### What I don't like about this PR / things I want to change / where further work is needed
There's a bigger question here of "do the benefits of this PR warrant it landing given it's complexity" that I try to address more in the benchmarking section.
Here though I just want to point out some parts of the PR that are incomplete / can hopefully be cleaned up. I wanted more general feedback on the PR as a whole before spending time cleaning it up further.

1) The new registration path is bad; it's implemented as a giant chain of duplicate functions, all ending in `_withKeys`, spread throughout `library.h`, `KernelFunction[_impl].[h|cpp]`, `make_boxed_from_unboxed_functor.h`, `CppSignature.h`, `op_registration.h` and `Metaprogramming.h`. The `_withKeys` suffix is also subject to change, I just left it as a placeholder so I could easily find all the call-sites. I think that I can write this all more cleanly with templated bools, but I ran into some issues trying that- when I tried merging the pairs of structs in `make_boxed_from_unboxed_functor.h`, the compiler looked like it was complaining about the fact that I was passing in different types in different branches of a `guts::if_constexpr`.
- One way I could fix it- maintain two copies of the structs in `make_boxed_from_unboxed_functor.h`, but template everything else on a boolean. The functions that create those structs can use `if_constexpr` to choose which struct to extract the `call` operator from.

2) `callWithDispatchKey`, `redispatch` and `callBoxed` now each have two variants, one taking in a DispatchKey and one taking in a DispatchKeySet (callBoxed's first variant doesn't take in anything). They are functionally similar, and I need the second variant, but I kept the first around to avoid being BC-breaking. Still, that gives us two pretty much repeated functions. I could write the first variant to call the second, I'd just need to benchmark it to ensure it gets inlined properly.

3) Need to confirm whether it's acceptable that autograd kernels no longer sample RecordFunction (see the notes in the benchmarking section). We get a bit of extra perf by not having to do this but I can add it back in if necessary.

4) The logic I added in `Metaprogramming.h` to remove the first argument of a function is pretty hacky. The only alternative that I see is to special-case the error-checks in FunctionSchema and OperatorKernel to ignore a first argument of type DispatchKeySet. Not sure that that's necessarily cleaner though.

5) I updated 3 of the kernels in `VariableTypeManual.cpp` to plumb dispatch keys, and it looks like all 3 of them are failing to compile on windows in phabricator because they aren't matching a BoxedKernelWrapper specialization ([here](https://www.internalfb.com/intern/ci/signal_trampoline/?phabricator_version_fbid=214217710245195&ci_signal_id=c2FuZGNhc3RsZV9pbnN0YW5jZV9hbGlhczpidWlsZC1vdnItc2VydmVy)). I need to dig into it more.

6) Need to re-write the autograd codegen with the new model on top of Jiakai's changes

7) Need to add unit tests


### Benchmarking results
Recap: This PR adds a lot of complexity to the dispatcher, and probably isn't worth landing unless there's a clear perf improvement.

One noticeable shortcoming of this PR - especially when tracing is disabled, it has a greater effect on "base operators" over "composite operators". Composite ops result in multiple dispatch chains, and all that this PR tries to guarantee is a single TLS read per dispatch chain. I tried printing out the number of times that TLS keys are read from for a number of simple operators before/after this PR, which I dumped [in this quip](https://fb.quip.com/MNBJAbUq0jEW#RSDACAxyNAu).
So for example, I'd expect this PR to improve `a += 1` (tls reads go from 7 -> 6) less than it improves `c = a + b` (tls reads go from 2 -> 1).

One (unpopular) option would be to pick a small number of simple/commonly used composite ops and implement the plumbing for them too. Although that would add even more complexity and doesn't provide a clear win without more measuring (e.g. since that would probably require adding yet another argument to the calling convention).


I tested 5 benchmarks using torch.Timer's C++ snippets:
```
    stmt="t3 = t1 + t2;",
    setup="auto t1 = torch::randn({1}); auto t2 = torch::randn({1}); auto t3 = torch::zeros({1});",

    stmt="t = torch::empty(size);",
    setup="auto t = torch::zeros({1, 2, 3}); auto size = t.sizes();",

    stmt="t = t1.view(-1);",
    setup="auto t = torch::zeros({1, 2, 3}); auto t1 = torch::zeros({1, 2, 3});",

    stmt="t += 1;",
    setup="auto t = torch::zeros({1, 2, 3});",

    stmt="t.resize_(0);",
    setup="auto t = torch::randn({1});",
```


**wall time results (ran each test for 10 minutes)**
| Test | Before | After | Delta |
| ------------- | ------------- | ------------- | ------------- |
| add_out  | 1.67 +- 0.02 us | 1.36 +- 0.01 us | -18.6% |
| empty | 427.50 +- 2.38 ns | 403.01 +- 2.78 us | -5.7% |
| view | 784.70 +- 10.35 ns | 707.76 +- 15.13 ns | -9.8% |
| add_inplace | 4.42 +- 0.02 us | 4.45 +- 0.02 us | +0.7% |
| resize_inplace | 195.12 +- 0.74 ns | 108.44 +- 0.49 ns | -45.4% |

Due to my paranoia I ran the tests again for 60 minutes each overnight on my devfair.


**wall time results (ran each test for 60 minutes)**
| Test | Before | After  | Delta |
| ------------- | ------------- | ------------- | ------------- |
| add_out | 1.70 +- 0.02 us | 1.41 +- 0.02 us | -17.10% |
| empty | 397.17 +- 2.16 ns | 395.37 +-2.19 us | -0.50% |
| view | 774.42 +- 2.41 us | 666.66 +- 7.91 us | -13.90% |
| add_inplace | 4.40 +- 0.04 | 4.22 +- 0.09 us | -4.10% |
| resize_inplace | 188.82 +- 0.96 ns | 107.05 +- 0.22 ns | -43.30% |

resize_inplace, add_out and view all performed much better than empty and add_inplace. That held up with the expectation I put above, since those 3 all halve the number of tls reads (2->1), while empty has the same number of tls reads (3->3) and adds_inplace goes from 7->6. Importantly, empty and add_inplace didn't get noticeably slower.

I couldn't get a matching benchmark from AIBench though. It actually showed large regressions, specifically in the `add_out` and `empty` tests. That's why I explicitly tests those in my benchmarks - I matched the snippet that was run by AIBench in my local tests. I don't have a good explanation for why I couldn't replicate the results in AIBench, although I'd like to think that the 60 minute tests that I ran locally are more representative. Two examples are [here](https://www.internalfb.com/intern/aibench/details/757195478/) and [here](https://www.internalfb.com/intern/aibench/details/3404018999/).


**instruction count results**
| Test | Before (instruction count) | After (instruction count) | diff in instructions | diff in __tls_get_addr_ count | delta |
| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |
| add_out | 628461 | 610210 | -18251 | -4400 | -2.90% |
| empty | 218216 | 218116 | -100 | 0 | ~0% |
| view | 324816 | 304016 | -20800 | -4400 | -6.40% |
| add_inplace | 1558265 | 1541325 | -16940 | -4400 | -1.10% |
| resize_inplace | 67314 | 47414 | -19900 | -4400 | -29.60% |

And here's an example instruction count delta. This one is for `resize_inplace`. Sorry, I ran `before.delta(after)`, so positive numbers are good :)
```
rows: 41
<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7fe0f3d54940>
    4400  /build/glibc-OTsEL5/glibc-2.27/elf/../sysdeps/x86_64/tls_get_addr.S:__tls_get_addr
    4200  VariableTypeManual.cpp:torch::autograd::VariableType::()::resize_(at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
    3700  Dispatcher.h:at::Tensor& c10::Dispatcher::callWithDispatchKey<at::Tensor&, at::Tensor&, Ar<l ... oryFormat>)> const&, c10::DispatchKey, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>) const'2
    3700  Dispatcher.h:at::Tensor& c10::Dispatcher::callWithDispatchKey<at::Tensor&, at::Tensor&, Ar<l ... emoryFormat>)> const&, c10::DispatchKey, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>) const
    2400  record_function.cpp:at::shouldRunRecordFunction(bool*)
    2400  DispatchKeyExtractor.h:c10::impl::dispatchTypeId(c10::DispatchKeySet, c10::DispatchKeySet)
    2000  TensorMethods.cpp:at::Tensor::resize_(Ar<long>, opt<c10::MemoryFormat>) const'2
    1600  Dispatcher.h:at::Tensor::resize_(Ar<long>, opt<c10::MemoryFormat>) const'2
    1400  LocalDispatchKeySet.cpp:c10::impl::tls_local_dispatch_key_set()
    1200  DispatchKeySet.h:c10::impl::dispatchTypeId(c10::DispatchKeySet, c10::DispatchKeySet)
    1000  llvmMathExtras.h:c10::impl::dispatchTypeId(c10::DispatchKeySet, c10::DispatchKeySet)
    1000  KernelFunction_impl.h:at::Tensor& c10::Dispatcher::callWithDispatchKey<at::Tensor&, at::Tens ... oryFormat>)> const&, c10::DispatchKey, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>) const'2
    1000  KernelFunction_impl.h:at::Tensor& c10::Dispatcher::callWithDispatchKey<at::Tensor&, at::Tens ... emoryFormat>)> const&, c10::DispatchKey, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>) const
     500  record_function.cpp:at::()::manager()
     500  WrapFunctionIntoFunctor.h:c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFun ... 0::MemoryFormat>)>::call(c10::OperatorKernel*, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
     500  RegisterCPU.cpp:c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoF ... 0::MemoryFormat>)>::call(c10::OperatorKernel*, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
     300  TensorBody.h:torch::autograd::VariableType::()::resize_(at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
     300  LegacyTypeDispatch.h:torch::autograd::VariableType::()::resize_(at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
     200  stl_vector.h:at::shouldRunRecordFunction(bool*)
     200  make_boxed_from_unboxed_functor.h:c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail: ... 0::MemoryFormat>)>::call(c10::OperatorKernel*, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
     200  OperatorEntry.h:c10::impl::OperatorEntry::lookup(c10::DispatchKey) const
     200  KernelFunction_impl.h:c10::impl::OperatorEntry::lookup(c10::DispatchKey) const
     200  /usr/include/c++/7/array:c10::impl::OperatorEntry::lookup(c10::DispatchKey) const
     100  LocalDispatchKeySet.h:c10::impl::tls_local_dispatch_key_set()
    -100  stl_list.h:torch::autograd::VariableType::()::resize_(c10::DispatchKeySet, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
    -100  Dispatcher.h:at::Tensor::resize_(Ar<long>, opt<c10::MemoryFormat>) const
    -200  make_boxed_from_unboxed_functor.h:c10::impl::wrap_kernel_functor_unboxed_withKeys_<c10::impl ... all(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
    -200  WrapFunctionIntoFunctor.h:c10::impl::wrap_kernel_functor_unboxed_withKeys_<c10::impl::detail ... all(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
    -200  OperatorEntry.h:at::Tensor::resize_(Ar<long>, opt<c10::MemoryFormat>) const
    -300  TensorBody.h:torch::autograd::VariableType::()::resize_(c10::DispatchKeySet, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
    -300  LegacyTypeDispatch.h:torch::autograd::VariableType::()::resize_(c10::DispatchKeySet, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
    -400  make_boxed_from_unboxed_functor.h:c10::impl::wrap_kernel_functor_unboxed_withKeys_<c10::impl ... all(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
    -400  RegisterCPU.cpp:c10::impl::wrap_kernel_functor_unboxed_withKeys_<c10::impl::detail::WrapFunc ... all(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
    -700  llvmMathExtras.h:at::Tensor::resize_(Ar<long>, opt<c10::MemoryFormat>) const
    -700  DispatchKeySet.h:c10::DispatchKeySet c10::DispatchKeyExtractor::getDispatchKeyUnboxed<*, long>(c10::DispatchKeySet, * const&, long const&) const
    -800  TensorMethods.cpp:at::Tensor::resize_(Ar<long>, opt<c10::MemoryFormat>) const
    -900  llvmMathExtras.h:torch::autograd::VariableType::()::resize_(c10::DispatchKeySet, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
   -1000  DispatchKeyExtractor.h:c10::DispatchKeySet c10::DispatchKeyExtractor::getDispatchKeyUnboxed<*, long>(c10::DispatchKeySet, * const&, long const&) const
   -1100  KernelFunction_impl.h:at::Tensor::resize_(Ar<long>, opt<c10::MemoryFormat>) const
   -1600  KernelFunction_impl.h:torch::autograd::VariableType::()::resize_(c10::DispatchKeySet, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)
   -4300  VariableTypeManual.cpp:torch::autograd::VariableType::()::resize_(c10::DispatchKeySet, at::Tensor&, Ar<long>, opt<c10::MemoryFormat>)

Total: 19900
```

Some observations:
- Aside from empty (no change), the other benchmarks all have a similar drop in total instruction counts (~18k), which seems reasonable since they're all removing one tls read, from the autograd kernel, on every op call.
  - That probably means that this PR will primarily be useful in training and tracing use cases- not for inference (once `torch.no_grad` lands).
- The instruction count deltas are all smaller than the wall-time deltas. That's probably due to __tls_get_addr being an expensive instruction
- In all cases where fewer tls reads occurred, we dropped instructions in more places than just __tls_get_addr. Taking the above as an example:
  - __ttls_get_addr (-4400). No surprises here.
  - dispatchTypeId (-4600). dispatchTypeId does other bitwise calculations in addition to the tls read, which we now only do once (plus the single mask in the autograd kernels)
  - tls_local_dispatch_key_set (-1400). This is a tiny functions that basically just checks a global flag and calls __tls_get_addr. Maybe that flag checking added up to 1400?
  - callWithDispatchKey (-9400). This is because I updated the autograd kernels to use `redispatch()` rather than `call()`. It looks like `callWithDispatchKey` isn't getting inlined, but `redispatch` is (see below)
  - shouldRunRecordFunction (-2200). Now that we call `redispatch` instead of `callWithDispatchKey` from autograd kernels, we don't use RecordFunction during the redispatch. I **think** that's the right behavior, since we've already used RecordFunction in the original `call()`, and it's not clear to me why we'd want to use RecordFunction multiple times in a single dispatch chain in only some cases. Other than that, `redispatch` and `callWithDispatchKey` aren't too different, so I'm not too sure why the delta for `callWithDispatchKey` is so high.
  - resize_ (+2000). My guess is that this is because the autograd kernels now do a bit more work, to manually mask the DispatchKeySet before calling back into the dispatcher

I'm also particularly confused by `add_out` from AIBench vs. my benchmarks, since locally I see both a wall-time and an instruction count drop, but the two AIBench runs I posted both show a regression. The same goes for `empty`, although I see mostly no change in my local benchmarks, vs. a regression in AIBench 


### Takeaways
For this PR to merit landing, we want to see:
- substantial benchmark wins. i.e. in the majority of benchmarks, we want to see that (gain from plumbing TLS keys) > (loss due to extra register usage)
- Limited code complexity increase. Specifically, that (value of benchmark wins) > (size of code complexity increase)

If my local benchmarks are trustworthy, I think we see some pretty sizable wins in most cases, and more importantly, no major regressions in the cases where this PR doesn't help (`empty`).

I think the PR is pretty unwieldy as is, but there are some steps I can take to make it better (namely, hopefully templating the op registration path so we don't have two sets of functions everywhere).

But open to other suggestions/opinions!


Differential Revision: [D25614042](https://our.internmc.facebook.com/intern/diff/D25614042)

[ghstack-poisoned]

---

# [<](2020-12-22.md) 2020-12-23 [>](2020-12-24.md)

