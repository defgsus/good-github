# [<](2020-01-17.md) 2020-01-18 [>](2020-01-19.md)

1,321,062 events, 738,793 push events, 1,056,051 commit messages, 67,715,295 characters


## [MMMiracles/-tg-station@747546c565...](https://github.com/MMMiracles/-tg-station/commit/747546c565769c8c0be5aff059be6408ff54d912)
##### 2020-01-18 01:21:36 by Rob Bailey

SEVERAL mask sprite fixes (#48744)

* society moment

god im so funny

* do you apes not know what consistency means?

seriously who the fuck made these sprites

---
## [newstools/2020-naija-news-agency@08a89d27ae...](https://github.com/newstools/2020-naija-news-agency/commit/08a89d27aed49ea6daf2147eb66cf8bacf296939)
##### 2020-01-18 05:23:32 by NewsTools

Created Text For URL [naijanewsagency.com/olamide-reportedly-marries-his-longtime-girlfriend-babymama-bukunmi-aisha/]

---
## [sthagen/pyre-check@7a703fcfd6...](https://github.com/sthagen/pyre-check/commit/7a703fcfd659bfd2be7447050944603751b2a49d)
##### 2020-01-18 05:29:24 by Jia Chen

Introduce APIs for parallel scheduling policy

Summary:
We'll need to go through a fair amount of background before one can make sense out of my proposal here.

Context
======

The problem we are trying to address here is to figure out a reasonable way to schedule the list of workloads passed into `Scheduler.map_reduce` (or `Scheduler.iter`). Past experience has shown that the scheduling algorithm has critical impact on the performance profile of both `pyre` and `pysa` (examples where a simple decision change lead to hundreds of seconds of perf win: D19385381, D13330658).

Here's a little bit of background on how the underlying `MultiWorker` module in `hack_parallel` is coded up. Supposed that we have `M `tasks and `N` workers. Conceptually, the way it works is that we take the list of tasks, break it apart into smaller chunks (aka. `bucket` in the code), and put all the chunks into a queue. Then we repeatedly poll from a pool of workers -- if all workers are busy, we block and wait for one that becomes non-busy; otherwise, we pull a task out of the queue, assign it to a non-busy worker, and poll for the next non-busy worker.

As you can see from the description above, there's really not a lot we can fine-tune given this framework -- picking a scheduling algorithm, under this context, essentially boils down to deciding how the list of `M` input tasks can be chunked.

Although the underlying `MultiWorker` API makes it possible to implement more advanced chunking algorithm, I think to avoid over-engineering we'll just stick with the simplest and dumbest chunking method possible for now: splitting the `M` jobs into chunks of equal length. Given that the input size `M` can vary across different invocations of `Scheduler.map_reduce`, naturally there are two ways we can do the split:
- The Fixed-Bucket-Size policy keeps the size of each chunk (henceforth denoted as `B`) constant, and let the number of chunks vary as `M / B`.
- The Fixed-Bucket-Count policy keeps the number of chunks (henceforth denoted as `C`) constant, and let the size of each chunk vary as `M / C`. Note that if this kind of policy is used, it usually makes more sense to let `C` scale with the number of workers -- hence in the code you won't find  `C` directly because it is always derived by multiplying the worker count and a multiplier.

Choosing Policies
=============

I've thought a lot about how we should choose between the Fixed-Bucket-Size policy and the Fixed-Bucket-Count policy. My current understanding is that there's no clear winner and it's always about tradeoffs:
- The Fixed-Bucket-Size policy tends to have more predictable **latency**
  + It's easier to estimate how long it takes for a worker to complete a given chunk of work if the chunk size is known.
- The Fixed-Bucket-Count policy tends to have more predictable **throughput**
  + It's easier to estimate the amount of IPC communication overhead if we know how many chunks need to be processed in total.

Following this line of thoughts, the first principle we should follow seems to be that we should do Fixed-Bucket-Size whenever progress needs to be shown to the user (because humans are more latency sensitive), and do Fixed-Bucket-Count otherwise for better overhead control. Note that I'm simplifying the issues a lot -- there are more perf/memory concerns regarding the worker GC if we are willing to dig into more details. But I want to get the big picture right first, before we can dive into micro-optimizations.

Fallback Behaviors
==============

Unfortunately, I can't conclude the discussion of scheduling policy at this point, as we do have some devils that hide in the edge cases.

The simple Fixed-Bucket-Size and Fixed-Bucket-Count policies described above work great when the number of task `M` is sufficiently large. But when the number becomes small, we run into troubles fairly quickly:
- The Fixed-Bucket-Size policy is prone to the problem of *starvation*: when the number of work `M` is not sufficient enough to fill `N` buckets (with `N` being the number of workers) of size `B`, some worker is going to be left with little to none work to do, while other workers will keep getting `B` chunks of works.
- The Fixed-Bucket-Count policy is prone to the problem of *excessive overhead*: when the number of work `M` is not sufficient to the extent that each bucket only contains very few work to do, the IPC/synchronization overhead between master-worker starts to dominate the wall clock. I've seen in practice cases where for a small amount of work, doing everything in master takes milliseconds while sending the same workloads to workers would take tens of seconds.

To mitigate these two issues, I propose two simple adaptations to the two policies mentioned before:
- First, we introduce the notion of *fallback behavior*, which kicks in when we find that following the original Fixed-Bucket-Size or Fixed-Bucket-Count policy may lead to issues due to insufficient input size.
  + For Fixed-Bucket-Size policy, if we detect that the number of buckets would be smaller than a threshold `C`, we fall back to a Fixed-Bucket-Count policy of count `C` instead to avoid starvation. The other way of looking at this is that we always cap the number of buckets to be at least `C`.
  + For Fixed-Bucket-Count policy, if we detect that the bucket size would be smaller than a threshold `B`, we fall back to a Fixed-Bucket-Size policy of size `B` to avoid excessive overhead. The other way of looking at this is that we always cap the bucket size to be at least `B`.
- Second, we introduce the notion of `sequential_cutoff`. This is going to be a parameter of our policy, and it means that if the input size `M` is smaller than `sequential_cutoff`, we unconditionally run everything sequentially in the master process. The exact value of `sequential_cutoff` may vary: if each task takes long time to compute (e.g. Pysa fixpoint iteration), we should use a small number; if each task only takes miliseconds (e.g. most of the environment table update), we should use a larger number. Note that this number should always be larger than 1: if `M` is 1, it is unconditionally better to do it in master than in worker.
  + In the code, `sequential_cutoff` is implicitly computed by multiplying the minimum bucket size `C` and the minimum bucket count `B`.

Overview of This Diff
===============

This diff tries to provide implementations for both Fixed-Bucket-Size-WIth-Fallback policy and the Fixed-Bucket-Count-With-Fallback policy. It also preserves the two policies currently used in production (denoted as "legacy" policies in the code) -- the goal is to keep this particular diff semantic-preserving, and gradually phase out the legacy policies in followup diffs.

See `schduler.mli` first to get an idea of how the `Scheduler` API evolves. My change should be fairly straightforward: the `bucket_size` parameter (which used to switch between our legacy version of Fixed-Bucket-Size and Fixed-Bucket-Count policy) is replaced with a full-blown policy type `Policy.t`.

The type `Policy.t` is kept private to ensure that one can only create them via the provided APIs. Under the hood it is nothing but a function that computes the bucket count.

Rest of the changes are there to cope with the `Scheduler` API updates.

Reviewed By: sinancepel

Differential Revision: D19402492

fbshipit-source-id: a9b627cdf9af083b021fe51e825168dc3abfaa45

---
## [mrakgr/The-Spiral-Language@f598d1673b...](https://github.com/mrakgr/The-Spiral-Language/commit/f598d1673b1d93041a13598a14df7025e71fcc5a)
##### 2020-01-18 13:16:06 by Marko Grdinić

"10:05am. I got up 20m ago. Let me see if I can go through vol 9 and then I will start.

So far, my impression of Dendro is that it is really good. Especially the characterization - Hell General acting like a cartoon villain and then being shown to be a 10 year old kid throwing a tantrum after getting BTFOd is quite credible. The author is quite talented to be able to show this. Dendro is more than just the MC being thrown in some fantasy world and being given powers - the border between the world is there, and it is getting used.

10:20am. Ok, focus me. Waste time in the right way.

10:40am. https://www.reddit.com/r/MachineLearning/comments/eq3da0/d_what_are_the_current_significant_trends_in_ml/

Right now, I am reading Dendro, but decided to check out this thread on a whim.

"Look into sparse evolutionary training, it’s using genetic algorithms to configure networks for learning. Also symbolic regression is starting to gain popularity again as more modern uses have been published last year. Type those in using google scholar and you’ll see some cool stuff. The SET technique above was published in Nature I believe. If you have troubles let me know I can find the papers for you ❤️"

This stuff is new to me, I'll definitely keep it in mind. I never really looked into symbolic regression for real either though the talk by Lipshuts that I watched way before I started this quest really sold me on its potential.

After everything is done, I will really be innovating in the direction of using evolution together with NNs.

https://www.reddit.com/r/MachineLearning/comments/eqdad9/d_a_sober_look_at_bayesian_neural_networks/

I'll look at this at a later date, let me get back to my vacationing.

1:25pm. Done with vol 9. I've really been indulging myself for the past 1.5 weeks.

I think that starting next week I will try taking things a bit more seriously. The way things are going is taking too long, so I get to testing the new language soon. I'll roll up my sleeves and put in a few extra hours if possible and get back to my old pace.

1:30pm. I really can't keep working on the language forever.

I need to get back into ML.

So for the near term, at least finishing the first half should be my main priority. Even if I decide to skip typechecking for some reason, I still need to beef up Spiral's compilation speeds. So that is work that absolutely needs to be done.

1:35pm. Just one more time. I will evolve the language one final time and then try again.

The way I was approaching ML reminds me of how in the past I'd approached roguelike games like Diablo. At that time the degree of caution needed to win at roguelikes was completely foreign to me.

Games like Angband I've tried getting into many, many times and every time I would get creamed and die. Back when I was a kid I liked using cheats, and I like exploring immersive worlds and collecting stuff. The challenge was secondary.

Me becoming capable of beating Angband, DCSS, DoomRL is a fairly recent phenomenon.

I guess when I decided to become a trader it brought a change in my mindset and approach towards the games. It is not just that I had a fundamentally different approach to these games - I did. It is also that I had changed my affinities - I started to enjoy playing in the new style. And that made a world of difference in my survivability.

It is really a pity that I could never get anywhere with trading. If you are aiming for longer term, beating the market is not too hard, but you'll never get anywhere with 3k which was my situation. I've seen good daytraders do it as a challenge, but they already had the necessary skills developed from before.

But that is a thing of the past now. If my AI plan succeeds, I will never have to trade again anyway, at least as far as financial instruments are concerned.

1:45pm. For the past 5 years I've been slowly building my skills. I've ignored all the shortcuts and did all the exercises.

I think I know a lot in both ML and PL now.

Just like with roguelikes, it is not that even back then I was too dumb to really beat them. It is just that I've been approaching ML too much like regular programming and am getting irate when I get pushback from reality.

Sure, I might not know how NNs and CFR really work. I cannot reason it out.

In the future there might be some genius who can do these things that I cannot.

1:50pm. But at a certain point one needs to accept that there are classes of algorithms that are simple, but due to various reasons (such as working in continuous spaces) cannot be reasoned about.

It is like - suppose I did not understand basic addition of natural numbers. Then I'd get obsessed about mastering the 'fundamentals', crack open the books and get down to the bare circuits.

I'd find that addition can be done in terms of various logic circuits and train myself to reason about them in those terms.

And yet, while this effort might gain me that reasoning ability, I would be no closer to understanding at all.

It is like a being with no visual system trying to understand geometry.

...

No doubt there are higher levels of intelligence not allowed to humans that might allow shedding light on these basic algorithms, but I can honestly say that I gave it my best shot. Whatever understanding I wanted to unearth with my 2019 effort is certainly beyond my ability.

Hard thinking and great effort will not suffice here.

1:55pm. Instead I will accept that what I had was simply naivety and change my approach to something more befitting of a person living in this era.

I'll think of ML as science itself, and think of backprop as nothing more than greedy improvement. I'll assume that there is no secret sauce - rather what I need to do is increase the roboustness of it in various domains through the sheer weight of my programming skill.

Forget about money and ML both.

What I need to do is find a way to make an expression of science. Reach the new perspective and grasp it. Program in a altogether different manner.

Leave the others to waste their time trying to fiddle with the rules and architectures.

This is in fact what I said I would do, but I got completely obsessed with gaining 'understanding'. The end result is that the more I learned, the further I move away from it.

2pm. If I moved with the assumption that algorithms were understanding itself - this is something that everybody deep down already believes and acts upon anyway, then I would have not gotten so completely lost. I would not have started crying blood and fuming fire as soon as I realized what a scam modern math is.

...

I really am deeply regretful that I could not realize all of this soon. Before I even stared work on ML, I should have realized that I do not really understand even much simpler algorithms that work in rational spaces. Had I done so, I would not have wasted so much time trying to drill a hole through reality.

I want redemption for this sin.

And the best way to do this is to dedicate myself wholly to programming once more.

This time, I won't ask for money, understanding, recognition or anything else from the art.

2:05pm. When I was a kid, I realized something deep in my core. At that point I did not know about the Singularity or anything like that, but I could tell - the era I lived in was special.

The games that can be played on the computer are special.

The world beyond the screen is the world the God only knows.

2:10pm. I need to do it. Failures and victories do not matter.

What is important is that I express my understanding of what understanding really is.

Do I really want to give up here and be a monkey for the rest of my life? Or do I want to act as one of the predecessors to the Inspired?

Things such as life can be left to lesser creatures. I have the boundary to break."

---
## [Aarqw12/kernel_daisy@fc3273781b...](https://github.com/Aarqw12/kernel_daisy/commit/fc3273781bf2a656f84fe957925ad1b322aa7b56)
##### 2020-01-18 16:11:22 by Nathan Chancellor

Merge 4.9.168 into kernel.lnx.4.9.r12-rel

Changes in 4.9.168: (91 commits)
        arm64: debug: Don't propagate UNKNOWN FAR into si_code for debug signals
        arm64: debug: Ensure debug handlers check triggering exception level
        ext4: cleanup bh release code in ext4_ind_remove_space()
        lib/int_sqrt: optimize initial value compute
        tty/serial: atmel: Add is_half_duplex helper
        tty/serial: atmel: RS485 HD w/DMA: enable RX after TX is stopped
        mm: mempolicy: make mbind() return -EIO when MPOL_MF_STRICT is specified
        i2c: core-smbus: prevent stack corruption on read I2C_BLOCK_DATA
        CIFS: fix POSIX lock leak and invalid ptr deref
        h8300: use cc-cross-prefix instead of hardcoding h8300-unknown-linux-
        tracing: kdb: Fix ftdump to not sleep
        gpio: gpio-omap: fix level interrupt idling
        include/linux/relay.h: fix percpu annotation in struct rchan
        sysctl: handle overflow for file-max
        enic: fix build warning without CONFIG_CPUMASK_OFFSTACK
        scsi: hisi_sas: Set PHY linkrate when disconnected
        mm/cma.c: cma_declare_contiguous: correct err handling
        mm/page_ext.c: fix an imbalance with kmemleak
        mm/vmalloc.c: fix kernel BUG at mm/vmalloc.c:512!
        mm/slab.c: kmemleak no scan alien caches
        ocfs2: fix a panic problem caused by o2cb_ctl
        f2fs: do not use mutex lock in atomic context
        fs/file.c: initialize init_files.resize_wait
        cifs: use correct format characters
        dm thin: add sanity checks to thin-pool and external snapshot creation
        cifs: Fix NULL pointer dereference of devname
        jbd2: fix invalid descriptor block checksum
        fs: fix guard_bio_eod to check for real EOD errors
        tools lib traceevent: Fix buffer overflow in arg_eval
        wil6210: check null pointer in _wil_cfg80211_merge_extra_ies
        crypto: crypto4xx - add missing of_node_put after of_device_is_available
        usb: chipidea: Grab the (legacy) USB PHY by phandle first
        scsi: core: replace GFP_ATOMIC with GFP_KERNEL in scsi_scan.c
        coresight: etm4x: Add support to enable ETMv4.2
        ARM: 8840/1: use a raw_spinlock_t in unwind
        iommu/io-pgtable-arm-v7s: Only kmemleak_ignore L2 tables
        mmc: omap: fix the maximum timeout setting
        e1000e: Fix -Wformat-truncation warnings
        mlxsw: spectrum: Avoid -Wformat-truncation warnings
        IB/mlx4: Increase the timeout for CM cache
        scsi: megaraid_sas: return error when create DMA pool failed
        perf test: Fix failure of 'evsel-tp-sched' test on s390
        SoC: imx-sgtl5000: add missing put_device()
        media: sh_veu: Correct return type for mem2mem buffer helpers
        media: s5p-jpeg: Correct return type for mem2mem buffer helpers
        media: s5p-g2d: Correct return type for mem2mem buffer helpers
        media: mx2_emmaprp: Correct return type for mem2mem buffer helpers
        vfs: fix preadv64v2 and pwritev64v2 compat syscalls with offset == -1
        HID: intel-ish-hid: avoid binding wrong ishtp_cl_device
        leds: lp55xx: fix null deref on firmware load failure
        iwlwifi: pcie: fix emergency path
        ACPI / video: Refactor and fix dmi_is_desktop()
        kprobes: Prohibit probing on bsearch()
        ARM: 8833/1: Ensure that NEON code always compiles with Clang
        ALSA: PCM: check if ops are defined before suspending PCM
        usb: f_fs: Avoid crash due to out-of-scope stack ptr access
        bcache: fix input overflow to cache set sysfs file io_error_halflife
        bcache: fix input overflow to sequential_cutoff
        bcache: improve sysfs_strtoul_clamp()
        genirq: Avoid summation loops for /proc/stat
        iw_cxgb4: fix srqidx leak during connection abort
        fbdev: fbmem: fix memory access if logo is bigger than the screen
        cdrom: Fix race condition in cdrom_sysctl_register
        e1000e: fix cyclic resets at link up with active tx
        ASoC: fsl-asoc-card: fix object reference leaks in fsl_asoc_card_probe
        efi/memattr: Don't bail on zero VA if it equals the region's PA
        ARM: dts: lpc32xx: Remove leading 0x and 0s from bindings notation
        soc: qcom: gsbi: Fix error handling in gsbi_probe()
        mt7601u: bump supported EEPROM version
        ARM: avoid Cortex-A9 livelock on tight dmb loops
        tty: increase the default flip buffer limit to 2*640K
        powerpc/pseries: Perform full re-add of CPU for topology update post-migration
        media: mt9m111: set initial frame size other than 0x0
        hwrng: virtio - Avoid repeated init of completion
        soc/tegra: fuse: Fix illegal free of IO base address
        HID: intel-ish: ipc: handle PIMR before ish_wakeup also clear PISR busy_clear bit
        hpet: Fix missing '=' character in the __setup() code of hpet_mmap_enable
        dmaengine: imx-dma: fix warning comparison of distinct pointer types
        dmaengine: qcom_hidma: assign channel cookie correctly
        netfilter: physdev: relax br_netfilter dependency
        media: s5p-jpeg: Check for fmt_ver_flag when doing fmt enumeration
        regulator: act8865: Fix act8600_sudcdc_voltage_ranges setting
        drm/nouveau: Stop using drm_crtc_force_disable
        x86/build: Specify elf_i386 linker emulation explicitly for i386 objects
        selinux: do not override context on context mounts
        wlcore: Fix memory leak in case wl12xx_fetch_firmware failure
        x86/build: Mark per-CPU symbols as absolute explicitly for LLD
        dmaengine: tegra: avoid overflow of byte tracking
        drm/dp/mst: Configure no_stop_bit correctly for remote i2c xfers
        ACPI / video: Extend chassis-type detection with a "Lunch Box" check
        Linux 4.9.168

Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>

Conflicts:
	drivers/hwtracing/coresight/coresight-etm4x.c
	drivers/usb/gadget/function/f_fs.c

---
## [Daodan317081/reshade-shaders-crosire@2879716bd1...](https://github.com/Daodan317081/reshade-shaders-crosire/commit/2879716bd19d88460c3afe4bde82e0801ba3d926)
##### 2020-01-18 19:58:55 by BlueSkyDefender

Update Depth3D.fx

Depth3D: Harden to help prevent cheating.                                                       -=let me know if I can do more=-

+Cursor Is now Bound to the 3D image only.
+Removed Depth Buffer Debug view to keep users from using this to cheat.
+Basic VR Compatibility added so that it can be used with VR Desktop apps.
+Basic Theater Mode added for Cellphone VR users.
+Renamed many items in the UI to help new users with controlling this shader.
+New Depth Detection Code.
+New Screen Boundary Detection added.
+Edge Handling added from SuperDepth3D

Fixed issues with the UI in Freestyle and automated many functions as I could to reduce UI clutter. The idea was to keep as much of the functionality as possible without sacrificing too much.

Fixed issues with NV System that was causing a black screen on my Testers PC. This was not easy, I had to remote into a user's pc halfway across the world since I needed someone with an NV Card that was able to help me. Seems like this may be an issue for me with porting my other shaders. Thank you, Durante - aka Dorinte & TheGordinho - aka Gordinho

The goal of Depth3D to allow users to experience the world of 3D Gaming by adding real depth to your game. -=This shader requires depth access to work=-

This shader will work with 3D TV, 3D Monitors, and the NEW VR HMDs. The VR Theater experience worth exploring in VR.
Here are two free applications for VR Headsets That I seen people use that anyone can try.
Virtual Space
https://store.steampowered.com/app/703480/Virtual_Space/
Big Screen
https://bigscreenvr.com/help/gettingstarted/sbs3d/

Older games and non-VR games can benefit from being played again with Stereo3D.
I have been working on My 3D Shader for some time now and learned how to improve things over time even with limitations. My hope is to share this experience with as many people as possible.
I would like this shader to be considered as a standard shader to be used as openly as NV discontinued 3DVision Stereo Software. Since I know many users in the 3D community still enjoy 3D gaming.

Please let me know if there is anything I can change. That can help. Thank you.

Noted Issues
One of the things that bug me with Depth3D It's hard to use for new users. So I will be making better tutorials when I have the hardware to do so for VR.

---

# [<](2020-01-17.md) 2020-01-18 [>](2020-01-19.md)

