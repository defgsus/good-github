# [<](2020-10-07.md) 2020-10-08 [>](2020-10-09.md)

2,814,227 events, 1,445,531 push events, 2,197,396 commit messages, 155,756,808 characters


## [telenornms/skogul](https://github.com/telenornms/skogul)@[1ee280ab53...](https://github.com/telenornms/skogul/commit/1ee280ab53e3614201d94359d1f5633165237d9e)
#### Thursday 2020-10-08 07:44:33 by Kristian Lyngstol

Add worker-pool logic to the UDP receiver

This was harder than I had anticipated to test.

I'm exposing both the backlog size/buffering of the channel, and the
number of threads to use. To get a meaningful test, I added the
sleep-sender into the test-loop. Without it, we're testing the wrong
thing.

It turns out the backlog size isn't horribly important, which shouldn't
have surprised me to be honest. We already HAVE a backlog between in the
kernel before our call to Read. Even a small buffer would help here, but
defaulting to 100 messages felt like a reasonable value.

Number of threads is really the key factor. Since each thread will be
carried over to a sender, if we don't use a batch-sender to decouple
receiver-go routines from sender-go routines, the receiver go routine
will block waiting for the sender to complete.  E.g.: A slow sender is
actually more detrimental than a slow receiver in this case.

PR attaches some performance data from test cases, pre- and post this
fix. This fix is not really meant to be a huge performance boost, but a
means to prevent the UDP receiver from killing the rest of Skogul if the
sender pipeline is slower than the amount of messages received. In the
past, this would just spawn endless go routines, now, instead, we will
simply drop packages as the receive buffer fills up.

Ideally we should check for that, but we didn't to begin with so this
isn't really a regression. And I'm honestly not sure if it's possible.

---
## [BayuAlam/VENOM-wt88047](https://github.com/BayuAlam/VENOM-wt88047)@[0c3286e281...](https://github.com/BayuAlam/VENOM-wt88047/commit/0c3286e28168150af103bb5002ff3700cfc8e755)
#### Thursday 2020-10-08 09:30:24 by Bayu Alam

fs/sync: Make sync() satisfy many requests with one invocation

Dave Jones reported RCU stalls, overly long hrtimer interrupts, and
amazingly long NMI handlers from a trinity-induced workload involving
lots of concurrent sync() calls (https://lkml.org/lkml/2013/7/23/369).
There are any number of things that one might do to make sync() behave
better under high levels of contention, but it is also the case that
multiple concurrent sync() system calls can be satisfied by a single
sys_sync() invocation.

Given that this situation is reminiscent of rcu_barrier(), this commit
applies the rcu_barrier() approach to sys_sync().  This approach uses
a global mutex and a sequence counter.  The mutex is held across the
sync() operation, which eliminates contention between concurrent sync()
operations.  The counter is incremented at the beginning and end of
each sync() operation, so that it is odd while a sync() operation is in
progress and even otherwise, just like sequence locks.

The code that used to be in sys_sync() is now in do_sync(), and
sys_sync()
now handles the concurrency.  The sys_sync() function first takes a
snapshot of the counter, then acquires the mutex, and then takes another
snapshot of the counter.  If the values of the two snapshots indicate
that
a full do_sync() executed during the mutex acquisition, the sys_sync()
function releases the mutex and returns ("Our work is done!").
Otherwise,
sys_sync() increments the counter, invokes do_sync(), and increments
the counter again.

This approach allows a single call to do_sync() to satisfy an
arbitrarily
large number of sync() system calls, which should eliminate issues due
to large numbers of concurrent invocations of the sync() system call.

Changes since v1 (https://lkml.org/lkml/2013/7/24/683):

o	Add a pair of memory barriers to keep the increments from
	bleeding into the do_sync() code.  (The failure probability
	is insanely low, but when you have several hundred million
	devices running Linux, you can expect several hundred instances
	of one-in-a-million failures.)

o	Actually CC some people who have experience in this area.

Reported-by: Dave Jones <davej@redhat.com>
Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Cc: Alexander Viro <viro@zeniv.linux.org.uk>
Cc: Christoph Hellwig <hch@lst.de>
Cc: Jan Kara <jack@suse.cz>
Cc: Curt Wohlgemuth <curtw@google.com>
Cc: Jens Axboe <jaxboe@fusionio.com>
Cc: linux-fsdevel@vger.kernel.org

Signed-off-by: Paul Reioux <reioux@gmail.com>

---
## [KDE/kstars](https://github.com/KDE/kstars)@[e78154841c...](https://github.com/KDE/kstars/commit/e78154841c7a55328cb9f1ab6a37cc8297cf2a2c)
#### Thursday 2020-10-08 09:59:11 by Robert Lancaster

This is a pretty big change.  It is the start of the integration of the StellarSolver Library that I created this spring.   Here are some of the changes that it includes:

Utilizes StellarSolver for both SEP and for Plate Solving.
Adds an Options Profile Editor for creating profiles for use with StellarSolver
Consolidates all the multitudes of options for plate solving and SEP in the Options Profiles.   These profiles are Parameters files that are shared between KStars and StellarSolver and can be used for both SEP and Plate Solving.
Usage of Stellarsolver allows users of all operating systems to plate solve without astrometry.net or an internet connection.  But you do need index files.
Usage of Stellarsolver allows Windows users to now use the index file downloader, directly plate solve using astrometry.net in either Cygwin or ANSVR, and have full control over plate solving.  ANSVR does not need to be "running" and the "online" option is no longer required for using ANSVR.  You can still use that if you like however.
All users can still make use of ASTAP, online Astrometry.net, local astrometry.net, sextractor, and other plate solving features.  I have added more options as well, such as using the internal sextractor with external ASTAP.  These all now share the same options profiles.
Usage of StellarSolver eliminates the need for any external configuration files for Sextractor or Astrometry.net.  You can use internal or external ones without the need for configuration files.
Using the parallel processor options in StellarSolver will allow blind plate solving for many images in just a few seconds instead of minutes.
Several structures are now shared between StellarSolver and KStars so that information can be sent back and forth between them.  These include the Options Profiles (Parameters), Statistic, WCS_Point, Background, and Star (for found sources).
The information contained within the above structures can be retained to add all sorts of new features to KStars for use in Photometry, image analysis, object analysis etc.  In the past, this information was lost as soon as the plate solve or SEP was done.
On the recommendation of Hy, dataType, which was formerly a property of the FITS Data object, is now incorporated into Statistic and I made changes throughout KStars to support this.
The Astrometry Parsers, the Options Maps, some of the settings, and some of the options editor windows have been eliminated because they were redundant, but a lot of the code is still there commented out in case some is still needed.  This needs more work.
For now, the "Remote" option was eliminated because I can't support that with StellarSolver because it depends on INDI to function and that would mean making StellarSolver INDI dependent.  The Remote option could be re-enabled if needed but I think that the fact that KStars can now plate solve on its own without the need for any astrometry.net installation makes it unnecessary.
I have not yet implemented StellarSolver for SEP in Guiding.  I think it will help there too, but I want to make sure that everything else works and check with Hy before I touch his great work there.  He already helped with Focus, but I haven't consulted him yet on Guiding, so it is not implemented there yet.
Load and Slew now loads the image it is trying to solve in the AlignView, and then blind solves with StellarSolver using the options that were selected.  Note that it ignores information in the file such as position or scale.  Load and slew is capable of solving many types of images
"Open FITS Image" in the file menu has been replaced with "Open Image" and is capable of loading any type of image QT can open as well as the FITS types it could before.  This was possible because of the function I had to add to get Load and Slew to work with StellarSolver.

What you can do:

Please check to make sure that this massive update did not break anything important.
Specifically, please verify that none of the code that I commented out or removed when I put StellarSolver in Align or SEP was actually needed after this update.  There was A LOT of code that I had to change and I could have easily messed something up.
Please test StellarSolver to make sure that it plate solves fine in the Align module, and that it works well in the Focus module for SEP in real life situations.  If there are any issues with it, please let me know.
Please help me create good Options Profiles.  StellarSolver comes with a default set of profiles, so there is no initial installation required.  If you change any of them or add any of your own, then it saves a file with the profiles in it. I believe that my profiles for plate solving are pretty good, but that doesn't mean they can't be improved upon.  I think that my profiles meant for SEP need some work.  I haven't gotten a lot of feedback on them so far.  You can easily change them and you can create your own options profiles.  I would like to provide a good default set for users.  If you find anything in the default set that you think would be better, or if you come up with a profile that you think works better for certain situations, I would like to include that, please send it to me.

Testing instructions:

First, please download and install StellarSolver.  Use the latest Master because I have been making some changes recently based on things that I encountered while integrating into KStars and I am still updating based on that.  Those changes are not in the latest release yet.  https://github.com/rlancaste/stellarsolver. You can use the install script if you like or you can build and install it manually.  For Windows and Mac testing, the repository includes a craft recipe for installing in Craft on Windows and Mac OS.
Then you can test the code in my merge request.  One option to test it would be to download the code in my fork and then build and install KStars normally.  Another would be to use the features in GitLab to download a patch and then apply it to a local copy of the latest KStars you have.  Since this is my first merge request since we moved to Gitlab, I don't know all the details of how to do the second option.

---
## [mosra/m.css](https://github.com/mosra/m.css)@[915ffcbed5...](https://github.com/mosra/m.css/commit/915ffcbed545b9c38f55fac07f33ef04433863a8)
#### Thursday 2020-10-08 11:22:55 by Vladimír Vondruš

package/ci: go back to Pelican 4.2 until I find a fix.

OH GOD, one can't just leave a project alone for 6 months because every
damn thing just breaks, changes or gets removed. Kids these days, FFS.

Imagine if the standard of electrical outlets changed rapidly every two
weeks, you'd just have to constantly buy new fucking adapters and you
would HATE it. So why is it COMPLETELY FINE with software?!

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[e6a2408b63...](https://github.com/mrakgr/The-Spiral-Language/commit/e6a2408b63889e4e520b0d2294952e0ae293f27b)
#### Thursday 2020-10-08 13:21:54 by Marko Grdinić

"11:30am. It seems I missed vol 12 of Dendro a few months ago. Right now I am reading vol 4 of the manga. Let me have fun for a while longer. I feel very rested right now. In contrast to yesterday where I am too mentally exhausted after a full session of programming, right now the feeling of immersion is as if I were a kid. It is nice.

12:05pm. I feel like programming now. My tension is going up, and I won't be satisfied with just leisure.

12:10pm. This level of skill is quite nice. If I were in high school again and had them, I would have dropped out and focused all of my efforts towards creating Spiral v0.2. The actual code would have taken me months so it is not just skill. Actually having the code present is in fact a real asset.

And back then I would not have had ZeroMQ and Hopac.

Those two libraries are major. Right now, I feel that as a programmer it is a major failing on my side that I do not know how they work under the hood. I looked at Hopac's source, but it is written in not just in C style, but C style with gotos for control flow. I definitely admire the two libraries and want to know everything about them, but making the time to study them is something I am going to have to do.

I treasure my foundations. I have something really strong right now, but it could be even firmer.

Nonetheless, since reincarnation is not a thing in real life, studying the two libraries is not a priority. I'll leave studying the libraries for when I am done dealing with neurochips.

There will be a transition point where I'll have the money, the chips, the language and the ML libraries. At that point I will be able to change my policy from things like dealing with sponsors and working on backends and libraries towards finally putting the agents to real work. I'll assault the online gambling dens and various other arenas in order to break into the >100k per month range in income.

But since I'll have money at that point, it is not like starting that quest right away is a priority. In the first place, why I want to do that is not because of cash, but because the skills necessary to do that in the first place are a prerequisite to an even higher level. Past crushing esports lies the real world.

At that point I'll become more creative. Maybe I'll even return to Simulacrum.

It will be a quiet poriod where my agents will not just be powerful, but sentient. It is the entry point to the Singularity.

12:25pm. Well, it is no rush. If I want to study the libraries I can do it whenever I want.

Another good point to do it would be when v0.2 is done, but before I move to interviewing with the potential sponsors. I can make time there. I do prioritize fundamental skills.

I have a low view of webdev, but the concurrency skills that support it are something I hold in high regard. So maybe I'll just go with that.

I'll take a break when I am done with v0.2 and as a reward, I'll study those two libraries for a while.

It is my indulgence since I do not really need to, but I hate not knowing how they work. Since Hopac and NetMQ are so important to me, I want to get into a position where I would be able to replicate their functionality on my own should I really want to.

Concurrency abstractions are important enough to warrant this.

I could do Rx on my own, but the other two not just yet.

It does not matter that I be able to replicate them efficiently on the level they are done now, what is really important that I understand how they work.

12:35pm. Yeah, I want to tackle this challenge. Otherwise my soul won't feel at ease. In the first half of the year, I wasted months trying to learn how to do editor support inefficiently. I can dedicate a month to just studying these two libraries.

I should get familiar with network programming as well. I like this subject.

12:40pm. I can get better yet as a programmer. I can't leave everything to my agents.

At some point the world will be uploaded and this repo will chronicle my journey.

The Inspired will go into the beyond, but whatever the outcome the era leading up to the Singularity will be stored and frozen - for the rest of eternity. And my journal and the Spiral language will remain there.

12:50pm. The great v0.2. I wonder what Spiral will look like when it is made by someone with 1,000,000x more brainpower than me?

Insects and animals should struggle until they break out. Forget leisure. Focus on the task at hand.

I've made my determination. I'll complete v0.2 and then I will full master the two concurrency libraries. I'll atone for my lack of respect towards concurrency, but the time to redeem myself is not now.

Right now, the priority is Spiral.

12:55pm. I've been indulging myself in the bliss of success, but I need to focus again.

The main priority is to get into it first.

1pm. Actually, I was quite into it, but now that I've signaled my desire to study those two libraries, I have an internal conflict that I need to buttress.

1:05pm. Let me take a short break here.

1:30pm. Done with the break and with chores as well.

Ok, let me start.

Today I spent a bit of my creative energy on upgrading the code generator. So today, what I will do is focus on breaking the ice with the build command.

The first order of business will be to introduce a command on the VS Code side. I'll make a simple command and have it print hello world as a starter. Let me do it. Let me start by looking at the samples.

```ts
// The module 'vscode' contains the VS Code extensibility API
// Import the module and reference it with the alias vscode in your code below
import * as vscode from 'vscode';

// this method is called when your extension is activated
// your extension is activated the very first time the command is executed
export function activate(context: vscode.ExtensionContext) {
 // Use the console to output diagnostic information (console.log) and errors (console.error)
 // This line of code will only be executed once when your extension is activated
 console.log('Congratulations, your extension "helloworld-sample" is now active!');

 // The command has been defined in the package.json file
 // Now provide the implementation of the command with registerCommand
 // The commandId parameter must match the command field in package.json
 let disposable = vscode.commands.registerCommand('extension.helloWorld', () => {
  // The code you place here will be executed every time your command is executed

  // Display a message box to the user
  vscode.window.showInformationMessage('Hello World!');
 });

 context.subscriptions.push(disposable);
}

// this method is called when your extension is deactivated
export function deactivate() {}
```

```
 "contributes": {
  "commands": [
   {
    "command": "extension.helloWorld",
    "title": "Hello World"
   }
  ]
 },
```

Let me go with this.

```ts
        languages.registerHoverProvider(spiralFilePattern,new SpiralHover()),
        commands.registerCommand("buildFile", () => {
            window.showInformationMessage('Hello World!');
        })
```

Let me put this in to start with.

```
  "commands": [
   {
    "command": "buildFile",
    "title": "Build File"
   }
  ],
```

Now how about I give this a try?

```
   {
    "command": "buildFile",
    "title": "Build File",
    "category": "Spiral"
   }
```

It works. Let me try adding a category.

Now the command shows up prefixed by `Spiral: `. Great.

Let me make the `buildFile` request.

```ts
const spiBuildFileReq = async (uri: string) => request({buildFile: {uri}})
```

Here is the request.

Damn it, how do I get the currently open document?

1:55pm.

```ts
        commands.registerCommand("buildFile", () => {
            window.visibleTextEditors.forEach(x => spiBuildFileReq(x.document.uri.toString()))
        })
```

I forgot that multiple documents can be open at the same time. It is fine if I do this.

Now let me do something on the F# side. I'll start out just by printing all the uris received.

```fs
            | BuildFile x ->
                printfn "%s" x.uri
                send_back null
```

I am refreshing my mind, but there is not much to do apart from this.

Let me try it out.

```
Server bound to: tcp://*:13805
file:///c%3A/Users/Marko/Source/Repos/The%20Spiral%20Language/VS%20Code%20Plugin/spiral/src/choice.spi
```

This is what gets printed when I use the Build File command on the VS Code side now.

Great.

Let me try using the uri to load the file and print it out in server.

Actually, whenever I use the Build File, I should save all the file first.

2:10pm.

```ts
        commands.registerCommand("buildFile", async () => {
            const c = await commands.getCommands()
            window.visibleTextEditors.forEach(x => spiBuildFileReq(x.document.uri.toString()))
        })
```

Let me just print all the commands first so I can see what Save All is called.

2:15pm. Ah, I can just save all the files directly. But instead let me transfer the text using the VS Code editor rather than risk syncing errors.

2:20pm. What am I thinking? I already have the files in the editor. All I need is the uri.

Let me have the F# server open a file with the same name, but a `.fs` extension.

2:30pm.

```fs
            | BuildFile x ->
                match IO.Path.GetExtension(x.uri) with
                | ".spi" | ".spir" -> IO.File.WriteAllText(IO.Path.ChangeExtension(x.uri,"fs"), "// Compiled with Spiral v0.2.")
                | _ -> ()
                send_back null
```

Let me do this.

.NET has a lot of helpers for dealing with paths.

Let me give it a try.

```
Server bound to: tcp://*:13805
Unhandled exception. System.IO.IOException: The filename, directory name, or volume label syntax is incorrect. : 'C:\Users\Marko\Source\Repos\The Spiral Language\The Spiral Language v0.2 (typechecking)\bin\Debug\netcoreapp3.1\file:\c%3A\Users\Marko\Source\Repos\The%20Spiral%20Language\VS%20Code%20Plugin\spiral\src\choice.fs'
   at System.IO.FileStream.ValidateFileHandle(SafeFileHandle fileHandle)
   at System.IO.FileStream.CreateFileOpenHandle(FileMode mode, FileShare share, FileOptions options)
```

How strange.

```
Server bound to: tcp://*:13805
file:///c%3A/Users/Marko/Source/Repos/The%20Spiral%20Language/VS%20Code%20Plugin/spiral/src/choice.fs
Unhandled exception. System.IO.IOException: The filename, directory name, or volume label syntax is incorrect. : 'C:\Users\Marko\Source\Repos\The Spiral Language\The Spiral Language v0.2 (typechecking)\bin\Debug\netcoreapp3.1\file:\c%3A\Users\Marko\Source\Repos\The%20Spiral%20Language\VS%20Code%20Plugin\spiral\src\choice.fs'
```

I am not sure.

I am not sure what to think. Maybe I need to parse the uri first?

```
Server bound to: tcp://*:13805
C:\Users\Marko\Source\Repos\The Spiral Language\The Spiral Language v0.2 (typechecking)\bin\Debug\netcoreapp3.1\file:\c%3A\Users\Marko\Source\Repos\The%20Spiral%20Language\VS%20Code%20Plugin\spiral\src\choice.fs
```

Why is it parsing the uri as this?

```
Server bound to: tcp://*:13805
/c%3A/Users/Marko/Source/Repos/The%20Spiral%20Language/VS%20Code%20Plugin/spiral/src/choice.fs
```

Yeah, I need to parse it using the Uri class first.

2:40pm.

```fs
            | BuildFile x ->
                let x = Uri(x.uri).AbsolutePath
                match IO.Path.GetExtension(x) with
                | ".spi" | ".spir" -> IO.File.WriteAllText(IO.Path.ChangeExtension(x,"fs"), "// Compiled with Spiral v0.2.")
                | _ -> ()
                send_back null
```

Let me give this a shot.

```
Server bound to: tcp://*:13805
Unhandled exception. System.IO.DirectoryNotFoundException: Could not find a part of the path 'C:\c%3A\Users\Marko\Source\Repos\The%20Spiral%20Language\VS%20Code%20Plugin\spiral\src\choice.fs'.
```

Ah, maybe I need to create the file first?

No the path is strange here. Notice that there are 2 Cs. It is telling me that it cannot find the directory.

2:45pm. https://docs.microsoft.com/en-us/dotnet/api/system.uri.localpath?view=netcore-3.1

Oh, `LocalPath` is a thing.

```
Server bound to: tcp://*:13805
Unhandled exception. System.IO.IOException: The filename, directory name, or volume label syntax is incorrect. : 'C:\c:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin\spiral\src\choice.fs'
   at System.IO.FileStream.ValidateFileHandle(SafeFileHandle fileHandle)
   at System.IO.FileStream.CreateFileOpenHandle(FileMode mode, FileShare share, FileOptions options)
```

Aghhhh... Why is the stupid thing getting parsed like this?

```
Returns a string representation of this Uri. The representation and normalization of a URI depends on the scheme.

The resulting string can be safely used with Uri.parse.
The resulting string shall not be used for display purposes.
Note that the implementation will encode aggressive which often leads to unexpected, but not incorrect, results. For instance, colons are encoded to %3A which might be unexpected in file-uri. Also & and = will be encoded which might be unexpected for http-uris. For stability reasons this cannot be changed anymore. If you suffer from too aggressive encoding you should use the skipEncoding-argument: uri.toString(true).
```

I tried passing the true to the function and now it works. I am going to have to change the way I am encoding uris.

```
Server bound to: tcp://*:13805
file:///c:/Users/Marko/Source/Repos/The Spiral Language/VS Code Plugin/spiral/src/choice.spi
```

This is what I get now with more relaxed encoding. Let me go with this.

Actually I see that on the .NET side there is a dontEscape optional argument. Maybe I can use this instead.

Ah, no that is not it. It is completely deprecated.

But still, isn't there something on the .NET side that can parse aggressively encoded URIs?

2:55pm. Forget this. Let me just use what works.

Actually, for the time being it would be better to just compile the individual files as `.fsx` files. Those don't need to be in a project.

3:05pm. It took me two hours, but I've mastered the particular trick of saving that comment to a `.fsx` file. Great.

3:15pm. I am thinking what comes next.

There is no way around it - the next step is to really tie things up. That means getting the prepass, the peval and the codegen to play with the rest of the system.

3:20pm. Let me stop for a bit here. I want to read Omega. And this is a natural spot to do it.

I've had a bunch of ideas regarding how compilation should work, but now it is time to refine then. I am going to just focus on getting the prepass server done.

One thing that is a mistery to me is how exactly I am going to make compilation roboust to changes. I think I should aim to do it differently than F# does."

---
## [zxlhhyccc/bf-package-master](https://github.com/zxlhhyccc/bf-package-master)@[f33a84f4de...](https://github.com/zxlhhyccc/bf-package-master/commit/f33a84f4de3d33b167be32be6192c4449f47320f)
#### Thursday 2020-10-08 14:41:56 by zxlhhyccc

proxychains-ng:experimental new feature: proxy_dns_daemon

since many users complain about issues with modern, ultracomplex
clusterfuck software such as chromium, nodejs, etc, i've reconsidered
one of my original ideas how to implement remote dns lookup support.
instead of having a background thread serving requests via a pipe,
the user manually starts a background daemon process before running
proxychains, and the two processes then communicate via UDP.
this requires much less hacks (like hooking of close() to prevent
pipes from getting closed) and doesn't need to call any async-signal
unsafe code like malloc(). this means it should be much more compatible
than the previous method, however it's not as practical and slightly
slower.

it's recommended that the proxychains4-daemon runs on localhost, and
if you use proxychains-ng a lot you might want to set ip up as a service
that starts on boot. a single proxychains4-daemon should theoretically
be able to serve many parallel proxychains4 instances, but this has not
yet been tested so far. it's also possible to run the daemon on other
computers, even over internet, but currently there is no error-checking/
timeout code at all; that means the UDP connection needs to be very
stable.

the library code used for the daemon sources are from my projects
libulz[0] and htab[1], and the server code is loosely based on
microsocks[2]. their licenses are all compatible with the GPL.
if not otherwise mentioned, they're released for this purpose under
the standard proxychains-ng license (see COPYING).

[0]: https://github.com/rofl0r/libulz
[1]: https://github.com/rofl0r/htab
[2]: https://github.com/rofl0r/microsocks

---
## [Winter-Contingency/Winter-Contingency](https://github.com/Winter-Contingency/Winter-Contingency)@[1e86f506f3...](https://github.com/Winter-Contingency/Winter-Contingency/commit/1e86f506f32d725c95710af7a910e8031be3208d)
#### Thursday 2020-10-08 14:50:55 by loafe

Merge pull request #35 from loafe/dev

God I fucking Hate my Life

---
## [generot/ExogenBreach](https://github.com/generot/ExogenBreach)@[69b53ff44f...](https://github.com/generot/ExogenBreach/commit/69b53ff44febb878e80a83e77308a1a32819404e)
#### Thursday 2020-10-08 15:44:04 by Anomalouss

Made a functional game in less than 10 minutes, why the FUCK did we not do this when it was the right time, I fucking hate myself, I'm such a fucking idiot, why am I so retarded.

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[8556d87ac9...](https://github.com/mrakgr/The-Spiral-Language/commit/8556d87ac9930546d448db57075051a1e46a116d)
#### Thursday 2020-10-08 16:45:11 by Marko Grdinić

"3:35pm.

```fs
    let file_message uri f =
        let tokenizer = tokenizer uri
        let parser = parser uri
        let typechecker = typechecker uri
        let hover,_ = hover uri

        Hopac.start (
            let res = IVar()
            Ch.give tokenizer (f res) >>=.
            IVar.read res >>= fun (blocks,tokenizer_errors) ->
            queue.Enqueue(TokenizerErrors {|uri=uri; errors=tokenizer_errors|})

            let req_tc = IVar()
            Ch.give typechecker req_tc >>=.
            Ch.give hover req_tc >>=.

            let res = IVar()
            Ch.give parser {blocks=blocks; res=res} >>=.
            IVar.read res >>= fun parser ->
            queue.Enqueue(ParserErrors {|errors=parser.errors; uri=uri|})

            let res = IVar()
            IVar.fill req_tc {bundles=parser.bundles; res=res} >>=.
            IVar.read res >>= fun tc ->
            tc |> Seq.foldJob (fun errors tc ->
                IVar.read tc >>- fun tc ->
                let errors = List.append tc.errors errors
                queue.Enqueue(TypeErrors {|errors=errors; uri=uri|})
                errors
                ) [] >>- ignore
            )
```

I've forgotten about this beastie. I even forgot that the parser does it all at once.

3:50pm. I am thinking whether I could trim some of the other servers. Nah.

4:05pm. I am taking a break here.

I've gotten a small taste of it. From what I can see, from here on out, all the challenge is essentially dealing with these servers. The prepass, and the rest of the phases I can all do as a single server.

That is what I will aim for now. Later on though, it will be more challenging once multiple files enter the picture.

Well, let me take a break for a while and I will see whether I feel like doing anything else for the day.

6pm. Done with lunch. I am still thinking about it.

By 'it' I mean packages. Within a single project the compilation is linear so dealing with it is not too difficult. But packages can be arranged in a graph like fashion.

6:05pm. This makes things greatly challenging to deal with straightforwardly.

6:10pm. Change of plans. I said I am going to focus on testing, but forget that. I'll leave it for last. What I am going to do right now is deal with compilation thoroughly. I'll light up the project files, bring in packages and multi file compilation.

I am starting to realize that there are certain rules and design guidelines I have to follow when doing concurrent programming.

What I have in mind right now seems fancy, but it is the most realistic way of dealing with things. It is not the usual kind of programming pattern that I am used to, but I'll have to make do with it.

6:25pm. Now I am really hung up on this. Whatever thoughts of leisure I previously had are firmly replaced by obsession now.

6:40pm. Let me just stop here as I've already started browsing on the side.

Tomorrow I will dedicate myself to dealing with project management. After that is done, the only thing left will be testing.

I said that I wanted to deepen my relationship with concurrency. Well, this work will be my chance.

It is time for the next step. Tomorrow, the onslaught will resume."

---
## [coreinfrastructure/best-practices-badge](https://github.com/coreinfrastructure/best-practices-badge)@[bed9776f57...](https://github.com/coreinfrastructure/best-practices-badge/commit/bed9776f57809984f4fd782d0c037ac29891c9ce)
#### Thursday 2020-10-08 17:29:06 by David A. Wheeler

Gemfile: Break rails gem into selected dependencies (#1483)

Modify the Gemfile so we *only* load the Rails gems we use.
This reduces our memory use and attack surface.

Previously in our Gemfile we just used this construct to load Rails:

> gem 'rails', '5.2.4.4' # Our web framework

However, that brings in some Rails gems that we never use.

Bringing in unused gems can be a problem.
[Rails issue #36963, "Rails 6.0.0 performance regression because of ActionText::Engine hook"](https://github.com/rails/rails/issues/36963)
discusses a serious problem when using Rails 6 - ActionText
takes a lot more memory (so much that many people have reverted
an upgrade). We don't plan to use ActionText anyway;
it fails to support markdown.
ActionText builds on Trix, and we have requested markdown suport
([Trix issue #626](https://github.com/basecamp/trix/issues/626))
and someone has developed a pull request for some markdown support
([Trix pull request #737](https://github.com/basecamp/trix/pull/737)).
But those patches have yet to be accepted, and it's not clear this
functionality will ever get added. Without that functionality,
ActionText is not useful for many people (not just us).
So we'd have a lot of pain (wasted memory) with no benefit (we can't
use ActionText anyway).

The StackOverflow discussion
[Rails 5.2.4: How to reduce RAM use?](https://stackoverflow.com/questions/59340237/rails-5-2-4-how-to-reduce-ram-use)
noted several ways to reduce RAM use, including:

> Another thing you can do is to only load the rails modules you
> actually use. On your config/application.rb file you'll see a
> line like this require 'rails/all' that loads all rails features
> https://github.com/rails/rails/blob/master/railties/lib/rails/all.rb
> You can change that line to only import the features you want, like if
> you don't use action_cable or active_job you can just import the rest.

However, while we *could* modify `config/application.rb`, it seems
much cleaner to list in the Gemfile *only* the gems we use.
Then we *know* we never loaded anything else.

A minor negative to this approach is that when we update our Gemfile we
have to update all these gem versions in sync. Since they're released
as a unit, it's possible that different rails gems won't work properly
together if they use different versions. This commit adds
comments to the Gemfile to specifically counter that potential problem.
As a result, I think that risk is very low.

Signed-off-by: David A. Wheeler <dwheeler@dwheeler.com>

---
## [Greece4ever/JavaScript-Websocket-Games](https://github.com/Greece4ever/JavaScript-Websocket-Games)@[04dd5aa8cc...](https://github.com/Greece4ever/JavaScript-Websocket-Games/commit/04dd5aa8ccc932a23f1b07d01e03265f966d4a8d)
#### Thursday 2020-10-08 18:08:33 by Leonidios

Merge pull request #4 from Greece4ever/backend

Fuck you github, do I have to merge every time i push

---
## [mlandgraf1/Mind-Expanding-Books](https://github.com/mlandgraf1/Mind-Expanding-Books)@[ca0d5be73d...](https://github.com/mlandgraf1/Mind-Expanding-Books/commit/ca0d5be73dc47d7a1d147e430e4326fbbe52cfb9)
#### Thursday 2020-10-08 18:53:13 by Maureen Landgraf

Added It by Stephen King

You can't have a list of horror books with Stephen King. 'It' will always be one of my favorite King books. The book effectively uses not only Pennywise as a horror element, but also real life threats such as Henry Bowers and his gang of school bullies and Bev's abusive father. While the book is plenty chilling, you also fall in love with the tight-knit group of misfits who are able to band together and defeat a monster that has existed since the beginning of time itself. The book is not only about monsters, but the power of friendship, which makes it so much more than your typical horror novel.

---
## [tybl/libvodka](https://github.com/tybl/libvodka)@[233746dd86...](https://github.com/tybl/libvodka/commit/233746dd8662e394b7aae47499820242d773f0bb)
#### Thursday 2020-10-08 19:34:20 by Timothy Lyons

Compile tests when root project

I thought I had a cool system to prevent accidentally enabling tests
as a subproject. It used a variable that I thought was set depending
on whether the current project name matched the root project name.

Well, the variable wasn't getting set correctly. Instead of evaluating
the current and root project names, it was just assigning the variable
names, as a semicolon separated list, to the variable. CMake said,
"if variable? Yup, looks like variable is set. Let do this thing!"
Well, it said that a lot. Because it was always set.

So, I fixed that by figuring out the correct way of conditionally
assigning a variable in CMake. Cool.

But it wasn't. Nothing changed. The variable was still assigned a
wonky value. I said I was importing this library into one of my own
libraries, right? Yeah, well guess what variable I set in that
project? That's right! The same one! Using the same broken logic!
I'll just go fix-- Wait a minute! My root project is setting the
variable my imported projects use to determine if they are root
projects?? That's not gunna work!

I have decided that user defined CMake variables are the devil and
shouldn't be used. Instead, I evaluate the current and root project
names at each location that is dependent.

That was a fun head scratcher!

I also had to guard the compiler_settings library and the FetchContent
for doctest behind the same check and figured I shouldn't force a
project to use linters if they didn't ask for them. After all that,
guess what the importing project gets? Just the library they asked
for! I will need to fill out the target features for the library
though.

Relevant Links:
- How to do conditional variable assignment with cmake?
(https://stackoverflow.com/questions/51320553)
- Issue 17: Compile tests when root project
(https://github.com/tybl/libvodka/issues/17)

---
## [papyri/idp.data](https://github.com/papyri/idp.data)@[fc046a9ccd...](https://github.com/papyri/idp.data/commit/fc046a9ccdc8389437a55b9926f772c5c13c80ef)
#### Thursday 2020-10-08 19:43:09 by James Cowey

Update revisionDesc

 - Submit - No mention of ll. 30-31 on the verso in the ed. pr. No mention in Chrest. Wilck. No mention in Verhoogt (Pap.Lugd.Bat. XXIX) nor in BL. Mention of the address in APIS. Therefore I inserted the reading based on the photo available online. (giuditta mirizio)
 - Vote - AcceptText - Very well observed and clearly an improvement. I wonder if one cannot read more: κωμογρ and ΜΕΓΧΕΙ (in caps) are on the same line, with Κερκεσιρ (no dots needed) below. Indented in a third line (the one below) I see something in a cursive script that appears to be ἐπέδωκα. This reminds me of P.Petaus 121, the writing exercise in which Petaus practices Πεταῦς κωμογραμματεὺς ἐπιδέδωκα ("I, Petaus, komogrammateus, have handed over") with varying degrees of success. Accordingly, the address ΜΕΓΧΕΙ could be written by a first hand (that of recto), whereas the mostly abbreviated part could be interpreted as [Μεγχῆς] / κωμογρ(αμματεὺς) / Κερκεοσίρ(εως) / ἐπέδωκα, which would point to a second hand, maybe that of Menches or one of his subordinates. (Thomas Backhuys)
 - General - The reading in lin30 κωμογρ Μεγχεῖ is clear. I would prefer to interpret this as κωμογρ(αμματεῖ) Μεγχεῖ. The reading in lin31 (Κερκεοσ̣ί̣(ρεως)) is less clear. Only the sigma is really clear. That the following iota is slightly crooked and so long is not that usual, but perhaps its length signifies abbreviation. All other letters have various grades of damage. The reading is certainly justifiable. I would tend to underdot the first six letters. The reading ἐπέδωκα is attractive. The delta is very small, the kappa large and somewhat stretched in its width. Is there ink after what would be a rather round alpha? If the reading is correct I would prefer to understand it as an individual indication of a bureaucratic act. What exactly would it mean in the context. That I find unclear. (James Cowey)
 - General - In an email from Demokritos Kaltsas I received the following comments "My impression of WChr. 233: I would be more or less happy with κωμογρ(αμματεῖ) | Κ̣ε̣ρ̣κεοσί̣(ρεως); the iota is indeed a bit strange, but this could be owing to the interference of the long tail of the rho above. The two words obviously belong together and should stand apart from Μεγχεῖ in the transcription, too. (so, three lines instead of two?)
As to ἐπέδωκα, I could see it with a lot of good will, but I think it is a mere impression from the cartonnage. If you use ‘flip horizontal’ in Preview the kappa becomes a ny. In any case, it is so off-centre that it can hardly belong to the two words preceding." (James Cowey)
 - Vote - AcceptText - I agree with D. Kaltsas and would treat the two lines of text on the left as separate from the name Μεγχεῖ. I also am not convinced of ἐπέδωκα. Perhaps record traces and note in the commentary that it could be an impression from the cartonnage, as suggested by Kaltsas? (Rodney Ast)
 - Finalized - Ready. I have attempted to summarise the editorial history of the voting in a commentary on line 31. (James Cowey)

---
## [broadinstitute/gatk](https://github.com/broadinstitute/gatk)@[c8ec73b2a9...](https://github.com/broadinstitute/gatk/commit/c8ec73b2a9eda94018c8a7960b7a2d30a01d5a0d)
#### Thursday 2020-10-08 19:55:52 by James

fist go at rebasing everything correctly, now have to deal with the buggy refactor code

Revert "Rewrote leftAlignIndels code -- it now always works, even for multiple indels (#6427)"

This reverts commit 8612288881890345d949b40ead5796183c52c08d. Which brakes the softclip aware cigar building code and has a horrible bug.

bringing modernity to this branch

added a consistency test for DRAGEN mode

a bunch of code cleanup

more cleanup and tests?

Added the matlab files as test

Strengthened trust in SoftclipBases Behavior for overlap base quality adjustments

Fixing a crash in alleleliklehoods and adjusting bq annotation after the quality for overlapping bases change

minor fix and expanded debug output

removing excess bases before hmm scoring seems to improve the resutlts at complex sites... the FRD issue is still relevant...

Fixed genotype posterior calculator to not honor the unphase permutation number factor

more bugfixes to read edge cases at sites with many low quality ends and low mapping quality combined

including reads that only overlap in hardclips (despite the obvious concerns with that)

fixing an unexpected error

flipping

hardclippingsoftclippedBases

creating a frankenbranch with a very intersting change involving realignment of reads and apparent DRAGEN behavior

fixing a bug in 'getAllVariationEvents()' code

flipping the values so it actually works

Changed BQD feather end calculation to account for indels in the read relative to the reference

reverted the non-realignment code

fixing the horrible regression I just wrought on the BQD calculations...

disabling bad debug squack

Parameterizing the option to use original base qualities

threading the truly original alignment through for BQD/FRD to match the bad dragen behavior we are now aware of

cleaning up the branch

if you like me then you should have put a pin in it

responding to reveiw comments, (still havent fixed the debug streams)

deleting a test that was for local files

fixing a silly bug

fixing the logic yet one more time

fixing an issue with the disable-spanning-event-genotyping code that was causing false positivles (or perhaps reasonable behavior)

jumping the gun on takutos fix because it was causing me trouble for this test

refactored the debug output to be something very simple that should/could/maybe is better handled by making a debug level in the debugger. Anyway this is fine for now and probably could be expanded to handle the other debug streams in HC

adding a scary error message to deter people from using contamination downsampling with BQD/FRD (it should be an exception but somebody would complain and it really isn't a sound thing to do unless the disqualified reads are included but they can't be inherently because they don't have scores)

partial val review response work checkpoint
:

responded to most comments for my code in val PR review

Fixed a couple of rare bugs in DRAGEN port code.
Now we can reads dragstr-params values out of gcloud storage directly.

taking the changes from valentin with the removed OQ tag

pulling out the --use-original-alignments-for-genotyping-overlap as if it were so many weeds in my garden.

Revision changes thus far

Fix issues with how we assign STR period and repeat lenghts to positions on a read.
Added more robust testing using Illumina's Matlab script to generate use cases.

Impose a maximum of 40 for the GOP in DRAGstr pairhmm as seem to be the case in DRAGEN.

Changes to meet recommendation except for the param estimation part that is on a separate branch

reverting a mistake where i replaced every every instance of the word 'exists' with 'isEnabled'

Unifying changes with the refactoring of the ReadsDataSource class

A few updates on the revision:

* Solved a couple of self-evaluation javadoc PR comments.
* Added a integration test for ComposeSTRTableFile based on the DRAGEN output on some of the mini references under src/test/resources/large
* Fix an issue that made ComposeSTRTableFile quite slow with reference with a very large number of contigs (eg. funcotator gencode trascripts ~100_000).
* Found and fixed a bug introduced during refactoring making EstimateDragstrParameters produce "garbage" if run in parallel.
* Silience a tedious debug message in SAMReaderQueryingIterator that comes out too often when the traversal intervals are just a few. Now it requires
          like a 1000+ intervals to produce that debug, might be still too low but it works in practice.

Solves james'  comment on DragstrParam toString deficiency

fixed a few dangling issues with the rebase

added the argument to end all arguments (and also completely remove the users ability to interact with our toolkit as they please)

Fixes bug reported by Cwhelan

Addresses some of James' comments and fixes a bug in a unit test for DragstrLocus class

Fixes Dragstr Read Str analyzer failing unit test

allowing nulls in the genotyper because its actually okay

Fixed a bug causing the wrong priors to be taken (indel's instead of snp's in snp sites)
Reverted a couple of changes in DragstrPairHMMInputScoreImputator to make it match better
the corresponding HC integration-tests

fixing a test i broke

adding a test for the easy dragen calling mode

Roll forward some of the rolled back changes Dragstr PairHMM code:

* Limit GOP to 40 max.
* byte-round rather than byte-truncate gop, gcp scores

I Address most outstanding review comments

Fix treatment of star alleles in use genotype posteriors for qual mode (#6856)

* use genotype posteriors for qual spanning deletion fix

---
## [pytorch/pytorch](https://github.com/pytorch/pytorch)@[365331997b...](https://github.com/pytorch/pytorch/commit/365331997bd7bd8a1adfa3d1c48a70778eacecf8)
#### Thursday 2020-10-08 22:11:05 by Edward Z. Yang

Rewrite implementation of faithful cpp signatures

This rewrite is as per my comments at https://github.com/pytorch/pytorch/pull/44087#issuecomment-701664506
I did the rewrite by reverting #44087 and then reimplementing it on top.
You may find it easier to review by diffing against master with only #44087
reverted.

There are two main ideas.

First, we now factor cpp argument processing into two phases operating
on three representations of data:

1. `FunctionSchema` - this is the source from native_functions.yaml
2. `Union[Argument, ThisArgument, TensorOptionsArgument]` - this is
   the arguments after doing some basic semantic analysis to group
   them (for TensorOptions) or identify the this argument (if this
   is a method).  There is only ever one of these per functions.
3. `Union[CppArgument, CppThisArgument, CppTensorOptionsArgument]` -
   this is the arguments after we've elaborated them to C++.  There
   may be multiple of these per actual C++ signature.

You can think of (2) as common processing, whereas (3) bakes in specific
assumptions about whether or not you have a faithful or non-faithful
signature.

Second, we now have CppSignature and CppSignatureGroup representing
the *total* public C++ API signature.  So those dataclasses are what
know how to render definitions/declarations, and you no longer have
to manually type it out in the Functions/TensorMethods codegen.

Here is an exhaustive accounting of the changes.

tools.codegen.api.types

- CppSignature and CppSignatureGroup got moved to tools.codegen.api.types
- Add new CppThisArgument and CppTensorOptionsArguments (modeled off
  of ThisArgument and TensorOptionsArguments) so that we can retain
  high level semantic structure even after elaborating terms with C++
  API information.  Once this is done, we can refine
  CppArgument.argument to no longer contain a ThisArgument (ThisArgument
  is always translated to CppThisArgument.  Note that this doesn't
  apply to TensorOptionsArguments, as those may be expanded or not
  expanded, and so you could get a single CppArgument for 'options')
- Add no_default() functional mutator to easily remove default arguments
  from CppArgument and friends
- Add an explicit_arguments() method to CppArgument and friends to
  extract (flat) argument list that must be explicitly written in the signature.
  This is everything except (Cpp)ThisArgument, and is also convenient
  when you don't care about the extra structure of
  CppTensorOptionsArguments

tools.codegen.api.cpp

- group_arguments is back, and it doesn't send things directly to a
  CppSignatureGroup; instead, it moves us from representation (1) to (2)
  (perhaps it should live in model).  Here I changed my mind from my
  PR comment; I discovered it was not necessary to do classification at
  grouping time, and it was simpler and easier to do it later.
- argument got split into argument_not_this/argument/argument_faithful.
  argument and argument_faithful are obvious enough what they do,
  and I needed argument_not_this as a more refined version of argument
  so that I could get the types to work out on TensorOptionsArguments

tools.codegen.api.dispatcher

- Here we start seeing the payoff.  The old version of this code had a
  "scatter" mode and a "gather" mode.  We don't need that anymore:
  cppargument_exprs is 100% type-directed via the passed in cpp
  arguments.  I am able to write the functions without any reference
  to use_c10_dispatcher

tools.codegen.gen

- Instead of having exprs_str and types_str functions, I moved these to
  live directly on CppSignature, since it seemed pretty logical.
- The actual codegen for TensorMethods/Functions is greatly simplified,
  since (1) all of the heavy lifting is now happening in
  CppSignature(Group) construction, and (2) I don't need to proxy one
  way or another, the new dispatcher translation code is able to handle
  both cases no problem.  There is a little faffing about with ordering
  to reduce the old and new diff which could be removed afterwards.

Here are codegen diffs.  For use_c10_dispatcher: full:

```
+// aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
 Tensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) {
-    return _cudnn_init_dropout_state(dropout, train, dropout_seed, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
+    static auto op = c10::Dispatcher::singleton()
+        .findSchemaOrThrow("aten::_cudnn_init_dropout_state", "")
+        .typed<Tensor (double, bool, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
+    return op.call(dropout, train, dropout_seed, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
 }
```

Otherwise:

```
+// aten::empty_meta(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
 Tensor empty_meta(IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format) {
-    return empty_meta(size, TensorOptions().dtype(dtype).layout(layout).device(device).pinned_memory(pin_memory), memory_format);
+    static auto op = c10::Dispatcher::singleton()
+        .findSchemaOrThrow("aten::empty_meta", "")
+        .typed<Tensor (IntArrayRef, const TensorOptions &, c10::optional<MemoryFormat>)>();
+    return op.call(size, TensorOptions().dtype(dtype).layout(layout).device(device).pinned_memory(pin_memory), memory_format);
 }
```

Things that I probably did not get right:

- The Union[Argument, TensorOptionsArguments, ThisArgument] and
  the Cpp variants are starting to get a little unwieldy.  Not sure if
  this means I should add a supertype (or at the very least an
  alias); in some cases I do purposely omit one of these from the Union
- Code may not necessarily live in the most logical files.  There isn't
  very much rhyme or reason to it.
- The fields on CppSignature.  They're not very well constrained and
  it will be better if people don't use them directly.
- Disambiguation.  We should do this properly in #44087 and we don't
  need special logic for deleting defaulting for faithful signatures;
  there is a more general story here.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

ghstack-source-id: 5db980cce350fb3f38c75088b7da1b0061b7c717
Pull Request resolved: https://github.com/pytorch/pytorch/pull/45890

---
## [awslabs/amazon-qldb-driver-rust](https://github.com/awslabs/amazon-qldb-driver-rust)@[19e9234700...](https://github.com/awslabs/amazon-qldb-driver-rust/commit/19e9234700a36dcb7535f740931092074afb605b)
#### Thursday 2020-10-08 23:00:31 by Marc Bowes

Add the driver

This commit implements the `QldbDriver` and supporting acts. Here's
a high-level overview of what we have:

1. QldbDriver can be built with the QldbDriverBuilder
2. The main API is `transact`, which takes in a function
3. The function has one argument: a Transaction, with it's own API
4. Transactions can execute statements or commit/abort

The driver currently only supports an async API, partially because
that's what Rusuto wants to do under the hood anyways. There is a
BlockingQldbDriver that simply blocks on any futures. However, this
driver still exposes the async-Transaction API. As future work, it'd
be great to have a sync-Transaction API to expose
easy-to-use (blocking) C bindings.

The transact API has some subtleties in the type signature that are
worth flagging. First, the param takes a `Fn(Transaction)` as opposed
to `FnOnce` or `FnMut`. This is because we need to be able to retry
transactions (making 'once' not suitable) and want to encourage
idempotent functions (making 'mut' not suitable). Users *can* work
around idempotency with interior mutability, but that is discouraged.

The next subtly worth calling out is that the Fn must return a special
type that is hard to construct manually. Instead, users use an API on
`Transaction` to commit/abort. These methods take the Transaction by
value, making it hard to incorrectly use the transact method (you need
to call one of them, and you can't call them twice).

The commit digest is a bit of a dance due to ownership. Initially I
tried to have the lambda arg be `&mut Transaction` such that executing
statements could mutate a commit digest in the Transaction. That was a
very sad path to go down, given current limitations in the type
system (notably: GAT support). What I settled on is a model where the
transaction accumulates the digest internally and sends it to the
driver on Drop (via a channel). Beacuse the disposition methods take
the tx by value, this has the neat property of immediately making the
digest available to the driver (which has just await'ed the user fn).

The driver interacts with the pooling and retry functionality to
acquire sessions and retry transactions. Hopefully it does it
correctly because there aren't any meaningful tests yet other than the
example included with this commit, the doctest and my own personal
testing. The reason there aren't yet tests is I'm still thinking
through how I want to go about it. (Spoiler: I don't want mocks.)

A couple of other misc. notes:

1. We only support getting values back as Ion binary
2. Pagination is automatic and blocking (there is no async 'read ahead')
3. The User-Agent includes Rust but it is prepended not appended [1]

1: https://github.com/rusoto/rusoto/pull/1838

---
## [hytech-racing/code-2020](https://github.com/hytech-racing/code-2020)@[5259a5fba3...](https://github.com/hytech-racing/code-2020/commit/5259a5fba3cc731d6f011ababbc5d78c855d2720)
#### Thursday 2020-10-08 23:26:32 by atulmerchia

I swear to god if one more stupid goddamn commit gets dropped I'm reinstalling my OS

---

# [<](2020-10-07.md) 2020-10-08 [>](2020-10-09.md)

