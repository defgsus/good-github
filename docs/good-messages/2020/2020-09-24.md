# [<](2020-09-23.md) 2020-09-24 [>](2020-09-25.md)

179,130 events, 92,689 push events, 141,746 commit messages, 10,437,828 characters


## [legodps/arc-article-pipeline](https://github.com/legodps/arc-article-pipeline)@[d136af4041...](https://github.com/legodps/arc-article-pipeline/commit/d136af404138df717cc49c96b26fba8a0e06901f)
#### Thursday 2020-09-24 22:06:17 by David Sander

[MAJOR] rename module, update config, add requirement versions

Been a couple weeks or so since I've made a last commit. Suffice to say, hard at work, just not on
things I could commit until today. I have managed to write up a script to parse through a particular
question file from the TQA question dataset. I'll be adding the code for that hopefully soon, though I
need to reorient with my professor to get a good idea of next steps.

This commit was mostly about some quality of life related stuff. Firstly, I noticed that running this
repeatedly, sucked. So I created a fallback for the `-f` configuration to look for a property in the
config file. So, priority to terminal command options, then to the config file, failing if neither
has a proper data filepath. This will allow me to test stuff more rapidly, which I'm happy about. In
addition I have renamed the module from pipeline to archiver. This more accurately represents what the
module does, and as such, could be run on its own. My ultimate plan would be to have something that
could end to end run the archiver and several other functions in succession. I have updated `readme.md`
with the configuration and updated running instructions.

Working with Elasticsearch I encountered a strange issue where my "index" commands were being rejected
due to some "read-only" property. Initially I thought this was because of a setting in Elasticsearch,
but further Googling determined that it was actually me not having enough disk space available. After
cleaning up my disk, and re-running, I had no issues. Another oddity I noticed, but is intermittent,
is that if I rerun the archiver with the same files, it might double (but no more if rerun past that)
the size of the index. I found no evidence that it was just duplicating information beyond the index
size, as the number of hits didn't seem to change. Since I can't really reproduce it, I'm keeping it
in mind if stuff comes up later.

Quick note, in my examination of the Elasticsearch documentation, I found out that the mapping
"doc type" is a deprecated function piece. I have removed it both from my configuration file and the
code to support this. I need to make some notes about what version of Elasticsearch I expect.

I raised test coverage to 100%! I had to make some alterations. I moved the Elasticsearch constructor
call to `__main__.py` as well as the importing of `elasticsearch.helpers.bulk`. I really tried to find
a way to mock those libraries but following the online documentation of the mocking library didn't help
nor did looking for my issues in particular. With this in mind, I think this solution is workable for
now and I can double back and fix this later when its not late in the evening and I have more patience.

Another important thing to note, I have added specific versions for the different libraries I use.
While earlier/later versions are likely to work, from my recent and horrible experience with another
library that did not version (they used specific git commits *shudder*) I never wish to inflict that
pain upon anyone else that would use my code. This way, the version is specified, so everyone knows.

---
## [legodps/arc-article-pipeline](https://github.com/legodps/arc-article-pipeline)@[6ae503ed8b...](https://github.com/legodps/arc-article-pipeline/commit/6ae503ed8bd71a6b5cd0037b09c63cf8e4b1bcf5)
#### Thursday 2020-09-24 22:06:17 by David Sander

Add `__main__.py` to allow for a terminal interface add property loading

With the experience of some of my other projects at work, doing the terminal interface was familiar
and not difficult (thankfully). This gives me a framework to expand this as time goes on. I also
added functionality to import configurations from a yaml configuration file. I debated between yaml
and json but since yaml doesn't require quite as much formatting, I decided to use yaml. I shifted
some of the hard coded values to the config file and I will try to get that tested when I next have
time to work on this (hopefully tomorrow, trying to put in ~an hour a day on this during the week).

I also added another unit/integration test, this time to test the loading of config yaml files.
Pretty simple stuff but it will help to have that later on. Doing my best to conform to the same
stringent work requirements surrounding python coding. Both for the best practices that they are
but so that others that might see this can understand it better. With the terminal interface now
open for use, and config files ready to use, I plan on working with the article parsing tomorrow
and digging into the format that I was given from a professor I'm working with. They are using
jsonl, which I'm kind of familiar with, part of me wishes it was just json though. Not sure how
best to handle the variety of formats people might use to provide articles outside of just me,
I'll probably look to that professor for some thoughts on that for ease of use. Ideally I would
avoid having an import per file type (e.g. .txt and .json and .jsonl, so on and so forth) but
we'll see what happens as I want to make this tool available for use by the group.

Tests passing, coverage raised but not 100% just yet. Will need to do some mocking to technically
reach that peak. Ideally once this system is set up I can add some larger integration and functional
tests to ensure it works properly at a larger scale.

---
## [legodps/arc-article-pipeline](https://github.com/legodps/arc-article-pipeline)@[c47138db53...](https://github.com/legodps/arc-article-pipeline/commit/c47138db530c394eb2eac1b49f3fb143036d79cd)
#### Thursday 2020-09-24 22:06:17 by David Sander

add in command to run ARC-Solver via benchmark and extract results from the run

Lot of progress here, really happy it went this smoothly. I thought it would be a royal pain to get
python to be able to communicate with ARC-Solver, but I figured it out. See ARC-Solver by default
uses Conda as an environment keeper. So I used `subprocess.run` in conjunction with the
`conda run` command and I was able to force conda to run the ARC-Solver script in the conda environment
that had been set up already. This is huge for me! To top it off, I was able to get the results from
the run of the script and get the number of correct and incorrect answered questions. We'll have to
see how things go, but maybe 10 more hours on this project will let me get to a place I can run the
benchmark and be confident in how it works. Definitely gonna need to unit test this, that'll be
difficult but doable.

Added some additional configs to benchmarkConfig.yaml, not all things make sense to enter via the
terminal, I'm considering removing those entirely so it is simple how to configure it, I'll think about
that some more. The next step is to set up the automation of running the index. I already have the
reset and copy test set functions all set up. There's some additional work I need to do to get
the articles associated with their files... that'll be a little sucky but I'll make it work.

Lacking a lot of coverage for arc_runner.py, I'll add it soon.

---
## [jtv/libpqxx](https://github.com/jtv/libpqxx)@[dde4eebc8c...](https://github.com/jtv/libpqxx/commit/dde4eebc8c97aa03e4295f9fe5962744bead7c4c)
#### Thursday 2020-09-24 23:22:11 by Jeroen T. Vermeulen

Deprecate `namedclass`.

I hate this class.  It has its use, sure, but it brought in too much
complexity â€” notably virtual inheritance!

The libpqxx 7.0 changes may have simplified the situation a bit, and I
now see just two, distinct (non-overlapping) use-cases:
 1. The `transaction` hierarchy.
 2. `transactionfocus` and its children.

In both these cases, `namedclass` exists to check for, and report,
mistakes such as "you tried to open a transaction while another one was
active on the same connection."  So two things: common base class for
polymorphism, and a human-readable name for an object which may or may
not have been given an individual name.

Let's see how far we can get if we separate these, and streamline them
individually.

For starters, I'm not at all convinced that it's really useful to have a
specific class name in the case of the transaction hierarchy.  As far as
the user is concerned, they're all "transaction."  And if the developer
wants more detail, they'll get far more from giving the transactions
helpful names.  There goes the vtable.  Just a `name()` on the
transaction will do.

I haven't looked at `transactionfocus` in as much detail yet.  It's the
case where we do want polymorphism that isn't already there.  So yeah, I
fear the vtable stays.  But virtual inheritance?  No, that no longer
looks necessary.  Right now it's only there (far as I can see right now)
because `subtransaction` is both a `transactionfocus` and a
`transaction_base`, which is a `namedclass` in its own right.  But if I
tear these two things apart, well, who cares?

And... a `std::string` for the class name?  A `std::string_view` will do
just fine, thank you.  That string is hardly going to change!  Yes, yes,
I know: another option would be to construct the full description right
when we construct the `transactionfocus`.  But who wants to pay that
cost all the time for something we'll use so rarely?

---

# [<](2020-09-23.md) 2020-09-24 [>](2020-09-25.md)

