# [<](2020-11-09.md) 2020-11-10 [>](2020-11-11.md)

2,953,102 events, 1,495,286 push events, 2,311,230 commit messages, 180,863,037 characters


## [swampservers/contrib](https://github.com/swampservers/contrib)@[489f909823...](https://github.com/swampservers/contrib/commit/489f90982374bd3e89bc593d27278c3648e928dd)
#### Tuesday 2020-11-10 02:03:53 by PYROTEKNIK

Laserpointer SWEP , BRRRAAPPPP SWEP, and helmet makes you corona-proof (#189)

* Redone all changes i guess

i fucked something up bad so here's everything again?

* Update weapon_laserpointer.lua

right click sound filtering so that it can't click out unless it's already been clicked in

* Update weapon_laserpointer.lua

fov_desired seems to fuck up so i'm using the fov input instead . It disconnects while the FOV is changing via suit zoom but it's better than randomly setting to the wrong FOV forever

* Update sv_vapeswep.lua

rare chance for vape to explode when exhaling

* Update weapon_laserpointer.lua

replaced drawsprite with drawquadeasy. beam ends now have random rotation for a flickery laser effect.
changed holdtype to knife

* Update weapon_laserpointer.lua

yoinked the hack for the vape to allow it to render on ponies

* Update weapon_laserpointer.lua

oops my hook wasn't being called correctly.

* Update weapon_laserpointer.lua

more visual tweaks

* Update weapon_laserpointer.lua

Requested changes, reformat.

* Update weapon_laserpointer.lua

holy shit hahaha what is this formatting

Co-authored-by: PYROTEKNIK <explosionjuice@gmail.com>

---
## [cryptoseb/cryptoseb.pw](https://github.com/cryptoseb/cryptoseb.pw)@[dd8d50f452...](https://github.com/cryptoseb/cryptoseb.pw/commit/dd8d50f4528a54650fcb451f503d53f412493a2c)
#### Tuesday 2020-11-10 05:20:52 by Crypto | Seb

Add files via upload

Working on coding a new website in html/css my hand. I'm not at all experienced, so this is pretty fun (and sometimes frustrating to all hell) to do. I'll post my progress here every once and a while. Taking any and all advice :)

---
## [dlama001/lab6](https://github.com/dlama001/lab6)@[0f413a96b5...](https://github.com/dlama001/lab6/commit/0f413a96b518b6e0a7f49729bef2b5de52f4c9ee)
#### Tuesday 2020-11-10 05:39:10 by dlama001

Update README.md

# lab6
---
title: "lab 6"
author: "dawa lama"
date: "11/2/2020"
output: html_document
---
```{r}
load("C:/Users/singh/OneDrive/Desktop/project new/lab 6/acs2017_ny_data.RData")
attach(acs2017_ny)
```
In this lab-6.As mention in question We are using logit and probit models. These are suited for when the dependent y variable takes values of just 0 or 1. We will look at labor force participation, which is an important aspect of the economy – who chooses to work (where those who are unemployed but looking for work are considered part of the labor force).

I am interested in examine the labor patterns and looking for job within the female population.It will be focused on that might affect women’s participation in the labor force that generally would not affect men, I will check is there lies the biasness or not.  Does number of children(MCHILD), marital status(MARST)and other miscellaneous factors to female effect on for force participation of female population. I set me minimum age of any natural person is 18 and legally they can be entered in laborforce when they turn 18 years so, we’ll restrict our data set so to include only females aged 18+.So, the age group 0-25 will cover the my both group of age 0-17 and 18-25 as 0-25.In my data set I run  This population of females aged 18+ has a total number of observations equal to 82755 with 109 variables.



```{r}
use_varb <- (AGE >= 18) & (female == 1) 
dat_use <- subset(acs2017_ny,use_varb) 
detach()
attach(dat_use)
```

Let create the data to use:

````{r }
dat_use$LABFORCE <- as.factor(dat_use$LABFORCE)
levels(dat_use$LABFORCE) <- c("Not in LF","in LF", "N/A")

dat_use$MARST <- as.factor(dat_use$MARST)
levels(dat_use$MARST) <- c("married spouse present","married spouse absent","separated","divorced","widowed","never married")
```


What is the difference between “NA” as label and Not in the Labor Force? Make sure you understand. (Hint, look at ages in each group).

In general it is a good idea to check summary stats before doing fancier models. What fraction of people, say, 55-65, are in the labor force? What about other age ranges? What would you guess are other important predictors? For example,
Ans: As a calculated below, yes age 55-65 

```{r}
library(kableExtra)
```

```{r}
dat_use$age_bands <- cut(dat_use$AGE,breaks=c(0,25,35,45,55,65,100)) #This piece of code allows to divide the variable into continues data form.
lf <- table(dat_use$age_bands,dat_use$LABFORCE) #Creates a variable containing a table of participation in the labor force by age group.

#The below piece of code creates a nice-looking table out of lf variable we created:
lf %>% 
  kbl() %>%
  kable_styling()
```
```{r}
#Here is a graph for visually showing participation in the labor force by age group:
plot(lf, main="Labor Force Participation by Age Group",
xlab="Age Group",
ylab="Participation", 
col = c("red", "blue"))
```
Since in USA people start the work after 18 because legally they are eligible to work but exactly I am not sure under 25 years female looking for job or not so I have include the female under 25(age 18-25 are working in USA) in my analysis and I found 6193 females is in Labor force in my data. In my the table above we see that the female population between 25-35 has a higher participation number in Laborforce and  above 65 has a lower participation rate than other age groups. we can see on table below
Age_group	   Not_in_LF	in LF   	N/A
(0,25]	       3778	      6193	  0
(25,35]	       2456      	10070	  0
(35,45]	       2699	      9168   	0
(45,55]	       3245	      10739	  0
(55,65]      	 6047	      8980	  0
(65,100]    	16751	      2629	  0



Now let use a Baseline model to creat the logistic regression
```{r}
model_logit1 <- glm(LABFORCE ~ AGE + MARST + NCHILD,
            family = binomial, data = dat_use)
summary(model_logit1)
library(stargazer)
stargazer(model_logit1, type="text") #Let's make it prettier and easier to interpret. 

```
In this my regression analysis we can see that all the valriables are statistically significance but  'separated' marital status is statistically insignificance. Yes there is relationship with labor force status to Marital status and no of child have by female population whose age is between 25-55.


```{r}

dat_use$LABFORCE <- droplevels(dat_use$LABFORCE)

NNobs <- length(dat_use$LABFORCE)
set.seed(12345) # just so you can replicate and get same "random" choices
graph_obs <- (runif(NNobs) < 0.1) # so something like just 1/10 as many obs
dat_graph <-subset(dat_use,graph_obs)  

 plot(LABFORCE ~ jitter(AGE, factor = 2), pch = 16, ylim = c(0,1), data = dat_graph, main = "Labor Force Participation by Age", xlab = "Age", ylab = "Labor Force Status", col = c("green","red"))


to_be_predicted <- data.frame(AGE = 25:55, MARST = "never married", NCHILD = 1)
to_be_predicted$yhat <- predict(model_logit1, newdata = to_be_predicted)

lines(yhat ~ AGE, data = to_be_predicted)
```
IN this graph we can see age group start from 15 actually that include the 18-25 year female population in labor force. In this graph we can see that some female who is over 85years old and still in labor force and working.


```{r}
#Transform "unmarried" variable into a factor:
dat_use$unmarried <- as.factor(dat_use$unmarried)
levels(dat_use$unmarried) <- c("Married","Unmarried")


model_logit2 <- glm(LABFORCE ~ AGE + unmarried + NCHILD,
            family = binomial, data = dat_use)

stargazer(model_logit2, type="text") #Let's make it prettier and easier to interpret
```
In above analysis we can see that all variables are statistically significance with '***' with 99% confidence interval even though there P value of NCHILD is higher than other its means females who have child is more likely less in laborforce. In more brief we can say that there is female with child is almost zero participation in labor force.

In model_logit3 I am going to do regression of Age, male, AFAm, Asian, hispanic, educ_HS, educ_college, marrital status, mortgage, NCHILD  is  a regression with age and age-squared with other variables

```{r}
model_logit3 <- glm(LABFORCE ~ AGE+ I(AGE^2)+female,
            family = binomial, data = dat_use)
summary(model_logit3)
```
```{r}
stargazer(model_logit3, type="text") #Let's make it prettier and easier to interpret
```
Here, we can see all variables are significance, the P value of age is more than Age square so Age is have lower participate in workforce than work square.

Now I am going to check that  sinlge white woman with college education and child in the Laborforce

```{r}
model_logit4 <- glm(LABFORCE ~ AGE + I(AGE^2) + female + AfAm + Asian  + Hispanic 
            + educ_hs + educ_somecoll + educ_college + educ_advdeg 
            + MARST + MORTGAGE+ NCHILD,
            family = binomial, data = dat_use)
summary(model_logit3)
to_be_predicted1 <- data.frame(AGE = 18:65, female = 1,white = 1, Hispanic = 0, AfAm =0, educ_hs = 0, educ_college = 1, educ_advdeg = 0 , NCHILD=1, MARST = "separated","divorced","widowed","never married" , MORTGAGE = "Yes, mortgaged/ deed of trust or similar debt" , Asian = 0 , educ_somecoll = 0)
```
In overall regression analysis we found that single female(separated from husband or single mother) with child is less participate and almost zero participate in laborforce.In above analysis we can see that all variables are statistically significance with '***' with 99% confidence interval even though there P value of NCHILD is higher than other its means females who have child is more likely less in laborforce. In more brief we can say that there is female with child is almost zero participation in labor force.

---
## [dlama001/lab6](https://github.com/dlama001/lab6)@[46d4e0621d...](https://github.com/dlama001/lab6/commit/46d4e0621d6defcaed8ebf78c5103c448bd2ea33)
#### Tuesday 2020-11-10 06:04:32 by dlama001

Now let use a Baseline model to creat the logistic regression  model_logit1 <- glm(LABFORCE ~ AGE + MARST + NCHILD,             family = binomial, data = dat_use) summary(model_logit1) ##  ## Call: ## glm(formula = LABFORCE ~ AGE + MARST + NCHILD, family = binomial,  ##     data = dat_use) ##  ## Deviance Residuals:  ##     Min       1Q   Median       3Q      Max   ## -2.2531  -0.9898   0.6414   0.9077   2.3304   ##  ## Coefficients: ##                              Estimate Std. Error z value Pr(>|z|)     ## (Intercept)                 3.0963820  0.0360046  86.000  < 2e-16 *** ## AGE                        -0.0510863  0.0005743 -88.959  < 2e-16 *** ## MARSTmarried spouse absent -0.3752237  0.0489033  -7.673 1.68e-14 *** ## MARSTseparated              0.0822411  0.0501306   1.641    0.101     ## MARSTdivorced               0.2791721  0.0257539  10.840  < 2e-16 *** ## MARSTwidowed               -0.8899649  0.0335031 -26.564  < 2e-16 *** ## MARSTnever married         -0.5237385  0.0223571 -23.426  < 2e-16 *** ## NCHILD                      0.0423664  0.0082909   5.110 3.22e-07 *** ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## (Dispersion parameter for binomial family taken to be 1) ##  ##     Null deviance: 112734  on 82754  degrees of freedom ## Residual deviance:  95913  on 82747  degrees of freedom ## AIC: 95929 ##  ## Number of Fisher Scoring iterations: 4 library(stargazer) ##  ## Please cite as: ##  Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. ##  R package version 5.2.2. https://CRAN.R-project.org/package=stargazer stargazer(model_logit1, type="text") #Let's make it prettier and easier to interpret.  ##  ## ====================================================== ##                                Dependent variable:     ##                            --------------------------- ##                                     LABFORCE           ## ------------------------------------------------------ ## AGE                                 -0.051***          ##                                      (0.001)           ##                                                        ## MARSTmarried spouse absent          -0.375***          ##                                      (0.049)           ##                                                        ## MARSTseparated                        0.082            ##                                      (0.050)           ##                                                        ## MARSTdivorced                       0.279***           ##                                      (0.026)           ##                                                        ## MARSTwidowed                        -0.890***          ##                                      (0.034)           ##                                                        ## MARSTnever married                  -0.524***          ##                                      (0.022)           ##                                                        ## NCHILD                              0.042***           ##                                      (0.008)           ##                                                        ## Constant                            3.096***           ##                                      (0.036)           ##                                                        ## ------------------------------------------------------ ## Observations                         82,755            ## Log Likelihood                     -47,956.540         ## Akaike Inf. Crit.                  95,929.080          ## ====================================================== ## Note:                      *p<0.1; **p<0.05; ***p<0.01 In this my regression analysis we can see that all the valriables are statistically significance but ‘separated’ marital status is statistically insignificance. Yes there is relationship with labor force status to Marital status and no of child have by female population whose age is between 25-55.  dat_use$LABFORCE <- droplevels(dat_use$LABFORCE)  NNobs <- length(dat_use$LABFORCE) set.seed(12345) # just so you can replicate and get same "random" choices graph_obs <- (runif(NNobs) < 0.1) # so something like just 1/10 as many obs dat_graph <-subset(dat_use,graph_obs)     plot(LABFORCE ~ jitter(AGE, factor = 2), pch = 16, ylim = c(0,1), data = dat_graph, main = "Labor Force Participation by Age", xlab = "Age", ylab = "Labor Force Status", col = c("green","red"))   to_be_predicted <- data.frame(AGE = 25:55, MARST = "never married", NCHILD = 1) to_be_predicted$yhat <- predict(model_logit1, newdata = to_be_predicted)  lines(yhat ~ AGE, data = to_be_predicted)  IN this graph we can see age group start from 15 actually that include the 18-25 year female population in labor force. In this graph we can see that some female who is over 85years old and still in labor force and working.  #Transform "unmarried" variable into a factor: dat_use$unmarried <- as.factor(dat_use$unmarried) levels(dat_use$unmarried) <- c("Married","Unmarried")   model_logit2 <- glm(LABFORCE ~ AGE + unmarried + NCHILD,             family = binomial, data = dat_use)  stargazer(model_logit2, type="text") #Let's make it prettier and easier to interpret ##  ## ============================================== ##                        Dependent variable:     ##                    --------------------------- ##                             LABFORCE           ## ---------------------------------------------- ## AGE                         -0.056***          ##                              (0.001)           ##                                                ## unmarriedUnmarried          -0.549***          ##                              (0.022)           ##                                                ## NCHILD                      0.043***           ##                              (0.008)           ##                                                ## Constant                    3.287***           ##                              (0.035)           ##                                                ## ---------------------------------------------- ## Observations                 82,755            ## Log Likelihood             -48,509.970         ## Akaike Inf. Crit.          97,027.940          ## ============================================== ## Note:              *p<0.1; **p<0.05; ***p<0.01 In above analysis we can see that all variables are statistically significance with ’***’ with 99% confidence interval even though there P value of NCHILD is higher than other its means females who have child is more likely less in laborforce. In more brief we can say that there is female with child is almost zero participation in labor force.  In model_logit3 I am going to do regression of Age, male, AFAm, Asian, hispanic, educ_HS, educ_college, marrital status, mortgage, NCHILD is a regression with age and age-squared with other variables  model_logit3 <- glm(LABFORCE ~ AGE+ I(AGE^2)+female,             family = binomial, data = dat_use) summary(model_logit3) ##  ## Call: ## glm(formula = LABFORCE ~ AGE + I(AGE^2) + female, family = binomial,  ##     data = dat_use) ##  ## Deviance Residuals:  ##     Min       1Q   Median       3Q      Max   ## -1.8554  -0.8690   0.6361   0.8017   3.8234   ##  ## Coefficients: (1 not defined because of singularities) ##               Estimate Std. Error z value Pr(>|z|)     ## (Intercept) -3.076e+00  6.232e-02  -49.36   <2e-16 *** ## AGE          2.310e-01  2.967e-03   77.88   <2e-16 *** ## I(AGE^2)    -2.901e-03  3.205e-05  -90.53   <2e-16 *** ## female              NA         NA      NA       NA     ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## (Dispersion parameter for binomial family taken to be 1) ##  ##     Null deviance: 112734  on 82754  degrees of freedom ## Residual deviance:  87126  on 82752  degrees of freedom ## AIC: 87132 ##  ## Number of Fisher Scoring iterations: 5 stargazer(model_logit3, type="text") #Let's make it prettier and easier to interpret ##  ## ============================================= ##                       Dependent variable:     ##                   --------------------------- ##                            LABFORCE           ## --------------------------------------------- ## AGE                        0.231***           ##                             (0.003)           ##                                               ## I(AGE2)                    -0.003***          ##                            (0.00003)          ##                                               ## female                                        ##                                               ##                                               ## Constant                   -3.076***          ##                             (0.062)           ##                                               ## --------------------------------------------- ## Observations                82,755            ## Log Likelihood            -43,562.980         ## Akaike Inf. Crit.         87,131.960          ## ============================================= ## Note:             *p<0.1; **p<0.05; ***p<0.01 Here, we can see all variables are significance, the P value of age is more than Age square so Age is have lower participate in workforce than work square.  Now I am going to check that sinlge white woman with college education and child in the Laborforce  model_logit4 <- glm(LABFORCE ~ AGE + I(AGE^2) + female + AfAm + Asian  + Hispanic              + educ_hs + educ_somecoll + educ_college + educ_advdeg              + MARST + MORTGAGE+ NCHILD,             family = binomial, data = dat_use) summary(model_logit3) ##  ## Call: ## glm(formula = LABFORCE ~ AGE + I(AGE^2) + female, family = binomial,  ##     data = dat_use) ##  ## Deviance Residuals:  ##     Min       1Q   Median       3Q      Max   ## -1.8554  -0.8690   0.6361   0.8017   3.8234   ##  ## Coefficients: (1 not defined because of singularities) ##               Estimate Std. Error z value Pr(>|z|)     ## (Intercept) -3.076e+00  6.232e-02  -49.36   <2e-16 *** ## AGE          2.310e-01  2.967e-03   77.88   <2e-16 *** ## I(AGE^2)    -2.901e-03  3.205e-05  -90.53   <2e-16 *** ## female              NA         NA      NA       NA     ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## (Dispersion parameter for binomial family taken to be 1) ##  ##     Null deviance: 112734  on 82754  degrees of freedom ## Residual deviance:  87126  on 82752  degrees of freedom ## AIC: 87132 ##  ## Number of Fisher Scoring iterations: 5 to_be_predicted1 <- data.frame(AGE = 18:65, female = 1,white = 1, Hispanic = 0, AfAm =0, educ_hs = 0, educ_college = 1, educ_advdeg = 0 , NCHILD=1, MARST = "separated","divorced","widowed","never married" , MORTGAGE = "Yes, mortgaged/ deed of trust or similar debt" , Asian = 0 , educ_somecoll = 0) In overall regression analysis we found that single female(separated from husband or single mother) with child is less participate and almost zero participate in laborforce.In above analysis we can see that all variables are statistically significance with ’***’ with 99% confidence interval even though there P value of NCHILD is higher than other its means females who have child is more likely less in laborforce. In more brief we can say that there is female with child is almost zero participation in labor force.

---
## [Aurorastation/Aurora.3](https://github.com/Aurorastation/Aurora.3)@[dd0b88a3ef...](https://github.com/Aurorastation/Aurora.3/commit/dd0b88a3ef3b9c60e8e846481fba49d606281543)
#### Tuesday 2020-11-10 06:14:50 by Geeves

Vampire Tweaks (#10476)


    Vampires with victim awareness enabled will no longer drop their victims to the floor.
    Stunned people can no longer interact with radios. This means that you cannot use a radio while being bloodsucked.
    Clicking the drain blood ability whilst draining blood will now cause you to stop sucking blood at the end of the next drain cycle.
    Gaining a vampire power is now displayed a bit nicer in the chat log.
    Vampires now have a UI element on the HUD for how much usable blood they have, what their frenzy counter is at, and to suck people.
    Removing vampire status from a player now removes their vampire abilities and such.
    Vampires now can only suck up until 950 units of usable blood.
    Vampire glare now stuns IPCs and cyborgs as well.
    Hulks no longer have funky punctuation.
    Veil walking now uses your mob sprite instead of a generic ghost sprite.
    Triggering veil walking now breaks all grabs on you.
    The vampire help menu has been revamped to be more up to date and stylish.

---
## [khrm/triggers](https://github.com/khrm/triggers)@[bc2eccf35e...](https://github.com/khrm/triggers/commit/bc2eccf35e939206ce7c0d469149e7850bbfc801)
#### Tuesday 2020-11-10 06:45:17 by Christie Wilson

Add @dibyom as an OWNER! 👑

@dibyom has been working closely with the triggers team for a while now!
He has reviewed 29 PRs to date
(https://tekton.devstats.cd.foundation/d/46/pr-reviews-by-contributor?orgId=1&var-period=d7&var-repo_name=tektoncd%2Ftriggers&var-reviewers="dibyom")
which is 1 short of our OWNER requirements at https://github.com/tektoncd/community/blob/master/process.md#owners
but actually represents nearly 50% of the total PRs so in my opinion
this is close enough!

His first review was ~Sept 4, and the project started on ~June 22, which
makes the project ~3 months old. Our policy
(https://github.com/tektoncd/community/blob/master/process.md#owners)
is that you should be involved in the project for 3 months or half the
lifespan which also makes us a bit short on that mark - happy to wait a
bit longer if folks want to, however I feel like @dibyom's contribution
has been significant and the project itself is so new that I think 1
month vs 1.5 months isn't a huge difference.

---
## [brenns10/cbot](https://github.com/brenns10/cbot)@[2f5fb62a6d...](https://github.com/brenns10/cbot/commit/2f5fb62a6da029809fe7168685e5bf192e4f3198)
#### Tuesday 2020-11-10 07:50:35 by Stephen Brennan

Implement a configurable reply plugin

Rather than writing custom C code every time you want to implement some
replies, might as well shove the replies into configuration files. This
implementation allows you to specify one or several responses (randomly
selected), whether the bot should be addressed or not, and the regex
trigger.

To ensure that the configurations are flexible, they should be able to
specify responses which include the user who sent the message, as well
as other contextual strings. The response also should be able to include
captured strings from the regex. To do this, we implement a string
formatter with syntax similar to Python's f-strings.

This can supersede the functionality of magic8, lod, and sadness
plugins, so they are removed. This implements #17 and inadvertently
closes #13 by deleting that code. Sorry @angelolicastro.

---
## [mysteriouspants/mysteriousbot](https://github.com/mysteriouspants/mysteriousbot)@[888a81a7a6...](https://github.com/mysteriouspants/mysteriousbot/commit/888a81a7a6b1a8a198f448467eafc925a9218962)
#### Tuesday 2020-11-10 08:07:25 by Christopher R. Miller

Migrate to async Serenity 0.9.1, Dockerized build

Migrates to Serenity 0.9.1 to take advantage of async. For as low
traffic as iDevGames experiences it's well and truly overkill, but it's
fun nonetheless.

Removes the VerbalMoralityHandler. It was unused, I was late in getting
the configuration in repo to a state consistent with what was actually
running.

Dockerizes the build process. Did you know that if I build on my Pop!_OS
20.10 machine and put that binary on my Ubuntu 20.04 server it does not
   work? Indeed, the glibc version on my laptop is newer and named
differently from the one on the server. This is a symptom of disparate
build and production environments, so by dockerizing the build we can
build and link against an environment that resembles our deployment
environment. We could go further and deploy a docker image itself, but I
think this has been plenty enough adventure for one evening already.

---
## [SerenityOS/serenity](https://github.com/SerenityOS/serenity)@[07a2d22c33...](https://github.com/SerenityOS/serenity/commit/07a2d22c3368d153f8e1afb8ce30646c7660f56d)
#### Tuesday 2020-11-10 11:03:55 by Andreas Kling

HackStudio: Scroll embedded terminals to bottom upon command execution

It was kinda annoying that you had to scroll down manually every time
you started a new build while looking at some error in the scrollback.

---
## [NighthawkSLO/Autosplitters](https://github.com/NighthawkSLO/Autosplitters)@[83193c13d9...](https://github.com/NighthawkSLO/Autosplitters/commit/83193c13d956751f3a523f531e76854042524a79)
#### Tuesday 2020-11-10 13:33:49 by Urinstein

Hotline fix (#7)

* Updated startup{} text descriptions

* hotline logic changes for Part splits

* Update Hotline.asl

* Added IGT output

Game Time now outputs the game IGT
Tracking deaths has been added as a setting
IL timing for level without IGT has been removed
Put most lines into nested ifs

* Added IGT output

Game Time now outputs the game IGT
Tracking deaths has been added as a setting
IL timing for level without IGT has been removed
Put most lines into nested ifs

* Update Hotline.asl

- fixed reset bug, where the timer woudl reset upon closing the game even when the reset option was unchecked
- removed death IGHT tracking, because it was litereally impossbiel to bug fix due to shitty sub-frame memory flush shenanigans

* Update Hotline.asl

Added settings:
- All Levels - Floors
- any% - Floors
- ILs - Floors
- Track Deaths in IGT (this time it hopefully works)

Put settings into groups and renamed a bunch of settings and room-pointers to make the naming patterns less stupid.

Finally added GoG version support for all IGT related features.

* Update Hotline.asl

- fixed a bug where death tracking on Biker levels in gog version did not work (added biker_dead variable for this)
- changed the version recognition method from ModuleMemorySize to Hash, because version reconition for gog version rarely and randomly failed
- removed setting "any% - parts (Urin) "any% - Parts (Jack)" has become simply "any% - Parts"
- added startRoom variable to IL splitting so the splitter only starts on levels with intro screen, not in the menu or wherever else

* Update Hotline.asl

fucked some shit up on last commit which broke the splitter

Co-authored-by: Daniel Turcich <me@danielTurcich.com>
Co-authored-by: Jon <jon.kuhar99@gmail.com>

---
## [tgstation/tgstation](https://github.com/tgstation/tgstation)@[52cb4a401d...](https://github.com/tgstation/tgstation/commit/52cb4a401db2eeda81381feb20466c5f041b5f8d)
#### Tuesday 2020-11-10 14:33:22 by ArcaneMusic

Cell chargers now respect the law of conservation of energy. (#54886)

## About The Pull Request

Hopefully I can make this PR as knowledge filled as possible, so sit down and buckle up, because we're about to talk about POWER USE.

So, machines use power. The station uses power. Without power, key, critical aspects of the game don't function, and keeping the station's lifeblood pumped with electricity is the primary goal of the engineering department. Cool. So there's currently 2 methods of "power use" that machines can use, that is, through the passive use of power (Assigned by the `idle_power_usage` and `active_power_usage` variables), or through the `use_power()` proc. What's interesting, is that neither proc actually draws directly from the cell of the APC, which is sort of assumed by a proc called `use_power()`, after all. Where the passive power draw aspect of an APC is done automatically as machines are processed, and then applied to the APC seperately, taking power per cycle, use_power just applies a temporary blip of power usage to one of the APC's power tracks (Equipment, Lighting, or Enviroment). One would assume then that this temporary power drain would apply for long enough that it would apply the intended cost to the cell, and then turn off.

But I'm making this PR, right?

So the most egregious issue this brings is in terms of the cell recharger. If you place a power cell into a cell recharger, the recharger calls use_power every processing tick in order to reduce the APC cell by an equivalent amount to what the cell is intending to gain in charge. After all, you're just moving the charge from the APC to the power cell, plus the processing charge required by the cell_charger. However, lets look a bit closer at how use_power actually works. For this example we'll use a default, unupgraded cell recharger attempting to put it's default 250 watts into an empty bluespace cell.
Because power is heavily linked to the area that the machinery is placed in, we snag the area, determine the power channel, and call a use power proc on the area. The area use power proc simply adds that 250 watts, or rather joules into the equipment power channel of the APC, a part of power usage. All of this is parroted over to the APC's processing side, where the actual cost to the power cell is calcualted as follows:
`var/cellused = min(cell.charge, GLOB.CELLRATE * lastused_total)	// clamp deduction to a max, amount left in cell
		cell.use(cellused)`
So that number, the 250 joules of power we're calling to the cell, is actually being multiplied by a global cellrate, which is applied to every power drain on the station, actually charging the APC cell a total of 0.5 kj.
Based on some rumentary math and some experimentation, I filled a full bluespace power cell with 40Mj of power using 351 Kj of power from a standard, stock APC cell with no other drain except the 5 joules of power draw from the cell recharger.

So: What does this mean?

- Power draw is completely fucking busted (We knew this).
- Using two power cells, an APC, and an inducer, you can create infinite energy, anywhere, at zero cost to the station.
- We really need to make cell recharging a direct power draw.

Thankfully, that last one actually fits the portfolio of being a fix!
This adds a new proc to machines called `directly_use_power`. It does what it says on the tin, directly charges the APC for instances where power is going 1-1 from a power cell to another cell, in order to prevent infinite energy exploits.

## Why It's Good For The Game

Power is all kinds of unbalanced. Attempting to enforce the concept that a single unit of power is equal to itself is probably a good step in the right direction and in all likelyhood appears to have been the original intent with cell chargers in the first place.

I'm self-aware enough to see that this has ramifications beyond just fixing an issue within the cell charger alone, so if maintainers want to close this until december that's perfectly fine, but this is one of those things that could really easily snap basic station balance in twain.

---
## [mosra/magnum](https://github.com/mosra/magnum)@[58fea774ce...](https://github.com/mosra/magnum/commit/58fea774ce08a328a6d3ef3f2c70f1026b42792b)
#### Tuesday 2020-11-10 15:53:57 by Vladimír Vondruš

Vk: initial APIs for binding a memory to an image.

You won't believe it, but it took me over a month of sitting on the
shitter until this design idea materialized out of [..] air. The whole
story, in order:

 - Vulkan doesn't allow one VkDeviceMemory to be mapped more than once.
   This is rather sad, because since Vulkan best practices suggest to
   allocate a large block and suballocate from that, the engine needs
   an extra layer that "emulates" mapping the suballocations for the
   users but behind the scenes it inevitably has to maps the whole
   VkDeviceMemory anyway and keep it mapped for as long as any of the
   sub-mappings is active.
-  Because if it would map just a certain suballocation and then the
   user would want to map another suballocation, it would have to
   discard the original mapping and create a new one spanning both
   suballocations and that has a risk of suddenly being in a different
   VM block, making all pointers to the previous mapping invalid.
 - The Vulkan Memory Allocator implements this approach of mapping the
   whole thing and because of all the bookkeeping it doesn't give a
   direct access to the underlying VkDeviceMemory, making it rather
   hard to integrate.

Here I realized that:

 - Most allocations won't need to be mapped ever, so the hiding and
   obfuscation done by VMA isn't needed for those --- and we want
   interoperability with 3rd party code, so preventing access to
   VkDeviceMemory it's out of question.
 - There's KHR_dedicated_allocation, which (probably?) wasn't around
   when VMA was originally designed. The extension was created because
   a dedicated allocation actually *does* make sense in certain
   cases and on certain architectures. Providing a way to make those
   thus shouldn't be something "temporary, until a real allocator
   exists" but rather a well-designed API that's there to stay.
 - Except for iGPUs, the usual way to populate a GPU buffer would be to
   first copy the data to some host-accessible scratch buffer and then
   do a GPU-side copy of that buffer to a device-local memory. The
   scratch buffer is very likely to have a vastly different
   suballocation scheme than GPU buffers (grow & discard everything
   once it's all uploaded, for example) so again trying to put the two
   under the same allocator umbrella doesn't make sense.

Thus:

 - To avoid implementing a full-blown allocators right from the start,
   we'll first provide convenience APIs only for dedicated allocations
   -- making it possible to transfer memory ownership to an
   Image/Buffer so it can be treated the same way as in GL, and later
   having the Image/Buffer constructor implicitly allocate a dedicated
   VkDeviceMemory.
 - This default allocation will be subsequently equipped with
   KHR_dedicated_allocation bits.
 - Thanks to the extensible/layered nature of the design, the user is
   still capable of being completely in control of allocations,
   managing VkDeviceMemory sub-allocations by hand.

Finally, once allocator APIs are figured out, the default Buffer/Image
behavior gets switched from a dedicated allocation to using an
allocator, and dedicated allocation will be only used if the
KHR_dedicated_allocation bit is requested.

---
## [megwrites/portfolio](https://github.com/megwrites/portfolio)@[1eba44d2b5...](https://github.com/megwrites/portfolio/commit/1eba44d2b516dc83e579f4d6624787a0e8b1c182)
#### Tuesday 2020-11-10 16:50:07 by megwrites

run the whites hosts adventurous events on Saturday

https://www.conwaydailysun.com/sports/events/run-the-whites-hosting-adventurous-events-saturday/article_5d47a9c2-1fbf-11eb-8fda-d3622ae8c64d.html

By Meg Gaucher

Special to The Conway Daily Sun

JACKSON — If you are a New England outdoor enthusiast or athlete seeking year-round mountain adventure, you may have skied the Whites — but have you run the Whites? On Saturday, Nov. 7, Run the Whites will host its annual 5/10K Blackout Mountain Race. Runners of all ages and abilities will lace up their trail shoes at the base of Black Mountain to start their ascent at 4:26 p.m.

advertisement
The course provides sections of verticals and looping turns, making for a fun and challenging mountain pursuit. Each 5K loop is equivalent to 1,000 feet of elevation, making for a 2,000 foot elevation gain for the 10k runners.

The race is kept quiet in the media to avoid large crowds, and runners will be wearing masks until they are safely distanced as they run, walk and race the peak of Black.

Running is experiencing a boom due to the pandemic, and experienced runners are also seeking new challenges, from routes along roads and trails, to the mountains. Whether you’re an ultra-runner motivated to break the course record, an outdoor athlete looking to check off a bucket-list run, or a beginner ready for a thrilling nighttime adventure, you’re sure to enjoy the Blackout Mountain Race experience.

Local legend Andrew Drummond started Run the Whites out of his ski shop, Ski the Whites, in the town of Jackson, New Hampshire when he saw an opportunity to service and showcase the world-class terrain of the White Mountains for runners. Like every passion, Drummond brings to life through organized experiences. Run the Whites events are fun and have captured the hearts, lungs, and legs of the trail and mountain running community.

“Run the Whites really came out of my off-season love for trail running,” said Drummond. “Most skiers revert to biking in the off season — but there’s something about running the mountains, especially at night with a headlamp...it’s a special experience. I wanted to bring that experience to our community.”

When the shop opened in 2016, Drummond proactively combatted the challenge of running a ski shop in the off season by transforming the shop into “Run the Whites” in the summer. The shop provides trail running gear and equipment, and thanks to his long-time girlfriend and US Ski Team Alum, Hilary McCloy, the Ski the Whites calendar is typically chock full of year-round community events to get people outside. In the summer, a weekly run called the Friday Night Vertical series captures the White Mountain weekend crowds and locals alike. Runners are able to come together for a fun run regardless of speed, experience or age.

“The first year, we had close to 40 people. I don’t even count the runners anymore,” Drummond shares. “The whole purpose of the runs we host is to have fun and connect with people, whether you’re here for the parking lot pizza or the race.”

advertisement
This year marked the third season of the weekly vertical series, which expanded through word of mouth and athlete-targeted app features, like Strava challenges. Verticals this summer were operationally challenged. To accommodate social distancing and safety protocols due to the pandemic, social distancing was encouraged through guidelines and signed waiver from all participants, and run starts were staggered. For example, anyone could show up after 6 p.m. and run Black on their own.

For the running community, progress and breaking personal records (PRs) require challenging opportunities that Run the Whites offers that no other speciality shop in the area does. Drummond has a sweet deal with Black Mountain to put on running and ski events like the legendary Friday Night Lights ski series or the crazy event called the Last Skier Standing. Run the Whites brings runners and off-season skiers to Black Mountain to experience the terrain in a totally different way. Runners are motivated. They want to get outside and crush miles. Many runners linger in the parking lot at the mountain base after they finish for pizza and conversation. It's an experience in itself as the last of the golden summer rays of sunlight linger Black.

Black Mountain has terrain coupled with cold and wind that really caters to the New Englander pride and resiliency. Ski the Whites events bring athletes from all over New England to experience it. California pro skier Colby Townsend visited Black to watch the final skiers left looping Black as part of Last Skier Standing. “You come here to Black Mountain and it’s like, oh, this is where the soul of skiing is,” shared Townsend. “You can’t buy this type of experience with a lift ticket; you need to run it to achieve it.

Ski the Whites was born of Drummond’s personal project of exploring the White Mountain range by ski, foot, or ice climb and sharing his journey through video and photography. He demoed Fischer ski gear through an agreement with the company and transported GoPros and film gear out of the back of his truck to make the 2 a.m. climb up Tuckerman’s Ravine to capture the journey down. His ability to sell the gear helped him stay on top financially and fuel his touring needs. Drummond’s passion project gained notice in the community and eventually led him to transition his project into a shop.

“I knew if I was out there doing what I love, it would work out,” says Drummond. An important element of Drummond’s success in opening a back country shop on the East Coast was his ability to share White Mountain back-country with the local community. Drummond’s YouTube channel has grown over the years and received as many as 146,695 views a month.

“In the back-country community, you usually only see imagery from out West,” shares Drummond. “The White Mountains are right out the back door, and our community needs to see content they can relate to.”

Ski the Whites partners with over 30 outdoor brands that specialize in skiing, biking, and trail running — all suitable activities for the White Mountains. The gear is Ski the Whites community tested and approved. Drummond’s podcast and media work continue to bring people closer to mountain sports, too. What does Ski the Whites offer that no other New England shop does? An honest commitment to helping the community access and enjoy the White Mountains of New Hampshire.

For more information about community events, products, services or coffee, check out their website. If you’re in town, pop by the shop. Andrew is always excited to share local insights and get you geared up and out the back door to the mountains. It’s a spot worth stopping by before your next outdoor trek, guaranteed.

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[3224760c85...](https://github.com/mrakgr/The-Spiral-Language/commit/3224760c8524f6ba00e181916df7b133622dad4d)
#### Tuesday 2020-11-10 17:02:43 by Marko Grdinić

"1:10pm. Almost done with breakfast. Let me finish this fruit and then come the chores.

1:50pm. Where did the last 40m go? Let me start the chores.

3:50pm. Done with chores and am done cooling down. Today I had a good morning session, but like for the past week, the chores are really a drain on my time. I think I should be done in a week with them and after that I won't be troubled by this. Let me open the IDE.

4pm. I am thinking.

```fs
type FileStream = abstract member Run : TokReq -> TokRes * ParserRes Promise * Infer.InferResult Stream * FileStream

type FileStreamState = {
    tokenizer : TokenizerStream
    parser : ParserStream
    typechecker : TypecheckerStream
    }

let file is_top_down =
    let rec loop (s : FileStreamState) =
        {new FileStream with
            member t.Run req =
                let a,tok = s.tokenizer.Run req
                let s = {s with tokenizer = tok}
                let par = s.parser.Run a >>-* fun (a,s') -> a, {s with parser=s'}
                let typ = par >>-* fun (b,s) ->
                    s.typechecker.Run(b.bundles) |> Stream.mapFun (fun (x,s') -> x,{s with typechecker=s'})
                //let c = s.typechecker.Run(b >>-* fun x -> x.bundles)
                //a,b,c |> Stream.mapFun fst,loop {
                //    tokenizer = tok
                //    parser = par
                //    typechecker = s.typechecker
                //    }
            }
    loop {tokenizer=tokenizer; parser=parser is_top_down; typechecker=typechecker 0 0 Infer.top_env_default}
```

Let me just get rid of all of this for the time being.

I know I said that the plan would be to implement this, but all I need to do is call tokenizer, parser and the typechecker in a sequence. I don't need a stream for that. My redesigns simplified the 3 stream servers that I can easily make use of them now.

4:05pm. Rather than program, let me spend some time doing planning. I am finally on the right track now. I need to reconsider things once more.

Let me remove `LazyServer.fs`. Not going to be using that, even as a reference.

4:30pm. The sheer size of the whole thing is giving me fatigue, but I think at this point I am done. All the pieces really are there. Is there anything at all stopping me from finishing this? Not as far I can see.

I had to innovate and advance my own understanding of concurrency to such great extend, but now that I've demonstrated my understanding of concurrent diffing, the world is my oyster. The techinque I've demonstrated in the 3 stream servers is something that will be applicable anywhere else. It will scale.

4:35pm. I feel like calling it a day, but for the next hour or so, how about I get to work on getting that file server going. I'll forget all I've done in the project files and leave that for later. Right now, I need to break the ice and put the things I have done into action. It is time I slowly start shelcing the stuff in `Server.fs`.

```fs
type ClientReq =
    | ProjectFileOpen of {|uri : string; spiprojText : string|}
    | ProjectFileChange of {|uri : string; spiprojText : string|}
    | ProjectFileDelete of {|uri : string|}
    | ProjectFileLinks of {|uri : string|}
    | ProjectCodeActionExecute of {|uri : string; action : ProjectCodeAction|}
    | ProjectCodeActions of {|uri : string|}
    | FileOpen of {|uri : string; spiText : string|}
    | FileChanged of {|uri : string; spiEdit : SpiEdit|}
    | FileTokenRange of {|uri : string; range : VSCRange|}
    | HoverAt of {|uri : string; pos : VSCPos|}
    | BuildFile of {|uri : string|}

type ClientErrorsRes =
    | FatalError of string
    | PackageErrors of {|uri : string; errors : RString []|}
    | TokenizerErrors of {|uri : string; errors : RString []|}
    | ParserErrors of {|uri : string; errors : RString []|}
    | TypeErrors of {|uri : string; errors : RString list|}
```

Let me bring these back in. The project file stuff I'll just return null for now. In fact, let me start out by returning null for everything.

4:45pm.

```fs
let port = 13805
let uri_server = sprintf "tcp://*:%i" port
let uri_client = sprintf "tcp://localhost:%i" (port+1)

open FSharp.Json
open NetMQ
open NetMQ.Sockets

let dir uri = FileInfo(Uri(uri).LocalPath).Directory.FullName

let main _ =
    use poller = new NetMQPoller()
    use server = new RouterSocket()
    poller.Add(server)
    server.Options.ReceiveHighWatermark <- System.Int32.MaxValue
    server.Bind(uri_server)
    printfn "Server bound to: %s" uri_server

    use queue_server = new NetMQQueue<NetMQMessage>()
    poller.Add(queue_server)

    use queue_client = new NetMQQueue<ClientErrorsRes>()
    poller.Add(queue_client)

    let buffer = Dictionary()
    let last_id = ref 0
    use __ = server.ReceiveReady.Subscribe(fun s ->
        let rec loop () = Utils.remove buffer !last_id (body <| NetMQMessage 3) id
        and body (msg : NetMQMessage) (address : NetMQFrame, x) =
            incr last_id
            let push_back (x : obj) =
                match x with
                | :? Option<string> as x ->
                    match x with
                    | None -> msg.Push("null")
                    | Some x -> msg.Push(sprintf "\"%s\"" x)
                | _ -> msg.Push(Json.serialize x)
                msg.PushEmptyFrame(); msg.Push(address)
            let send_back x = push_back x; server.SendMultipartMessage(msg)
            let send_back_via_queue x = push_back x; queue_server.Enqueue(msg)
            let send_ivar_back_via_queue () = let res = IVar() in Hopac.start (IVar.read res >>- send_back_via_queue); res
            loop ()
        let msg = server.ReceiveMultipartMessage(3)
        let address = msg.Pop()
        msg.Pop() |> ignore
        let (id : int), x = Json.deserialize(Text.Encoding.Default.GetString(msg.Pop().Buffer))
        if !last_id = id then body msg (address, x)
        else buffer.Add(id,(address,x))
        )

    use client = new RequestSocket()
    client.Connect(uri_client)

    use __ = queue_client.ReceiveReady.Subscribe(fun x ->
        x.Queue.Dequeue() |> Json.serialize |> client.SendFrame
        client.ReceiveMultipartMessage() |> ignore
        )

    use __ = queue_server.ReceiveReady.Subscribe(fun x -> x.Queue.Dequeue() |> server.SendMultipartMessage)

    poller.Run()
    0
```

Here is essentially the main function except all the messages will be getting back nulls.

4:45pm. I am getting distracted. Let me just do this. I always seem to be short on time lately, but that is no excuse to slack when I am finally in the swing of things. Let me do one more thing for the day.

Let me do an imperative file server. It should be easy. This is just to test things. Then I'll bring back the project file server - this one probably tomorrow.

Then I'll be back to par. Except now the foundation will be there to tie this all together. Once I do that, I will be able to move to the prepass. And after that partial evaluation and codegen.

Seriously, by the time I've gotten to the prepass, I'll be 80-90% done anyway and the hardest parts will have been cleared.

The hardest part over the past 1.5 month was getting the requisite understanding. All the concurrency study I did in the first half of 2020 was not enough to prepare me for this. I needed to make an extra step.

Now that I've gotten this far I should be ready.

4:50pm. Let me start slowly moving into the end game for the compilation pipeline. I want to be done with this all by the end of the month. There is certainly enough time to do this if I can catch my wind.

Just sitting on my ass is not going to summon that momentum. The strenuous push is what will move the mountain of lethargy.

```fs
type FileServer =
    abstract member Open : {|uri : string; spiText : string|} -> unit
    abstract member Changed : {|uri : string; spiEdit : SpiEdit|} -> unit
    abstract member Hover : {|uri : string; range : VSCRange|} -> unit
```

Last time I just shoved the file server all inside the main function, and now I find myself confused over that. So this time I'll do it properly.

```fs
type FileServer =
    abstract member Open : {|uri : string; spiText : string|} -> unit
    abstract member Changed : {|uri : string; spiEdit : SpiEdit|} -> unit
    abstract member Hover : {|uri : string; pos : VSCPos|} -> string option
    abstract member TokenRange : {|uri : string; range : VSCRange|} -> VSCTokenArray
```

Let me do it like this. The errors will be a separate mechanism.

Now let me just do this thing.

```fs
let file_server fatal_errors tokenizer_errors parser_errors type_errors =
    let dict = Dictionary()
    {new FileServer with
        member _.Open req = ()
        member _.Changed req = ()
        member _.Hover req = None
        member _.TokenRange req = [||]
        }
```

Here is the stump for it.

5:25pm. Had to take some time off for lunch. Let me at least deal with this today. I'll test it out tomorrow.

5:30pm. No, just what am I doing here. With an interface I cannot do the backpressure trick I need a proper server.

I'll put in some overtime today. If needed I'll do this till 8pm. I'll get the file server out of the way today. Playtime can wait.

5:45pm. Forget what I just wrote. I do not feel like it.

At this point all I can really do is rush the server. I mean, if I was on a deadline and need to get it to work ASAP, then yes, I'd just paste the old stuff here and light it up.

But it is not like I get anything in particular for meeting my self imposed goal.

I do not just need to do a server - I need to do a new server. I need to think through the issues of it. Right now I am looking at the old stuff, and feel a sense of fatigue over it. The code feels old and rotted. I do not want to touch that trash.

Yesterday I had time to think what I wanted to do in the morning and things went quite smoothly.

5:55pm. Let me close here. I keep saying - it is just a server, but really - it is not. For every single idea, I've had to go through a lot rehearsal in my mind before I committed it to code.

It is just a server, but I am going to have to agonize over it a lot. I'll have to do it, and later I am going to have to take it down and rebuild it again.

I feel completely drained of inspiration with regards to programming right now.

Hard things might be hard, but easy thing make up for it by being tedious. I need to get a handle on it before I can do anything. I need to dwell upon it and first make it live in my mind. Then things will start moving.

I cannot do this any faster than its natural flow."

---
## [alagner/exult](https://github.com/alagner/exult)@[21d78c8b0b...](https://github.com/alagner/exult/commit/21d78c8b0bea4bd5ffd65eb71c55fc73a4dc3496)
#### Tuesday 2020-11-10 19:58:15 by Alagner

Prototype: remove checks for weather effects

Early stage of prototype, lots to do still,
so please don't be too harsh on the review, it is
not meant to be merged yet; I wanted to consult if
you like the whole idea, if so, I will be continuing work
on this brach. Otherwise, I will simple delete that.

The whole concept is this:
there is a `paintable` interface, `Weather_effect`
class and `Special_effect` class (name to be decided,
can be `Other_effect`, anything really):
`class Weather_effect : public paintable`
`class Other_effect : public paintable`.
Now, there are three lists:
`list<smartptr<Weather_effect>> //obvious`
`list<smartptr<Other_effect>> //obvious`
`list<paintable*> paintables` pointers to both elements.
Adding via overloaded methods (not yet added here).
Pardon the dynamic_cast, it will disappear once
the class hierarchy is remade, but honestly,
is_weather is just another kind of custom made
dynamic_cast anyway.

Potentially (but not sure now) this can additionally
be wrapped into helper class, say ManagedEffect<T>,
that would store a reference to `paintables` and autodelete
the observer pointer from there once the effect is removed
from its corresponding unique_ptr list.

Second thought: I wonder if `remove_effect` should not be
changed to sth like `relaseManagedEffect` and return the smart
pointer. This way it could return it in `handle_event` so
it should be more explicit that object is soon going to
"commit suicide" (by leaving scope in unique_ptr), thus leading
to less likely use_after_free (see previous PR).

---
## [ezio84/abc_frameworks_base](https://github.com/ezio84/abc_frameworks_base)@[a4f9de152d...](https://github.com/ezio84/abc_frameworks_base/commit/a4f9de152dcab4a25b7d512726bd9340ebeedfbb)
#### Tuesday 2020-11-10 21:16:51 by ezio84

Fix 2tap2wake after Ambient Pulsing on some devices

like taimen and walleye, instead sunfish (and probably newer pixels)
doesn't need this

To apply, override the config_has_weird_dt_sensor bool in the device tree

----
TL;DR
for some reason, on taimen and walleye, after ambient pulsing
gets triggered by adb with the official "com.android.systemui.doze.pulse"
intent or by our custom "wake to ambient" features, the double tap
sensor dies if you follow this steps:
- screen is OFF
- trigger ambient pulsing with a double tap to wake (if custom wake to
  ambient feature is enabled), or the official intent by adb, or with
  music ticker or any other event
- after ambient display shows up, don't touch anything and wait till the
  screen goes OFF again
- double tap to wake, again
- the double tap sensor doesn't work at all and device doesn't wake up

Now, funny thing, after the steps above, if you cover then uncover the
proximity/brightness sensor with the hand, then double tap to wake
again, the wake gesture works as expected.

When covering/uncovering the proximity/brightness sensor, this happens:
11-10 22:02:00.916   967   998 I ASH     : @ 1993.460: ftm4_disable_sensor: disabling sensor [double-tap]
11-10 22:02:02.013   967   998 I ASH     : @ 1994.556: ftm4_enable_sensor: enabling sensor [double-tap]

When you switch screen ON with power button, the doze screen states do
the same: the sensor gets disabled then enabled again if device goes
to DOZE idle state.

Instead, after Ambient pulsing, when the pulsing finishes, the sensor
is still enabled, so the disable/enable event doesn't happen this
time. And that's why, for some reason, it doesn't respond anymore.
----

So, in a nutshell: i've no idea why this sh#t happens lol,
but with a super lazy hacky tricky dirty bloody nooby line change,
we can force the sensor disable/enable event when the device goes
to DOZE state.

Change-Id: I8ce463a6e435e540e3ca93336c5dba7a95771b56

---
## [aospa-whyred/android_frameworks_base](https://github.com/aospa-whyred/android_frameworks_base)@[591f709fd1...](https://github.com/aospa-whyred/android_frameworks_base/commit/591f709fd18d106e6fac8eee92c59624b684965d)
#### Tuesday 2020-11-10 21:59:41 by orgesified

Revert "[caf-ginkgo] SystemUI: QSPanel: Display an extra row when media player is hidden"

fuck this shit, it looks ugly to me after getting used to 2 rows
also i'd love to be as close to aosp looks

This reverts commit ddcb8e67b841e4d5db4c7e294d3efa6ea670ca7e.

---
## [Volkarl/Soundche](https://github.com/Volkarl/Soundche)@[6591bfba3e...](https://github.com/Volkarl/Soundche/commit/6591bfba3eba75c83ab64467f63332987013154f)
#### Tuesday 2020-11-10 22:16:08 by Ubuntu

Toyed way too much with dockerfiles I hate my life

---
## [soupi/learn-haskell-blog-generator](https://github.com/soupi/learn-haskell-blog-generator)@[0c32661055...](https://github.com/soupi/learn-haskell-blog-generator/commit/0c32661055d1a19ae2371013cd35eec8baac88d6)
#### Tuesday 2020-11-10 23:04:28 by soupi

Define a markup language (`data` types)

Our simple markup language will contain the following features:

- Headers - prefixing a number of '@'
- Paragraphs - grouped lines
- Unordered lists - a group or lines each prefixed with '- '
- Unordered lists - a group or lines each prefixed with '# '
- Code blocks -     a group of lines each prefixed with `> `

We have implemented the html counterpart of a few of these,
but we haven't implemented a few headers and code blocks, perhaps
you'd like to give it a try?

So how do we represent this in Haskell types? We would like to
represent it kinda similarily to our html edsl - the body of our html
was a list of "content", we could have a markup be a list of contents
as well (headers, paragraphs, lists, etc.). But we need to be able to
distinguish them from one another.

For that, we have `data` definitions. `data` gives us the ability to
create custom types by grouping multiple types together and having
alternative structures. It looks like this:

```
data <Type-name> <type-args>
  = <Data-constructor> <types>
  | ...
```

It looks really similar to newtype, but there are two important
differences:

1. In the <types> part you can write many types (Like `Int String Bool`)
   but for newtypes you can only write one.
2. You can have alternative structures using `|`, newtypes have no
   alternatives.

Let's see a couple of examples of data types:

1. Bool

```
data Bool
  = True
  | False
```

We created a new data type named `Bool` with the possible values `True` or `False`.

2. Person

```
data Person
  = Person String Int -- where the first is the name and the second is
                      -- the age
```

We created a new data type named `Person`. Values of the type `Person`
look like this:

```
Person <some-string> <some-int>
```

For example:

```
Person "Gil" 31
```

3. Tuple

```
data Tuple a b
  = Tuple a b
```

This is pretty similar to `Person`, but we can plug any type we want
for this definition. For example:

```
Tuple "Clicked" True :: Tuple String Bool

Tuple 'a' 'z' :: Tuple Char Char
```

This type has special syntax in Haskell:

```
("Clicked", True) :: (String, Bool)

('a', 'z') :: (Char, Char)
```

4. Either

```
data Either a b
  = Left a
  | Right b
```

Similar to Tuple but instead of having only one constructor, we have
two. This means that we can choose which side we want. Here are a
couple of Values of type `Either String Int`:

```
Left "Hello"

Right 17
```

This type is useful for modeling errors. Either we succeeded and got
what we wanted (The `Right` constructor with the value), or we didn't
and got an error instead (The `Left` constructor with a string or a
custom error type).

Here we use it to model the different kinds of content types we have
in our markup language.

---

You might ask "Why do we even need to represent the markup as type?
Why don't we convert it into html as soon as we parse it
instead?". That's a good question and a valid strategy. The reason we
first represent it as a Haskell type is for flexibility and
modularity.

If the parsing code is coupled with html generation, we lose the
ability to preprocess the markup document. For example we might want
to take only a small part of the document (for summary) and present
it, or create a table of content from headers. Or maybe we'd like to
add other targets and not just html - maybe markdown format or a gui reader?

Parsing to an "abstract data type" (ADT) representation (one that does
not contain the details of the language, that for example we use '#' for
ordered lists) gives us the freedom to do so much more than just
converting to html that it's usually worth it in my opinion unless you
really need to optimize the process.

---
## [BigHeadGeorge/chowder](https://github.com/BigHeadGeorge/chowder)@[c8c0274a3d...](https://github.com/BigHeadGeorge/chowder/commit/c8c0274a3d42fabbe13f3a7f45ada3569fd2bcb9)
#### Tuesday 2020-11-10 23:44:35 by tuckerrrrrrrrrr

Move the TAG_End case in nbt_tag_seek_iter to after the loop

It still works the same, but I think it makes more sense to check if you
were looking for a TAG_End after the loop because the only way you'll
get there is if you find the TAG_End for the root compound of that
search. The old way, checking every tag found AND making sure you're in
the root compound, is kinda dumb

Hopefully there's not some magic case I'm not thinking of where this new
solution behaves differently

---

# [<](2020-11-09.md) 2020-11-10 [>](2020-11-11.md)

