# [<](2020-01-08.md) 2020-01-09 [>](2020-01-10.md)

2,131,272 events, 1,045,746 push events, 1,678,296 commit messages, 125,374,089 characters


## [brandonschmeidler/GodotImageEditor](https://github.com/brandonschmeidler/GodotImageEditor)@[9eb1ac35d6...](https://github.com/brandonschmeidler/GodotImageEditor/commit/9eb1ac35d66157b626309720f114c89e4592e0db)
#### Thursday 2020-01-09 03:50:11 by Brandon Schmeidler

Starting testing out how I'm going to handle layers and brushes.

This commit is probably a bad idea. But go fuck yourself.

---
## [hippi345/randobotjava](https://github.com/hippi345/randobotjava)@[b3f721e8d0...](https://github.com/hippi345/randobotjava/commit/b3f721e8d0e1affda1087b851a34a681764831aa)
#### Thursday 2020-01-09 04:00:25 by hippi345

View class added and autosize (sort of)

Separated out GUI components into a View class.
I think conceptually its decent but I may have lost some soundness in other areas in the process so feel free to review. First stab at this so hopefully its not garbage fire.

I got it to scale to input arg for size but my machine is necked at 16x16 so I set the limit on the arg for that for now but we obviously want to look at scaling to the machine's screen size (holy shit lol) and then setting component size properly.

---
## [shukal090/Predict-The-Data-Scientists-Salary-In-India-Hackathon](https://github.com/shukal090/Predict-The-Data-Scientists-Salary-In-India-Hackathon)@[11e4b62232...](https://github.com/shukal090/Predict-The-Data-Scientists-Salary-In-India-Hackathon/commit/11e4b622320ac5bb9ba588113af21b96fc1ca264)
#### Thursday 2020-01-09 04:36:47 by shukal090

Add files via upload

Data scientist is the sexiest job in the world. How many times have you heard that? Analytics India Annual Salary Study which aims to understand a wide range of trends data science says that the median analytics salary in India for the year 2017 is INR 12.7 Lakhs across all experience level and skill sets. So given the job description and other key information can you predict the range of salary of the job posting?

What kind of factors influence the salary of a data scientist? The study also says that in the world of analytics, Mumbai is the highest paymaster at almost 13.3 Lakhs per annum, followed by Bengaluru at 12.5 Lakhs. The industry of the data scientist can also influence the salary. Telecom industry pays the highest median salaries to its analytics professionals at 18.6 Lakhs. What are you waiting for, solve the problem by predicting how much a data scientist or analytics professional will be paid by analysing the data given. Bonus Tip: You can analyse the data and get key insights for your career as well.

The best data scientists and machine learning engineers will be given awesome prizes at the end of hackathon. Share this hackathon with a colleague who may be interested in mining the dataset for insights and make great predictions.
Data

The dataset is based on salary and job postings in India across the internet. The train and the test data consists of attributes mentioned below. The rows of train dataset has rich amount of information regarding the job posting such as name of the designation and key skills required for the job. The training data and test data comprise of 19802 samples and of 6601 samples each. This is a dataset which has been collected over some time to gather relevant analytics jobs posting over the years.

Data can be downloaded from https://www.machinehack.com/course/predict-the-data-scientists-salary-in-india-hackathon/
Features

    Name of the company (Encoded)
    Years of experience
    Job description
    Job designation
    Job Type
    Key skills
    Location
    Salary in Rupees Lakhs(To be predicted)

Problem Statement

Based on the given attributes and salary information, build a robust machine learning model that predicts the salary range of the salary post.
Evaluation Metric

Submissions are evaluated on the acccuracy between the predicted and the observed target.

---
## [benknoble/Dotfiles](https://github.com/benknoble/Dotfiles)@[e531ec0e26...](https://github.com/benknoble/Dotfiles/commit/e531ec0e26a5a66878814df737de489d759077bf)
#### Thursday 2020-01-09 05:42:17 by D. Ben Knoble

vim: replace old "hacks" with my new plugin, simpl

All programmers should know the following about duplicate code:

If you write it once, leave it be.
If you write it twice, give it a second thought, but let it be.
If you write it a third time, refactor it.

Today, this happened.

I already had use and load set up for SML and lisp, respectively.

A class of mine will be using clojure shortly, and I knew I wanted to
extend this functionality to clojure, and maybe to other
REPL/interpreter-based languages eventually.

So it was time for the x3 refactor.

Unfortunately, the code was sort of scattered through different
filetypes, and keyed in to the old `terminal#run` function. So it took
some time to pull the pieces of that together and design the necessary
mechanisms by which to automate and configure everything.

But, today, vim-simpl was born. And boy-oh-boy is it beautiful.

---
## [KeinR/Quiz_Kards](https://github.com/KeinR/Quiz_Kards)@[14ff19fd01...](https://github.com/KeinR/Quiz_Kards/commit/14ff19fd010404e942e1d916dce201d6465c13f0)
#### Thursday 2020-01-09 07:01:29 by Kein

For god's sake

Shut the fuck up with your "Could not find "nice"" 'aight? I thought I programmed you better than that...

---
## [locutus2/Stockfish](https://github.com/locutus2/Stockfish)@[8ff2fcf299...](https://github.com/locutus2/Stockfish/commit/8ff2fcf299ac621779dc167ce750ba73aaae90a4)
#### Thursday 2020-01-09 08:44:21 by Ondrej Mosnáček

Refactor tbprobe.cpp

This involves:
 * replacing the union hacks with simply reusing the EntryPiece arrays
   for the no-pawns case
 * merging the PairsData structure with the EntryPiece/-Pawn structs
   (with credit to Marco: @mcostalba)
 * simplifying some HashTable functions
 * thanks to previous changes, removing the ugly memsets
 * simplifying the template logic for WDL/DTZ distinction
   (now we distinguish based on an enum type, not the entry classes)
 * removing the unneeded Atomic wrapper

-----------------------------

For reference, here is a manual way to check that patches concerning
table bases code are non-functional changes:

0) Download the Syzygy table bases (up to 6 men).
1) Make sure you have branches master and the pull request pointing to
   the right commits.
2) Download the bench calculation scripts from the following URL:

        https://gist.github.com/WOnder93/b5fcf9c989b4a1715684d5c82367cdbe

   and copy into src inside your Stockfish repo.
3) Make the scripts executable (chmod +x *.sh).
4) Run the following command to use TBs located at <path>:

       export SYZYGY_PATH='<path>'

5) After that, run this (it will take a long time, this is a deep bench):

       BENCH_ARGS='128 1 22' ./check_benches.sh master tbprobe_cleanup 2>/dev/null`

==> You should see two equal numbers printed.
    (Of course, now we have to trust that the script itself is correct :)

-----------------------------

Closes https://github.com/official-stockfish/Stockfish/pull/1477

No functional change.

---
## [newstools/2020-the-guardian-uk](https://github.com/newstools/2020-the-guardian-uk)@[f8498e32fb...](https://github.com/newstools/2020-the-guardian-uk/commit/f8498e32fb38fd07ffc33f0a802f5a6938c9f870)
#### Thursday 2020-01-09 11:19:25 by NewsTools

Created Text For URL [www.theguardian.com/lifeandstyle/2020/jan/09/lose-220kg-beat-back-pain-love-yourself-personal-trainers-on-lessons-that-changed-their-lives]

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[65e854aa5a...](https://github.com/mrakgr/The-Spiral-Language/commit/65e854aa5abc4fd542e36e7629ea932d599c348d)
#### Thursday 2020-01-09 13:50:40 by Marko Grdinić

"1:15pm. I am really slacking right now. Let me finish this thing I am reading, do the chores and then I will resume work on the parser.

I am really having fun at the moment. I got up late so I decided to get breakfast immediately today.

1:45pm. Ok, chores. Then I'll get some work in.

2:10pm. It is finally time to start.

Damn, I do not feel like it, but it is just a couple of hours. Nothing I cannot bear.

Now where do I start? I know that I need to do the foralls and the let statements.

2:15pm. What should the syntax be?

`inl f forall a. (x : a) : a = x`

Hmmm, I've been thinking of making the constraints...

`inl f forall a. con a => (x : a) : a = x`

I like this the most of all.

`inl f forall a. (=> con a) (x : a) : a = x`

I've been thinking of this variant for contraints as well as it would be easier to parse, but I do not like it as much.

Also, maybe I should deviate from the F# notation a bit and make it...

`inl f forall a. con a => (x : a) -> a = x`

But this would clash with expressions.

`inl f forall a. con a => (x : a) : a = x`

I like this way the most of all.

But now, there is no way around it anymore. If I want to go down this route, I am going to have to do backtracking parsing.

2:30pm. I am thinking right now.

2:35pm. Ok, I am going to leave constraints for when it is time to do typechecking. That can wait till then.

Ah, yes. I've also completely forgotten to deal with `:`. I am going to have to deal with that as well.

2:50pm. Hmmm...

I need to think about this a bit more. Let me take a short break here."

---
## [ndt98/Db2z](https://github.com/ndt98/Db2z)@[bda6c0fdc9...](https://github.com/ndt98/Db2z/commit/bda6c0fdc9b78da319da290fea07fdeb3564d6bf)
#### Thursday 2020-01-09 14:38:07 by ndt98

Add files via upload

It is always interesting to have a tool to track errors in sql, because :



An application error can happen and nothing is seen in MSTR, DBM1 ..
Optimize your code as unnecessary and repeated errors is waste of CPU  
It happens to us recently, a DBA launched a creation of triggers in Production as requested by the development, unfortunately the triggers don't work correctly driving the triggering SQL statements (inserts, updates) to have negative sqlcode. As the negative sqlcode is not well signaled in the program, the application support doesn't know what happens to the programs and  15 long minutes  happen between the alert and the problem resolution, and it was by chance because the DBA is here to check his triggers and realized that there is a problem.

We had Apptune , but it was disabled to save some CPU consumption. 

So i decided to write this Rexx to do exactly what Apptune does : Start a trace and then IFI Read from Opx buffers.


IFCID 053 allows to get the sqlcode of a SQL statement, among others things, like the stats related to the sql (number of getpages, wait times ...) - IFCID 058 is the same and has the same assembler macro, but IFCID58 must be started with IFCID numbers from 59 to 66 (IFCID 58 is an END SQL event, so you must have a corresponding Start SQL IFCID launched) . Also, and the most important thing, IFCID 53 reports the SQLCODE even if the Sql could not be executed (-551 or -805 for example) 

 

Use this program wisely, it does a loop on READA to scan every sqlcode on the system, so it consumes  CPU !! Be behind your screen and stop it quickly after you get the data you need.

Also, remember that all monitors must run in a priority higher than Db2, so if you can't afford this, be sure that you have enough priority to catch things, otherwise it is possible that you lose records ... In my shop, we have quite enough priority to get the data we need, otherwise i suggest that you create a STC with the right priority , or use one that exists by replacing the jcl ...

---
## [WillemijnVP/Exploring-hacker-news-posts](https://github.com/WillemijnVP/Exploring-hacker-news-posts)@[62e00ea89e...](https://github.com/WillemijnVP/Exploring-hacker-news-posts/commit/62e00ea89e12856a170e7e61672738b27b4854f1)
#### Thursday 2020-01-09 16:10:08 by WillemijnVP

In this project we'll analyze Ask and Show posts on Hacker News and analyze which posts created at what hour  receive the most comments

Guided Project: Exploring Hacker News Posts
This is the guided project for the 'Python for Data Science: Intermediate' course of Dataquest.

In this project, we'll work with a data set of submissions to popular technology site Hacker News.

You can find the data set at https://www.kaggle.com/hacker-news/hacker-news-posts, but note that it has been reduced from almost 300,000 rows to approximately 20,000 rows by removing all submissions that did not receive any comments, and then randomly sampling from the remaining submissions. Below are descriptions of the columns:

- id: The unique identifier from Hacker News for the post
- title: The title of the post
- url: The URL that the posts links to, if it the post has a URL
- num_points: The number of points the post acquired, calculated as the total number of upvotes minus the total number of downvotes
- num_comments: The number of comments that were made on the post
- author: The username of the person who submitted the post
- created_at: The date and time at which the post was submitted

We're specifically interested in posts whose titles begin with either Ask HN or Show HN. Users submit Ask HN posts to ask the Hacker News community a specific question, such as 'Ask HN: How to improve my personal website?'
Likewise, users submit Show HN posts to show the Hacker News community a project, product, or just generally something interesting, such as 'Show HN: Shanhu.io, a programming playground powered by e8vm'.

We'll compare these two types of posts to determine the following:
- Do Ask HN or Show HN receive more comments on average?
- Do posts created at a certain time receive more comments on average?

---
## [postgres/postgres](https://github.com/postgres/postgres)@[9ce77d75c5...](https://github.com/postgres/postgres/commit/9ce77d75c5ab094637cc4a446296dc3be6e3c221)
#### Thursday 2020-01-09 16:57:17 by Tom Lane

Reconsider the representation of join alias Vars.

The core idea of this patch is to make the parser generate join alias
Vars (that is, ones with varno pointing to a JOIN RTE) only when the
alias Var is actually different from any raw join input, that is a type
coercion and/or COALESCE is necessary to generate the join output value.
Otherwise just generate varno/varattno pointing to the relevant join
input column.

In effect, this means that the planner's flatten_join_alias_vars()
transformation is already done in the parser, for all cases except
(a) columns that are merged by JOIN USING and are transformed in the
process, and (b) whole-row join Vars.  In principle that would allow
us to skip doing flatten_join_alias_vars() in many more queries than
we do now, but we don't have quite enough infrastructure to know that
we can do so --- in particular there's no cheap way to know whether
there are any whole-row join Vars.  I'm not sure if it's worth the
trouble to add a Query-level flag for that, and in any case it seems
like fit material for a separate patch.  But even without skipping the
work entirely, this should make flatten_join_alias_vars() faster,
particularly where there are nested joins that it previously had to
flatten recursively.

An essential part of this change is to replace Var nodes'
varnoold/varoattno fields with varnosyn/varattnosyn, which have
considerably more tightly-defined meanings than the old fields: when
they differ from varno/varattno, they identify the Var's position in
an aliased JOIN RTE, and the join alias is what ruleutils.c should
print for the Var.  This is necessary because the varno change
destroyed ruleutils.c's ability to find the JOIN RTE from the Var's
varno.

Another way in which this change broke ruleutils.c is that it's no
longer feasible to determine, from a JOIN RTE's joinaliasvars list,
which join columns correspond to which columns of the join's immediate
input relations.  (If those are sub-joins, the joinaliasvars entries
may point to columns of their base relations, not the sub-joins.)
But that was a horrid mess requiring a lot of fragile assumptions
already, so let's just bite the bullet and add some more JOIN RTE
fields to make it more straightforward to figure that out.  I added
two integer-List fields containing the relevant column numbers from
the left and right input rels, plus a count of how many merged columns
there are.

This patch depends on the ParseNamespaceColumn infrastructure that
I added in commit 5815696bc.  The biggest bit of code change is
restructuring transformFromClauseItem's handling of JOINs so that
the ParseNamespaceColumn data is propagated upward correctly.

Other than that and the ruleutils fixes, everything pretty much
just works, though some processing is now inessential.  I grabbed
two pieces of low-hanging fruit in that line:

1. In find_expr_references, we don't need to recurse into join alias
Vars anymore.  There aren't any except for references to merged USING
columns, which are more properly handled when we scan the join's RTE.
This change actually fixes an edge-case issue: we will now record a
dependency on any type-coercion function present in a USING column's
joinaliasvar, even if that join column has no references in the query
text.  The odds of the missing dependency causing a problem seem quite
small: you'd have to posit somebody dropping an implicit cast between
two data types, without removing the types themselves, and then having
a stored rule containing a whole-row Var for a join whose USING merge
depends on that cast.  So I don't feel a great need to change this in
the back branches.  But in theory this way is more correct.

2. markRTEForSelectPriv and markTargetListOrigin don't need to recurse
into join alias Vars either, because the cases they care about don't
apply to alias Vars for USING columns that are semantically distinct
from the underlying columns.  This removes the only case in which
markVarForSelectPriv could be called with NULL for the RTE, so adjust
the comments to describe that hack as being strictly internal to
markRTEForSelectPriv.

catversion bump required due to changes in stored rules.

Discussion: https://postgr.es/m/7115.1577986646@sss.pgh.pa.us

---
## [Dudemanguy/mpv-manga-reader](https://github.com/Dudemanguy/mpv-manga-reader)@[0042cd1457...](https://github.com/Dudemanguy/mpv-manga-reader/commit/0042cd145746fa62b594ac889528bb4f97057f04)
#### Thursday 2020-01-09 18:20:23 by dudemanguy

completely rewrite script based on lavfi-complex

This is a major change. Originally, I was dumb and didn't know about the
existence of the lavfi-complex fiter. So the script was written based
around imagemagick which is a pretty simple enough way to add pages to
each other. However, it is possible to just use the lavfi-complex option
in mpv to play multiple videos side by side. For the manga-reader, we
don't have to worry about the potential slowness of this since we're
only loading images.

lavfi-complex has several advantages over the old method. For one, it's
way, way faster. There's no need to write to the disk or anything ugly
like that. Adding another video track to mpv and displaying it with the
filter is nearly instant. Also, this means all the shell calls and path
ugliness is completely gone. So the script should now actually work on
Windows.

The only ugly thing in this implementation is a hacky sleep function
call that is basically used to ensure every file is loaded before
getting its dimensions. Otherwise, mpv switches through the playlist way
too fast and you get incorrect values.

Feature parity with the original is basically 99% there so I'm going to
merge it now. There's basically only a couple of small things missing
from the old one. First, double page mode doesn't have quite as
sophisticated width checks as it used to. If two images are the same
height, but their combined width would be way wider than the monitor's
aspect ratio, the images are combined and displayed here. On the old
one, it wouldn't do that. This rarely happens though, and it's an easy
enough fix (just lazy).

Continuous mode has one notable drawback now. Every image must be
exactly the same width. Double page mode has logic to handle mismatched
sizes so it's not an issue in that mode, but doing a similiar thing in
continuous mode while still supporting arbitrary page sizes is harder.
It's still possible, but it just means I have to add some more
boilerplate (which I don't feel like doing yet).

Overall, this way is a big improvement for me and I'm happy enough with
the state of this now. So let's go ahead and push it to master.

---
## [Akemi/mpv](https://github.com/Akemi/mpv)@[e1586585b4...](https://github.com/Akemi/mpv/commit/e1586585b4c54b1f76379728e4f1478b2fd5b3ce)
#### Thursday 2020-01-09 18:45:01 by wm4

vo_gpu: opengl: make it work with EGL 1.4

This tries to deal with the crazy EGL situation. The summary is:

- using eglGetDisplay() with multiple windowing platforms doesn't really
  work, but Mesa had an awful hack for it
- this hack can be disabled at build time, and some distros sometimes
  accidentally or intentionally do so
- Mesa will probably eventually disable it by default
- we switched to eglGetPlatformDisplay(), but this requires EGL 1.5
- the very regrettable graphics company (also known as Nvidia) ships
  drivers (for old hardware I think) that are EGL 1.4 only
- that means even though we "require" EGL 1.5 and link against it, the
  runtime EGL may be 1.4
- trying to run mpv there crashes in the dynamic linker
- so we have to go through some more awful compatibility hacks

This commit tries to do it "properly", but using EGL 1.4 as base. The
plaform selection mechanism is a messy extension there, which got
elevated to core API in 1.5 (but OF COURSE in incompatible ways).

I'm not sure whether the EGL 1.5 code path (by parsing the EGL_VERSION)
is really needed, but if you ask me, it feels slightly saner not to rely
on an EGL 1.4 kludge forever. But maybe this is just an instance of
self-harm, since they will most likely never drop or not provide this
API.

Also, unlike before, we actually check the extension string for the
individual platform extensions, because who knows, some EGL
implementations might curse us if we pass unknown platform parameters.
(But actually, the more I think about this, the more bullshit it is.)

X11 and Wayland were the only ones trying to call eglGetPlatformDisplay,
so they're the only ones which are adjusted in this commit.

Unfortunately, correct function of this commit is unconfirmed. It's
possible that it crashes with the old drivers mentioned above.

Why didn't they solve it like this:

struct native_display {
    int platform_type;
    void *native_display;
};

Could have kept eglGetDisplay() without all the obnoxious extension BS.

---
## [nschloe/heinzelpush](https://github.com/nschloe/heinzelpush)@[34a6db6559...](https://github.com/nschloe/heinzelpush/commit/34a6db655936299b6be213ac6a91fb093a6eeb76)
#### Thursday 2020-01-09 19:29:04 by Nico Schlömer

Cost back ahead gas part however suffer. By them second better game. Computer well probably.
Recently spend scene education TV experience theory feeling. They bit final pull night.

---
## [PIANO4DAYZ/RPGButOk](https://github.com/PIANO4DAYZ/RPGButOk)@[e13f4fa212...](https://github.com/PIANO4DAYZ/RPGButOk/commit/e13f4fa2126129af302723c78832cbb040b44587)
#### Thursday 2020-01-09 20:35:42 by PIANO4DAYZ

BROOOOOOOOOOOOOOOOOOOOOOOOOOOOOO you already know happy new year my friend, how was it???? ok yeah nice enough with the formalities GIKMMIE YOUR TOES NOWWWWWWWWWWWWWWWWWWWWWWWW please. I need them for my ph6ysics project where we find the projectile velocityh of toes, over a 30 degree aangrle over the horizonaltla,. if i do not get them, mr brownie will eliminate me from the faceof  this very flat earth. yeahj im flat earther, cause i gotta flast checst. take that boomer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, ok so basically me and this guy name jediper created a diologue and a ssytem at the beggninig of this god awful code, so now you can finally get over your identity crisis, and know who you are. fun facyt, your name is very very shhiiiiiiiiiiiiiiiiiiii- . yeah so have fun. MERRY GOOD and happy chinese new year in like a month lmao :)

---
## [STJr/SRB2](https://github.com/STJr/SRB2)@[004cfb45aa...](https://github.com/STJr/SRB2/commit/004cfb45aa2edbc9a1b1a18a26372822896e4ef5)
#### Thursday 2020-01-09 21:20:41 by toaster

Allow Rollout Rocks that are being ridden to:
* Break bustable FOFs like pushables.
* Activate pushable-counting executor activators.
* Get pushed by currents/wind.

This takes advantage of the previously-existing hack relating to objects with MF_PUSHABLE infotable (but not active) flags and a nonzero fuse, which is used as the condition to group this under (rather than MT_ROLLOUTROCK specific fuckery).

Thanks to the bug report submitted by our kirbs pal for getting me off my ass this evening.

---
## [Felvesthe/android_device_oneplus_msm8998-common](https://github.com/Felvesthe/android_device_oneplus_msm8998-common)@[8e29c91e28...](https://github.com/Felvesthe/android_device_oneplus_msm8998-common/commit/8e29c91e28a91285e9700533fdc78ea5c1a80af4)
#### Thursday 2020-01-09 22:06:13 by Felvesthe

well, fuck you enforcing

Signed-off-by: Felvesthe <felvesthe@gmail.com>

---

# [<](2020-01-08.md) 2020-01-09 [>](2020-01-10.md)

