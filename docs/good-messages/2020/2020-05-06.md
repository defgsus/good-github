# [<](2020-05-05.md) 2020-05-06 [>](2020-05-07.md)

2,746,305 events, 1,398,034 push events, 2,211,345 commit messages, 154,590,859 characters


## [tgstation/tgstation](https://github.com/tgstation/tgstation)@[a65d550743...](https://github.com/tgstation/tgstation/commit/a65d5507430ad40ea1a4540d43f4dc966a72b74c)
#### Wednesday 2020-05-06 00:27:17 by moo

Cooked food can now be exported through cargo, Take 2. (#50707)

* Oh jesus that took FOREVER. Why is there so much god damn food?

* First pass balance changes for single click crafts

* Yes, I know Qust is going to kill me. I'm aware.

* merge conflict fixed.

* Fuck

* Updates based on dunking element.

* Accidently bundled changes.

* Fixes Silver_spawned variable to work outside of luminescents.

---
## [maceghost/SayurStall](https://github.com/maceghost/SayurStall)@[5f403c49d6...](https://github.com/maceghost/SayurStall/commit/5f403c49d68f5a203e8d8abb29fed02fa9900958)
#### Wednesday 2020-05-06 01:23:12 by maceghost

Pretty big push all in one, here we go:

Started an entirely new look for the site, based on suggestions from commissioner. Still same basic functionality but a softer look, friendlier home pagel, friendlier shop experience, drop down cart modals, better organization for products, drop down for category list. Add animations, animations make sites look better;

Initially attempted to change the navigation because of the introduction of more serious header and footer, but realized it would take too much effort at this point. I wanted a router element in app.html that was smart about mobile or desktop, and then within either of those, have another router that routed pages sandwiched between a permanent header and footer. Too complicated, realized I could do the same with just holding a view variable in service and ngIfing the different components between header and footer. The result is a site that only ever loads one page. Works for mobile but seems somewhat unnatural on browser. To be determined if thatâ€™s the best practice;

So, added a footer component, an info page component, and a homepage component;

To make a better category and subcategory filter, I simplified the hierarchy and made the app work with just categories and subcategories rather than aisles;

Updated the api endpoint to match the new dns of the Django server. The frontend app and the Django server are currently running on the same server. This probably needs to change;

Updated index.html to include the correct web fonts, instantly making the site look 100% better;

I completely re-vamped the shop page component and the header component.
Gave the header a search bar, as well as an animated cart list modal on hover of cart button, and an animated drop down select of categories of items on hover over shop link.
During this process I discovered the following:

Some trickery with css up at the top navbar.
Whatâ€™s the best way to make a good hover path between a menu hover link and the drop down that animates below it?
You canâ€™t really use ngif or hidden if you want css animations because apparently they need to be in the Dom for the animations to work
I did come upon a cool little method of dynamically activating an animation based on some other action happening elsewhere pn the page, not necessarily the div where the animation should happen. This allows me to animate a menu to fade in from bottom of screen based on hovering over a navigation link.
But so I ended up trying to have this parent, relative position element that has the link in it, and then have an â€˜animationâ€™ container div thatâ€™s absolutely positioned within that parent relative element and then within that is a statically positioned div with the content in it, on hover over the link, the innermost div which starts with a margin up top to make it low in the screen then transitions up with a translation
Ran into issues then having mouse leave and enter events on the animated menu, fixed it by disabling pointer events on it until it was â€˜activatedâ€™ by hovering over link
Also an issue with the link and animation container in same div cause the hover events of the link to be eclipsed by the other div. this I ficxesd by making the link also absolutely positioned within the relative parent and giving it a higher z index

Is there a better way?
Should it be this hard?

In terms of the shop page:

I wanted a side nav that popped in and out, and within it, a tree structure to represent selection of categories and subcategories. Initially attempted to do this with angular material side navbars and material trees, but these things add so much bloat for no reason, it was easier to just make it from scratch.
In terms of the product view, added the option to view a grid or list, so had to build both those views. I discovered the magic of css grid styling. Just use a grid. So simple. For list items, grids give each element a certain amount of space, and the list item expands to fit the page. With the grid, the elements themselves can be given dimensions, and then set the grid to a single, auto-fitting column, this allows the grid to responsively add columns according to the size of screen. I found with images, it was necessary to give them specific dimensions to preserve aspect ratios and make the image look good, and then have the element containing it be a certain dimension based on that. So the image was the source of dimension.

Also added an entirely new item detail view when you click on the item in the list.

Still need to:

Make the cart modal work better
Move the item detail view to be within the shop grid area to make user more comfortable
Animate the category slide
The category select on the shop is buggy, the subcategories wonâ€™t select
Do the info page.
Do the home page banners etc, which includes animating the slide at the top.

---
## [sarpik/turbo-schedule](https://github.com/sarpik/turbo-schedule)@[3e7369bd2a...](https://github.com/sarpik/turbo-schedule/commit/3e7369bd2ad59af4e8344ab9dec0e67e84eb8735)
#### Wednesday 2020-05-06 01:46:21 by Kipras Melnikovas

refactor(scraper): Update `scrapeAndDoMagicWithLessonsFromParticipants` to the pipe operator! ðŸ”¥ðŸ”¥

Oh boy, I tried to extend typescript, webpack, babel, ts-loader, ttypescript
myself, but it looks like you cannot change stuff unless you modify the
private stuff, which is a shame.

Good news - someone implemented this at the private scope (the compiler itself),
meaning we can use it xoxo

Signed-off-by: Kipras Melnikovas <kipras@kipras.org>

---
## [BurntSushi/dotfiles](https://github.com/BurntSushi/dotfiles)@[819ade78ce...](https://github.com/BurntSushi/dotfiles/commit/819ade78ce941068ba05230fab9c2a19f585fa2f)
#### Wednesday 2020-05-06 02:00:25 by Andrew Gallant

disable them god damn inlay hints from rust-analyzer

I don't understand why something this would be enabled by default. Maybe
neovim doesn't know how to display them well, but they are SUPER
annoying. And they are also inconsistent.

Kudos to otaviosalvador on twitter for helping me figure out what these
were even called in the first place:
https://twitter.com/burntsushi5/status/1257848082089291776

We also completely disable diagnostics from RA because I hate neovim's
floating popup windows. It's much nicer to just see the error messages
in the status bar. I think those are from `cargo check` run by the
normal Rust vim plugin.

So I think now all I use RA for is goto-definition. But it's worth it
because it is MUCH better than ctags.

---
## [jayjayb772/NodeSelectorAPI](https://github.com/jayjayb772/NodeSelectorAPI)@[39387a728c...](https://github.com/jayjayb772/NodeSelectorAPI/commit/39387a728cc678e6c7fa976a6406e41893e52189)
#### Wednesday 2020-05-06 03:42:03 by jayjayb772

DATABASES UPGRADE HELL YEAH WOOOOOWWWW LOTS OF CODE OH FUCK

---
## [intentodemusico/Atribunet](https://github.com/intentodemusico/Atribunet)@[c8ffaa390e...](https://github.com/intentodemusico/Atribunet/commit/c8ffaa390eb4e0bf3030a52ade6c758abc4c5b05)
#### Wednesday 2020-05-06 04:04:09 by ?

I'M FUCKING LITERALLY GODDAMN GOD!!

Este fix sÃ­ estÃ¡ bien HOT:
Socket 10/10
MQTT 10/10
Algoritmo antiguo guardado solo para recordar
Archivos de test y todo eso, de todas formas ya estaba bien, solo lo cambiÃ© de sitio

---
## [Fletchlingboy/MegaMemePack](https://github.com/Fletchlingboy/MegaMemePack)@[287c1f792b...](https://github.com/Fletchlingboy/MegaMemePack/commit/287c1f792bcc04188d1c456e154109811a7e1ff4)
#### Wednesday 2020-05-06 04:11:30 by Evan

i just spent 2 hours making rango i hate my life i wish i was dead

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[8eb9746dee...](https://github.com/mrakgr/The-Spiral-Language/commit/8eb9746deecfaa33f9c9d06e62add4a625c4e4f5)
#### Wednesday 2020-05-06 09:59:50 by Marko GrdiniÄ‡

"10:35am. I got very little sleep tonight. I went to bed, but my brain just would not turn off. Amongst other things, I've been thinking about a new feature for heap records, but I decided against it. I am going to make unpacking them manually by using `indiv` or individual field accesses a requirement. This was my original intention.

10:40am. I am checking out extensions names in the VS Code marketplace. `.spr`, `.spa` and `.spl` are taken, but `.spi` is empty.

So `.spi` will it be. `.spi` will be the extension name for Spiral modules.

10:50am. Regrets...I've really made so many in my Spiral programming. The language was not designed right. I fell too much in love in its power to notice that I should not be putting my full strength into every swing. I was just too much into exploring a new world to really notice how low my productivity really was. Now, it has been thoroughly explored and I am a much wiser person.

It has been an excellent exercise, but I sure am sick of that by now. There has been enough exercise. This time I will aim for my goal with exquisite skill.

10:55am. The only thing I did right in the last 3 months is getting to intermediate stage with reactive extensions. Skill at that will last me and give me confidence. Well, and finally breaking through with a crappy, but popular language like Typescript.

It feels like math all over again. Having to eat shit is something I have real difficulty with.

11am. I am not really slacking right now. I am in a high tension state and yet not moving. It is not like I am trying to decide what I should first either.

11:05am. It is not like I lack desire - I will definitely start this soon.

11:10am. I am just looking inwards and taping into my purpose. From January 2019 to now, things have been all wrong. The way Spiral v0.2 is wrong as well. I was just so pissed at those math papers that I dropped them like a sack of shit.

This is not the right mindset. You go forward because you believe you can win. Because you believe that the strength and the weight of your experience will allow you to prosper.

11:15am. The reason to make a language is because you believe it will allow to create even greater vistas of power.

I need to believe - that the ensembles really are the big thing for the next wave. That I need Spiral to tap into the power of those neurochips.

Reasoning points in that direction, but I really channeled a lot of will back in 2018 only it to have it not be enough. It has left me burned. I need less skill, and more youthful vigor. I guess beyond a certain point in life, you always feel like you are old. I might end up living forever if I win, but at almost 33 it feels like I am an old man.

It is not like I want to sacrifice my present for my future.

If the future wants me to get there, then it should endeavor to make the present interesting.

11:20am. I really do want to programming in Spiral v0.2. That language will really be great.

11:25am. Everything in the way the last 5 years have unfolded deserves contempt. It would be so wonderful to continually climb towards greater capabilities in the field of RL, in the language of your choosing.

It is really a simple dream that I have. Though its implications are sinister, the day to day work should be plain.

The ML field is completely incompetent and yet I cannot beat it. Should that be funny or tragic?

10:30am. Looking inside myself, I find that my desire to program in v0.2 is quite real. This more than anything else is what will drive me forward from here.

The last 5 years have been long. I am not a beginer anymore. I've always had talent, but not the wealth of skill and experience to this degree.

There is no way the next 5 years will play out the same way the last 5 have.

Even though typechecking and doing editor support should be a significant challenge - just consider how long it took me to get Spiral into an usable state to begin with. But it honestly feels to me now that those grand challenges are just another encounter that I will pass through without much trouble.

A naive heuristic is telling me that this will be hard, but my programming intuition is telling me that it won't.

11:40am. I want to prove...that failure and lack of progress is not something that should accompany doing ML and PL side by side. That this madness can by curtailed and channeled productively.

11:45am. There is no better cure for insanity than victory. At that point it becomes genius. Like playing a game one is good at, there is a certain calmness in skill.

11:50am. There has never been a need for madness, or doubt and fear. Should I not know by now what most of the principles of ML are?

Scaling them will be harsh and for that I need the right tools.

11:55am. Let me take a break here.

Today I am too strained to really program or do anything else for that matter. Just like two days ago, I will take the time to dwell and collect my threads of thought. Let this inner war rage to completion and then I will start my charge once more. Not because I am out of options, but because I know what I need to do.

The first wave of ML is over, but the second one will start in the nearby future. When that happens I will be at my finest."

---
## [AJSmithJohnson/302-BossGame](https://github.com/AJSmithJohnson/302-BossGame)@[bcb52036d5...](https://github.com/AJSmithJohnson/302-BossGame/commit/bcb52036d5699f96bca1f116a108bb7d8525b297)
#### Wednesday 2020-05-06 10:40:51 by AJamesSmith

Have a lot of changes

//FOR TITLE SCENE
+blocked out geometry
+created basic rotation script to give the geometry some life
+created three bezier paths, Two for the camera, one for the UI
+Switched up the follow path script
+Objects have a NewBezierScript they can use
+This script takes an array of paths and animation times and allows the player to have the functionality to switch between paths
+ The new script and the old script have both been modified to allow for looping paths now
+The camera has the basic fly through animation I want and follows and switches paths accordingly so does the UI
+I changed the look at script so that it is a bit more basic. I was having trouble getting the other script to work so now it takes a target and just uses unities built in look at to focus on that
+ I also added in a variable to control if we want to look at the target or not that can be flipped on and off
+I have a script that functions as a rudimentary event system that tells the UI to animate once the Camera is along the right path
+I combined some of the code we used for the basic character controller from way back in week 4 or 5 and tweaked and combined it with our new procedural rig so it now turns and moves with the camera and the camera follows it.

---
## [rythin-sr/cod4](https://github.com/rythin-sr/cod4)@[f338575b2a...](https://github.com/rythin-sr/cod4/commit/f338575b2a62afd58af3d8e68452aa6b8183e397)
#### Wednesday 2020-05-06 13:40:42 by Cassie

fuck this game and its stupid quicksave freezing stupid ass fuck

---
## [Khalvat-M/kernel_samsung_msm8974](https://github.com/Khalvat-M/kernel_samsung_msm8974)@[e5739859f4...](https://github.com/Khalvat-M/kernel_samsung_msm8974/commit/e5739859f492c6fcbfd140d03e4d4190fd6f3d60)
#### Wednesday 2020-05-06 14:09:03 by Thomas Gleixner

tick: Upstream fixes

 * Addresses the issue of timers being scheduled on offline CPUs.

tick: Don't invoke tick_nohz_stop_sched_tick() if the cpu is offline

commit 5b39939a4 (nohz: Move ts->idle_calls incrementation into strict
idle logic) moved code out of tick_nohz_stop_sched_tick() and missed
to bail out when the cpu is offline. That's causing subsequent
failures as an offline CPU is supposed to die and not to fiddle with
nohz magic.

Return false in can_stop_idle_tick() if the cpu is offline.

Reported-and-tested-by: Jiri Kosina <jkosina@suse.cz>
Reported-and-tested-by: Prarit Bhargava <prarit@redhat.com>
Cc: Frederic Weisbecker <fweisbec@gmail.com>
Cc: Borislav Petkov <bp@alien8.de>
Cc: Tony Luck <tony.luck@intel.com>
Cc: x86@kernel.org
Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1305132138160.2863@ionos
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

timers: Consolidate base->next_timer update

Another bunch of mindlessly copied code. All callers of
internal_add_timer() except the recascading code updates
base->next_timer.

Move this into internal_add_timer() and let the cascading code call
__internal_add_timer().

Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Gilad Ben-Yossef <gilad@benyossef.com>
Cc: Frederic Weisbecker <fweisbec@gmail.com>
Link: http://lkml.kernel.org/r/20120525214819.189946224@linutronix.de

timers: Create detach_if_pending() and use it

Most callers of detach_timer() have the same pattern around
them. Check whether the timer is pending and eventually updating
base->next_timer.

Create detach_if_pending() and replace the duplicated code.

Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Gilad Ben-Yossef <gilad@benyossef.com>
Cc: Frederic Weisbecker <fweisbec@gmail.com>
Link: http://lkml.kernel.org/r/20120525214819.131246037@linutronix.de

timers: Add accounting of non deferrable timers

The code in get_next_timer_interrupt() is suboptimal as it has to run
through the cascade to find the next expiring timer. On a completely
idle core we should only do that when there is an active timer
enqueued and base->next_timer does not give us a fast answer.

Add accounting of the active timers to the now consolidated
attach/detach code. I deliberately avoided sanity checks because the
code is fully symetric and any fiddling with timers w/o using the API
functions will lead to cute explosions anyway. ulong is big enough
even on 32bit and if we really run into the situation to have more
than 1<<32 timers enqueued there, then we are definitely not in a
state to go idle and run through that code.

This allows us to fix another shortcoming of get_next_timer_interrupt().

Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Gilad Ben-Yossef <gilad@benyossef.com>
Cc: Frederic Weisbecker <fweisbec@gmail.com>
Link: http://lkml.kernel.org/r/20120525214819.236377028@linutronix.de

timers: Improve get_next_timer_interrupt()

Gilad reported at

 http://lkml.kernel.org/r/1336056962-10465-2-git-send-email-gilad@benyossef.com

"Current timer code fails to correctly return a value meaning that
 there is no future timer event, with the result that the timer keeps
 getting re-armed in HZ one shot mode even when we could turn it off,
 generating unneeded interrupts.

 What is happening is that when __next_timer_interrupt() wishes
 to return a value that signifies "there is no future timer
 event", it returns (base->timer_jiffies + NEXT_TIMER_MAX_DELTA).

 However, the code in tick_nohz_stop_sched_tick(), which called
 __next_timer_interrupt() via get_next_timer_interrupt(),
 compares the return value to (last_jiffies + NEXT_TIMER_MAX_DELTA)
 to see if the timer needs to be re-armed.

 base->timer_jiffies != last_jiffies and so tick_nohz_stop_sched_tick()
 interperts the return value as indication that there is a distant
 future event 12 days from now and programs the timer to fire next
 after KTIME_MAX nsecs instead of avoiding to arm it. This ends up
 causing a needless interrupt once every KTIME_MAX nsecs."

Fix this by using the new active timer accounting. This avoids scans
when no active timer is enqueued completely, so we don't have to rely
on base->timer_next and base->timer_jiffies anymore.

Change-Id: I874ee5e5f837a228cebf5e6a084baf520d33cd0f
Reported-by: Gilad Ben-Yossef <gilad@benyossef.com>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Frederic Weisbecker <fweisbec@gmail.com>
Link: http://lkml.kernel.org/r/20120525214819.317535385@linutronix.de
Signed-off-by: Khalvat-M <mmba313@gmail.com>

---
## [macwannevil/repo1](https://github.com/macwannevil/repo1)@[29af1d1179...](https://github.com/macwannevil/repo1/commit/29af1d11799043f70d3fccd5afcda20c7142b647)
#### Wednesday 2020-05-06 15:29:08 by macwannevil

# Redis configuration file example. # # Note that in order to read the configuration file, Redis must be # started with the file path as first argument: # # ./redis-server /path/to/redis.conf  # Note on units: when memory size is needed, it is possible to specify # it in the usual form of 1k 5GB 4M and so forth: # # 1k => 1000 bytes # 1kb => 1024 bytes # 1m => 1000000 bytes # 1mb => 1024*1024 bytes # 1g => 1000000000 bytes # 1gb => 1024*1024*1024 bytes # # units are case insensitive so 1GB 1Gb 1gB are all the same.  ################################## INCLUDES ###################################  # Include one or more other config files here.  This is useful if you # have a standard template that goes to all Redis servers but also need # to customize a few per-server settings.  Include files can include # other files, so use this wisely. # # Notice option "include" won't be rewritten by command "CONFIG REWRITE" # from admin or Redis Sentinel. Since Redis always uses the last processed # line as value of a configuration directive, you'd better put includes # at the beginning of this file to avoid overwriting config change at runtime. # # If instead you are interested in using includes to override configuration # options, it is better to use include as the last line. # # include /path/to/local.conf # include /path/to/other.conf  ################################## MODULES #####################################  # Load modules at startup. If the server is not able to load modules # it will abort. It is possible to use multiple loadmodule directives. # # loadmodule /path/to/my_module.so # loadmodule /path/to/other_module.so  ################################## NETWORK #####################################  # By default, if no "bind" configuration directive is specified, Redis listens # for connections from all the network interfaces available on the server. # It is possible to listen to just one or multiple selected interfaces using # the "bind" configuration directive, followed by one or more IP addresses. # # Examples: # # bind 192.168.1.100 10.0.0.1 # bind 127.0.0.1 ::1 # # ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the # internet, binding to all the interfaces is dangerous and will expose the # instance to everybody on the internet. So by default we uncomment the # following bind directive, that will force Redis to listen only into # the IPv4 loopback interface address (this means Redis will be able to # accept connections only from clients running into the same computer it # is running). # # IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES # JUST COMMENT THE FOLLOWING LINE. # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ bind 127.0.0.1  # Protected mode is a layer of security protection, in order to avoid that # Redis instances left open on the internet are accessed and exploited. # # When protected mode is on and if: # # 1) The server is not binding explicitly to a set of addresses using the #    "bind" directive. # 2) No password is configured. # # The server only accepts connections from clients connecting from the # IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain # sockets. # # By default protected mode is enabled. You should disable it only if # you are sure you want clients from other hosts to connect to Redis # even if no authentication is configured, nor a specific set of interfaces # are explicitly listed using the "bind" directive. protected-mode yes  # Accept connections on the specified port, default is 6379 (IANA #815344). # If port 0 is specified Redis will not listen on a TCP socket. port 6379  # TCP listen() backlog. # # In high requests-per-second environments you need an high backlog in order # to avoid slow clients connections issues. Note that the Linux kernel # will silently truncate it to the value of /proc/sys/net/core/somaxconn so # make sure to raise both the value of somaxconn and tcp_max_syn_backlog # in order to get the desired effect. tcp-backlog 511  # Unix socket. # # Specify the path for the Unix socket that will be used to listen for # incoming connections. There is no default, so Redis will not listen # on a unix socket when not specified. # # unixsocket /tmp/redis.sock # unixsocketperm 700  # Close the connection after a client is idle for N seconds (0 to disable) timeout 0  # TCP keepalive. # # If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence # of communication. This is useful for two reasons: # # 1) Detect dead peers. # 2) Take the connection alive from the point of view of network #    equipment in the middle. # # On Linux, the specified value (in seconds) is the period used to send ACKs. # Note that to close the connection the double of the time is needed. # On other kernels the period depends on the kernel configuration. # # A reasonable value for this option is 300 seconds, which is the new # Redis default starting with Redis 3.2.1. tcp-keepalive 300  ################################# GENERAL #####################################  # By default Redis does not run as a daemon. Use 'yes' if you need it. # Note that Redis will write a pid file in /var/run/redis.pid when daemonized. daemonize no  # If you run Redis from upstart or systemd, Redis can interact with your # supervision tree. Options: #   supervised no      - no supervision interaction #   supervised upstart - signal upstart by putting Redis into SIGSTOP mode #   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET #   supervised auto    - detect upstart or systemd method based on #                        UPSTART_JOB or NOTIFY_SOCKET environment variables # Note: these supervision methods only signal "process is ready." #       They do not enable continuous liveness pings back to your supervisor. supervised no  # If a pid file is specified, Redis writes it where specified at startup # and removes it at exit. # # When the server runs non daemonized, no pid file is created if none is # specified in the configuration. When the server is daemonized, the pid file # is used even if not specified, defaulting to "/var/run/redis.pid". # # Creating a pid file is best effort: if Redis is not able to create it # nothing bad happens, the server will start and run normally. pidfile /var/run/redis_6379.pid  # Specify the server verbosity level. # This can be one of: # debug (a lot of information, useful for development/testing) # verbose (many rarely useful info, but not a mess like the debug level) # notice (moderately verbose, what you want in production probably) # warning (only very important / critical messages are logged) loglevel notice  # Specify the log file name. Also the empty string can be used to force # Redis to log on the standard output. Note that if you use standard # output for logging but daemonize, logs will be sent to /dev/null logfile ""  # To enable logging to the system logger, just set 'syslog-enabled' to yes, # and optionally update the other syslog parameters to suit your needs. # syslog-enabled no  # Specify the syslog identity. # syslog-ident redis  # Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7. # syslog-facility local0  # Set the number of databases. The default database is DB 0, you can select # a different one on a per-connection basis using SELECT <dbid> where # dbid is a number between 0 and 'databases'-1 databases 16  # By default Redis shows an ASCII art logo only when started to log to the # standard output and if the standard output is a TTY. Basically this means # that normally a logo is displayed only in interactive sessions. # # However it is possible to force the pre-4.0 behavior and always show a # ASCII art logo in startup logs by setting the following option to yes. always-show-logo yes  ################################ SNAPSHOTTING  ################################ # # Save the DB on disk: # #   save <seconds> <changes> # #   Will save the DB if both the given number of seconds and the given #   number of write operations against the DB occurred. # #   In the example below the behaviour will be to save: #   after 900 sec (15 min) if at least 1 key changed #   after 300 sec (5 min) if at least 10 keys changed #   after 60 sec if at least 10000 keys changed # #   Note: you can disable saving completely by commenting out all "save" lines. # #   It is also possible to remove all the previously configured save #   points by adding a save directive with a single empty string argument #   like in the following example: # #   save ""  save 900 1 save 300 10 save 60 10000  # By default Redis will stop accepting writes if RDB snapshots are enabled # (at least one save point) and the latest background save failed. # This will make the user aware (in a hard way) that data is not persisting # on disk properly, otherwise chances are that no one will notice and some # disaster will happen. # # If the background saving process will start working again Redis will # automatically allow writes again. # # However if you have setup your proper monitoring of the Redis server # and persistence, you may want to disable this feature so that Redis will # continue to work as usual even if there are problems with disk, # permissions, and so forth. stop-writes-on-bgsave-error yes  # Compress string objects using LZF when dump .rdb databases? # For default that's set to 'yes' as it's almost always a win. # If you want to save some CPU in the saving child set it to 'no' but # the dataset will likely be bigger if you have compressible values or keys. rdbcompression yes  # Since version 5 of RDB a CRC64 checksum is placed at the end of the file. # This makes the format more resistant to corruption but there is a performance # hit to pay (around 10%) when saving and loading RDB files, so you can disable it # for maximum performances. # # RDB files created with checksum disabled have a checksum of zero that will # tell the loading code to skip the check. rdbchecksum yes  # The filename where to dump the DB dbfilename dump.rdb  # The working directory. # # The DB will be written inside this directory, with the filename specified # above using the 'dbfilename' configuration directive. # # The Append Only File will also be created inside this directory. # # Note that you must specify a directory here, not a file name. dir ./  ################################# REPLICATION #################################  # Master-Replica replication. Use replicaof to make a Redis instance a copy of # another Redis server. A few things to understand ASAP about Redis replication. # #   +------------------+      +---------------+ #   |      Master      | ---> |    Replica    | #   | (receive writes) |      |  (exact copy) | #   +------------------+      +---------------+ # # 1) Redis replication is asynchronous, but you can configure a master to #    stop accepting writes if it appears to be not connected with at least #    a given number of replicas. # 2) Redis replicas are able to perform a partial resynchronization with the #    master if the replication link is lost for a relatively small amount of #    time. You may want to configure the replication backlog size (see the next #    sections of this file) with a sensible value depending on your needs. # 3) Replication is automatic and does not need user intervention. After a #    network partition replicas automatically try to reconnect to masters #    and resynchronize with them. # # replicaof <masterip> <masterport>  # If the master is password protected (using the "requirepass" configuration # directive below) it is possible to tell the replica to authenticate before # starting the replication synchronization process, otherwise the master will # refuse the replica request. # # masterauth <master-password>  # When a replica loses its connection with the master, or when the replication # is still in progress, the replica can act in two different ways: # # 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will #    still reply to client requests, possibly with out of date data, or the #    data set may just be empty if this is the first synchronization. # # 2) if replica-serve-stale-data is set to 'no' the replica will reply with #    an error "SYNC with master in progress" to all the kind of commands #    but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG, #    SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB, #    COMMAND, POST, HOST: and LATENCY. # replica-serve-stale-data yes  # You can configure a replica instance to accept writes or not. Writing against # a replica instance may be useful to store some ephemeral data (because data # written on a replica will be easily deleted after resync with the master) but # may also cause problems if clients are writing to it because of a # misconfiguration. # # Since Redis 2.6 by default replicas are read-only. # # Note: read only replicas are not designed to be exposed to untrusted clients # on the internet. It's just a protection layer against misuse of the instance. # Still a read only replica exports by default all the administrative commands # such as CONFIG, DEBUG, and so forth. To a limited extent you can improve # security of read only replicas using 'rename-command' to shadow all the # administrative / dangerous commands. replica-read-only yes  # Replication SYNC strategy: disk or socket. # # ------------------------------------------------------- # WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY # ------------------------------------------------------- # # New replicas and reconnecting replicas that are not able to continue the replication # process just receiving differences, need to do what is called a "full # synchronization". An RDB file is transmitted from the master to the replicas. # The transmission can happen in two different ways: # # 1) Disk-backed: The Redis master creates a new process that writes the RDB #                 file on disk. Later the file is transferred by the parent #                 process to the replicas incrementally. # 2) Diskless: The Redis master creates a new process that directly writes the #              RDB file to replica sockets, without touching the disk at all. # # With disk-backed replication, while the RDB file is generated, more replicas # can be queued and served with the RDB file as soon as the current child producing # the RDB file finishes its work. With diskless replication instead once # the transfer starts, new replicas arriving will be queued and a new transfer # will start when the current one terminates. # # When diskless replication is used, the master waits a configurable amount of # time (in seconds) before starting the transfer in the hope that multiple replicas # will arrive and the transfer can be parallelized. # # With slow disks and fast (large bandwidth) networks, diskless replication # works better. repl-diskless-sync no  # When diskless replication is enabled, it is possible to configure the delay # the server waits in order to spawn the child that transfers the RDB via socket # to the replicas. # # This is important since once the transfer starts, it is not possible to serve # new replicas arriving, that will be queued for the next RDB transfer, so the server # waits a delay in order to let more replicas arrive. # # The delay is specified in seconds, and by default is 5 seconds. To disable # it entirely just set it to 0 seconds and the transfer will start ASAP. repl-diskless-sync-delay 5  # Replicas send PINGs to server in a predefined interval. It's possible to change # this interval with the repl_ping_replica_period option. The default value is 10 # seconds. # # repl-ping-replica-period 10  # The following option sets the replication timeout for: # # 1) Bulk transfer I/O during SYNC, from the point of view of replica. # 2) Master timeout from the point of view of replicas (data, pings). # 3) Replica timeout from the point of view of masters (REPLCONF ACK pings). # # It is important to make sure that this value is greater than the value # specified for repl-ping-replica-period otherwise a timeout will be detected # every time there is low traffic between the master and the replica. # # repl-timeout 60  # Disable TCP_NODELAY on the replica socket after SYNC? # # If you select "yes" Redis will use a smaller number of TCP packets and # less bandwidth to send data to replicas. But this can add a delay for # the data to appear on the replica side, up to 40 milliseconds with # Linux kernels using a default configuration. # # If you select "no" the delay for data to appear on the replica side will # be reduced but more bandwidth will be used for replication. # # By default we optimize for low latency, but in very high traffic conditions # or when the master and replicas are many hops away, turning this to "yes" may # be a good idea. repl-disable-tcp-nodelay no  # Set the replication backlog size. The backlog is a buffer that accumulates # replica data when replicas are disconnected for some time, so that when a replica # wants to reconnect again, often a full resync is not needed, but a partial # resync is enough, just passing the portion of data the replica missed while # disconnected. # # The bigger the replication backlog, the longer the time the replica can be # disconnected and later be able to perform a partial resynchronization. # # The backlog is only allocated once there is at least a replica connected. # # repl-backlog-size 1mb  # After a master has no longer connected replicas for some time, the backlog # will be freed. The following option configures the amount of seconds that # need to elapse, starting from the time the last replica disconnected, for # the backlog buffer to be freed. # # Note that replicas never free the backlog for timeout, since they may be # promoted to masters later, and should be able to correctly "partially # resynchronize" with the replicas: hence they should always accumulate backlog. # # A value of 0 means to never release the backlog. # # repl-backlog-ttl 3600  # The replica priority is an integer number published by Redis in the INFO output. # It is used by Redis Sentinel in order to select a replica to promote into a # master if the master is no longer working correctly. # # A replica with a low priority number is considered better for promotion, so # for instance if there are three replicas with priority 10, 100, 25 Sentinel will # pick the one with priority 10, that is the lowest. # # However a special priority of 0 marks the replica as not able to perform the # role of master, so a replica with priority of 0 will never be selected by # Redis Sentinel for promotion. # # By default the priority is 100. replica-priority 100  # It is possible for a master to stop accepting writes if there are less than # N replicas connected, having a lag less or equal than M seconds. # # The N replicas need to be in "online" state. # # The lag in seconds, that must be <= the specified value, is calculated from # the last ping received from the replica, that is usually sent every second. # # This option does not GUARANTEE that N replicas will accept the write, but # will limit the window of exposure for lost writes in case not enough replicas # are available, to the specified number of seconds. # # For example to require at least 3 replicas with a lag <= 10 seconds use: # # min-replicas-to-write 3 # min-replicas-max-lag 10 # # Setting one or the other to 0 disables the feature. # # By default min-replicas-to-write is set to 0 (feature disabled) and # min-replicas-max-lag is set to 10.  # A Redis master is able to list the address and port of the attached # replicas in different ways. For example the "INFO replication" section # offers this information, which is used, among other tools, by # Redis Sentinel in order to discover replica instances. # Another place where this info is available is in the output of the # "ROLE" command of a master. # # The listed IP and address normally reported by a replica is obtained # in the following way: # #   IP: The address is auto detected by checking the peer address #   of the socket used by the replica to connect with the master. # #   Port: The port is communicated by the replica during the replication #   handshake, and is normally the port that the replica is using to #   listen for connections. # # However when port forwarding or Network Address Translation (NAT) is # used, the replica may be actually reachable via different IP and port # pairs. The following two options can be used by a replica in order to # report to its master a specific set of IP and port, so that both INFO # and ROLE will report those values. # # There is no need to use both the options if you need to override just # the port or the IP address. # # replica-announce-ip 5.5.5.5 # replica-announce-port 1234  ################################## SECURITY ###################################  # Require clients to issue AUTH <PASSWORD> before processing any other # commands.  This might be useful in environments in which you do not trust # others with access to the host running redis-server. # # This should stay commented out for backward compatibility and because most # people do not need auth (e.g. they run their own servers). # # Warning: since Redis is pretty fast an outside user can try up to # 150k passwords per second against a good box. This means that you should # use a very strong password otherwise it will be very easy to break. # # requirepass foobared  # Command renaming. # # It is possible to change the name of dangerous commands in a shared # environment. For instance the CONFIG command may be renamed into something # hard to guess so that it will still be available for internal-use tools # but not available for general clients. # # Example: # # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52 # # It is also possible to completely kill a command by renaming it into # an empty string: # # rename-command CONFIG "" # # Please note that changing the name of commands that are logged into the # AOF file or transmitted to replicas may cause problems.  ################################### CLIENTS ####################################  # Set the max number of connected clients at the same time. By default # this limit is set to 10000 clients, however if the Redis server is not # able to configure the process file limit to allow for the specified limit # the max number of allowed clients is set to the current file limit # minus 32 (as Redis reserves a few file descriptors for internal uses). # # Once the limit is reached Redis will close all the new connections sending # an error 'max number of clients reached'. # # maxclients 10000  ############################## MEMORY MANAGEMENT ################################  # Set a memory usage limit to the specified amount of bytes. # When the memory limit is reached Redis will try to remove keys # according to the eviction policy selected (see maxmemory-policy). # # If Redis can't remove keys according to the policy, or if the policy is # set to 'noeviction', Redis will start to reply with errors to commands # that would use more memory, like SET, LPUSH, and so on, and will continue # to reply to read-only commands like GET. # # This option is usually useful when using Redis as an LRU or LFU cache, or to # set a hard memory limit for an instance (using the 'noeviction' policy). # # WARNING: If you have replicas attached to an instance with maxmemory on, # the size of the output buffers needed to feed the replicas are subtracted # from the used memory count, so that network problems / resyncs will # not trigger a loop where keys are evicted, and in turn the output # buffer of replicas is full with DELs of keys evicted triggering the deletion # of more keys, and so forth until the database is completely emptied. # # In short... if you have replicas attached it is suggested that you set a lower # limit for maxmemory so that there is some free RAM on the system for replica # output buffers (but this is not needed if the policy is 'noeviction'). # # maxmemory <bytes>  # MAXMEMORY POLICY: how Redis will select what to remove when maxmemory # is reached. You can select among five behaviors: # # volatile-lru -> Evict using approximated LRU among the keys with an expire set. # allkeys-lru -> Evict any key using approximated LRU. # volatile-lfu -> Evict using approximated LFU among the keys with an expire set. # allkeys-lfu -> Evict any key using approximated LFU. # volatile-random -> Remove a random key among the ones with an expire set. # allkeys-random -> Remove a random key, any key. # volatile-ttl -> Remove the key with the nearest expire time (minor TTL) # noeviction -> Don't evict anything, just return an error on write operations. # # LRU means Least Recently Used # LFU means Least Frequently Used # # Both LRU, LFU and volatile-ttl are implemented using approximated # randomized algorithms. # # Note: with any of the above policies, Redis will return an error on write #       operations, when there are no suitable keys for eviction. # #       At the date of writing these commands are: set setnx setex append #       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd #       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby #       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby #       getset mset msetnx exec sort # # The default is: # # maxmemory-policy noeviction  # LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated # algorithms (in order to save memory), so you can tune it for speed or # accuracy. For default Redis will check five keys and pick the one that was # used less recently, you can change the sample size using the following # configuration directive. # # The default of 5 produces good enough results. 10 Approximates very closely # true LRU but costs more CPU. 3 is faster but not very accurate. # # maxmemory-samples 5  # Starting from Redis 5, by default a replica will ignore its maxmemory setting # (unless it is promoted to master after a failover or manually). It means # that the eviction of keys will be just handled by the master, sending the # DEL commands to the replica as keys evict in the master side. # # This behavior ensures that masters and replicas stay consistent, and is usually # what you want, however if your replica is writable, or you want the replica to have # a different memory setting, and you are sure all the writes performed to the # replica are idempotent, then you may change this default (but be sure to understand # what you are doing). # # Note that since the replica by default does not evict, it may end using more # memory than the one set via maxmemory (there are certain buffers that may # be larger on the replica, or data structures may sometimes take more memory and so # forth). So make sure you monitor your replicas and make sure they have enough # memory to never hit a real out-of-memory condition before the master hits # the configured maxmemory setting. # # replica-ignore-maxmemory yes  ############################# LAZY FREEING ####################################  # Redis has two primitives to delete keys. One is called DEL and is a blocking # deletion of the object. It means that the server stops processing new commands # in order to reclaim all the memory associated with an object in a synchronous # way. If the key deleted is associated with a small object, the time needed # in order to execute the DEL command is very small and comparable to most other # O(1) or O(log_N) commands in Redis. However if the key is associated with an # aggregated value containing millions of elements, the server can block for # a long time (even seconds) in order to complete the operation. # # For the above reasons Redis also offers non blocking deletion primitives # such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and # FLUSHDB commands, in order to reclaim memory in background. Those commands # are executed in constant time. Another thread will incrementally free the # object in the background as fast as possible. # # DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled. # It's up to the design of the application to understand when it is a good # idea to use one or the other. However the Redis server sometimes has to # delete keys or flush the whole database as a side effect of other operations. # Specifically Redis deletes objects independently of a user call in the # following scenarios: # # 1) On eviction, because of the maxmemory and maxmemory policy configurations, #    in order to make room for new data, without going over the specified #    memory limit. # 2) Because of expire: when a key with an associated time to live (see the #    EXPIRE command) must be deleted from memory. # 3) Because of a side effect of a command that stores data on a key that may #    already exist. For example the RENAME command may delete the old key #    content when it is replaced with another one. Similarly SUNIONSTORE #    or SORT with STORE option may delete existing keys. The SET command #    itself removes any old content of the specified key in order to replace #    it with the specified string. # 4) During replication, when a replica performs a full resynchronization with #    its master, the content of the whole database is removed in order to #    load the RDB file just transferred. # # In all the above cases the default is to delete objects in a blocking way, # like if DEL was called. However you can configure each case specifically # in order to instead release memory in a non-blocking way like if UNLINK # was called, using the following configuration directives:  lazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del no replica-lazy-flush no  ############################## APPEND ONLY MODE ###############################  # By default Redis asynchronously dumps the dataset on disk. This mode is # good enough in many applications, but an issue with the Redis process or # a power outage may result into a few minutes of writes lost (depending on # the configured save points). # # The Append Only File is an alternative persistence mode that provides # much better durability. For instance using the default data fsync policy # (see later in the config file) Redis can lose just one second of writes in a # dramatic event like a server power outage, or a single write if something # wrong with the Redis process itself happens, but the operating system is # still running correctly. # # AOF and RDB persistence can be enabled at the same time without problems. # If the AOF is enabled on startup Redis will load the AOF, that is the file # with the better durability guarantees. # # Please check http://redis.io/topics/persistence for more information.  appendonly no  # The name of the append only file (default: "appendonly.aof")  appendfilename "appendonly.aof"  # The fsync() call tells the Operating System to actually write data on disk # instead of waiting for more data in the output buffer. Some OS will really flush # data on disk, some other OS will just try to do it ASAP. # # Redis supports three different modes: # # no: don't fsync, just let the OS flush the data when it wants. Faster. # always: fsync after every write to the append only log. Slow, Safest. # everysec: fsync only one time every second. Compromise. # # The default is "everysec", as that's usually the right compromise between # speed and data safety. It's up to you to understand if you can relax this to # "no" that will let the operating system flush the output buffer when # it wants, for better performances (but if you can live with the idea of # some data loss consider the default persistence mode that's snapshotting), # or on the contrary, use "always" that's very slow but a bit safer than # everysec. # # More details please check the following article: # http://antirez.com/post/redis-persistence-demystified.html # # If unsure, use "everysec".  # appendfsync always appendfsync everysec # appendfsync no  # When the AOF fsync policy is set to always or everysec, and a background # saving process (a background save or AOF log background rewriting) is # performing a lot of I/O against the disk, in some Linux configurations # Redis may block too long on the fsync() call. Note that there is no fix for # this currently, as even performing fsync in a different thread will block # our synchronous write(2) call. # # In order to mitigate this problem it's possible to use the following option # that will prevent fsync() from being called in the main process while a # BGSAVE or BGREWRITEAOF is in progress. # # This means that while another child is saving, the durability of Redis is # the same as "appendfsync none". In practical terms, this means that it is # possible to lose up to 30 seconds of log in the worst scenario (with the # default Linux settings). # # If you have latency problems turn this to "yes". Otherwise leave it as # "no" that is the safest pick from the point of view of durability.  no-appendfsync-on-rewrite no  # Automatic rewrite of the append only file. # Redis is able to automatically rewrite the log file implicitly calling # BGREWRITEAOF when the AOF log size grows by the specified percentage. # # This is how it works: Redis remembers the size of the AOF file after the # latest rewrite (if no rewrite has happened since the restart, the size of # the AOF at startup is used). # # This base size is compared to the current size. If the current size is # bigger than the specified percentage, the rewrite is triggered. Also # you need to specify a minimal size for the AOF file to be rewritten, this # is useful to avoid rewriting the AOF file even if the percentage increase # is reached but it is still pretty small. # # Specify a percentage of zero in order to disable the automatic AOF # rewrite feature.  auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb  # An AOF file may be found to be truncated at the end during the Redis # startup process, when the AOF data gets loaded back into memory. # This may happen when the system where Redis is running # crashes, especially when an ext4 filesystem is mounted without the # data=ordered option (however this can't happen when Redis itself # crashes or aborts but the operating system still works correctly). # # Redis can either exit with an error when this happens, or load as much # data as possible (the default now) and start if the AOF file is found # to be truncated at the end. The following option controls this behavior. # # If aof-load-truncated is set to yes, a truncated AOF file is loaded and # the Redis server starts emitting a log to inform the user of the event. # Otherwise if the option is set to no, the server aborts with an error # and refuses to start. When the option is set to no, the user requires # to fix the AOF file using the "redis-check-aof" utility before to restart # the server. # # Note that if the AOF file will be found to be corrupted in the middle # the server will still exit with an error. This option only applies when # Redis will try to read more data from the AOF file but not enough bytes # will be found. aof-load-truncated yes  # When rewriting the AOF file, Redis is able to use an RDB preamble in the # AOF file for faster rewrites and recoveries. When this option is turned # on the rewritten AOF file is composed of two different stanzas: # #   [RDB file][AOF tail] # # When loading Redis recognizes that the AOF file starts with the "REDIS" # string and loads the prefixed RDB file, and continues loading the AOF # tail. aof-use-rdb-preamble yes  ################################ LUA SCRIPTING  ###############################  # Max execution time of a Lua script in milliseconds. # # If the maximum execution time is reached Redis will log that a script is # still in execution after the maximum allowed time and will start to # reply to queries with an error. # # When a long running script exceeds the maximum execution time only the # SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be # used to stop a script that did not yet called write commands. The second # is the only way to shut down the server in the case a write command was # already issued by the script but the user doesn't want to wait for the natural # termination of the script. # # Set it to 0 or a negative value for unlimited execution without warnings. lua-time-limit 5000  ################################ REDIS CLUSTER  ###############################  # Normal Redis instances can't be part of a Redis Cluster; only nodes that are # started as cluster nodes can. In order to start a Redis instance as a # cluster node enable the cluster support uncommenting the following: # # cluster-enabled yes  # Every cluster node has a cluster configuration file. This file is not # intended to be edited by hand. It is created and updated by Redis nodes. # Every Redis Cluster node requires a different cluster configuration file. # Make sure that instances running in the same system do not have # overlapping cluster configuration file names. # # cluster-config-file nodes-6379.conf  # Cluster node timeout is the amount of milliseconds a node must be unreachable # for it to be considered in failure state. # Most other internal time limits are multiple of the node timeout. # # cluster-node-timeout 15000  # A replica of a failing master will avoid to start a failover if its data # looks too old. # # There is no simple way for a replica to actually have an exact measure of # its "data age", so the following two checks are performed: # # 1) If there are multiple replicas able to failover, they exchange messages #    in order to try to give an advantage to the replica with the best #    replication offset (more data from the master processed). #    Replicas will try to get their rank by offset, and apply to the start #    of the failover a delay proportional to their rank. # # 2) Every single replica computes the time of the last interaction with #    its master. This can be the last ping or command received (if the master #    is still in the "connected" state), or the time that elapsed since the #    disconnection with the master (if the replication link is currently down). #    If the last interaction is too old, the replica will not try to failover #    at all. # # The point "2" can be tuned by user. Specifically a replica will not perform # the failover if, since the last interaction with the master, the time # elapsed is greater than: # #   (node-timeout * replica-validity-factor) + repl-ping-replica-period # # So for example if node-timeout is 30 seconds, and the replica-validity-factor # is 10, and assuming a default repl-ping-replica-period of 10 seconds, the # replica will not try to failover if it was not able to talk with the master # for longer than 310 seconds. # # A large replica-validity-factor may allow replicas with too old data to failover # a master, while a too small value may prevent the cluster from being able to # elect a replica at all. # # For maximum availability, it is possible to set the replica-validity-factor # to a value of 0, which means, that replicas will always try to failover the # master regardless of the last time they interacted with the master. # (However they'll always try to apply a delay proportional to their # offset rank). # # Zero is the only value able to guarantee that when all the partitions heal # the cluster will always be able to continue. # # cluster-replica-validity-factor 10  # Cluster replicas are able to migrate to orphaned masters, that are masters # that are left without working replicas. This improves the cluster ability # to resist to failures as otherwise an orphaned master can't be failed over # in case of failure if it has no working replicas. # # Replicas migrate to orphaned masters only if there are still at least a # given number of other working replicas for their old master. This number # is the "migration barrier". A migration barrier of 1 means that a replica # will migrate only if there is at least 1 other working replica for its master # and so forth. It usually reflects the number of replicas you want for every # master in your cluster. # # Default is 1 (replicas migrate only if their masters remain with at least # one replica). To disable migration just set it to a very large value. # A value of 0 can be set but is useful only for debugging and dangerous # in production. # # cluster-migration-barrier 1  # By default Redis Cluster nodes stop accepting queries if they detect there # is at least an hash slot uncovered (no available node is serving it). # This way if the cluster is partially down (for example a range of hash slots # are no longer covered) all the cluster becomes, eventually, unavailable. # It automatically returns available as soon as all the slots are covered again. # # However sometimes you want the subset of the cluster which is working, # to continue to accept queries for the part of the key space that is still # covered. In order to do so, just set the cluster-require-full-coverage # option to no. # # cluster-require-full-coverage yes  # This option, when set to yes, prevents replicas from trying to failover its # master during master failures. However the master can still perform a # manual failover, if forced to do so. # # This is useful in different scenarios, especially in the case of multiple # data center operations, where we want one side to never be promoted if not # in the case of a total DC failure. # # cluster-replica-no-failover no  # In order to setup your cluster make sure to read the documentation # available at http://redis.io web site.  ########################## CLUSTER DOCKER/NAT support  ########################  # In certain deployments, Redis Cluster nodes address discovery fails, because # addresses are NAT-ted or because ports are forwarded (the typical case is # Docker and other containers). # # In order to make Redis Cluster working in such environments, a static # configuration where each node knows its public address is needed. The # following two options are used for this scope, and are: # # * cluster-announce-ip # * cluster-announce-port # * cluster-announce-bus-port # # Each instruct the node about its address, client port, and cluster message # bus port. The information is then published in the header of the bus packets # so that other nodes will be able to correctly map the address of the node # publishing the information. # # If the above options are not used, the normal Redis Cluster auto-detection # will be used instead. # # Note that when remapped, the bus port may not be at the fixed offset of # clients port + 10000, so you can specify any port and bus-port depending # on how they get remapped. If the bus-port is not set, a fixed offset of # 10000 will be used as usually. # # Example: # # cluster-announce-ip 10.1.1.5 # cluster-announce-port 6379 # cluster-announce-bus-port 6380  ################################## SLOW LOG ###################################  # The Redis Slow Log is a system to log queries that exceeded a specified # execution time. The execution time does not include the I/O operations # like talking with the client, sending the reply and so forth, # but just the time needed to actually execute the command (this is the only # stage of command execution where the thread is blocked and can not serve # other requests in the meantime). # # You can configure the slow log with two parameters: one tells Redis # what is the execution time, in microseconds, to exceed in order for the # command to get logged, and the other parameter is the length of the # slow log. When a new command is logged the oldest one is removed from the # queue of logged commands.  # The following time is expressed in microseconds, so 1000000 is equivalent # to one second. Note that a negative number disables the slow log, while # a value of zero forces the logging of every command. slowlog-log-slower-than 10000  # There is no limit to this length. Just be aware that it will consume memory. # You can reclaim memory used by the slow log with SLOWLOG RESET. slowlog-max-len 128  ################################ LATENCY MONITOR ##############################  # The Redis latency monitoring subsystem samples different operations # at runtime in order to collect data related to possible sources of # latency of a Redis instance. # # Via the LATENCY command this information is available to the user that can # print graphs and obtain reports. # # The system only logs operations that were performed in a time equal or # greater than the amount of milliseconds specified via the # latency-monitor-threshold configuration directive. When its value is set # to zero, the latency monitor is turned off. # # By default latency monitoring is disabled since it is mostly not needed # if you don't have latency issues, and collecting data has a performance # impact, that while very small, can be measured under big load. Latency # monitoring can easily be enabled at runtime using the command # "CONFIG SET latency-monitor-threshold <milliseconds>" if needed. latency-monitor-threshold 0  ############################# EVENT NOTIFICATION ##############################  # Redis can notify Pub/Sub clients about events happening in the key space. # This feature is documented at http://redis.io/topics/notifications # # For instance if keyspace events notification is enabled, and a client # performs a DEL operation on key "foo" stored in the Database 0, two # messages will be published via Pub/Sub: # # PUBLISH __keyspace@0__:foo del # PUBLISH __keyevent@0__:del foo # # It is possible to select the events that Redis will notify among a set # of classes. Every class is identified by a single character: # #  K     Keyspace events, published with __keyspace@<db>__ prefix. #  E     Keyevent events, published with __keyevent@<db>__ prefix. #  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ... #  $     String commands #  l     List commands #  s     Set commands #  h     Hash commands #  z     Sorted set commands #  x     Expired events (events generated every time a key expires) #  e     Evicted events (events generated when a key is evicted for maxmemory) #  A     Alias for g$lshzxe, so that the "AKE" string means all the events. # #  The "notify-keyspace-events" takes as argument a string that is composed #  of zero or multiple characters. The empty string means that notifications #  are disabled. # #  Example: to enable list and generic events, from the point of view of the #           event name, use: # #  notify-keyspace-events Elg # #  Example 2: to get the stream of the expired keys subscribing to channel #             name __keyevent@0__:expired use: # #  notify-keyspace-events Ex # #  By default all notifications are disabled because most users don't need #  this feature and the feature has some overhead. Note that if you don't #  specify at least one of K or E, no events will be delivered. notify-keyspace-events ""  ############################### ADVANCED CONFIG ###############################  # Hashes are encoded using a memory efficient data structure when they have a # small number of entries, and the biggest entry does not exceed a given # threshold. These thresholds can be configured using the following directives. hash-max-ziplist-entries 512 hash-max-ziplist-value 64  # Lists are also encoded in a special way to save a lot of space. # The number of entries allowed per internal list node can be specified # as a fixed maximum size or a maximum number of elements. # For a fixed maximum size, use -5 through -1, meaning: # -5: max size: 64 Kb  <-- not recommended for normal workloads # -4: max size: 32 Kb  <-- not recommended # -3: max size: 16 Kb  <-- probably not recommended # -2: max size: 8 Kb   <-- good # -1: max size: 4 Kb   <-- good # Positive numbers mean store up to _exactly_ that number of elements # per list node. # The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size), # but if your use case is unique, adjust the settings as necessary. list-max-ziplist-size -2  # Lists may also be compressed. # Compress depth is the number of quicklist ziplist nodes from *each* side of # the list to *exclude* from compression.  The head and tail of the list # are always uncompressed for fast push/pop operations.  Settings are: # 0: disable all list compression # 1: depth 1 means "don't start compressing until after 1 node into the list, #    going from either the head or tail" #    So: [head]->node->node->...->node->[tail] #    [head], [tail] will always be uncompressed; inner nodes will compress. # 2: [head]->[next]->node->node->...->node->[prev]->[tail] #    2 here means: don't compress head or head->next or tail->prev or tail, #    but compress all nodes between them. # 3: [head]->[next]->[next]->node->node->...->node->[prev]->[prev]->[tail] # etc. list-compress-depth 0  # Sets have a special encoding in just one case: when a set is composed # of just strings that happen to be integers in radix 10 in the range # of 64 bit signed integers. # The following configuration setting sets the limit in the size of the # set in order to use this special memory saving encoding. set-max-intset-entries 512  # Similarly to hashes and lists, sorted sets are also specially encoded in # order to save a lot of space. This encoding is only used when the length and # elements of a sorted set are below the following limits: zset-max-ziplist-entries 128 zset-max-ziplist-value 64  # HyperLogLog sparse representation bytes limit. The limit includes the # 16 bytes header. When an HyperLogLog using the sparse representation crosses # this limit, it is converted into the dense representation. # # A value greater than 16000 is totally useless, since at that point the # dense representation is more memory efficient. # # The suggested value is ~ 3000 in order to have the benefits of # the space efficient encoding without slowing down too much PFADD, # which is O(N) with the sparse encoding. The value can be raised to # ~ 10000 when CPU is not a concern, but space is, and the data set is # composed of many HyperLogLogs with cardinality in the 0 - 15000 range. hll-sparse-max-bytes 3000  # Streams macro node max size / items. The stream data structure is a radix # tree of big nodes that encode multiple items inside. Using this configuration # it is possible to configure how big a single node can be in bytes, and the # maximum number of items it may contain before switching to a new node when # appending new stream entries. If any of the following settings are set to # zero, the limit is ignored, so for instance it is possible to set just a # max entires limit by setting max-bytes to 0 and max-entries to the desired # value. stream-node-max-bytes 4096 stream-node-max-entries 100  # Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in # order to help rehashing the main Redis hash table (the one mapping top-level # keys to values). The hash table implementation Redis uses (see dict.c) # performs a lazy rehashing: the more operation you run into a hash table # that is rehashing, the more rehashing "steps" are performed, so if the # server is idle the rehashing is never complete and some more memory is used # by the hash table. # # The default is to use this millisecond 10 times every second in order to # actively rehash the main dictionaries, freeing memory when possible. # # If unsure: # use "activerehashing no" if you have hard latency requirements and it is # not a good thing in your environment that Redis can reply from time to time # to queries with 2 milliseconds delay. # # use "activerehashing yes" if you don't have such hard requirements but # want to free memory asap when possible. activerehashing yes  # The client output buffer limits can be used to force disconnection of clients # that are not reading data from the server fast enough for some reason (a # common reason is that a Pub/Sub client can't consume messages as fast as the # publisher can produce them). # # The limit can be set differently for the three different classes of clients: # # normal -> normal clients including MONITOR clients # replica  -> replica clients # pubsub -> clients subscribed to at least one pubsub channel or pattern # # The syntax of every client-output-buffer-limit directive is the following: # # client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds> # # A client is immediately disconnected once the hard limit is reached, or if # the soft limit is reached and remains reached for the specified number of # seconds (continuously). # So for instance if the hard limit is 32 megabytes and the soft limit is # 16 megabytes / 10 seconds, the client will get disconnected immediately # if the size of the output buffers reach 32 megabytes, but will also get # disconnected if the client reaches 16 megabytes and continuously overcomes # the limit for 10 seconds. # # By default normal clients are not limited because they don't receive data # without asking (in a push way), but just after a request, so only # asynchronous clients may create a scenario where data is requested faster # than it can read. # # Instead there is a default limit for pubsub and replica clients, since # subscribers and replicas receive data in a push fashion. # # Both the hard or the soft limit can be disabled by setting them to zero. client-output-buffer-limit normal 0 0 0 client-output-buffer-limit replica 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60  # Client query buffers accumulate new commands. They are limited to a fixed # amount by default in order to avoid that a protocol desynchronization (for # instance due to a bug in the client) will lead to unbound memory usage in # the query buffer. However you can configure it here if you have very special # needs, such us huge multi/exec requests or alike. # # client-query-buffer-limit 1gb  # In the Redis protocol, bulk requests, that are, elements representing single # strings, are normally limited ot 512 mb. However you can change this limit # here. # # proto-max-bulk-len 512mb  # Redis calls an internal function to perform many background tasks, like # closing connections of clients in timeout, purging expired keys that are # never requested, and so forth. # # Not all tasks are performed with the same frequency, but Redis checks for # tasks to perform according to the specified "hz" value. # # By default "hz" is set to 10. Raising the value will use more CPU when # Redis is idle, but at the same time will make Redis more responsive when # there are many keys expiring at the same time, and timeouts may be # handled with more precision. # # The range is between 1 and 500, however a value over 100 is usually not # a good idea. Most users should use the default of 10 and raise this up to # 100 only in environments where very low latency is required. hz 10  # Normally it is useful to have an HZ value which is proportional to the # number of clients connected. This is useful in order, for instance, to # avoid too many clients are processed for each background task invocation # in order to avoid latency spikes. # # Since the default HZ value by default is conservatively set to 10, Redis # offers, and enables by default, the ability to use an adaptive HZ value # which will temporary raise when there are many connected clients. # # When dynamic HZ is enabled, the actual configured HZ will be used as # as a baseline, but multiples of the configured HZ value will be actually # used as needed once more clients are connected. In this way an idle # instance will use very little CPU time while a busy instance will be # more responsive. dynamic-hz yes  # When a child rewrites the AOF file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. aof-rewrite-incremental-fsync yes  # When redis saves RDB file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. rdb-save-incremental-fsync yes  # Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good # idea to start with the default settings and only change them after investigating # how to improve the performances and how the keys LFU change over time, which # is possible to inspect via the OBJECT FREQ command. # # There are two tunable parameters in the Redis LFU implementation: the # counter logarithm factor and the counter decay time. It is important to # understand what the two parameters mean before changing them. # # The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis # uses a probabilistic increment with logarithmic behavior. Given the value # of the old counter, when a key is accessed, the counter is incremented in # this way: # # 1. A random number R between 0 and 1 is extracted. # 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1). # 3. The counter is incremented only if R < P. # # The default lfu-log-factor is 10. This is a table of how the frequency # counter changes with a different number of accesses with different # logarithmic factors: # # +--------+------------+------------+------------+------------+------------+ # | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   | # +--------+------------+------------+------------+------------+------------+ # | 0      | 104        | 255        | 255        | 255        | 255        | # +--------+------------+------------+------------+------------+------------+ # | 1      | 18         | 49         | 255        | 255        | 255        | # +--------+------------+------------+------------+------------+------------+ # | 10     | 10         | 18         | 142        | 255        | 255        | # +--------+------------+------------+------------+------------+------------+ # | 100    | 8          | 11         | 49         | 143        | 255        | # +--------+------------+------------+------------+------------+------------+ # # NOTE: The above table was obtained by running the following commands: # #   redis-benchmark -n 1000000 incr foo #   redis-cli object freq foo # # NOTE 2: The counter initial value is 5 in order to give new objects a chance # to accumulate hits. # # The counter decay time is the time, in minutes, that must elapse in order # for the key counter to be divided by two (or decremented if it has a value # less <= 10). # # The default value for the lfu-decay-time is 1. A Special value of 0 means to # decay the counter every time it happens to be scanned. # # lfu-log-factor 10 # lfu-decay-time 1  ########################### ACTIVE DEFRAGMENTATION ####################### # # WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested # even in production and manually tested by multiple engineers for some # time. # # What is active defragmentation? # ------------------------------- # # Active (online) defragmentation allows a Redis server to compact the # spaces left between small allocations and deallocations of data in memory, # thus allowing to reclaim back memory. # # Fragmentation is a natural process that happens with every allocator (but # less so with Jemalloc, fortunately) and certain workloads. Normally a server # restart is needed in order to lower the fragmentation, or at least to flush # away all the data and create it again. However thanks to this feature # implemented by Oran Agra for Redis 4.0 this process can happen at runtime # in an "hot" way, while the server is running. # # Basically when the fragmentation is over a certain level (see the # configuration options below) Redis will start to create new copies of the # values in contiguous memory regions by exploiting certain specific Jemalloc # features (in order to understand if an allocation is causing fragmentation # and to allocate it in a better place), and at the same time, will release the # old copies of the data. This process, repeated incrementally for all the keys # will cause the fragmentation to drop back to normal values. # # Important things to understand: # # 1. This feature is disabled by default, and only works if you compiled Redis #    to use the copy of Jemalloc we ship with the source code of Redis. #    This is the default with Linux builds. # # 2. You never need to enable this feature if you don't have fragmentation #    issues. # # 3. Once you experience fragmentation, you can enable this feature when #    needed with the command "CONFIG SET activedefrag yes". # # The configuration parameters are able to fine tune the behavior of the # defragmentation process. If you are not sure about what they mean it is # a good idea to leave the defaults untouched.  # Enabled active defragmentation # activedefrag yes  # Minimum amount of fragmentation waste to start active defrag # active-defrag-ignore-bytes 100mb  # Minimum percentage of fragmentation to start active defrag # active-defrag-threshold-lower 10  # Maximum percentage of fragmentation at which we use maximum effort # active-defrag-threshold-upper 100  # Minimal effort for defrag in CPU percentage # active-defrag-cycle-min 5  # Maximal effort for defrag in CPU percentage # active-defrag-cycle-max 75  # Maximum number of set/hash/zset/list fields that will be processed from # the main dictionary scan # active-defrag-max-scan-fields 1000

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[33b27e999a...](https://github.com/mrakgr/The-Spiral-Language/commit/33b27e999a4fd8f2241b2626d267b0eb732d3b9d)
#### Wednesday 2020-05-06 15:53:17 by Marko GrdiniÄ‡

"3:40pm.

///

I am finally familiar enough with various web-related technologies to start work on a VS Code plugin for my own language [Spiral](https://github.com/mrakgr/The-Spiral-Language/tree/v0.2). On the JS/TS side of things, the [`node-ipc`](https://www.npmjs.com/package/node-ipc#basic-examples) seems like what I need, but on the F# side I am not really sure what I should pick to do a server.

I do not need much. Ideally on the .NET side, I want a library that is able to do IPC communication of JSON strings via fast native sockets in a cross platform fashion. If possible, I'd like to avoid doing my own library using .NET primitives.

Using fast native sockets isn't even a requirement, but merely a bonus. I do need the server and the client to be cross platform however.

I vaguely remember 1-2 months ago when I looked at it that Giraffe supports native sockets, so I'll look into that now, but I thought I'd ask here as well. There is a lot of stuff to process [in its documentation](https://github.com/giraffe-fsharp/Giraffe/blob/master/DOCUMENTATION.md).

As a side note, it seems that `node-ipc` does not use ports for native socket IPC, but some of the [.NET IPC stuff](https://docs.microsoft.com/en-us/dotnet/api/system.runtime.remoting.channels.ipc.ipcchannel?view=netframework-4.8) that I've found does. Why is that?

///

I am thinking what I should write, but this is the best I can come up with. Let me go with it.

3:50pm. https://kotlinlang.org/

```
// Target either the JVM or JavaScript. Write code in Kotlin and decide where you want to deploy to

import kotlin.browser.window

fun onLoad() {
    window.document.body!!.innerHTML += "<br/>Hello, Kotlin!"
}
```

Oh this thing compiles to JS. That is interesting.

...Well, a lot of the languages do these days. Nevermind that for now.

3:55pm. https://www.reddit.com/r/fsharp/comments/gek8cv/what_should_i_be_using_to_do_ipc_for_a_language/

Right now I am just browsing the F# sub.

4:05pm. https://www.reddit.com/r/fsharp/comments/g5cija/f_tooling_state_after_net_core_churn/fo529g9?utm_source=share&utm_medium=web2x
"This is because the notion of adding a file is actually more complex than IntelliSense, unintuitively. In Visual Studio, adding a file triggers the following:

* MSBuild evaluation to recognize the new file

* File ordering logic is kicked off to determine _where_ in the dependency order that file needs to go based on the _kind_ of file addition, the _kind_ of file being added, and _where_ the file is being added. For example, this could be an existing file being added to a subfolder that needs to be ordered above a set of linked files. Gotta get that right, otherwise IntelliSense will complain about not knowing what certain symbols are.

* Design-time build (a massive thing in and of itself) is kicked off, generating data that is fed into an abstraction know as a Workspace

* The Workspace then passes updated data to the F# compiler service, which triggers typechecking to make sure everything is up to date

This is accomplished by a vast amount of code built by multiple fully staffed teams. No such machinery exists in VSCode for either F# or C#; what's there is very basic and represents the best efforts of a very small number of people. That's why it's wonky sometimes :)"

I might want to link this in the review.

https://www.reddit.com/r/fsharp/comments/g5cija/f_tooling_state_after_net_core_churn/
> I don't know if this is good practice or not, but I just have Paket installed in Windows via Chocolatey. This way, I don't have to do download Paket to each folder, and I can just start a project with paket init.

Interesting. I might try this in the future should I need to. Hopefully it won't slip my mind.

4:20pm. https://www.youtube.com/watch?v=jByCiwycoPM&feature=youtu.be
F# 5.0 Preview

Let me watch this. Right now, my mind is too fatigued to really grind documentation so I'll spend my time in different ways. I'll get back to reading the Giraffe docs after that.

https://suave.io/

Ah, I was reading the docs for this a few months ago. This is the thing I was thinking about when I said Giraffe.

4:30pm. https://youtu.be/jByCiwycoPM?t=383

Here he talks about attracting Python devs who love succinctness of Python, but not writing code.

https://youtu.be/jByCiwycoPM?t=488

He mentions that the F# compiler code base is 300k LOC. Yeah, that is huge by any measure.

> Translating that into C# terms that would be 2-3M LOC.

Oh wow.

5:15pm. https://youtu.be/jByCiwycoPM?t=1808

Almost done with this. Since the focus was on UI experience, this is really convincing me to give F# 5 a try in its preview state. I might want to go for it. The actually language changes are fine, but they are minor in terms of utility.

5:35pm. I keep taking these breaks. Let me just stop here.

Actually trying to process information today is just giving me a headache. I really am in no state to be doing this. I am going through the Suave documentation, but I can't find where I saw that mention of protocols. I'll leave this as work for tomorrow.

For F# 5 I think I'll just wait until it comes out. I do not want the hassle of installing it right now.

Today what I did was give those `ipc-node` examples a try. Tomorrow, my goal will be to get Suave to interact with the Node IPC server. I'll use numbered ports just to get things started and establish a base. For the next steps if I cannot do them, I'll ask around more. The Suave library has an issue page, and I might get a reply by tomorrow in that thread.

5:45pm. Every time I've been dead tired during the day due to a restless night I've always had restful sleep the night following it, so tomorrow I should get back into the groove.

I've done plenty of TS at this point and know how to feed all the information that I need to VS Code. I just have to establish the servers and I can begin integrating what I've done of v0.2 so far with VS code. The TS side is essentially set, all I have to figure out how do the server on the F# side.

This will not take me too long. Let me take off here."

---
## [thwaitesproject/thwaites](https://github.com/thwaitesproject/thwaites)@[389f372ea3...](https://github.com/thwaitesproject/thwaites/commit/389f372ea3b5062c3906e323dce004ccf3469ccc)
#### Wednesday 2020-05-06 16:44:30 by Will Scott

Added 2d and 2.5d runscripts for ice step domain based on Kimura et al 2013

2d run seems to be stable with Kv= 0.1m^2/s near the open ocean and linearly
scaling down as height of the layers decreases towards the grounding line.
Run to 100 days ok.

Before this I tried the values of Kv = 1e-5m^2/s in the paper, but it crashed
before 1 day. There is some instability that seems to happen near the
grounding line wall when the domain has completely flushed out and
you get flow back into the cavity.

I tried constant Kv = 1e-4,1e-3 and 1e-2. 1e-4 and 1e-3 both crashed before 1 day
but 1e-2 ran to 44 days. by 20 days though an instability was forming near
the grounding line.

I tried a 2.5d setup because I thought (naively?) that the velocities would be
slower with the u component. This didnt seem to happen and with Kv = 0.1m^2/s
the model crashed at 21 days.

The aspect ratio of the grid dx/dz = 1e3/(100/9) = 90 at the grounding line
and is (1e3/1e2)= 10 at the ice front and outside the cavity. So from a numerics
point of view not so obvious that Kh should be much bigger than Kv. In the fluidity
paper Kh = 100m^2/s. I havent tried lower values of Kh in case that makes it better.

Also importantly I haven't tried lowering the timestep. (set at 200s)

Or the grid resolution. On the plus side the run with 4 cores is very fast
1min = 1 day. And the melt rate profile is smooth due to the layered mesh.
The magnitudes also seem about right 1e-5m/a cf with fluidity results. we
are using different constants for GammaT/S though. mainly out of laziness on my
part and not wanting to mess up the melt param last night.

I thought that layer convergence near the grounding line wall might be affecting
melt rates (which definitely go up and down with the incoming flow) so
I added the linear ramp back in. For Kv = 0.01 this made the 2d simulation
crash quicker only lasting at 21 days compared to 44.

At this point raised open ocean Kv by another order of magnitude to 0.1m^2/s
(THis is actually the value given in isomip for when convective instability's
occur on a 32 layer grid - here only 9 layers)

The run was stable up and reached 100 days. It was more diffuse than Kimura 13
though. i.e the bottom layer was 0.96 degc compared to 1.8 degc when the initial
conditons and source regions were 2degc. it is quite a slow restoring rate though
Trestoring = 10 days at edge and goes down to 30 days at 90km.

So reasons for why we can't exactly replicate kimura 13:
- I've made a silly mistake with the setup. could well be possible because I set it up
last night...
- There is something wrong with our setup/model so we are inherently less stable
(less numerical diffusion?) than Fluidity. If it is just numerical diffusion then
is this necessarily a bad thing?
- if this is the case would lowering timestep or increasing resolution help?
- possibly to do with ratio of Kv/Kh. which may be related to aspect  ratio of grid too.
- or something inherently unstable about the 2d setup... maybe in 3d more drag. should
be quite easy to check because I have the mesh. just need to update boundary ids. This
seems like wishful thinking though.

Also needed to update .geo file for layered mesh because without defining
a physical surface of the 2d mesh gmsh didnt save the 2d part of the mesh!
Also note to self firedrake needs boundary id tags not descriptions you can add
in gmsh.

Finally also updated the 2.5d mitgcm comparison run to include the option for
outputs of arrays needed for matplotlib plotting to compare with mitgcm.

---
## [Monkeypac/tm-twitch-map-command](https://github.com/Monkeypac/tm-twitch-map-command)@[986b60adce...](https://github.com/Monkeypac/tm-twitch-map-command/commit/986b60adce318ae6d3f7023857753a356e40342d)
#### Wednesday 2020-05-06 17:18:27 by Thomas Magalhaes

fix update map command crash

* Situation
When loading a map, pressing the "Manually update" button, the game
crashes.

* Investigation:
I reproduced instantly the bug by doing the EXACT same thing:
open MP -> RPG -> Solo -> Deep Blue -> Manually update -> crash
Let's call this Situation A.

I tried to find what was in cause there.
I added so many prints.
I suspected the Twitch connection socket having a problem being used
at the same time in two routines.
I suspected a max time for a routine execution.
I suspected the RenderMenu function to not allow this shit.
==> NOPE

Then I tried another map than DeepBlue (Eldar). WOW it worked.
But then even stranger. Once Eldar worked, I could go to DeepBlue and
update the command. WEIIIIIIIIIIRD. Let's call this Situation B.

Okay so I tried again to go to DeepBlue from a Restart aaaand crash.

In the meanwhile, I had tested the AutoUpdate and could then eleminate
the problem being linked to the RenderMenu button, it crashed too.

I then noticed something in the logs when running Situation B.
- On eldar
- loads map details
- loads from MX
- Sends message
- On DeepBlue
- loads map details
- Sends message

Oh, no MX for DeepBlue. Let me check that with Situation A.
You get it, now it loads from MX so cool, problem is from MX AND
bonus: I have another bug about not being able to load from MX on a
second map.

Call MX for DeepBlue manually.
Okay, there is a big comment section for this one that I don't have
for the other maps. Like a big one but not THAT big.
There are also some weird characters in there. ... Uhuh

```
    "Comments": "[align=center][b]Deep Blue[/b]\r\n[i]a duo Trial by
    [user]14727[/user] & PLTX[/i][/align]\r\n[hr]\r\n[i]Here is a
    project that was pretty time-consuming in the making.  :build:
    :wait: \r\n[/i]\r\n[b]GPS at every CP![/b]\r\n\r\nWe went for a
    [i]summer-themed ambience[/i], so you can stay inside to enjoy the
    summer instead of being a healthy human being!\r\n\r\nTrial is
    about finishing, time doesn't matter. So focus and keep
    calm.\r\n\r\n[i]Mod : \"PoolMod\" by
    Popgun[/i]\r\n\r\n[b][u]Updated[/u][/b] [i]17/08/18[/i]\r\n[item]
    fixed tiny cut at cp3 where it was possible to skip one balance
    beam\r\n[item] changed start of cp9 for something easier\r\n[item]
    last cp is sliiightly easier\r\n\r\n[u]This track is available
    online on this server
    :[/u]\r\nmaniaplanet://#join=trialtm2@RPG@tmrpg\r\n[hr]\r\n[item]
    [b]Full run video :[/b]
    [url=https://youtu.be/J6udGvesSfg]https://youtu.be/J6udGvesSfg[/url]
    :done: \r\n[hr]\r\n[align=center][b] :award:  People who finished
    :[/b]\r\n\r\nSkandeaR _ ~1h50\r\nMatt _ 31mins\r\nBren _
    35mins\r\nBRandOM _ 2h10\r\nAriana G. _ ~1h50\r\nNarcor _
    ~21mins\r\nWingobear _ ~2h30\r\nAgent _ ~1h50\r\nMicmo _
    ~2h\r\nL94 _ ~6h\r\nMilmy _ ~8h\r\nKeby _ 1h05\r\nErizel _
    ~2h45\r\nJe suis pas _ ~2h12\r\nOprx _ ~4h06\r\nSpam _
    ~5h30\r\nriolu! _ ~5h30\r\nJe suis pas la _ 12h42\r\nYannex _
    ~1h10[/align]",
```

But wait, I don't use the Comments. And there are maps on MX that have
large Comments sections too that do work.

...

~Keeps searching~

...

OH wait, I print the whole response from MX in the logs. Don't tell me
that ... YEP. The print didn't like some characters.

* Fix
Remove a print. (that's pretty much all)

Also, fix the bug about not being able to query MX a second time.

---
## [Bakinak/MTA20834-Semester-Project](https://github.com/Bakinak/MTA20834-Semester-Project)@[5617548401...](https://github.com/Bakinak/MTA20834-Semester-Project/commit/56175484012f20b403c684894c3ba8a2a5e55835)
#### Wednesday 2020-05-06 17:59:14 by Bakinak

BIG CHANGES TO FEEDBACK

In this version, you are no longer guaranteed feedback when correctly steadying the boat. Instead, first  time you do it, you have 60% + modifier to get feedback. Thereafter, only 60 %, so to better mimic shit BCI accuracy, as we were basically cheating by always providing feedback before.

Also reverted previous change to make boat only tilt once for each wave, now it will continue to tilt as long as it is in the wave, to encourage players to keep steadying, and thereby experience shit accuracy.

---
## [seni1/Data-Engineer-Nanodegree-Assignments](https://github.com/seni1/Data-Engineer-Nanodegree-Assignments)@[ae024239f6...](https://github.com/seni1/Data-Engineer-Nanodegree-Assignments/commit/ae024239f6171f1161d48efbf63d9e845d0ed637)
#### Wednesday 2020-05-06 18:46:09 by Seni Kamara

Create intro_NoSQL

When Not to Use SQL:
Need high Availability in the data: Indicates the system is always up and there is no downtime
Have Large Amounts of Data
Need Linear Scalability: The need to add more nodes to the system so performance will increase linearly
Low Latency: Shorter delay before the data is transferred once the instruction for the transfer has been received.
Need fast reads and write
Here is a helpful blog that describes the different types of NoSQL databases. You can bookmark it to review this later too.

Eventual Consistency:
Over time (if no new changes are made) each copy of the data will be the same, but if there are new changes, the data may be different in different locations. The data may be inconsistent for only milliseconds. There are workarounds in place to prevent getting stale data.

Commonly Asked Questions:
What does the network look like? Can you share any examples?
In Apache Cassandra every node is connected to every node -- it's peer to peer database architecture.

Is data deployment strategy an important element of data modeling in Apache Cassandra?
Deployment strategies are a great topic, but have very little to do with data modeling. Developing deployment strategies focuses on determining how many clusters to create or determining how many nodes are needed. These are topics generally covered under database architecture, database deployment and operations, which we will not cover in this lesson. Here is a useful link to learn more about it for Apache Cassandra.

In general, the size of your data and your data model can affect your deployment strategies. You need to think about how to create a cluster, how many nodes should be in that cluster, how to do the actual installation. More information about deployment strategies can be found on this DataStax documentation page

Citation for above slides:
Here is the Wikipedia page cited in the slides.

Cassandra Architecture
We are not going into a lot of details about the Apache Cassandra Architecture. However, if you would like to learn more about it for your job, here are some links that you may find useful.

Apache Cassandra Data Architecture:

Understanding the architecture
Cassandra Architecture
The following link will go more in-depth about the Apache Cassandra Data Model, how Cassandra reads, writes, updates, and deletes data.

Cassandra Documentation

CAP Theorem:
Consistency: Every read from the database gets the latest (and correct) piece of data or an error

Availability: Every request is received and a response is given -- without a guarantee that the data is the latest update

Partition Tolerance: The system continues to work regardless of losing network connectivity between nodes

Additional Resource:
You can also check out this Wikipedia page on the CAP theorem.

Commonly Asked Questions:
Is Eventual Consistency the opposite of what is promised by SQL database per the ACID principle?
Much has been written about how Consistency is interpreted in the ACID principle and the CAP theorem. Consistency in the ACID principle refers to the requirement that only transactions that abide by constraints and database rules are written into the database, otherwise the database keeps previous state. In other words, the data should be correct across all rows and tables. However, consistency in the CAP theorem refers to every read from the database getting the latest piece of data or an error.
To learn more, you may find this discussion useful:

Discussion about ACID vs. CAP
Which of these combinations is desirable for a production system - Consistency and Availability, Consistency and Partition Tolerance, or Availability and Partition Tolerance?
As the CAP Theorem Wikipedia entry says, "The CAP theorem implies that in the presence of a network partition, one has to choose between consistency and availability." So there is no such thing as Consistency and Availability in a distributed database since it must always tolerate network issues. You can only have Consistency and Partition Tolerance (CP) or Availability and Partition Tolerance (AP). Remember, relational and non-relational databases do different things, and that's why most companies have both types of database systems.

Does Cassandra meet just Availability and Partition Tolerance in the CAP theorem?
According to the CAP theorem, a database can actually only guarantee two out of the three in CAP. So supporting Availability and Partition Tolerance makes sense, since Availability and Partition Tolerance are the biggest requirements.

If Apache Cassandra is not built for consistency, won't the analytics pipeline break?
If I am trying to do analysis, such as determining a trend over time, e.g., how many friends does John have on Twitter, and if you have one less person counted because of "eventual consistency" (the data may not be up-to-date in all locations), that's OK. In theory, that can be an issue but only if you are not constantly updating. If the pipeline pulls data from one node and it has not been updated, then you won't get it. Remember, in Apache Cassandra it is about Eventual Consistency.


Data Modeling in Apache Cassandra:
Denormalization is not just okay -- it's a must
Denormalization must be done for fast reads
Apache Cassandra has been optimized for fast writes
ALWAYS think Queries first
One table per query is a great strategy
Apache Cassandra does not allow for JOINs between tables
Commonly Asked Questions:
I see certain downsides of this approach, since in a production application, requirements change quickly and I may need to improve my queries later. 
Isn't that a downside of Apache Cassandra?
In Apache Cassandra, you want to model your data to your queries, and if your business need calls for quickly changing requirements, you need to create a new table to process the data. 
That is a requirement of Apache Cassandra. If your business needs calls for ad-hoc queries, these are not a strength of Apache Cassandra. 
However keep in mind that it is easy to create a new table that will fit your new query.

Additional Resource:
Here is a reference to the DataStax documents on [Apache Cassandra].(https://docs.datastax.com/en/dse/6.7/cql/cql/ddl/dataModelingApproach.html)


Primary Key
Must be unique
The PRIMARY KEY is made up of either just the PARTITION KEY or may also include additional CLUSTERING COLUMNS
A Simple PRIMARY KEY is just one column that is also the PARTITION KEY.
A Composite PRIMARY KEY is made up of more than one column and will assist in creating a unique value and in your retrieval queries
The PARTITION KEY will determine the distribution of data across the system
Here is the DataStax documentation on Primary Keys.


Clustering Columns:
The clustering column will sort the data in sorted ascending order, e.g., alphabetical order. 
Note: this is a mistake in the video, which says descending order.
More than one clustering column can be added (or none!)
From there the clustering columns will sort in order of how they were added to the primary key
Commonly Asked Questions:
How many clustering columns can we add?
You can use as many clustering columns as you would like. You cannot use the clustering columns out of order in the SELECT statement. 
You may choose to omit using a clustering column in your SELECT statement. 
That's OK. Just remember to use them in order when you are using the SELECT statement.

Additional Resources:
Here is the DataStax documentation on Composite Partition Keys.
This Stackoverflow page provides a nice description of the difference between Partition Keys and Clustering Keys.

WHERE clause
Data Modeling in Apache Cassandra is query focused, and that focus needs to be on the WHERE clause
Failure to include a WHERE clause will result in an error
Additional Resource
AVOID using "ALLOW FILTERING": Here is a reference in DataStax that explains ALLOW FILTERING and why you should not use it.

Commonly Asked Questions:
Why do we need to use a WHERE statement since we are not concerned about analytics? Is it only for debugging purposes?
The WHERE statement is allowing us to do the fast reads. With Apache Cassandra, we are talking about big data -- think terabytes of data -- so we are making it fast for read purposes. 
Data is spread across all the nodes. By using the WHERE statement, we know which node to go to, from which node to get that data and serve it back. For example, imagine we have 10 years of data on 10 nodes or servers. 
So 1 year's data is on a separate node. By using the WHERE year = 1 statement we know which node to visit fast to pull the data from.

---
## [cornell-dti/samwise](https://github.com/cornell-dti/samwise)@[536737261f...](https://github.com/cornell-dti/samwise/commit/536737261f87ccf2296ae779c70a90e5b2939df9)
#### Wednesday 2020-05-06 20:55:57 by Sam

Seamless drag-and-drop experience

The drag and drop user experience is far from being perfect.
To make order persistent, we have to save the updated order in the firestore. Although firestore seems to have ['latency compensation'](https://firebase.google.com/docs/firestore/query-data/listen#events-local-changes) that can avoid a roundtrip before updated data is pushed to remote, the listener update is still not **synchronous**, which causes a big trouble in dnd that relies an a synchronous state change.

In the past, we introduced `localTasks` as a cache in focus view to solve the problem. We are able to mostly avoid the dropped task bouncing back to the original position, but there are still some occasional flashes. In future view, such caching mechanism has not been implemented, so the resulting experience is even worse.

Recently, I encountered a similar problem in my side projects, and I found a more principled and easier way to solve it. Surprisingly, this easier way also makes the UX better. An crucial observation is that

> Redux store update is synchronous.

Therefore, instead of `setState` locally in a component, why don't we do similar stuff to the redux store? Then we can avoid creating two versions of truth. It's also easier to implement, since the functions we call to update on firebase already works on change sets, and our redux store updates also work on change set.

After this change, we are able to completely remove those ugly local caches and make the app more responsive!

Try to drag and drop within focus view, from focus view completed to uncompleted, within a day in future view and between days.
No more flashes!

---
## [basshelal/Wudooh](https://github.com/basshelal/Wudooh)@[dd327c3f5b...](https://github.com/basshelal/Wudooh/commit/dd327c3f5bd8346e5c98f15b590762c526539e29)
#### Wednesday 2020-05-06 21:48:16 by Bassam Helal

Publishing has proven to be extremely difficult and complicated, things aren't working anywhere except chrome, I'm sleeping on this for now.

I hate publishing and I especially hate Firefox and their shitty tools and platform.
Fuck you Firefox!

---
## [andrew0030/Pandoras-Creatures](https://github.com/andrew0030/Pandoras-Creatures)@[5d8f751b77...](https://github.com/andrew0030/Pandoras-Creatures/commit/5d8f751b774dd448c9bbb83e8a456110c8dc25ea)
#### Wednesday 2020-05-06 23:19:22 by andrew0030

Made ISTER code a few light years better

What the hell was I thinking when I coded this shit, well lesson learned, dont code for over 10 hours.

---

# [<](2020-05-05.md) 2020-05-06 [>](2020-05-07.md)

