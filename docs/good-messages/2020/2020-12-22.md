# [<](2020-12-21.md) 2020-12-22 [>](2020-12-23.md)

2,609,449 events, 1,294,100 push events, 2,032,921 commit messages, 159,334,479 characters


## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[59eff877e4...](https://github.com/mrakgr/The-Spiral-Language/commit/59eff877e425a6d473e2bd5d1174f5180be5cfdd)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"9:05am. I am up. I've rested.

9:10am. Without a doubt, XCOM 2 on Legendary is great. Remember the first 3 days on Veteran which were fun incarnate and then it was all easy missions after that. Well right now, I've been playing it for far more than that and every mission has had me on the edge of my seat. Every time I finish the session, I am covered in sweat from the tension of the decision making. This is despite my room being cool.

9:15am. If only I could get the wins in the game to matter in the real world I would be fulfilled as a person. It is why I am striving to get better at all aspects of programming.

Games were not supposed to be like this. Every bit of effort I put into the game, I could have put to advance other aspects of my life. Do the games exist only to make me a evolutionary dead end. No! I will not let it be like that!

The purpose of AI is to draw power from the games. They have the power to do so. In contrast to humans, the potential of beings who can control their own evolution is infinite.

9:20am. I just can't take money seriously. Maybe that is why I failed at trading - I could never give it the thought it deserved. Still, nobody and nothing will get away from the simple principle that optimizing for power is the true purpose. Not money, and not people. When I have the power, I will have everything else.

Today's mission is to do the monthly review.

9:25am. I did not think I would get into CFR so much. As an algorithm, it is definitely worth studying. Two player zero sum games are the essence of everything. Even just in its depth first form, it has a lot to say about how sampling should be done.

It is one of the core algorithms.

9:30am. Though I feel I have the responsibility to figure out averaging, I really like CFR+ a lot more. It is a lot conceptually cleaner as it only works based off regrets sums. It just makes more sense. Had I been the one doing research into this, I'd be much more likely to come up with CFR+ than CFR itself. Thinking in terms of types, the strategies are like oranges while regrets are apples. CFR+ does not have oranges and only has apples.

Even though they are both floats, I do have the notion that values have underlying latent types beyond those that can be typechecked so I am basing this judgement based on that. That it works better is just the icing on the cake.

With CFR+ I do not have to figure out why the weighting is done only by first player's probability either.

9:35am. Let me start the review.

---

Last month, feed up with backprop I made that rant about reversibility. At that time, I had no notion of what I would do with it. I really thought it would take me ages to get anywhere and I've been hoping I would get some good insight from studying homotopy type theory. Instead, for the first two weeks while I was watching [math lectures](https://www.youtube.com/channel/UCriFv3G22iOUidUhkIGXuhw) I would be assaulted by inspiration. I've made reversibility a principle of credit assignment and my subconscious was responding, constantly feeding me a steady stream of ideas. The highlight of the month is me making this [draft paper](https://github.com/mrakgr/The-Spiral-Language/blob/6bfd104cdf27cd6460d0e59fbf490c2bb354eab0/Learning/Papers/principle%20of%20reversibility.md) on how to combine those ideas with current deep learning methods.

Just so there is no misunderstanding, my vision last month was higher than this. There really should be a path from type theory to learning, if type theory is to be a grand theory of computation. I've yet to draw any real inspiration from that. The draft material is nothing more than an appetizer.

But at the very least, now I have an understanding of how to do something that has eluded me ever since I started work on RL seriously 1.5 years ago. Namely how to do stable credit assignment in all circumstances. Together with exploration, I have everything I need to make powerful RL agents.

Put together, these ideas are good enough for me to get back into the ring. I threw in the towel back in January, and now it is time for rematch. Unlike before, I am not going to be doing it in Spiral.

I've learned my lesson with that. Developing a language while at the same time working on ML, while good for Spiral is not that good for me as a ML practitioner. The burden is much larger. I've gotten my fill of Spiral. Spiral is a bet on the future - eventually the algorithms will all be there, and the neuromorphic hardware will be there as well and that will be Spiral's turn to shine. I had never intended for it to be vehicle for doing ML experiments which I ended up using it as.

I created it because it was so difficult to get good GPU capabilities in F#, which I wanted in a ML library. I was a true believer in deep learning, and my view was is if you want it to be done properly you need to do it yourself. So I wanted to do it properly, and that I did. It is personally disappointing to me that nobody else really felt that way - I thought there would be a likeminded person or two with a love of functional programming wanting to make his own library with excellent GPU support. I know that Spiral has many uses beyond that - rather than marketing it here, it would have been better to push it to the game and embedded crowd. I have no interest in that though. But still I guess that shows that my quest was pretty unique.

Well, at any rate, the GPUs themselves are a big disappointment when it comes to RL. Their memory is hard to manage and they are actually slower than CPUs when batching is not done. They might be suitable for supervised learning, but reinforcement learning not so much. A better kind of hardware is definitely needed.

So until that happens, I am going to do what I tried in January. I am going to force myself to just do the ML experiments in Python to save time for the parts that need deep learning capabilities. I'll do the sane thing for once and make sure the algorithms are there before I pull the trigger on Spiral again. The v0.1 requires a rewriting of the libraries which I do not want to spend time doing at the moment.

---

Let me talk a little about math. I've been doing theorem proving in mostly Coq, and Agda since late April and for the first three weeks of last month, I've been watching math lectures. I really wish I could say that I managed to get anything out of this to improve my programming, but that has not happened yet. Though I've gotten better at synchronizing the reasoning, my Coq proofs are still far short of the internal models I use when doing regular programming.

Still, compared to me of the past years, I now at least know what a proof is. I actually would not have been able to answer this decisively before April.

With this newfound insight and drawing on the weight of my experience as a programmer, I can state that Wildberger is right that a lot of the fields of mathematics are [logically weak](https://youtu.be/JpEd1Mmgggc) and that [rigor has gone down over time](https://youtu.be/Nnp4qZX9H1U). Thanks to the experience of the past few months, I can now understand that my math weakness is not entirely due to lack of talent.

It is going to be an absolute pain to fix all of this and to put math on a firm foundation. The benefit of this being done will be worth it. Meeting the high standard of having all math checkable by a computer will make learning it much easier. It will also eliminate the room for error in mathematical results. Most importantly, it will kill the general quackery of mathematics.

Personally, I'd really like to see how Bayesian statistics might be done without necessarily assuming that one can integrate a function from negative to positive infinity. Or the use of reals. As it turns out, it is possible to do differentiation and integration with only rationals.

Because of the way things are being done right now, it is not at all straightforward to take some ML paper and put the proofs into a proof assistant which is an ability I'd hoped to attain with my theorem proving efforts. I'd like to live in a world where that would be possible.

There is going to be resistance to getting there, and a necessary part of it is getting rid of [some kinds of reasoning](https://www.homeschoolmath.net/teaching/proof_square_root_2_irrational.php) that are a firm fixture in modern mathematics. Towars that end even though I'll be busy with ML from here on out, I'll make it a resolution to study type theory by going through the Cubical library in full, and go through Wildberger's lectures by the end of next year. Honestly, just watching lectures, or studying theorem proving all day like I have done till now is too boring for me, but by spreading out the effort I might be able to attain something in the longer term. I've gone at it full time for over a few months, but as expected, that is not sustainable for me.

---

As my goal was originally to master poker first before moving on to bigger and better things, currently I am [studying CFR](https://int8.io/counterfactual-regret-minimization-for-poker-ai/). I've already made a translation of the Java [`Intro to CFR`](http://modelai.gettysburg.edu/2013/cfr/cfr.pdf) paper [to F#](https://github.com/mrakgr/The-Spiral-Language/tree/3d8b4df4d4f71d9e133f2c2f55c8e983459a537e/Learning/CFR/Intro) months ago, and the next step in my plan is to actually understand what the algorithm is doing. Along the way, I intend to translate the sampling algorithms in Marc Lanctot's 2013 [thesis](http://mlanctot.info/files/papers/PhD_Thesis_MarcLanctot.pdf) to F#, along with the sampling variants from the newer papers.

This should be done by the end of October. After that I'll get to implementing the reversibility ideas in my draft and combining the CFR algorithms with neural nets. This will give me elite poker playing agents for any kind of variant of the game. It should not take me too long now that I've made up my mind to go after it directly rather than using it as an excuse to work on a language. I'll stop assuming the world will just give me what I need and take it myself.

---

11:45am. This should be fine. Writing has an effect of strengthening one's resolve.

Let me take a break here. One thing I've wanted to do for a while is watch a talk by Michael Bowling. Since I am taking a break in general today, let me do that for a bit. I'll read the book after that."

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[487c5dbb55...](https://github.com/mrakgr/The-Spiral-Language/commit/487c5dbb55236d361135e4d05eefeb313acfe478)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"12:50pm. Let me resume.

1:05pm. 54/149. My focus is starting to go down, but I need to do it. Read this thing. Internalize it. And then move on.

Yeah, I know it is sad. I hate it how I spent so much time trying to figure out omnipotence, life and why this universe exists only to finally crack it and have it be as useless as an algorithm in an era without computers. It is always like that. I spent so much time making Spiral and creating a whole new style of programming, only for GPUs to be a huge disappointment for RL and the algorithms that I need to not even be here. Or lately, I wanted to master theorem proving and succeeded at it only to realize that the fields of mathematics that I am actually interested in do not care about logical consistency.

It is ironic that during my trading days all I aimed for was money, but ended up figuring out the self improvement loop.

Maybe I should consider a career in painting?

Fuck my life. All that I've felt lately is loneliness at the power that I desire being away from me for so long.

I really have no idea what I am going to do if discrepancy reduction turns out to be a bust.

The next 3 months will be a lonesome road. But I will walk it as it is the only choice.

If I can get discrepancy reduction to stabilize RL and make it consistent...then everything will become alive. The plan I've set will come into motion. Spiral will have a reason for existing and will be a fragment that I will be able to tap into when the time comes. The neuromorphic chips with terrabytes of non-volatile memory will be the death knell of human era. If I can only make this little thing work then the doors will open.

To be sure, even if discrepancy reduction and all the rest of the pieces that I have in mind work, there will still be issues of long term credit assignment and meta-learning. I do not have an answer to those. But I do not have to answer that right now. What I need the most is stable RL that works on toy games like poker.

That is the absolutely most important thing.

1:25pm. Maybe it is fine if I die again. The old me which strove for omnipotence for so long died when he figured out the self improvement loop. Then the new me who wanted to make up for turning his back on programming took his place.

I guess that one died back in January when I threw in the towel on ML. In 2019 I accepted that just being good at programming is not enough. So I've been seeking something to fill the void.

1:30pm. I still have not found it. But with just a little more effort I might.

Let me resume the thesis.

2pm. 56/149. Here is the first sampling algorithm. I had time to think about it. Let me implement it.

2:35pm. Had to take a break. Let me finally do some programming.

2:40pm. First let me do tha pruning optimization.

I'll also clean up the intro a little and get rid of the unsightly FSI stuff. I need to pare down the honesty a tad for the sake of things flowing smoothly.

```
// The refined design for the CFR agent. Here I've finally succeeded in factoring out the inference logic from the game logic.
// This results in quite a generic implementation that could be used for other games. Having the correct design in mind will
// also make implementing the more complicated FSI-CFR much easier.
```

It is embarrassing to have comments like this in only to change my mind about implementing FSI later.

2:50pm.

```
--> Timing now on

Average game value: -0.040474
Real: 00:00:18.019, CPU: 00:00:17.687, GC gen0: 7114, gen1: 9, gen2: 0
val it : unit = ()

--> Timing now off
```

2:55pm.

```
--> Timing now on

Average game value: -0.040474
Real: 00:00:02.779, CPU: 00:00:02.750, GC gen0: 1017, gen1: 8, gen2: 0
val it : unit = ()

--> Timing now off
```

The pruning optimization really works. The reason why it works so well is because regrets frequently swing between singularities.

2:55pm. Now with that bit of cleanup out of the way, let me finally start the implementation of the outcome sampling CFR.

3:05pm. Ugh, what a pain, but I must do it.

3:25pm.

```
let sample (actions : 'action []) (distribution : float []) =
    if actions.Length <> distribution.Length then failwith "The two arrays must be equally sized."
    let r = rng.NextDouble()
    let rec loop i cumulative_probability =
        if i < actions.Length then
            let cumulative_probability = cumulative_probability + distribution.[i]
            if r < cumulative_probability then actions.[i]
            else loop (i+1) cumulative_probability
        else
            failwith "impossible"
    loop 0 0.0

let sample_uniform (actions : 'action []) = actions.[rng.Next(actions.Length)]
```

Right now I am just shuffling thing around as I make room in my head for how to do this.

```
let sample (actions : 'action []) (distribution : float []) =
    if actions.Length <> distribution.Length then failwith "The two arrays must be equally sized."
    let r = rng.NextDouble()
    let rec loop i cumulative_probability =
        if i < actions.Length then
            let cumulative_probability = cumulative_probability + distribution.[i]
            if r < cumulative_probability then actions.[i], distribution.[i]
            else loop (i+1) cumulative_probability
        else
            failwith "impossible"
    loop 0 0.0

let sample_uniform (actions : 'action []) = actions.[rng.Next(actions.Length)], 1.0 / float actions.Length
```

Let me do it like this actually.

3:35pm. Things are really coming back to me now. It has really been a while since I did real programming. Theorem proving might qualify as that, but it is just so different from the kind of design work that I would consider real programming.

With real programming my issues stem from organizational difficulties, but while working in Coq and Agda my focus was on very low level correctness issues. I think that if I eventually get good enough at theorem proving that the kind of thinking I am doing in Coq and Agda will start to resemble more the one in Spiral and F#, but for now that is not the case. It does not matter.

Coq and Agda were such a struggle, but this - this! This is what I am good at!

3:40pm. Let me put up the algorithm again. Here is what I am going to do...

```
OutcomeSampling(h, i, t, πi, π−i, s):
```

First of all, that `s`. I am going to split it into two so as to avoid having to pass that parameter explicitly. In the change node I will just multiply the two together to get the same effect.

3:50pm. Now, the actual algorithm...I am going to have to split it into two.

3:50pm.

```
if P(I) = i then σ' (I) ← e · Unif(I) + (1 − e)σ(I)
```

This part confuses me a bit. Shouldn't `e` be dependent on the number of iterations?

Also is there any point to calculating path tail? Maybe it is an evolutionary offshot from other algorithms?

Well, whatever.

Let me go for it.

Let me make the two response functions and then I will figure out how to put them together. No point in just sitting here thinking about it.

4pm.

```
let inline reponse_first node one two actions next =
    let propositional_distribution = regret_match node.regret_sum
    let sampling_distribution = Array.map (fun prob -> one.uniformity_coef * (1.0 / float (Array.length actions)) + (1.0 - one.uniformity_coef) * prob) propositional_distribution
    let action = sample actions sampling_distribution
    ()
```

So far so good. This is going great.

4:05pm.

```
let inline reponse_first node one two actions next =
    let propositional_distribution = regret_match node.regret_sum
    let sampling_distribution = Array.map (fun prob -> one.uniformity_coef * (1.0 / float (Array.length actions)) + (1.0 - one.uniformity_coef) * prob) propositional_distribution
    let action, prob = sample actions sampling_distribution
    let utility = next (action, {one with }
```

Ah, no wait. I need the probability for both the sampling and the proposal distribution.

4:15pm.

```
type Probability = {sample : float; proposal : float} with
    static member (*) (a, b) = {sample=a.sample*b.sample; proposal=a.proposal*b.proposal}
```

```
let inline reponse_first node one two actions next =
    let proposal_distribution = regret_match node.regret_sum
    let sampling_distribution = Array.map (fun prob -> one.uniformity_coef * (1.0 / float (Array.length actions)) + (1.0 - one.uniformity_coef) * prob) proposal_distribution
    let action, sample_probability = sample actions sampling_distribution proposal_distribution
    let utility = next (action, {one with probability=one.probability * sample_probability})
    ()
```

This is nice. Now what is next?

```
W · (π^σ(z[I]a, z) − π^σ(z[I], z)) 4.12
 −W · π^σ(z[I], z) 4.15
```

4:40pm.

```
let inline reponse_first node one two actions next =
    let proposal_distribution = regret_match node.regret_sum
    let sampling_distribution = Array.map (fun prob -> one.uniformity_coef * (1.0 / float (Array.length actions)) + (1.0 - one.uniformity_coef) * prob) proposal_distribution
    let action, node_probability, i = sample actions sampling_distribution proposal_distribution
    let utility = next (action, {one with probability=one.probability * node_probability})
    let W = utility * two.probability.proposal // for some reason importance sampling is not being done here
    for i'=0 to actions.Length-1 do
        let r =
            if i = i' then -W * node_probability.proposal
            else W * (one.probability.proposal * proposal_distribution.[i] - node_probability.proposal)
        node.regret_sum.[i] <- node.regret_sum.[i] + r
```

Hmmm...seems fine. Though the algorithm is not actually doing importance sampling for some reason. I am going to have to read the explanation more carefully.

Now let me do the other side.

5:10pm.

```
let inline chance dice one next =
    let dice = sample_uniform dice
    /// Outcome sampling does not consider the probabilities in chance nodes
    next {one with state=dice}

let inline response_first node one two actions next =
    let proposal_distribution = regret_match node.regret_sum
    let sampling_distribution = Array.map (fun prob -> one.uniformity_coef * (1.0 / float (Array.length actions)) + (1.0 - one.uniformity_coef) * prob) proposal_distribution
    let action, node_probability, i' = sample actions sampling_distribution proposal_distribution
    let utility, tail_probability = next (action, {one with probability=one.probability * node_probability})
    let W = utility * two.probability.proposal // for some reason importance sampling is not being done here
    for i=0 to actions.Length-1 do
        let r =
            if i = i' then -W * node_probability.proposal
            else W * (one.probability.proposal * proposal_distribution.[i] - node_probability.proposal)
        node.regret_sum.[i] <- node.regret_sum.[i] + r
    -utility, tail_probability * node_probability.proposal

let inline respose_second node one two actions next =
    let proposal_distribution = regret_match node.regret_sum
    let action, node_probability, _ = sample_uniform' actions proposal_distribution
    let utility, tail_probability = next (action, {one with probability=one.probability * node_probability})
    for i=0 to actions.Length-1 do
        node.strategy_sum.[i] <- node.strategy_sum.[i] + float (one.count - !node.count) * two.probability.proposal * node_probability.proposal
        node.count := one.count
    -utility, tail_probability * node_probability.proposal

let terminal one two x = x * one.probability.sample * two.probability.sample, 1.0
```

This should be it for the outcome sampling algorithm. I'll leave testing it for tomorrow.

The algorithm is not too large, but there are plenty of details that I needed to consider for it.

It took me a while to structure all of this.

Let me reread the relevant section of the thesis again. I am really mostly concerned why importance sampling is not being done for W.

`node_probability.proposal)`

No wait. This can't be right...

5:20pm.

```
        let r =
            if i = i' then -W * node_probability.proposal
            else W * (one.probability.proposal * (proposal_distribution.[i] - 1.0))
```

Still, making an exception for the current action does not make any sense to me. It would make sense if the selected action's probability is literally 1, but that won't always be the case.

Well, whatever.

5:35pm.

```
Unlike in Vanilla CFR and chance-sampled CFR, the average strategy updates are applied on the opponent’s turns to enforce the unbiasedness of the update to the average strategy.
```

Hmmm...

```
This is equivalent to performing (t − cI) updates to the average all at once, assuming the player had played the recently-computed strategy for all (t − cI) iterations since it was last sampled. Doing this is incorrect, as the increment to the average strategy should be the sum of terms π^σ t i (I) over iterations {t, t + 1, · · · }.
```

I see. So that is what is meant by optimistic.

...Ah, also I understand why `W` is not being divided by the sampling probability. Probably because the utility itself is getting divided by that, so there would duplication going on.

5:45pm. I'll call it a day here. Tomorrow I will try it out on Dudo and see if I can get it to work. I'll do the rest of the algorithms after that. It will be easier once I get this first one going.

5:50pm. Somehow, reading this thesis and implementing the algorithms within feels like exactly the thing I should be doing. I really do wonder how they can prove anything at all about these things. I really wish I understood how such proofs work. Coming to an understanding of that will be a long term theme for me.

It really has been a long while since I programmed last. The immersion of programming brings me that familiar high."

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[6de54eb956...](https://github.com/mrakgr/The-Spiral-Language/commit/6de54eb95616bb704149ab9e5c8d9a6808cb8063)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"7:35am. I actually woke up a while ago and could not get back to sleep. Well, let me dive into it.

...Well, as soon as I get over my groginess anyway. I've been thinking a lot about CFR. Until it was pointed out to me, I really did not see the connection between it and Minimax, so I've been going back and forth between mapping a Minimax solution to a CFR one in my head. Up to now, I've thought that the averaging of strategies was the most mysterious thing about CFR, but now I definitely think it is the summing of regret.

7:45am. I do not really have a good insight of what strategy averaging should be doing.

I sort of see something.

Suppose you have strategies `[1,0,0]*2`, `[0,1,0]*1` and `[0,0,1]*0.5`, the you should be able to add them together to get `[2,1,0.5]`. Normalize that and you get a new average strategy. It is a Bayesian approach.

The whole reweighting of things with path probabilities keeps going between making complete and no sense for me.

7:50pm. If anything I am even more confused about some aspects of vanilla CFR than when I started. Why is strategy multiplied by the path probability of just player one? And why is the regret multiplies by the path probability of just player two?

If we are talking about path probabilities, should it not be the total path? That would be the Bayesian thing.

7:55pm. I do not understand regret matching. Is there something special about it? Because it feel to me that just using the max action would be fine.

No actually, even before that what exactly is the purpose of even subtracting the sampling value from the true value?

8:20am. Uah, I am really obsessing about this. If I were designing a Bayesian tabular RL algorithm, it would look a bit like vanilla CFR, but there so many pieces that do not match it.

10:15am. I've been deep in thought all this time. I think I understand most of CFR now.

As expected, the discrepancy between the algorithm in the thesis and the way I implemented it is key.

At its core, CFR is a Bayesian algorithm that picks a player to optimize while holding the opponent steady and alternates between the two. Probably my implementation is right, but the way it done obscures that vital fact that I needed to understand the algorithm in the first place.

With the alternating optimization concept in mind, the rest of the mysteries can be explained in terms of optimization.

Let me go from the top.

```
let inline response node one two actions next =
    let action_distribution = regret_match node.regret_sum
    add node.strategy_sum (fun i -> one.probability * action_distribution.[i])

    let util, util_weighted_sum =
        array_mapFold2 (fun s action action_probability ->
            let util =
                if action_probability = 0.0 && two.probability = 0.0 then 0.0 // the pruning optimization
                else next (action, {one with probability=one.probability*action_probability})
            util, s + util * action_probability
            ) 0.0 actions action_distribution
    add node.regret_sum (fun i -> two.probability * (util.[i] - util_weighted_sum))

    -util_weighted_sum
```

I still do not understand regret matching just yet, or the way strategy is updated, but I understand `add node.regret_sum (fun i -> two.probability * (util.[i] - util_weighted_sum))` in particular. I understand why there is not `one.probability *` in there.

Because contrary to the initial notion, there are not just two players strictly speaking.

The algorithm rather than alternating between two players, or rather in addition to that, creates a simulacrum for every discrete choice. Then it fuses the probability masses of all those extras with the player being optimized.

This by itself is simple enough to understand in terms of probabilitic programming, but it will also go the extra mile to adjust the utility depending on player one's original for the sake of variance reduction. This is the subtraction by the weighted sum `(util.[i] - util_weighted_sum)`.

Because all the probabilities are either 1.0 (in the choice taken) or 0.0, there is no need to multiply by `one.probability`. For creating the extras, the algorithm is not really sampling from player one's strategy.

But for player two, it is indeed sampling his actions. Hence it multiplies by his path probabilities in `two.probability * (util.[i] - util_weighted_sum)`.

Notably, those simulacrum players are in fact regret sums.

I've always found it really strange that a player would have two policies. One that oscillates between extreme values and the other which is the target of all the optimization and yet is never used. Yet, if I imagine that regret sums are set to zero at the beginning every episode, then the whole scheme starts to make sense.

Well, ignoring regret matching which I do not understand. A lot of in the algorithm above come prepacked with a lot of optimizations whose purpose is unknown.

```
let inline response node one two actions next =
    let action_distribution = regret_match node.regret_sum
    add node.strategy_sum (fun i -> one.probability * action_distribution.[i])
```

Though it would be more work, it would really be a lot better if strategies are updated after regret sums are finished being calculated. Right now I have no idea what is going on here.

One more thing...

```
            let util =
                if action_probability = 0.0 && two.probability = 0.0 then 0.0 // the pruning optimization
                else next (action, {one with probability=one.probability*action_probability})
            util, s + util * action_probability
```

It really would make a lot more sense if action probabilities here were calculated based on the `strategy_sum` as those are the player's actual policies while regret sums are temporaries. Well, this stuff is just variance reduction so it being done incorrectly should not affect the validity of convergence, only the performance of the algorithm.

It would make the pruning optimization inapplicable though.

10:50am. CFR cannot be described directly in terms of probabilistic programming. From that perspective it has several meta features that are specific to the structure of the problem it is designed to solve which PP does not cover within its framework. But nonetheless, I find the perspective helpful for understanding what is going on.

I think that at this point I understand roughly half of it.

I do not feel like programming the MCCFR variants today. What I want to do is backtrack and figure out the reasoning behind regret matching. These updates are really strange and are not something I would have ever expected.

```
    let action_distribution = regret_match node.regret_sum
    add node.strategy_sum (fun i -> one.probability * action_distribution.[i])
```

These two lines are a complete mystery to me. Regret sums almost make sense to me, but this this thing...just does not fit.

11am. Ok, fine. After all this thinking, I guess it is time to do some reading. Let me go through the thesis from start so I can hunt down the references. I am yet to see CFR explained properly. I should really just look for the paper it was introduced along with regret matching.

[124; 123].
43; 42

http://martin.zinkevich.org/publications/regretpoker.pdf
Regret Minimization in Games with Incomplete Information

This is the 2008 in which CFR was introduced. Let me read it.

11:30am. I am looking at eq 4 here and am thinking 'does this make sense'? Does it make sense for the average strategy to be path dependent?

11:35am. "To this end, we can use Blackwell’s algorithm for approachability to minimize this regret
independently on each information set"

I am going to have to look up that algorithm.

12pm. 9/14. Fuck! I do not understand the proofs at all!

I need to do something so as to move beyond that. Just how would I plug RL theorems into Coq or Agda?

That capability would very much improve my confidence of them.

https://math-comp.github.io/mcb/
Mathematical Components

This is a Coq book that I found linked in one of the SO questions just now.

http://web.eecs.umich.edu/~jabernet/eecs598course/fall2013/web/notes/lec22_112013.pdf

This paper in Blackwell's theorem.

12:15pm. The paper that introduced CFR was even more useless in terms of insight than my most pessimistic assumption. What a waste of time.

12:20pm. I really need to take a break here.

No. Even though I wrote all of the above, I know of no principled way to go from the probabilistic programing perspective, to what is currently here. I am just straining my mind to come up with the above interpretation.

The algorithm as it stands is just a mismash of weird things.

I haven't met any of my goals with regard to theorem proving at all. I need to be much, much better at it to get through this hurdle. Shit. Should I abandon ship and go read the Coq book I just linked to?

12:25pm. Well, I'll do my best either way. One day I will get through this hell of ambiguity. If I ever figure out a principled way of reasoning through the CFR algorithm, I will have accomplished a wonderful thing."

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[c3e33276fa...](https://github.com/mrakgr/The-Spiral-Language/commit/c3e33276fab08b16af249c62240fb9a6e24db727)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"10:05am. __fmease__: "So irrational numbers don't exist because they are not computable? In the field of computer science, computability (and time & space complexity) is one of the most important concepts obviously. It's worth pursuing new algorithms/data structures. But should we limit ourselves to computability in the field of mathematics as well? I don't think so."

me: "Why not?

I agree that there is a tradeoff there. Making logical consistency a principle of mathematics (and not just programming) would necessitate a radical rethink of various areas. This is a practical burden. It is also a mental burden. It is a bit like selling a stock at a lower price than one bought it - it feels like you take an immediate loss, whereas if you hold on there is always hope that it might go up and you'll be able to sell it at a profit.

But the fact is, as soon the stock went down, the loss is real even if you do not sell it. And if you accept the loss, you can free up the remaining capital to invest in more promising ventures.

Mathematics, or more precisely, the mathematical community might be beyond base monetary concerns, but the fact is there are absolutely such things as cognitive budgets. You, me, the whole community have only a limited amount of focus that can be allocated to do research in areas of interest. We might not dabble in stocks, but we constantly do credit assignment throughout our life to things that are important.

By internalizing a principle you narrow you field of search. You lose the ability to do some of the things you could do before, but you also gain the option to go further in areas you might not have thought possible.

Internalizing the right principles is important. Without principles, there would be nothing but the brute forcing of infinities."

10:20am. I've been writing that post. I really need to get to work on that reply to Lanctot. I saw the email by him at 11pm yesterday, but I was too tired to compose a reply.

...Actually rather than writing rant, let me just thank him for the references and I'll go into rant mode when I actually finish the studies. Hopefully he would not mind me pasting these into a public journal.

Marc Lanctot: "Mi Marko,

Cool. Here are a few replies.

On Sun, Sep 29, 2019 at 8:49 AM Marko Grdinic <mrakgr@gmail.com> wrote:
> It took me a while to figure out why in the regret matching equation, only the probability of the opposing player is used.

Yes, exactly. You want to think of it as a measure of regret given that player i plays to reach I with probability 1. So the "counterfactual" part of the regret is only taking into account the reach probability of the other players (since the reach probability of the player to play at I is equal to 1). This is an interesting way to define values (it's fundamentally different than the way values are defined in reinforcement learning -- see Section 3.2 of https://arxiv.org/abs/1810.09026)

> So for updating regrets, what CFR is doing is assuming that the current player is using an uniform strategy for exploration - at least in the preceding nodes.

No that's not quite right (but I see why you think of it that way). The nodes visited are as if they were sampled from a uniform distribution over all states.. but that's a sampling distribution, that's not the same as the actual current strategy used by player i being uniform.

When computing counterfactual values and counterfactual regret in vanilla CFR, the sampling parts do not come in since the values are not estimates, they are exact. The sampling distributions are more relevant in the Monte Carlo variants.

> Now that just leaves figuring out the strategy averaging.

> Considering what I wrote previously, strategy averaging assumes the the current player is following the strategy based off the current regrets, but that the opponent is using an uniform strategy. I guess not considering opponent probabilities does make sense on toy games like rock paper scissors, but going beyond that is an open problem for me. The same goes for why strategy averaging works in general.

The averaging of a strategy for player i is completely independent of the opponent strategy. The weighting by the reach probably is to make sure that the average strategy computed in extensive-form is the same as the equivalent exponentially larger normal form game. This is an application of Kuhn's theorem: every extensive-form game has an equivalent (exponentially large) normal form representation (see page 134 of http://www.masfoundations.org/mas.pdf). The average strategy is being computed in a way that ensures it is equivalent to the average in this normal form equivalent.

The construction of the sequence-form might help with this as well (Chapter 5 of that same book), of the XFP paper might be also a good reference: http://proceedings.mlr.press/v37/heinrich15.pdf . The best reference was Gintis's Game Theory Evolving, but they unfortunately removed it when they moved to second edition. I'll try to see if I can find those few pages that explain it, as there is currently no good reference."

Marc Lanctot: "Here's some overview on behavior vs mixed strategies: https://www.youtube.com/watch?v=tT0E7PaDVck"

10:40am. I've made a brief reply and asked him about the softmax and exponential averaging.

At any rate, let me start going through the references.

Let me read the first two chapter of the PRG book and then I will take a look at the references.

10:55am. Ok, ok, focus me. Stop browsing social media.

The book!

11:25am. 27/403. Holy shit. This book goes from a simple example in the first few pages, to the absolute arcane short afterwards. I can't follow all these sums, exponentials, Greek letters and inequalities. Even figuring just a single thing here feels like it could take me days. Change of plans, I will skim the book.

12pm. https://www.youtube.com/watch?v=49PHeGZzttM
"Trading without Regret" by Dr. Michael Kearns

I am going to drop the book as it is too complicated. It is a prime exhibit of 'needs to be put into a proof assistant'. It is way beyond what I expected.

I am going to take a break here. The title of this caught my attention while I was looking for no regret learning yesterday, so I'll watch it later. I guess I'll be going through Lanctot's references earlier than I thought I would. Maybe I will get somewhere today."

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[3d8b4df4d4...](https://github.com/mrakgr/The-Spiral-Language/commit/3d8b4df4d4f71d9e133f2c2f55c8e983459a537e)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"2:05pm. Almost done. Let me chill for a bit longer and then I will start.

2:25pm. https://arxiv.org/abs/1407.5042
Solving Large Imperfect Information Games Using CFR+

This is the paper that introduced CFR+. I'll leave it for later.

2:30pm. http://www.ii.uni.wroc.pl/~lukstafi/pmwiki/uploads/AGT/Prediction_Learning_and_Games.pdf

Somebody posted this book in a Reddit thread. Let me take a look.

I am looking up resources for no-regret learning.

2:35pm. Rather than make that mail later, let me do it now.

Maybe Lanctot will give me a reply in which case I'll be using up time of both people, but I have no intention of of pestering them for every little thing. I'll get these basic questions out of the way and then do the rest on my own unless it is something vital.

3pm.

"Hello. I have a few questions about the CFR algorithm and the paper.

It took me a while to figure out why in the regret matching equation, only the probability of the opposing player is used. At first I thought that the current player's probability was just dropped, but then I realized that this dropping off is equivalent to multiply by an uniform probability in each branch.

So for updating regrets, what CFR is doing is assuming that the current player is using an uniform strategy for exploration - at least in the preceding nodes. This is what imagine an equivalent sampling algorithm would be doing. The depth first search is essentially emulating a sampling algorithm in a deterministic manner.

1) That having said, I do not at all see the reason behind the average strategy updates. It assumes the the current player is following the strategy based off the current regrets, but that the opponent is using an uniform strategy.

I guess not considering opponent probabilities does make sense on toy games like rock paper scissors, but I can't at all figure why that reasoning would be valid in extended form games.

2) Why does strategy averaging work? Is there a simple explanation of this. I got plugged the 'Kuhn's theorem and the equivalence between mixed strategies and behavioral strategies' paper, but I find it too complicated an answer for what is in my mind a simple question.

I wish the paper went into depth on this.

3) I am not having much success understanding the first proof in the appendix. Taken at its face, the lemma that full regret is <= that the sums of individual regrets makes sense.

But in eq 8 the full counterfactual regret is defined, and then it is redefined to be something else in eq 9.

Is this supposed to mean that eq 8 and 9 are supposed to be equal? If so, what are the steps to go from 8 to 9?

Similarly between 9 and 10, the max is split and the equality because an <=. How does that work logically?

- Marko Grdinić"

Let me go with this. I am looking at his homepage and it has not been updated for 8 years. I hope he is still alive.

3:05pm. Now where was I? Let me check out that book. After that I'll go through the CFR+ paper.

3:15pm. I really hope he does not might me posting these in the journal.

"Hi Marko,

I have not been through this in a while, but I'm quite sure that Eq. 9 should be an inequality (less than or equal to) so not equivalent to Eq 8. This is because all it's doing is putting a max a \in A(I); it's upper-bounding the full counterfactual regret by saying it's the regret at I plus the regret of the subtree with the largest regret under it. This is in general looser than the mixed strategy (Eq 8) which does not necessarily put all of its weight on the maximal one.

succ(I' | I,a) certainly includes the other player's policy, because it depends on how the other players act at the intermediate nodes. Note that the definition has \sigma in the superscript, i.e. it's succ^{\sigma}(I' | I, a).

Hope this helps,
Marc"

I am sorry for doubting you. I guess you were busy. Yeah, I am nasty today.

"Thank you for the clarification. So assuming that eq 9 is an inequality, what exactly is going on going from eq 9 to 10 on a logical basis? How can the max operation be split as if it were distributive?

I know that I am nitpicking about details - sorry about that, but the only kind of theorem proving that I understand is the kind that is done in theorem provers like Coq or Agda. I want to see if I could at least attain an understanding of the first proof.

The lemma that the full regret can be split into sums of individual regrets makes sense to me, but when I try to cache myself into the proof, on eq 10 I realize that this split of the max might result in the inequality flipping. I just can't ground it on a logical basis and I can't see what in the structure of the equation 9 makes such an operation viable."

3:25pm. Let me get back to reading the book.

3:40pm. Actually let me take a break. Somehow I am completely drained. Unlike yesterday it does not feel like I did much of anything today.

4:15pm.

```
> Not sure why you'd think this step would flip the inequality...?

`max_{a} (f_1(a) + f_2(a))` restricts the action passed to both functions to be equal while optimizing them like in `max_a(f_1(a)) + max_a(f_2(a))` means the actions can be independent.  

Let me demonstrate with a bit of F# code.

---
type Actions = A | B | C

let f = function
    | A -> 1
    | B -> 0
    | C -> -100

let g = function
    | A -> -100
    | B -> 0
    | C -> 1

let max_comb = [|f A + g A; f B + g B; f C + g C|]
let max_indep = [|f A + g B; f A + g C; f B + g A; f B + g C; f C + g A; f C + g B|]

---

val max_comb : int [] = [|-99; 0; -99|]
val max_indep : int [] = [|1; 2; -100; 1; -200; -100|]

---

When I run the script as you can see from `max_comb` and `max_indep`, optimizing the components independently might result in better results than optimizing them together. When optimizing the max together the best solution is `f B + g B = 1` while for independent optimization the best solution is `f A + g B = 2`.

This is why I can't follow the step from 9 -> 10.
```

I wrote this out and then realized that this exactly proves what he said. Ah damn it. I've been making a mental error all this time. Somehow I've been under the impression that since the subject is regret minimization that the max should be minimizing the quantity. It just slipped past me somehow.

Gah! So it makes perfect sense.

4:55pm. Had to take some time to make a reply. Also finished studying the 6 page CFR+ paper. It is really quite a simple algorithm.

Revisiting CFR+ and Alternating Updates
https://arxiv.org/abs/1810.11542

5:15pm. Efficient CFR for Imperfect Information Games with Instant Updates
https://realworld-sdm.github.io/paper/27.pdf

I think I am going to call it a day here. I do not feel like doing it anymore. My focus is near its lowest point and has been for a while now.

Today it feels like all the real work was done during the night while I was in bed. I've only been gathering study materials while in front of the screen. I got that book, but I could not manifest even an iota of focus.

Tomorrow what I will do is take some time off to do the monthly review.

Then I will focus on going through that 400 page book on no regret learning. Hopefully I will figure out why the strategy averaging works after that. Hopefully that will improve my understanding and confidence enough that I will be able to return to implementing the sampling algorithm from Lanctot's thesis after that."

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[f6a7a2eb16...](https://github.com/mrakgr/The-Spiral-Language/commit/f6a7a2eb166e2a50cd1e52c1ec3c919112c6f1cd)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"10:15am. Let me finally start.

Throughout my life, when I consumed fiction I always looked for hints how to make the character powers realistic. Whether it be Symphogear or something else, right now nothing really comes close to my desire of showing truth about power. Deep down, I am really hungry for realism about the transcendent.

Simulacrum was a step forward, but even if I were to continue it, I have no idea what powers would be realistic. I want to know at any cost. No matter how much media comes out, it will never come close. I need to do it myself.

Killing yourself for power is cool, and radical when it was introduced, but I am satisfied just saying that it is viable. I do not want Simulacrum to just be about proselyting that view. I want to be the one to bring it closer to reality.

10:20am. To do that I need to get closer to the essence of AI.

I haven't gotten any reply from Lanctot yet. To be honest, I am starting to wonder whether he can even answer the questions about the proof given that he could not point out that player one's sampling distribution is uniform in CFR and just hid behind the proof and the definition. Or maybe just does not feel like it. Well, it is not like he has any obligation to do so, but it is a bad look for the field if it turns out that the very first proof makes no sense.

The point of them is not for the authors to look smart, but to convince me of something. So far it is utterly failing at that.

It just reinforces Wildberger's view that mathematical rigor has steadily gone down since Euclid's time.

Now that I understand formal theorem proving, unlike before I have a taste of what a proof is and is not. I know what I proof is and I have the ability to pass judgment. The ideal proof is like reading a step by step manual about something. The RL proofs that I've seen fall far short of that ideal.

And ideally, one would gather up the proofs from the papers and work on putting them into a proof assistant. This is more like a PhD project, and I do not want to pursue it personally.

10:25am. Leaving proof issues aside, the CFR algorithm itself is definitely worth studying. It is really a challenge to figure out what it does given all the different dynamics criss-crossing about.

I need to disentangle them.

Today I will work on figuring out how strategy averaging works. Suppose you were playing RPS, then it makes sense to not consider the probabilities of the other player when updating the strategy.

Still, I have no idea how to generalize this understanding to more complex games.

10:45am. Focus me. Let me finally start reading the paper.

11:30am. ...The paper is useless. It is unreadable in its complexity.

11:45am. "There is a well-known connection between regret, average strategies, and Nash equilibria."

Yeah, what is that connection? The one time I need a reference it is not there.

https://pdfs.semanticscholar.org/0184/855c7baafdbcadcab967d4bfa7d4f8b86285.pdf

"For each individual iteration of our training, the regrets may be temporarily skewed in such a way that an important strategy in the mix has a negative regret sum and would never be chosen. Regret sums and thus individual iteration strategies are highly erratic. What converges to a minimal regret strategy is the average strategy across all iterations. This is computed in a manner similar to getStrategy above, but without the need to be concerned with negative values."

12:10pm. https://int8.io/counterfactual-regret-minimization-for-poker-ai/#Why_Nash_Equilibrium

Maybe I am getting too pessimistic and impatient. Maybe I should just sit down and read the paper on Kuhn's theorem, but let me take a look at the alternatives first.

12:05pm. https://pdfs.semanticscholar.org/0184/855c7baafdbcadcab967d4bfa7d4f8b86285.pdf

The Java Intro to CFR just links to the original CFR paper, but does not explain why the average strategy converges to an equilibrium.

12:10pm. https://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10110/10196

Let me read this thing.

12:20pm. 4/6. This paper is blisteringly complex. I can't understand it as I do not understand the methods being compared to.

12:30pm. Let me stop here. I am not as irritated as before, but I am not making much progress. I'll read a few papers and then just mail Zinkevich to ask him about strategy averaging, and maybe the proofs in the appendix."

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[5984822e65...](https://github.com/mrakgr/The-Spiral-Language/commit/5984822e659fceaee852cf1bca8508e139e9e685)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"1:10pm. Done with breakfast, but let me chill a while longer.

1:30pm. For a bit longer. After that I will resume in full force.

1:55pm. Done with the break.

https://www.youtube.com/watch?v=KzZPueNROz8
The Tea Time Talks: Kris De Asis, Finite-Horizon Temporal Difference Methods (June 5)

This caught my interest so let me go through it. After that I'll resume studying the proofs.

2:15pm. https://youtu.be/KzZPueNROz8?t=818

Actually you could divide the horizons by two and then the learning would be sublinear in 2^(-H).

This idea could be promising, but regardless, using Zap + discrepancy reduction would be more promising.

I wish I'd thought of this idea myself though.

https://youtu.be/KzZPueNROz8?t=1142

Yeah, this is what one would expect.

2:35pm. This is such a simple and elegant idea. I love it.

2:45pm. Well, forget it. My idea is even better. Let me go back to figuring out CFR first.

2:50pm. Okay, I can agree that Lemma 5 is right baring the holes and slight logical inconsistencies. I understand what the full regret is, so saying that the full regret is <= immediate regret + full regret of successors is probably true. Right now I do not like the proof at all though. Unless I can get an explanation I will just pretend it is not there. Let me move on to the next thing.

Now that I focused on the definitions more closely though, I can see why the action probabilities of player under consideration are not used. Because the regret is minimized against the idealized simulacrum which has pure strategies only.

For what I can tell using the probabilities of both does work as one might expect, but it has a lot more variance. This just goes to show that if you are going to pick a target, then it should be an ideal one.

3pm. That having said, that only explains why the player one probabilities are not considered. Why do the player two probabilities have to play a role?

3:10pm. Node sharing purposes. I've been thinking about these lines. For example, if there are chance nodes that are not uniform you'd want to reflect that. For a game tree where there is no sharing, regular Minimax would do, but once sharing enters the picture to attain integrity, you'd want to multiply by the probabilities.

3:15pm. Ok, I had suspected that much yesterday.

Still, what confused me is that by my estimate, I am pretty sure that Dudo should have no sharing apart from the uniform chance nodes. That means that the Minimax style of updating should have worked and yet, the results were clearly wrong. The variance absolutely exploded in a way I cannot explain.

3:40pm. Focus me, focus. It is already this late. Where does time leave? I have no idea.

10/14. My focus is getting lower. It was high during the first proof, but since I could not internalize it I've gotten dejected.

I need to figure out how they go from the regret matching proof, to an iterative proof over the whole tree.

4:15pm. No I can't see it. I admit I've been skimming the later sections rather than full out focusing like for the first proof, but from what I can tell, they just generalize the Blackwell theorem and then using the theorem that `full regret <= sums of immediate regrets`, they conclude the case. But this is the issue for me, I am really having trouble figuring out what a system of mutually interactive nodes is doing. By comparison, what the individual nodes do is trivial. One can use the softmax or one can do regret matching, and both would work. I admit that out of everything that could be done, regret matching makes the most sense.

Is there a way to put a bound on `full regret <= sums of immediate regrets`? That would be interesting. That `<=` tells me absolutely nothing about the differential.

4:25pm. The next thing on the menu would be `Kuhn's Theorem and the equivalence between mixed strategies and behavioral strategies`. Forget about regret matching for a bit.

I agree with the basic logic that the less you have to multiply from the lower nodes, the better. The less unnecessary dependencies the better. And I agree that the environmental probabilities should be taken into consideration. And I agree that adding a dependency on the self when optimizing the self would make the variance go higher.

Let's say I understand those aspects of CFR.

I still do not understand the way the average strategy is computed at all. I cannot think about that in terms of optimization.

Maybe the reason why that is so confusing is because I am trying to connect it to the regret matching parts, but the strategy averaging is in fact a completely separate mechanism?

4:30pm. Say you were doing sampling - without all the corrections in outcome sampling. Then the frequency of updates of the average strategy would be dependent on the number of time it has been used.

...But that would not just depend on player one, but also on player two's probabilities.

Hmmm...I have no idea. No idea.

4:35pm. But I guess I am slowly getting closer to getting some sort of understanding. When I first started, I did not at all see the distinction between strategy averaging and the rest of the algorithm. Or rather, I saw it, thought it was very strange, but still considered that there is a dependency between regret matching and it. But now I am pretty sure there isn't one. Strategy averaging seems to be more like a Bayesian thing than anything else. Regret matching is optimization, but strategy averaging is not.

I should just dive into that paper, but I do not feel like it. Let me do the chores and then I will call it a day.

Maybe I'll do more once I get back."

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[566aeb0aac...](https://github.com/mrakgr/The-Spiral-Language/commit/566aeb0aacb524498a2e26e9a6051bd64848e2b5)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"8:05am. The game that actually matters is the CFR game.

8:25am. Let me chill for a bit. A new Asuka chapter is out. After that I will start. Today I will definitely finish outcome sampling.

8:40am. Let me start. I'll go through Lanctot's thesis from the top and then deal with outcome sampling. The rest of the algorithms should not be difficult either.

9:25am. 30/149. Here strategies and regrets are updated right away. That clinches it. I am definitely updating the strategy sus at the wrong time. I'll have to look at the CFR tutorial again.

...No wait, actually the one in the paper is equivalent to what I have now apart from player alternation.

9:30am. Ah, the strategy sum update refers to the strategy that was just used. Ah, I see. It really was like that intentionally.

9:45am. 53/149. What exactly is the difference between `pi(z[i],z)` and `pi(z[i])`?

28/149. Why is the average strategy weighted only by the path probability of the player using it?

10am. Let me check out the Java CFR paper and then I will fire a question to Lanctot. I want to see if it does the same thing as I do or if I made a mistake in my transcription.

...Oh, he was the second author of the `An Introduction to Counterfactual Regret Minimization` paper. I hadn't realized that at all till now.

10:10am. Ah, just now I figured out how to do FSI-CFR elegantly. It finally all clicks for me. I know how to do it in breadth first fashion without necessarily having to turn the game into an abstract representation first.

10:20am. Ah, well, there are still other aspects of the algorithm I do not understand so I will give it a rest for the time being.

I am not sure how to do the updates and propagate rewards yet. And the benefit of the algorithm is only in the case of imperfect recall.

10:25am. Ah, no wait. I get it. True, to make things simpler, I am going to have to propagate probabilities, rewards for both players separately which might lead to traversals being done twice, but that is the only way I can think of to maintain consistency.

10:30am. Ah, I get what those equations were.

`pi(z[i],z)` means `pi (from: z[i] to: z)` and `pi(z[i])` means `pi (near_to: z[i])`.

Let me take a look at it again.

10:35am. So I probably got outcome sampling wrong. I guess now I know what those tail probabilities are for.

Let me take a little break here. I need to go over it from the top.

10:50am. 28/149. It should have been obvious that my reading of how utility is calculated is completely wrong simply based on how regular CFR does it.

11:15am. Ok. I think I understand the notation used in the CFR papers completely now. I can be surprisingly dense when it comes to figuring this out. I can't go from words to mental models very easily.

That having said, I do not understand why the utility is weighted by just the opponent's action probabilities. The same for goes for strategy updates. Let me fire that mail, and then I'll stop to have breakfast. Compared to yesterday when I spent the entire morning obsessing, the kind of think of thinking I am doing now is lighter in comparison. I'll definitely have steam to do some programming later.

12pm. Fired off that email. Let me have breakfast here.

With that asked, I'll be able to lighten my load for a bit.

At any rate, now that I understand the notation, I won't have much trouble implementing the sampling algorithms. I'll get this all done in the next few days if I do not get hung up some aspects...which I am prone to do. Well, a week should be enough.

1pm. https://www.reddit.com/r/ProgrammingLanguages/comments/d7gya5/unary_operators_not_good_investment_in_terms_of/

Oh, it seems I was not the only one who disliked implementing unary operators.

1:35pm. Let me resume. It seems that I forgot to commit the entries earlier."

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[9971814054...](https://github.com/mrakgr/The-Spiral-Language/commit/9971814054b7edfe6387c6d2f816f7c53b775083)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"1:25pm. It is this early, and I am so tired.

1:30pm. That bit of reasoning I had to do to come up with a halfway decent explanation of CFR really took it out of me.

I've changed my mind - it really is right. The fact that the probability distributions can be factored and updated in such a manner should be right. I can't get an exact handle, but there is something there and it is not a figment of my imagination.

1:35pm. Let me resume even though I'd rather just drop here.

I need to study the Blackwell theorem for a while.

http://web.eecs.umich.edu/~jabernet/eecs598course/fall2013/web/notes/lec22_112013.pdf

1:45pm. This seems to be a convex optimization kind of lecture paper. Too complex. And I am out of focus at this point anyway.

2:05pm. No, I give up. I do not have any more energy. That bit of deep thought that I did earlier in the morning took it out of me. There is no way I am feeling like studying those thick papers in order to figure out Blackwell's theorem right now.

The worst thing about all of this is that regular RL of all things would not work on poker. Otherwise TD or Q learning would have had more success in this arena.

The reason would have to be...

Yeah, exploration. No doubt about it.

Maybe deep architectures that look many steps back in ways tabular representation cannot would have more luck.

I am going to need something more powerful than vanilla deep nets in order to be able to cope with opponent adaptation.

Right now I am thinking how I would adapt CFR to deep nets and it feels like it would be hell. Really, what do I do? I have no idea...

Well it might be possible to do CFR with deep nets.

https://arxiv.org/pdf/1809.04040.pdf
Solving Imperfect-Information Games via Discounted Regret Minimization

There is something about linear CFR here.

" (5) CFR minimizes external regret (Zinkevich et al. 2007), so it converges to a coarse correlated equilibrium (Hart and Mas-Colell 2000). In two-player zero-sum games, this is also a Nash equilibrium. In two-player zero-sum games, if both players’ average regret satisfies R T i T ≤ , then their average strategies hσ¯ T 1 , σ¯ T 2 i are a 2-Nash equilibrium (Waugh 2009). Thus, CFR is an anytime algorithm for finding an - Nash equilibrium in two-player zero-sum games.

Although CFR theory calls for both players to simultaneously update their regrets on each iteration, in practice far better performance is achieved by alternating which player updates their regrets on each iteration. However, this complicates the theory for convergence (Farina, Kroer, and Sandholm ; Burch, Moravcik, and Schmid 018).

CFR+ is like CFR but with the following small changes.First, after each iteration any action with negative regretis set to zero regret. Formally, CFR+ chooses its strategyon iteration T + 1 according to Regret Matching+ (RM+),which is identical to Equation (3) but uses the regret-likevalue QT(I, a) = max{0, QT −1(I, a) + rt(I, a)} ratherthan RT+(I, a). Second, CFR+ uses a weighted average strategy where iteration T is weighted by T rather than usinga uniformly-weighted average strategy as in CFR. The bestknown convergence bound for CFR+ is higher (that is, worsein exploitability) than CFR by a constant factor of 2. Despitethat, CFR+ typically converges much faster than CFR and usually even faster than O(1"

Hmmm...I see. So now I finally know what CRF+ is. But I do not feel like reading the rest of this right now.

https://arxiv.org/pdf/1811.00164.pdf
Deep Counterfactual Regret Minimization

This is from May 2019.

2:30pm. This paper is a bit interesting. Let me read it and then I will take a real break.

2:50pm. "This bucketing is done using K-means clustering on domain-specific features. The lossless abstraction only clusters together situations that are strategically isomorphic (e.g., flushes that differ only by suit), so a solution to this abstraction maps to a solution in the full game without error."

I had no idea k-means could be used losslessly.

2:55pm. ...Later they talk about some kind of lossy kmeans it seems. Whatever.

I am done for the day. Let me go get some air while I do the chores.

Tomorrow I will go over Lanctot's thesis from the top. Today when I get back, I'll take a look at the Coq book I've linked to."

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[a9150d72d4...](https://github.com/mrakgr/The-Spiral-Language/commit/a9150d72d4519419d57e0183cf5a6a8b2fec226e)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"11:55am. https://youtu.be/REeaT2mWj6Y
Inconvenient truths about sqrt(2) | Real numbers and limits Math Foundations 80 | N J Wildberger

Ah, I did not know that he had a video about `sqrt(2)`. Based on the description, we seem to be in a complete agreement. I'll watch this video then during the break.

12:45pm. https://youtu.be/REeaT2mWj6Y?t=1576

This particular bit of math is completely new to me.

I am going to swap the link in `[some kinds of reasoning](https://www.homeschoolmath.net/teaching/proof_square_root_2_irrational.php)` with this lecture by Wildberger. Wildberger is great. I should listen to whatever he has to say. I wanted to scorched earth that entire line of reasoning, but he cut to the root in a way I could not.

1:15pm. Where was I? I am done with both breakfast and that lecture by Wildberger. Since I did not get a chance to do so already, let me read Tog and see if the Satanophany raw is out.

After that I think I will go for a lecture by Bowling.

4:50pm. I didn't think I would end up doing housework for 3.5 hours since that last entry. Damn, I am tired. Maybe it is good that this happened today just as I was taking a break.

5pm. Let me take a look at that talk by Bowling.

6:10pm. Ah, screw that talk. I am done with lunch and I want to rest.

6:15pm. Tomorrow as per plan, I will read the no-regret learning book."

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[14258a7663...](https://github.com/mrakgr/The-Spiral-Language/commit/14258a766336bccc434d9b5a9db65a97b8bf96a2)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"1pm. Let me watch that video.

1:15pm. https://youtu.be/49PHeGZzttM?t=562

This video has been quite good so far. It introduces no-regret learning quite well.

1:20pm. https://youtu.be/49PHeGZzttM?t=844
"There are modern connections to Black-Scholes. In particular it is possible to derive the Black-Scholes pricing formula without any probabilistic assumptions whatsoever by essentially viewing the pricing process as a game between a price setter and an arbitrageur both of whom are playing a no regret algorithm. This was done in the last few years or so."

The video is from 2017.

1:40pm. https://youtu.be/49PHeGZzttM?t=1808
"
* Could ask for no regret to best strategy in risk-adjusted metrics:
  - Sharpe ratio: mean(returns)/std(returns)
  - mean - variance: mean(returns) - std(returns)
* Strong negative results
  - no-regret provably impossible
  - lower bounds on competitive ratio for any algorithm
"

2pm. https://youtu.be/49PHeGZzttM?t=2713

Maybe I will take a look at the first paper for this.

https://www.cs.princeton.edu/~arora/pubs/MWsurvey.pdf
The Multiplicative Weights Update Method - a Meta Algorithm and Applications

2:15pm. Let me take a break. Then I'll read this paper. Then come the references by Lanctot.

3pm. That took longer than I thought it would.

Let me get back on track. The paper.

...Ah, yeah. What was the softmax update again? Isn't it exactly the same as the outcome sampling one?

`ret {in={prob label}; map=inl {prob label} -> (prob - label) / temp} (adjoint input)`

So it is `probability - label`.

```
let r = if i = action_index then W * (1.0 - proposal_distribution.[i]) else -W
```

Yeah, I am a fool. This is exactly that...no wait it is not. If it were it would be something like...

```
let r = if i = action_index then W * (1.0 - proposal_distribution.[i]) else -W * proposal_distribution.[i]
```

3:35pm. Thunder. The weather is turning bad again. Let me turn off. I'll resume later.

6:20pm. Damn, that took a while to pass.

At the very least I read that paper for the first time. It is pretty mindblowing that the multiplicative updates can be used to approximately solve various LP and NP-Hard problems. I could not understand most of it, but I can tell that this paper is to be kept in mind. Solving constraints is a major function of biological brains, and I am on the lookout for how that could be plausibly be done on the computer.

CFR and now this...I've been living in a very small box this whole time it seems. The number of ways multiplicative updates can work is staggering. I need to study this. I need to master this. If I can do it, I will be able to make human level agents by the time I reach the end of this path. It will be a new beginning.

I have my work cut out for me. I am just going to keep plugging away at it.

But by now I am starting to be convinced - vanilla RL is not going to cut it. CFR is essentially a two phase (slow/fast policy) actor-critic method, but while I am going to need it, unfortunately I have no idea how I am going to use it together with neural nets. Or how I am going to generalize it to regular RL from simple, finite games like poker. I do not know how to tie the two fields together.

I hope there are some good ideas in the variance reduction papers.

Anyway, I got a reply from Lanctot to the softmax question. I really do appreciate him being so helpful.

```
> Let me just ask a quick question about the softmax. The PRG book mentioned exponential averaging, which reminded me that I've yet to see any mention of that in any of the CFR papers that I've read. I am guessing that was tried and abandoned in favor of regret matching. Was that the case?

Yes, the exponential algorithms are more common in the online learning literature because they have better theoretical bounds (and I think easier to analyze), and they are just very commonly used.

In CFR there are three very good reason to use regret matching: (i) RM just plain works better in practice, (ii) RM does not have a parameter that requires tuning (i.e. learning rate, step size), and (iii) RM gives you policies where actions have probability zero, which means you can apply some simple pruning in the sequential tree setting since the reach probabilities go to zero and you can skip the subtrees because the updates are zero). If you're curious how these come in, take a look at the CFR implementation in OpenSpiel.
```

These are all good points. I'd add that regret matching is more biologically plausible as well, since it does not require computing those exponentials like in softmax.

6:35pm. Let me close with a quote from the paper.

> We feel that our meta algorithm and its analysis are simple and useful enough that they should be viewed as a basic tool taught to all algorithms students together with divide-and-conquer, dynamic programming, random sampling, and the like.

I agree. They presented more than enough evidence to convince me that this is worth studying.

By the end of the year, I should be able to gain something significant from this."

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[561b4f3df2...](https://github.com/mrakgr/The-Spiral-Language/commit/561b4f3df26f97400bb4ccdfd467784d0bcb70e8)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"8:45am. I am going to paste a few snippets from the mail here.

```

> Why in the strategy update is the strategy weighted by the player's probability of reaching the infoset?

It's because that's how you compute the average strategy properly in extensive form games. (The average strategy is the one that converges to a Nash equilibrium over time).

If you want to dig deeper, you can look for "Kuhn's Theorem and the equivalence between mixed strategies and behavioral strategies".

> 2) Conversely, in the utility calculation, why - as seen from the start, is the utility weighed by just the opponent's probability of reaching the infoset and not both of them?

Ah yes, that is the definition of counterfactual value and counterfactual regret: it's exactly the novelty behind the CFR algorithm. It's because of this special form of regret that can do regret minimization in the extensive-form games and still get them to converge to the Nash equilibrium (via the CFR Theorem).  So the answer is essentially "because that's the definition" :)

> No matter how much I wrack my brain, I can't figure out the reasoning behind this.

There is a much more subtle reason that would only be clear to you if you go through all the CFR theorem proofs. The proofs simply cannot work unless you define regret in this way.
```

He also says that alternating updates CFR has better performance both in theory and in practice than the simultaneous ones. This is what I've observed yesterday. But finally, 'figure out the proofs' card got played against me.

8:50am. I've sensed that I am not going to be able to avoid this forever. Up to now I've been skipping proofs and I did not have the foundation to internalize them properly, but I am going to have to slow down and do this thing. This is why I've doing those Coq and Agda proofs for the past few months.

8:55am. I guess I will be stuck on this for a while. Let me take a look at that thing by Kuhn.

10:10am. 9/14 of 'Regret Minimization in Games with Incomplete Information'. In Eq 8 and 9 why are there two different definitions of the full counterfactual regret?

Let me fire a quick mail.

I also do not understand why `u(σ|D(I)→σ' , I') = u(σ|D(I')→σ' , I')`. Also why `succ(I'|I,a)` does not depend on the opponent's strategy is not explained.

11:05am. 'Define D(I) to be the information sets of player i reachable from I (including I).'

Ah, wait. I've been imaging this as only involving the immediate successors, but it should be more like all the way to the end. Why did I imagine only reaching the successors?

11:30am. Ok, I think I get theorem 3.

> I also do not understand why `u(σ|D(I)→σ' , I') = u(σ|D(I')→σ' , I')`.

It is not so much that `u(σ|D(I)→σ' , I') = u(σ|D(I')→σ' , I')`, but assuming that you have `u(σ|D(I)→σ' , I')` then you could probably construct `u(σ|D(I)→σ' , I') = u(σ|D(I')→σ' , I')`. So a more precise statement would be that `u(σ|D(I)→σ' , I')` implies `u(σ|D(I)→σ' , I') = u(σ|D(I')→σ' , I')`.

> Also why does `succ(I'|I,a)` not depend on the opponent's strategy?

I worded this badly. I meant to ask why it does not depend on the strategy of the player under consideration - meaning not the opponent, but the current player. And to answer this question, it is because `a` and `o'` are being maximized, so their probabilities of being picked will always be 1.0. Would that be right?

I guess this gives me a hint why CFR optimization does not consider the probabilities of the player being optimized.

> In Eq 8 and 9 why are there two different definitions of the full counterfactual regret?

I am guessing this is how pen and paper math goes. This is the first real proof I've ever tried to study. For the past few months as a part of my training I only did Coq and Agda proofs. I wonder if anybody tried putting the stuff these papers into a proof assistant?

...Is what I want to write...but that can't be right. Let me just add the first two. I still need for him to clarify eq 9 for me more.

I still do not understand why those two definitions of the full counterfactual regret in eq 8 and eq 9 are supposed to be equal. I am also not sure that the direction of the less-than-equals sign in eq 10 makes sense.

12pm. Let me take a break here. I've really been fully immersed into this proof. I guess him telling me to just study the thing is enough to serve as a trigger. Wonderful.

I really hope he is up for answering my questions because I do not think I will manage to get through the proof on my own."

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[ffd9e29512...](https://github.com/mrakgr/The-Spiral-Language/commit/ffd9e2951295d6457c96b7d462fd639ae1451b90)
#### Tuesday 2020-12-22 12:37:12 by Marko Grdinić

"3:30pm.

```
type Particle<'state, 'history> =
    {
    state: 'state
    probability: Probability
    uniformity_coef: float
    count: uint32

    infosets: Dictionary<'history, Node>
    is_updateable: bool
    }
```

Stuff just keeps getting added to this. The check for `is_updateable` does not even make sense in the context.

I'll do things a bit differently then.

Let me modify the other example.

3:45pm.

```
let chance dice one_probability next =
    let prob = 1.0 / float (Array.length dice)
    prob * Array.fold (fun s dice -> s + next (dice, one_probability * prob)) 0.0 dice

let response is_updateable infosets history actions one_probability two_probability next =
    let node = infosets (fun _ -> node_create actions) history
    let action_distribution = regret_match node.regret_sum
    let util, util_weighted_sum =
        array_mapFold2 (fun s action action_probability ->
            let util =
                if action_probability = 0.0 && two_probability = 0.0 then 0.0 // the pruning optimization
                else next (action, one_probability*action_probability)
            util, s + util * action_probability
            ) 0.0 actions action_distribution

    if is_updateable then
        add node.strategy_sum (fun i -> one_probability * action_distribution.[i])
        add node.regret_sum (fun i -> two_probability * (util.[i] - util_weighted_sum))

    -util_weighted_sum
```

Let me do it like this. I've factored out all the implementation details that I could. What I will do is write wrappers for this.

4:15pm. I refactored the second version a little, but now it is time to make it right.

Somehow fatigue is hitting me right now. Let me just get this all to typecheck and compile and then I will be done. Refactoring the thing won't be hard.

4:40pm.

```
let inline chance dice next =
    /// Outcome sampling does not consider the probabilities in chance nodes
    next (sample_uniform dice)

let inline response_regret infosets uniformity_coef history actions one_probability two_probability next =
    let node = memoize infosets (fun _ -> node_create actions) history
    let proposal_distribution = regret_match node.regret_sum
    let sampling_distribution = Array.map (fun prob -> uniformity_coef * (1.0 / float (Array.length actions)) + (1.0 - uniformity_coef) * prob) proposal_distribution
    let action, node_probability, action_index = sample actions sampling_distribution proposal_distribution
    let utility, tail_probability = next action (one_probability * node_probability)
    let W = utility * two_probability.proposal // for some reason importance sampling is not being done here
    for i=0 to actions.Length-1 do
        let r =
            if i = action_index then -W * tail_probability
            else W * tail_probability * (proposal_distribution.[i] - 1.0)
        node.regret_sum.[i] <- node.regret_sum.[i] + r
    -utility, tail_probability * node_probability.proposal

let inline response_strategy infosets count history actions one_probability next =
    let node = memoize infosets (fun _ -> node_create actions) history
    let proposal_distribution = regret_match node.regret_sum
    let action, node_probability, _ = sample_uniform' actions proposal_distribution
    let utility, tail_probability = next action (one_probability * node_probability)
    for i=0 to actions.Length-1 do
        node.strategy_sum.[i] <- node.strategy_sum.[i] + float (count - node.count) * one_probability.proposal * node_probability.proposal
        node.count <- count
    -utility, tail_probability * node_probability.proposal

let terminal one_probability two_probability x = x * one_probability.sample * two_probability.sample
```

Well, this should be fine, but...

```
        let r =
            if i = action_index then -W * tail_probability
            else W * tail_probability * (proposal_distribution.[i] - 1.0)
```

I get the feeling that things will get updated in the wrong direction here. Am I really sure things are supposed to go like so?

4:50pm.

```
        let r =
            if i = action_index then -W * tail_probability
            else W * tail_probability * (1.0 - proposal_distribution.[i])
```

Hmmm...is it like this? There is no rule that says it has to necessarily be symmetric.

I have no idea. This is confusing. What is that probability supposed to be in that context.

...Yeah, it is most likely like this. It has to be it. I am going to have to study the C++ code to make sure though.

I guess I won't be running this today. Let me check out the C++ code just to be sure.

```
    // don't need to worry about keeping track of sampled chance probs for outcome sampling
    return cfros(ngs, player, depth+1, bidseq, reach1, reach2, sprob1, sprob2, updatePlayer, suffixreach, rtlSampleProb);
```

Ok, that is good to know.

```
      double U = updatePlayerPayoff * oppreach / rtlSampleProb;
```

Oh, here it is doing importance sampling correction.

```
  double newsuffixreach = 1.0;
...
  ctlReach = newsuffixreach;
  itlReach = newsuffixreach*is.curMoveProbs[action];
...
      double U = updatePlayerPayoff * oppreach / rtlSampleProb;
      double r = 0.0;
      if (a == takeAction)
        r = U * (ctlReach - itlReach);
      else
        r = -U * itlReach;
```

This makes no sense. `newsuffixreach` is acting as `tail_probability` here.

Did I miss an update to `newsuffixreach` somewhere?

5:10pm. No it does not seem so. Rather, I am wrong about there being a multiply by tail probability.

```
            if i = action_index then -W
            else W * (1.0 - proposal_distribution.[i])
```

So it is like this then? Still, note that the branches of the if statement are swapped in the C++ code.

Since the particular outcome sampling here is called stochastic averaging, let me take a look at the pseudocode for it. WIll the branches be swapped in it as well?

5:15pm. No, the pseudocode description is the same.

5:20pm. Ah, I have no idea. Let me check out other papers and if I cannot figure it out, I'll try asking Lanctot about it.

http://mlanctot.info/files/papers/nips09mccfr.pdf

Here is one paper by him.

I am leaning towards his thesis being wrong.

5:45pm. Done with lunch. I need to ask Lanctot a few things. Here would the questions be...

1)

```
let W = utility * two_probability.proposal // for some reason importance sampling is not being done here
```

Why is the importance sampling correction not being applied?

2) Point out the branch swapping error in the thesis.

3) Also note that what `pi(z[i]a,z)` means in the context is ambiguous.

4) Ask about what the node count is supposed to be. I am guessing it is the episode count.

5:50pm. I really do not feel like composing the mail now. I had enough for the day. I'll do it tomorrow. While I wait for the reply, I'll make use of the C++ code."

---
## [dwat3r/aoc2020](https://github.com/dwat3r/aoc2020)@[a213165f38...](https://github.com/dwat3r/aoc2020/commit/a213165f3843e0a4884b89a09c165ef00733e6ee)
#### Tuesday 2020-12-22 15:52:37 by dwat3r

day22 part2 finally oh my fucking god read that damn description THROUGH

---
## [Charlotte-Vrijen/Like-parent-like-child-Event-at-Noorderzon-2019](https://github.com/Charlotte-Vrijen/Like-parent-like-child-Event-at-Noorderzon-2019)@[8074564b21...](https://github.com/Charlotte-Vrijen/Like-parent-like-child-Event-at-Noorderzon-2019/commit/8074564b21500316bb95a3212ba2a46379a3f983)
#### Tuesday 2020-12-22 16:18:44 by Charlotte Vrijen

Add files via upload

Like parent, like child? Event during Noorderlichten at Noorderzon 2019 / Zo ouder, zo kind? Evenement tijdens Noorderlichten op Noorderzon 2019
Contributors: Charlotte Vrijen, Anoek Oerlemans, Jennifer Richards

Note: All contributors contributed equally to this project.

[Dutch version below]

Noorderlichten is an event at Noorderzon, organized by the Young Academy Groningen (YAG) to bring scientific research outside academia and give children and other non-scientists the opportunity to experience what it is like to do research.

Jennifer Klop-Richards and Anoek Sluiter-Oerlemans of the University Medical Center Groningen and Charlotte Vrijen of the University of Groningen organized the Noorderlichten activity ‘Like parent, like child?’. During this activity, children and their parents were invited to investigate for themselves how similar they were in looks, physics, character and emotions. With different instruments they compared their hair color, freckles, their ability to roll up their tong, but also characteristics like being a morning or evening person, and starting tasks right away or delaying them. Finally, their emotional responses were compared during a sometimes very frustrating giant Bibberspiral. We used posters to explain the assignments and give background information to the children and their parents, for example about the role of genes and parenting in explaining why some children show great similarity to their parents while others do not. The local newspaper ‘Dagblad van het Noorden’ followed a 7-year-old participant and her father during their quest for similarities, see the article (in Dutch).

Anyone who is interested can download the posters from this OSF page.

Questions? Please send an e-mail to c.vrijen@rug.nl, a.m.sluiter-oerlemans@umcg.nl, or j.s.klop-richards@umcg.nl

DUTCH VERSION

Noorderlichten is een evenement op Noorderzon dat georganiseerd werd door de Young Academy Groningen. Het doel van Noorderlichten was om wetenschappelijk onderzoek ook toegankelijk te maken voor niet-wetenschappers, en in het bijzonder kinderen de mogelijkheid te geven zelf te ervaren hoe het is om onderzoek te doen.

Jennifer Klop-Richards en Anoek Sluiter-Oerlemans van het Universiteir Medisch Centrum Groningen en Charlotte Vrijen van de Rijksuniversiteit Groningen organiseerden het Noorderlichten evenement ‘Zo ouder, zo kind?’. Kinderen en hun ouders werden uitgenodigd om zelf te gaan onderzoeken in hoeverre ze op elkaar lijken in uiterlijke kenmerken, innerlijke kenmerken en emoties. Uitgerust met verschillende onderzoeksinstrumenten vergeleken ze hun haarkleur, sproeten, of ze hun tong konden oprollen, maar ook of ze meer een avond- of een ochtendpersoon waren, en of ze altijd direct aan een taak begonnen of uitstelgedrag vertoonden. Tenslotte werden de emotionele reacties van ouder en kind vergeleken tijdens een soms erg frustrerende reuzen bibberspiraal. We hebben posters gebruikt om de opdrachten uit te leggen en achtergrond informatie te geven aan de ouders en kinderen, bijvoorbeeld over de rol van genen en opvoeding bij het verklaren waarom sommige kinderen veel gelijkenissen vertonen met hun ouders en anderen veel minder. Het Dagblad van het Noorden heeft een 7-jarige deelneemster en haar vader gevolgd tijdens hun onderzoek naar gelijkenissen, zie hier het artikel.

Iedereen die geinteresseerd is kan de posters die we hebben gebruikt downloaden van deze OSF pagina.

Vragen? Stuur een e-mail naar c.vrijen@rug.nl, a.m.sluiter-oerlemans@umcg.nl, of j.s.klop-richards@umcg.nl

---
## [0cjs/8bitdev](https://github.com/0cjs/8bitdev)@[819fd6305d...](https://github.com/0cjs/8bitdev/commit/819fd6305dcef146e7ab311f86cbf7c025411cdf)
#### Tuesday 2020-12-22 16:27:58 by Curt J. Sampson

WIP src/mc68/pmon regscmd: Functionality to set saved registers

We change the command from `s` to `r` because the latter is just much more
instinctive and easier to remember. We deal with the loss of R)ead by
changing read, write and verify all to be under the T)ransfer command,
which should be fine because that command will have a ton of other options
anyway.

There's still a design problem with this, as showed up with some
frustrating debuggin during unit testing: since the args are always
adjacent, it's easy to type `a3b4` meaning to set A=3 and B=4, but that's
interpreted as A=3B 4= followed by a parse error. Currently, we're thinking
of allowing optional commas in to separate arguments to help mitigate this.

The code is a bit sloppy and has a good deal of duplication, and the tests
are showing issues similar to (but not the same as) the examine tests. But
this is going on master anyway because it's unlikely to have broken
anything working to this point and we want feedback on this sooner rather
than later.

---
## [NoNoTeal/tts-bot](https://github.com/NoNoTeal/tts-bot)@[aa24887b08...](https://github.com/NoNoTeal/tts-bot/commit/aa24887b084609be6ebdcb07a07edf55f314f8f8)
#### Tuesday 2020-12-22 18:05:20 by nonoteal

1.2.3

COMMANDNO PATCH NOTES:
• Added disable on startup feature for command
• Adjusted maintence mode a little
• Added commandstatuses
• Adjusted eval
• Fixed breaking bug for imposter
• Fixed breaking bug for guild-load where unloaded commands get pulled as undefined from a loaded area.
• Moved bitNumberToArray to Util file
• Unload, load, reload allows groups that have spaces in them.

TTS PATCH NOTES:
• FIXED TIME BECAUSE FUCKIN DARKLYNX108 THOUGHT 29 HOURS IN A DAY WAS NORMAL, WELL FUCK YOU BITCH.
• Added fish.js
• Fix-fucked rep.js
• Added rawsay.js
• Made so commands don't need to be executed in guild.

---
## [rorydale/pointbreakradio](https://github.com/rorydale/pointbreakradio)@[c0e65b1b1d...](https://github.com/rorydale/pointbreakradio/commit/c0e65b1b1d68a16c5ad00d7dc24d64af515f038f)
#### Tuesday 2020-12-22 18:16:07 by Rory Dale

2020-12-22

Tuesday, December 22nd, 2020 - the new music show! More new music recently released and from the last year. I've really grown to enjoy doing these new music shows and discovering new material from a variety of artists. Today's oldest is Paul McCartney at 78, the youngest...? I'm not actually sure! As always, I don't play edited versions of songs, and some of the songs (and commentary) contain the words fuck, shit, anus, and cunt. Big hugs everyone, they're just mouth noises, and they only make sense to English speaking listeners.

---
## [clearlinux-pkgs/pyOpenSSL](https://github.com/clearlinux-pkgs/pyOpenSSL)@[0197d61e46...](https://github.com/clearlinux-pkgs/pyOpenSSL/commit/0197d61e46392aa7350b0bfaa6690b1da9009fbd)
#### Tuesday 2020-12-22 23:54:16 by Patrick McCarty

pyOpenSSL: Autospec creation for update from version 19.0.0 to version 20.0.1

Adrián Chaves (1):
      Remove asserts (#904)

Alex Gaynor (45):
      Reopen master for 19.1 (#810)
      Update test for us not supporting 0.9.8 (#812)
      Update install docs for suppoted versions of OpenSSL (#813)
      Removed deprecated Type aliases (#814)
      Make all of the examples py3 syntax friendly (#816)
      Deprecated NPN (#820)
      OpenSSL always has SNI (#821)
      Remove tests of long functionality (#832)
      Delete mk_simple_certs which is definitely not the pyca recommended way to generate certs (#833)
      Delete certgen.py (#834)
      Delete proxy.py (#835)
      Delete SecureXMLRPCServer.py (#836)
      Delete README.rst (#839)
      Delete examples/sni/ (#838)
      Delete examples/simple/ (#837)
      Delete .mention-bot (#840)
      Remove deleted files from MANIFEST.in (#841)
      Run tests in random order to weed out issues (#842)
      Mark this test as flaky (#850)
      Fixes #868 -- test on py38 (#870)
      Random cleanup around our usage of binary_type (#879)
      Make the code slightly resillient to python4 (#880)
      Drop support for Python 3.4, since cryptography dropped it (#884)
      Make tests pass in the future (now the present) (#888)
      Run twisted tests under py3 (#905)
      Uhhh, fix twisted tests to actually run twisted (wat) (#906)
      Drop OpenSSL 1.0.1 (#908)
      Update to a more modern macOS image (#909)
      Paint it Black by the Rolling Stones (#920)
      Remove RPM build script that we have no idea if it works (#923)
      use larger keys in ssl tests (#922)
      _only_ update the image (#925)
      Deprecated pkcs7 and pkcs12 support (#921)
      fixes #934 -- kill dead link (#936)
      lock old issues (#937)
      I'm back in black (#941)
      fixed tests for twisted change (#950)
      Drop CI for OpenSSL 1.0.2 (#953)
      Stop testing py35 with cryptography master (#958)
      Migrate CI to GHA (#960)
      Added dependabot for GHA (#961)
      Remove leakcheck (#965)
      remove py2 w/ cryptography master (#977)
      Fix for running on OpenSSL 1.1.0 + CI (#978)
      Prepare for 20.0.1 release (#979)

Arne Schwabe (2):
      Fix spelling of set in set_verify docstring (#959)
      Keep reference to SSL verify_call in Connection object (#956)

Benjamin Peterson (2):
      Remove dead examples directory from flake8 invocation. (#914)
      Remove deprecated tsafe module. (#913)

Christian Clauss (2):
      Fix Travis CI build config validation issues (#918)
      Tox.ini; Test on Python 3.9 and make flake8 stricter (#966)

Daniel Holth (2):
      Update SSL.py docstring (#849)
      use _ffi.from_buffer() to support bytearray (#852)

David Benjamin (2):
      Make test_ssl pass in an IPv6-only environment (#827)
      Fix generated test X.509 certificates. (#917)

Felix Yan (1):
      Correct typos in crypto.py (#949)

Hugo van Kemenade (1):
      Fix for Python 4 (#862)

Huw Jones (1):
      crypto._PassphraseHelper: pass non-callable passphrase using callback (#947)

Hynek Schlawack (1):
      Stop lawyers from emailing Hynek (#856)

Mark Williams (2):
      Raise an Error with "no cipher match" even with TLS 1.3 (#818)
      ALPN: complete handshake without accepting a client's protocols. (#876)

Maximilian Hils (2):
      Add SSL.Context.set_keylog_callback (#910)
      Context.set_verify: allow omission of callback (#933)

Michael Lazar (1):
      Fix typo in debug command output (#957)

Mrmaxmeier (1):
      Fix PKey.check for some broken keys (#897)

Nathaniel J. Smith (1):
      Expose OP_NO_TLSv1_3 (#861)

Paul Kehrer (16):
      more infra changes (#809)
      skip NPN tests if NPN is not available (#822)
      bump the minimum cryptography version (#875)
      version bump for 19.1 release (#877)
      reopen master, call it 20.0 because let's be honest with ourselves (#878)
      make our CI less frustrating (#926)
      update cert fixtures and simplify tests (#927)
      newer pypy (#928)
      use modern message digests (#930)
      use SSLv23_METHOD so we get "best TLS" in most tests. (#931)
      remove npn support entirely. you should be using alpn (#932)
      focal time (#929)
      travis.com now (#939)
      fix a memleak (#967)
      20.0.0 before 2020 is even over (#968)
      reopen master for next dev cycle (#969)

Rosen Penev (1):
      Switch to new notBefore/After APIs (#843)

Shane Harvey (1):
      Allow accessing a connection's verfied certificate chain (#894)

Sándor Oroszi (2):
      Allow using an OpenSSL hashed directory for verification in X509Store (#943)
      Allow using additional untrusted certificates for chain building in X509StoreContext (#948)

Wayne Werner (1):
      Provide a valid digest option (#811)

jalberdi004 (1):
      Fixing issue #798 (#907)

---

# [<](2020-12-21.md) 2020-12-22 [>](2020-12-23.md)

