# [<](2020-05-07.md) 2020-05-08 [>](2020-05-09.md)

2,556,224 events, 1,339,468 push events, 2,044,972 commit messages, 134,576,795 characters


## [TheNeoGamer42/WaspStation-1.0](https://github.com/TheNeoGamer42/WaspStation-1.0)@[56fe7ce642...](https://github.com/TheNeoGamer42/WaspStation-1.0/commit/56fe7ce642bffda5e70f6a1b9f5f69768d076ed6)
#### Friday 2020-05-08 00:20:41 by TheNeoGamer42

I swear to fuck you better fucking work now you piece of shit

---
## [hockeybuggy/hockeybuggy.com](https://github.com/hockeybuggy/hockeybuggy.com)@[2dd31459d8...](https://github.com/hockeybuggy/hockeybuggy.com/commit/2dd31459d8acce5bcafa8881c92f417d2886199f)
#### Friday 2020-05-08 03:15:13 by Douglas Anderson

Switch to Gatsby

It feels like most of what I do in this repo is switch static site
generators.

I switched to Hugo pretty recently and was mostly happy with it. A
single executable is nice, but I never really got comfortable with the
templateing system. I have on the other hand been writing React for 4
years now (which isn't a crazy long time, but it mostly just snuck up on
me). I used Gatsby as the static site generator for a friends website
and liked it's development experience a little more.

This commit sets up a basic typescript Gatsby app. It starts the process
of recreating the `hugo-coder` project. I feel a little bit weird about
copying the work but I am not quite sure how to correctly attribute the
work.

---
## [newstools/2020-sundiata-post](https://github.com/newstools/2020-sundiata-post)@[e4e5c9eae6...](https://github.com/newstools/2020-sundiata-post/commit/e4e5c9eae62cabc686fdd0e55c8a947a4ed8f8dd)
#### Friday 2020-05-08 04:51:01 by NewsTools

Created Text For URL [sundiatapost.com/tonto-dikeh-forgives-friends-for-allowing-her-release-a-horrible-song-in-2012/]

---
## [OpenGM8/GM8Emulator](https://github.com/OpenGM8/GM8Emulator)@[b66050195a...](https://github.com/OpenGM8/GM8Emulator/commit/b66050195a91754df5b55d4c80393a9f17ca85a4)
#### Friday 2020-05-08 05:16:06 by viri

game/window: fix mystery segfaults (fuck winapi)

So I forgot that WindowProc can be invoked with shit like WM_SIZE by
CreateWindowEx and of course the data pointer literally is not inserted
at that point so now I gotta add a check for that EVERYWHERE!!!

---
## [TheNeoGamer42/WaspStation-1.0](https://github.com/TheNeoGamer42/WaspStation-1.0)@[3eba331aa0...](https://github.com/TheNeoGamer42/WaspStation-1.0/commit/3eba331aa0a11e38f2985a84123ca1baae3803bf)
#### Friday 2020-05-08 07:10:48 by TheNeoGamer42

makes the L666 shoot sniper rounds because no mercy fuck you

---
## [TheNeoGamer42/WaspStation-1.0](https://github.com/TheNeoGamer42/WaspStation-1.0)@[74e7c46ad3...](https://github.com/TheNeoGamer42/WaspStation-1.0/commit/74e7c46ad37757c18cc126a8104de489e5d80d8c)
#### Friday 2020-05-08 07:21:46 by TheNeoGamer42

okay maybe a little less fuck you but still fuck you with an L666

---
## [saqib-ali/hivemined](https://github.com/saqib-ali/hivemined)@[56d1af3dd3...](https://github.com/saqib-ali/hivemined/commit/56d1af3dd3404c43a76c3094ceb0980ec0399fa0)
#### Friday 2020-05-08 07:45:26 by Saqib Ali

U.Group - Full Stack Engineer - Artificial Intelligence / Machine Learning. Details - Postdoctoral Research Fellow/Research Officer (Machine Learning &amp; Data Fusion) - Jobs - University of Queensland. Laplace’s Demon: A Seminar Series about Bayesian Machine Learning at Scale «  Statistical Modeling, Causal Inference, and Social Science. Udemy - Senior Machine Learning Engineer. Udemy - Senior Machine Learning Engineer. Machine Learning Expert for Autonomy, Lead - MITRE Careers. Laplace’s Demon: A Seminar Series about Bayesian Machine Learning at Scale «  Statistical Modeling, Causal Inference, and Social Science. Lecture 11: Introduction to Machine Learning | Lecture Videos | Introduction to Computational Thinking and Data Science | Electrical Engineering and Computer Science | MIT OpenCourseWare. Catalogue of new Herbig Ae/Be and classical Be stars. A machine learning approach to Gaia DR2 - NASA/ADS. Machine Learning Researcher/Engineer | ResearchJobs.cz.

---
## [rgommers/scipy](https://github.com/rgommers/scipy)@[21d03dbc98...](https://github.com/rgommers/scipy/commit/21d03dbc98fd24b7865aa63dfd5eae2d2e1c363c)
#### Friday 2020-05-08 08:53:10 by Ralf Gommers

DOC: add "performance improvements" to roadmap.

Note that this was one of the big ticket items in the 2019 NSF proposal
for the SciPy ecosystem:
https://figshare.com/articles/Mid-Scale_Research_Infrastructure_-_The_Scientific_Python_Ecosystem/8009441

For that proposal we got a lot of direct feedback from outreach to
prominent users in domains like economics, physics, and biology.

Example quotes:

"... my top-level concern as a person who supports users is whether
Python and the Scientific Python stack as a whole needs a rethink as
architectures become more parallel and individual cores become less and
less powerful. ... We don't have bright lines for Python users like we
have for C, C++, Fortran for performance portability. If your code is in
C++ we can talk about directive-based parallelism or specific libraries
or compilers that will help you minimize the amount of code you have to
rewrite to move across architectures (to be fair: it's rarely magic).
But things seem to lag in Python."

"Beyond individual features I think there's another issue worth thinking
about. Using things like numba efficiently is becoming more and more
useful to researchers, as special model types and solution methods may
not be written in vectorized form very easily or "beautifully". However,
since the whole chain has to be written using numpa for the JIT to fully
materialize itself, it's kind of an issue that scipy does not generally
support this way to solving problems in python (by that I mean something
that's not just fast because you're using Numpy and vector operations).
The leads to researchers writing their own optimizers, that are
essentially duplicates of well-known methods such as brent and golden
section search, multilinear interpolation and other methods, to speed up
their code. SciPy should handle this part of the problem-solving (it
does, but just not in a way that's efficient to the numba-users), but
currently we risk a lot of code duplication and bugs. I'm not involved
in scipy so I'm not fully aware of their stance of numba vs some of the
other possibilities out there, but the issue of being unable to
efficiently use @njit is a real issue in my opinion."

[ci skip]

---
## [snicolet/Stockfish](https://github.com/snicolet/Stockfish)@[6745ef2d19...](https://github.com/snicolet/Stockfish/commit/6745ef2d1950e5bdb8f3d77c15b109dc28b3d219)
#### Friday 2020-05-08 09:22:27 by Stéphane Nicolet

Introduce anti-suicide feature

In some recent tournament games, Stockfish exhibited the following
self-destructing behaviour. Stockfish was suffering in a long shuffle
session, having a bad evaluation in a blocked or semi-blocked position
for about 40 moves and yet the eval was sort of flatlined, indicating
that the opponent engine (Leela) had trouble converting the position.
Then, not long before the 50-moves draw rule would be reached reached,
the opponent would play its pieces to some strange places and SF would
push a pawn, thinking she would get a slightly "less worse" evaluation.
However, the slightly less worse evaluation would prove to be delusional,
the position with a sacrificed pawn crackable and SF eventually lost
these games.

This issue was discussed in the following thread:
https://github.com/official-stockfish/Stockfish/issues/2620

This commit is our best attempt to patch this issue, so that SF gets
more patient in worse positions and try to play for 50 moves as much
as possible and not suicide. The implementation uses pure evaluation
methods rather than search, damping down the eval after 25 moves of
shuffling (damping factor is linear, starting from 1.0 after 25 shuffling
moves and reaching 0.04 after 50 moves of shuffling). This damping
puts the burden on the attacking playing to prove that it can break
the fortress, as now the search will get more and more optimistic
for the defending player to be able to reach a draw by 50 moves rule.

This solution seems to work as intended for the few cases extracted
from tournament losses, according to tests done by @vondele in the
following comments:
https://github.com/official-stockfish/Stockfish/issues/2620#issuecomment-616779979
https://github.com/snicolet/Stockfish/commit/a66d3c01bb453d91e21e50b0b431a67ca906e3c4#commitcomment-38963042

In Fishtest, the best result we managed to get after extensive testing
was a double yellow with Elo-gaining bounds (this patch), maybe because
the problem is quite rare at the short time controls we use in our tests
compared to the longer time controls used in tournament games:

STC:
LLR: -2.97 (-2.94,2.94) {-0.50,1.50}
Total: 201928 W: 38274 L: 38174 D: 125480
Ptnml(0-2): 3452, 23520, 46844, 23772, 3376
https://tests.stockfishchess.org/tests/view/5eb281dd2326444a3b6d3499

LTC:
LLR: -2.94 (-2.94,2.94) {0.25,1.75}
Total: 90232 W: 11446 L: 11353 D: 67433
Ptnml(0-2): 631, 8421, 26967, 8418, 679
https://tests.stockfishchess.org/tests/view/5eb34a862326444a3b6d37ff

Bench: 4834675

---
## [snicolet/Stockfish](https://github.com/snicolet/Stockfish)@[bcbda8932e...](https://github.com/snicolet/Stockfish/commit/bcbda8932ebbe0585c8ad9a189e078fdc8328ee1)
#### Friday 2020-05-08 09:30:30 by Stéphane Nicolet

Introduce anti-suicide feature

In some recent tournament games, Stockfish exhibited the following
self-destructing behaviour. Stockfish was suffering in a long shuffle
session, having a bad evaluation in a blocked or semi-blocked position
for about 40 moves and yet the eval was sort of flatlined, indicating
that the opponent engine (Leela) had trouble converting the position.
Then, not long before the 50-moves draw rule would be reached reached,
the opponent would play its pieces to some strange places and SF would
push a pawn, thinking she would get a slightly "less worse" evaluation.
However, the slightly less worse evaluation would prove to be delusional,
the position with a sacrificed pawn crackable and SF eventually lost
these games.

This issue was discussed in the following thread:
https://github.com/official-stockfish/Stockfish/issues/2620

This commit is our best attempt to patch this issue, so that SF gets
more patient in worse positions and try to play for 50 moves as much
as possible and not suicide. The implementation uses pure evaluation
methods rather than search, damping down the eval after 25 moves of
shuffling (damping factor is linear, starting from 1.0 after 25 shuffling
moves and reaching 0.04 after 50 moves of shuffling). This damping
puts the burden on the attacking player to prove that he can break
the fortress, as now the search will get more and more optimistic
for the defending player to be able to reach a draw by 50 moves rule.

This solution seems to work as intended for the few cases extracted
from tournament losses, according to tests done by @vondele in the
following comments:
https://github.com/official-stockfish/Stockfish/issues/2620#issuecomment-616779979
https://github.com/snicolet/Stockfish/commit/a66d3c01bb453d91e21e50b0b431a67ca906e3c4#commitcomment-38963042

In Fishtest, the best result we managed to get after extensive testing
was a double yellow with Elo-gaining bounds (this patch), maybe because
the problem is quite rare at the short time controls we use in our tests
compared to the longer time controls used in tournament games:

STC:
LLR: -2.97 (-2.94,2.94) {-0.50,1.50}
Total: 201928 W: 38274 L: 38174 D: 125480
Ptnml(0-2): 3452, 23520, 46844, 23772, 3376
https://tests.stockfishchess.org/tests/view/5eb281dd2326444a3b6d3499

LTC:
LLR: -2.94 (-2.94,2.94) {0.25,1.75}
Total: 90232 W: 11446 L: 11353 D: 67433
Ptnml(0-2): 631, 8421, 26967, 8418, 679
https://tests.stockfishchess.org/tests/view/5eb34a862326444a3b6d37ff

Bench: 4834675

---
## [titola/neuropa](https://github.com/titola/neuropa)@[7f0d4028b6...](https://github.com/titola/neuropa/commit/7f0d4028b6c1aa1f50b4706d007276e7a49e3709)
#### Friday 2020-05-08 09:31:18 by Tito Latini

The idiots are a bit edgy between 3:00 and 3:30 a.m.

Sound of rain from a basin full of water under the bed base.

I'm using simple solutions to humiliate the financiers, the sellers
of these criminal "systems", and the handicapped scientists behind
that shit. Sometimes they change the name but the sewer is the same.

    "Nun je da' retta Roma
     che t'hanno cojonato
     sto morto a pennolone
     è morto suicidato.

     Se invece poi te dicheno
     che un morto s'é ammazzato
     allora è segno certo
     che l'hanno assassinato."

P.S. Ora capisco perché qualcuno si porta il materasso da casa quando
viene in Italia.

---
## [snicolet/Stockfish](https://github.com/snicolet/Stockfish)@[7620c09abd...](https://github.com/snicolet/Stockfish/commit/7620c09abdac2c7c957aa080c6402f9938d0bef5)
#### Friday 2020-05-08 09:32:44 by Stéphane Nicolet

Introduce anti-suicide feature

In some recent tournament games, Stockfish exhibited the following
self-destructing behaviour. Stockfish was suffering in a long shuffle
session, having a bad evaluation in a blocked or semi-blocked position
for about 40 moves and yet the eval was sort of flatlined, indicating
that the opponent engine (Leela) had trouble converting the position.
Then, not long before the 50-moves draw rule would be reached reached,
the opponent would play its pieces to some strange places and SF would
push a pawn, thinking she would get a slightly "less worse" evaluation.
However, the slightly less worse evaluation would prove to be delusional,
the position with a sacrificed pawn crackable and SF eventually lost
these games.

This issue was discussed in the following thread:
https://github.com/official-stockfish/Stockfish/issues/2620

This commit is our best attempt to patch this issue, so that SF gets
more patient in worse positions and tries to play for 50 moves as much
as possible and not suicide. The implementation uses pure evaluation
methods rather than search, damping down the eval after 25 moves of
shuffling (damping factor is linear, starting from 1.0 after 25 shuffling
moves and reaching 0.04 after 50 moves of shuffling). This damping
puts the burden on the attacking player to prove that he can break
the fortress, as now the search will get more and more optimistic
for the defending player to be able to reach a draw by 50 moves rule.

This solution seems to work as intended for the few cases extracted
from tournament losses, according to tests done by @vondele in the
following comments:
https://github.com/official-stockfish/Stockfish/issues/2620#issuecomment-616779979
https://github.com/snicolet/Stockfish/commit/a66d3c01bb453d91e21e50b0b431a67ca906e3c4#commitcomment-38963042

In Fishtest, the best result we managed to get after extensive testing
was a double yellow with Elo-gaining bounds (this patch), maybe because
the problem is quite rare at the short time controls we use in our tests
compared to the longer time controls used in tournament games:

STC:
LLR: -2.97 (-2.94,2.94) {-0.50,1.50}
Total: 201928 W: 38274 L: 38174 D: 125480
Ptnml(0-2): 3452, 23520, 46844, 23772, 3376
https://tests.stockfishchess.org/tests/view/5eb281dd2326444a3b6d3499

LTC:
LLR: -2.94 (-2.94,2.94) {0.25,1.75}
Total: 90232 W: 11446 L: 11353 D: 67433
Ptnml(0-2): 631, 8421, 26967, 8418, 679
https://tests.stockfishchess.org/tests/view/5eb34a862326444a3b6d37ff

Bench: 4834675

---
## [tjsd/ATM3-Expert](https://github.com/tjsd/ATM3-Expert)@[9b1a6c1032...](https://github.com/tjsd/ATM3-Expert/commit/9b1a6c1032d811c04f5f9d9b7c08b48f0ba2cc66)
#### Friday 2020-05-08 12:34:56 by tjsd

Update corrupted_starlight.json

Changing the blood magic Demon Will Clusters to 4 Demon Will Crystals, so you can automate the creation of Corrupted Starmetal Ingot

---
## [lloydarnold/turtle-time](https://github.com/lloydarnold/turtle-time)@[aed23c453d...](https://github.com/lloydarnold/turtle-time/commit/aed23c453d0c30de2a92af2a14f423b6a5f4920c)
#### Friday 2020-05-08 14:08:20 by Lloyd Arnold

Add more shit to .gitignore because IntelliJ

IntelliJ is CLUNKY dear god it builds a lot of shit I don't understand
don't want and don't think that I need but hey c'est la vie

---
## [saqib-ali/hivemined](https://github.com/saqib-ali/hivemined)@[3589274335...](https://github.com/saqib-ali/hivemined/commit/35892743359a46219c4dedd594c9d5323d67fa9c)
#### Friday 2020-05-08 15:45:23 by Saqib Ali

Machine Learning Research Scientist - Hologic Careers. ISE Department’s Systems of Autonomous and Connected Vehicles Systems graduate class uses Machine Learning to model real world problems - 2020 - School of Engineering and Computer Science - News - OU Magazine - Oakland University. MWM - Machine Learning Engineer. Sift Healthcare Launches Rev/Track Reporting Tool, Leveraging AI And Machine Learning To … | into.AI. Cezanne OnDemand | Vacancies | Research Assistant/Associate - Machine Learning for Improved Cyber Security. U.Group - Full Stack Engineer - Artificial Intelligence / Machine Learning. Details - Postdoctoral Research Fellow/Research Officer (Machine Learning &amp; Data Fusion) - Jobs - University of Queensland. Laplace’s Demon: A Seminar Series about Bayesian Machine Learning at Scale «  Statistical Modeling, Causal Inference, and Social Science. Udemy - Senior Machine Learning Engineer. Udemy - Senior Machine Learning Engineer.

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[1f23dc308b...](https://github.com/mrakgr/The-Spiral-Language/commit/1f23dc308b39ee891462fde1bf73ac31b1f28b40)
#### Friday 2020-05-08 16:16:49 by Marko Grdinić

"1pm. I am slacking a bit too much. But let me just go on for a while longer. It is finally time for the big push that will last me months. The prelude is done. The new journal file has been opened. So I want to savor the feeling of tension for a bit more.

I've been thinking of starting with a file watcher, but that is not really the first thing I should do. Right - the first thing would be to be able to compile individual files. Then I will have to implement the basic Spiral project format.

Once that bit of work is done, then I can do my first server.

And though I say I have v0.2 done, that was only true back in January. I am going to do have to do some redesigning - keyword args will have to become symbols, I'll also add operator precedent parsing like in Haskell and Agda. I also want to do those syntax tricks with symbols that will make them smooth to use.

```
| TyKeyword of KeywordTag * TypedData []
```

This will be going out. I'll split these into bare symbols and their pairs, so that `a: 1` will be compiled into `.a, 1`. This is all so as to enable a duality and make it easy to access records by applying `r (a: 1)` like so. I'll make it so that if a pair is applied to a record, it will first apply the symbol in the left side and then the right.

This will also allow me to have special names for functions like...

```
inl add: a with: b = a + b
```

The way to call this will be to write `Add: 1 with: 2`. If it is in a module, then I could simply do `r (add: 1 with: 2)`. Just a little change in capital letter will be enough to signal the intent of the code. Alternative way for this would be to do `r (.add_with_, 1, 2)`. The `:`s will be converted into `_`s under the hood. This will work well.

I haven't seen this in any other language, but after seeing Pharo's syntax I've wanted to put some of that goodness into Spiral for a while now.

I am going to remove being able to do selective imports. Right now I already have trouble with renaming of types, but after I put in custom operator precedence and constraints, that is going to become hell. So it is best to just avoid that can of worms.

None of these changes will be particularly difficult, but they will take some time and effort.

1:15pm. Let me do the chores here.

1:30pm. Done with chores. Now let me have these oranges...and then I'll slack, and then I'll start this thing. My goal will be to do only a little each day until I get the ball rolling.

1:55pm. Let me finally start. It is time to get something done.

Right...the first thing I should do is move the `spiral-plugin` folder into the main Spiral repo. The VS Code plugin is something that should be developed alongside with the language.

Let me do that.

2:05pm. The way it shows up is strange.

I erased the `.gitignore` in the folder itself and the `.git` folder for it, but it is not returning to normal functioning.

2:15pm. I really made a mistake not erasing the `.git` folder that was already in the project to begin with, but after some fiddling around, I managed to get it to commit properly.

Now, let me get rid of the `node-ipc` files. They are an eyesore.

2:20pm. I'll name the package `spiral-vscode-ide`.

...Hmmm, in the parser, one thing I need to take seriously would be tabs. I've been thinking about this - I should get rid of the dependency on FParsec. I am not using it to parse the tokens anyway at this point, and the tokenizer is simple enough that I can implement it on my own. That would allow me to better control inlining and improve performance.

I'll need to refactor the whole parser for performance. F# is not good at optimizing temporary data structures like Haskell and I am going to have to give it a guiding hand as well as go through it with a disassembler to make sure of what is going in.

But that comes later. I'll work on making the parser fast when I have some decent sized programs to make sure that my modifications are having effect.

```
// TODO: Hash consing might not be doing what I want due to a misunderstanding of how weak references work. Investigate this.
```

What was this about? Was this about that reactive extensions book bug that made me unsure about weak references. Yeah, it is nothing.

2:40pm.

```
let test40: SpiralModule =
    {
    name="test40"
    prerequisites=[]
    description="Do the keyword arguments get parsed correctly?"
    code=
    """
inl main _ =
    inl add left:right: = left + right
    add left:1 right: 2
    + add
        left: 3
        right: 7
    |> dyn
    """
    }
```

It is really amazing how great the new syntax is. I will make it even better.

2:55pm. But speaking of syntax...

Right now I am thinking how to do syntax for configuration files. I am thinking of my own, but I do not want to put too much effort making this good in case I do some plugin for the editor. So maybe JSON or HTML, or even Lispy syntax?

I am not sure.

JSON is not good because I can't write `Submodule:` and list out the files.

3pm. Ok, I will do my own thing. Let me start by defining the schema and then I will start working on the parser for it. I guess I won't be breaking the dependency on FParsec that easily.

3:05pm.

```
// Everything that deals with Spiral project files themselves goes here
module Config

type FileHierarchy =
    | File of string
    | Directory of string * FileHierarchy list

type Schema = {
    files : FileHierarchy list
}
```

Let me start by doing this.

```
type Schema = {
    default_name : string
    files : FileHierarchy list
}
```

Ok, now the most important thing to do is to just start the parser. I can hammer out the rest of the details once I figure out how to parse the file hierarchy.

...Actually, rather than do it like this, let me make it a map.

...No wait, what am I thinking? The map is unordered, I can't use it!

Though I will have to use a set in the parser to make sure no duplicates slip in.

There is one thing I will allow - having a file be both a module and a directory.

...Should I do that? I am not sure...

Ok, I'll go with that, but I will disallow duplicate directories, much like duplicate files.

3:15pm. Is that really necessary though?

3:25pm. Yeah, I do not want duplicate directories even though I could make the parser do that. It would just make things chaotic. I want the stuff in a directory to all be either before or after some other directory in the module.

In addition to `open`, I am going to have to put in `include`...

3:25pm. This is complicated. You know what, let me just go with my original plan of no duplicates whatsoever. I can always just have the `Main` module in a folder to indicate what the right import should be.

3:35pm.

```
open System

Text.Encoding.ASCII.GetBytes(Environment.NewLine) // 13uy, 10uy
```

I should not have used `Environment.NewLine` for splitting strings as that thing is just "\r\n".

```
let x = str.Split([|"\r\n";"\r";"\n"|],System.StringSplitOptions.None)
```

I should have done this instead and covered all the cases.

3:45pm. Let me finally start on the parser instead of just thinking about it.

```
Skips over any sequence of zero or more whitespaces (space (' '), tab ('\t') or newline ("\n", "\r\n" or "\r")).
```

You know what, I am going to just implement my own spaces in order to avoid having FParsec.

4:05pm. I could not find any references to how FParsec handles tab positions.

```
let is_big_var_char c = is_var_char c || c = '\\'
```

Not going to allow '\\' to be part of variable name. That was a bad idea.

4:20pm.

```
let rec file_hierarchy_list p =
    let i = column p
    let expr p = if i <= column p then file_or_directory p else Reply(ReplyStatus.Error,expected "file or directory on the same indentation")
    between (skipChar '[') (skipChar ']') (many expr) p
```

I am really agonizing how to parse the `;` here. That was a pain in the ass in the regular Spiral, and I want to simplify things for me.

```
            | ";"  ->
                if pos.line = d.semicolon_line then let r = d.FailWith(InvalidSemicolon) in d.Skip'(-1); r
                else precedence_associativity d x (fun a b -> l "" (unop ErrorNonUnit a) b)
```

Ah, in v0.2 I finally did the smart thing and made it an operator. I forgot about that.

Let me take a short break here.

4:40pm. Let me resume.

4:50pm.

```
let file_hierarchy p =
    let s = HashSet(HashIdentity.Structural)

    let rec file_hierarchy_list p =
        let i = column p
        let expr p = if i <= column p then file_or_directory p else Reply(ReplyStatus.Error,expected "file or directory on the same indentation")
        between (skipChar '[' >>. spaces) (skipChar ']' >>. spaces) (many expr) p

    and file_or_directory p =
        pipe2
            (many1Satisfy2L is_big_var_char_starting is_var_char "uppercase file" >>= fun x p ->
                if s.Add x then Reply(x) else Reply(ReplyStatus.Error, messageError <| sprintf "The module %s has already been declared." x)
                )
            (opt (skipChar ':' >>. spaces >>. file_hierarchy_list))
            (fun name -> function
                | Some files -> Directory(name,files)
                | None -> File name
                ) p

    file_hierarchy_list p
```

Once again, it bothers me that I cannot have local mutually recursive definitions in Spiral.

4:55pm. It is really a grave omission.

5:20pm. I figured it out! Wow.

The way to do mutually recursive nested definitions is quite simple - I have to inline them during the prepass, much like global recursive functions. What I have to do then is not do the renaming immediately, but instead do the usual free variable propagation of unique ids. But in addition to that, what I have to do is for every function keep track of unresolved recursives. They come in 3 forms.

```
let rec f x = r + g x
and g x = r + f x
```

```
let rec f x =
    let rec g x = r + f x
    ...
```

```
let rec f x = r + f x
```

Those recursive functions are not unbound, but their free variables are not yet resolved. So during the first pass, that is what I have to do.

Once I do the prepass and I have known and unresolved free variables, for the unresolved ones it is actually easy to them recursively by getting a set of free vars with shortcutting. The shortcutting is just the usual memoization by reference. This would work for the recursive cases. Once you have that, just put them in and do the usual renaming.

5:30pm. The beauty of this scheme is that you do not have recursive data structures in the partial evaluator. Those are all inlined already. I am in absolute awe that something simple and cool like this is possible.

Though I should be working on the parser for the config file, this is extremely cool.

With this I will be able to take full advantage of lexical scope.

I do not need to hold myself back here - rather than just inlining the globals, why not inline all the functions. Free up the partial evaluator. This will lead to other optimizations down the line.

There is one restriction right now when I added it that did not feel comfortable with me.

```
let f (Q(x) | W(y)) = x + y
```

In Spiral unlike in F#, this is a valid pattern. But just to keep it, I needed to add that rule that locals cannot shadow globals.

Why don't I make it the same as F#? That...

```
let f (Q(x) | W(x)) = ...
```

...all sides of the union pattern must have the same name? Yeah, that is something I sorely need isn't it?

If I do this I will be able to make shadowing consistent across the whole of the language.

...Well, actually, I do not need to inline the locals too much...

5:40pm. No, I should. The less work I have to do during partial evaluation the better.

5:45pm. I should also add optimizations for simple patterns. As opposed to clauses, I should endeavor not to copy paste like a lunatic everywhere...

Though I'll leave that for last.

...Yeah, just forget inlining of the locals. That can come for the future. But inlining of the recursives whether they be local or global functions is an absolute necessity. The same goes for the pattern matching change to allow pervasive shadowing.

v0.2 adds a lot of stuff that will only be possible to the top level, but mutually recursive functions should not have been one of them.

```
let file_hierarchy p =
    let s = HashSet(HashIdentity.Structural)

    let rec file_hierarchy_list p =
        let i = column p
        let expr p = if i <= column p then file_or_directory p else Reply(ReplyStatus.Error,expected "file or directory on the same indentation")
        between (skipChar '[' >>. spaces) (skipChar ']' >>. spaces) (many expr) p

    and file_or_directory p =
        pipe2
            (many1Satisfy2L is_big_var_char_starting is_var_char "uppercase file" >>= fun x p ->
                if s.Add x then Reply(x) else Reply(ReplyStatus.Error, messageError <| sprintf "The module %s has already been declared." x)
                )
            (opt (skipChar ':' >>. spaces >>. file_hierarchy_list))
            (fun name -> function
                | Some files -> Directory(name,files)
                | None -> File name
                ) p

    file_hierarchy_list p
```

Being able to declare that variable `s` where mutually recursive functions can access it is so great. It is so expressive and so powerful. So right. It is truly a thing of beauty.

I thought about it a lot over the past few months, and every time I thought about doing this same thing with interfaces, I cringed inside. I really had the feeling that doing this should be possible somehow in a simple manner, but just could not reach it.

6pm. Spiral v0.2 is going to be great. I really have a clear path forward to doing something good here.

...Also, that this idea is possible is great lesson how it is possible to do optimizations in two steps that could not be done in one. Everything is connected to some degree, and this does have implications for ML.

6:10pm. Let me call it a day here.

It has really been a while since I've had new insights. This is me, the real me. It feels so great to get back to what I should have been doing.

This I will do it right.

Tomorrow I will finish this config parser and get that server going. Then I will work on editor support and language in tandem. Apart from the symbol (keywords args) change, for once I do not think I'll have to do any refactoring in the partial evaluator."

---
## [thefrontside/bigtest](https://github.com/thefrontside/bigtest)@[bc17f3af88...](https://github.com/thefrontside/bigtest/commit/bc17f3af880f9b630c642a5a932218da69a59f92)
#### Friday 2020-05-08 17:09:58 by Charles Lowell

Automatically launch agents from the orchestrator/cli

We want to be able to weave the starting and stopping of browsers into
test flow seamlessly, whether in CI, or during development.

This adds the capability to manage a set of browsers to the
orchestrator, and then specify which managed browsers to launch when
the orchestrator spins up.

The tack we take is to create a set named of "drivers" associated with
the project that specify how to launch a runtime that will contain an
agent so that tests can be run. At this moment, "driver" is really a
synonym for "webdriver local", but the vocabulary was left intentionally
more abstract in order to accomodate other kinds of drivers such as
remote webdrivers as well as iOS, Android, and Node drivers.

In the `.bigtest.json` config, this adds the "drivers" field and a
"launch" field. Each entry in the "drivers" field specifies a driver
configuration that can be used by the "launch" field (and also by the
`--launch` command line option). The idea is that the "driver" is just
an opaque blop of configuration that can be passed to the underlying
driver when it is launched. For example, you might have two drivers
for chrome, one that uses the default window size, and another that
launches with a small window in order to test the small breakpoint in
a responsive layout:

```json
"drivers": {
  "chrome": {
    "browserName": "chrome",
    "headless": true,
  },
  "chrome-small": {
    "browserName": "chrome",
    "headless": true,
    "width": 600
  },
  "launch": {
    "chrome": {}
  }
}
```

Now, when starting the server, the orchestrator will see the
"chrome" option in the project config's "launch" field, and
automatically launch that driver with the configuration in the driver
field merged with whatever is in the launch field. While
"chrome-small" is _available_ to be launched, it will not be launced
by default in the absence of a `--launch` parameter. However, if you
_do_ want to launch it, here's how you would:

```
$ bigtest server --launch chrome-small
```

In order to facilitate having a single opaque configuration parameter,
the `@bigtest/webdriver` package was refactored to take a single
parameter.

TODOs and Open Questions:
-------------------------

- [ ] Not super happy with the additions to Project config. The
  problem is, that from the command line you want to be able override
  configuration like setting "headless" to `false`, so the format of
  the "launch" field has to have the same format as the "driver"
  field, but that's somewhat confusing since the purpose of the
  "launch" field isn't to provide configuration, just specify _which_
  browsers to launch. Feels more like it should be an array like
  `"launch": ["chrome"']``. I only did it this way so that
  `createServer` could take a `ProjectOptions` object and not have to
  diverge.
- [ ] The argument parsing for the `--launch` flag feels really quite
  flimsy. It parses everything after the `@` sign as [`JSON5`][1] with
  an implicit bracket, which feels hackish, but more importantly it
  lacks the things you expect from a robust CLI like:
    - at a minumum validate of options, so `--launch chrome@blah:true`
      is an error as would be `--launch chrome@headless:5`
    - completition: `--launch X` should complete X from the list of
      available drivers. `--launch chrome@` should should completation
      of the chrome drivers attributes, etc..
- [ ] Don't want to make the config confusing so that it's constantly
  a source of frustration. Still, drivers will have complex
  configuration, especially for remote cloud-based drivers that will
  need to specify driver urls and auth tokens. Maybe making
  configuration sparse?
- [ ] There is a potential race condition lurking in the browser
  launching resource. According to the webdriver spec, the navigation
  command will return once the `load` event has been dispatched on the
  document. This makes sense, but we really don't want to consider the
  launch complete until we receive the corresponding `connect` event
  from the agent as it calls home to activate itself. If we try to run
  the test before this connect event happens, or worse, if the agent
  code somehow fails after the `load` event happens, we'll think the
  agent should be connected when in fact it is not.

[1]: https://json5.org

---
## [wrye-bash/wrye-bash](https://github.com/wrye-bash/wrye-bash)@[a13813caf4...](https://github.com/wrye-bash/wrye-bash/commit/a13813caf4f49060860339968257c611c01180de)
#### Friday 2020-05-08 18:09:56 by MrD

WIP Subrecords EEE TTT

EEE docs return None from pack subrecords data
EEE MelUInt32('INDX', '')
EEE and header.label not in {'CELL','WRLD'}:

A dream come True. There is too much goodness in here so some random
notes:

- dumpData overrides went down. Now we actually override the part of the
behavior we need - we can further centralize/optimize etc
- ModWritter is gone - another level of function calls, another API for
new (and seasoned) devs to grok, and most importantly packing/loading
logic all around the place
- strings handling is centralized - defining packSub0 and co away from
the usages was difficult to follow. Plus this is a part of the code that
needs work - note discoveries on decode performance, the encodings
handling/naming is difficult to follow and rework for py3 plus cstrip
must disappear
- packSub does not do this weird "call me in two ways" - this is handled
by pack_subrecord_data
- used some itertools and comprehensions to avoid copying data, drop
inlines etc. I did use some `map`s here
- note _collect_array_data was already pack_subrecord_data - we now don't
need the sio however :) Plus to me it's much easier to reason about a
function than to reason about the stream that I write on - seeing:

-    def _collect_array_data(self, record):
+    def pack_subrecord_data(self, record):
         """Collects the actual data that will be dumped out."""
-        array_data = MelArray._DirectModWriter(sio())
-        if self._prelude:
-            self._prelude.dumpData(record, array_data)

I already wonder how come dumpData does not write headers in line 3 -
cause the definition of _DirectModWriter two lines above does not help
much. All this is past - now functions do one thing - only real
complication is the strings one - the packSub0 (sic) is replaced with
_dump_bytes and package_subrecord_data - was doing too much (actually
data was always encoded (see usages) no need for if checks) and too
little (copy/pasting packSub and buggily at that) TTTTT
- subType -> mel_sig
- I threw slots in but don't feel we should go and slot all subclasses -
this is still WIP
- subrecords attribute removed from MelRecord, making all instances
lighter - more to follow

limits are things like:

MelUInt32('INDX', '').packSub(out, struct_pack(u'=I', part_attr))

although I still find this more readable - would like to absorb the
packing in there. The attr being '' (EEE rename to u'unused') is
just exposing our current model a bit too bluntly: records are *not*
composed of MelBase instances - those just set/dump the attributes of
the records passed in from/to the stream they are passed in. I guess this
was done to save memory and it's probably a good idea - but defining the
records as sets of MelBase instances might also be a good idea - that's
pt4

_MelSimpleStruct(MelStruct) -> _MelSimpleStruct(MelBase) TTT

get rid of quite a few processing - I may have missed some needed override

Mopy/bash/brec/basic_elements.py: avoid concatenating strings

packSub0 call self.packSub TTT

I don't want to copy data so this is XXX but note:

- if data is None: was never (see also rewrite later)
- data was always encoded (see usages) no need for if checks
- comparison was wrong? if lenData < 0xFFFF: is if lenData <= 0xFFFF:
in packSub TTT
- note subheader format is hardcoded

SSS runs! BP crashes EEE

Traceback (most recent call last):
  File "bash\gui\events.py", line 166, in _post
    result = listener(*listener_args)
  File "bash\balt.py", line 904, in _conversation_wrapper
    return func(*args, **kwargs)
  File "bash\basher\patcher_dialog.py", line 190, in PatchExecute
    patchFile.init_patchers_data(enabled_patchers, SubProgress(progress, 0, 0.1)) #try to speed this up!
  File "bash\patcher\patch_files.py", line 167, in init_patchers_data
    patcher.initData(SubProgress(progress, index))
  File "bash\patcher\patchers\importers.py", line 707, in initData
    record):
  File "bash\bolt.py", line 902, in __eq__
    return self._cs == Path.__getCase(other)
  File "bash\bolt.py", line 433, in __getCase
    return os.path.normcase(os.path.normpath(name))
  File "C:\Users\MrD\venvs\wrye-wx4-win\lib\ntpath.py", line 419, in normpath
    if path.startswith(('\\\\.\\', '\\\\?\\')):
AttributeError: 'int' object has no attribute 'startswith'

Turns out:

<DABoethiaChosen09 [NPC_:(Oblivion.esm, 0A2AE5)]>

combatStyle = (bolt.Path(u'Oblivion.esm'), 0)

So we were not expecting a long fid?
I chose a bit arcane mel_sig cause sub_rec_sig was too verbose and because
code already thinks in terms of "mod elements". It's searchable and should
be confined to current uses

_MelField/MelStruct._packer

less function calls and more local scope, but importantly stop thinking
in terms of format and think in terms of operations. struct_pack uses are
a nice metric (less is better :P)

FFF skip group

---
## [nschilling10/PuFF_magnetic_nozzle_charging_model](https://github.com/nschilling10/PuFF_magnetic_nozzle_charging_model)@[010eee8ff5...](https://github.com/nschilling10/PuFF_magnetic_nozzle_charging_model/commit/010eee8ff5c2bc25452ee8ea278237751130331d)
#### Friday 2020-05-08 18:14:23 by Nathan Schilling

Model run with good recharge, and good thruster characteristics. Trying to get halfway descent Isp out of heavy U235 is a pain in the ass. 1keV doesn't really cut it - the best Isp I could get was like 5000 sec, which is basically the same as an ion engine. Usually runs had an Isp in the Hall thruster range (1800-2200 sec) which is crap. I figured out I had to increase the temperature from 1keV to 50keV. I'm not sure Rob can do that implosion. BUT Isp and impulse bit go up. Netron and heatign load go up by an order of magnitude however, pushing hte limits of design. Playing with the model I realized that a smaller nozzle is better for increased velocity, but a bigger nozzle is better for recharge. Also Nfcc, Rfcc, and I1_0 all affect recharge but also the final velocty. I should probably do a 3D trade study to compare all three, but I'm not sure how to graph it. I think a bigger nozzle is better for recharge because the plasma has mroe time to expand - if Nfcc is too high and I1_0 is too high, the inital magnetic field is too high and the plasma doesn't get to fully expand in the nozzle. Conversely, if it's too low, the plasma slams into the nozzle wall. Anyway, the run in this commit is a good one to look at, reference, and check out. It has an Isp of 7000sec, impulse bit of 10 kNs and a yield of about 1 GJ. It dumps about 20MJ/sr and 50MJ/sr neturon and photon energy fluences however, which is bad.

---
## [rapid7/metasploit-framework](https://github.com/rapid7/metasploit-framework)@[5dbb9e8ccc...](https://github.com/rapid7/metasploit-framework/commit/5dbb9e8ccc51328c3ddfb7b1e12f5da37b443510)
#### Friday 2020-05-08 18:24:02 by OJ

Fix packet ordering check

A long time ago prior to supporting both encrypted packets and packet
pivots, a bit of code existing in the packet dispatcher that reordered
packets before passing them on to the internal workings. This reordering
would prioritise responses first, it would put "channel close" messages
at the end, and the rest would go in between. It's a bit gross, but it
is what it is.

The key here is to note that for this ordering to happen, the code needs
to be able to access the packet header (to determine if it's request or
response), and to access the packet body (to get access to the method
and check if it's a channel close message).

When packet encryption came in this wasn't too much of a concern because
the packet decryption could happen as soon as the packet came off the
wire. This meant that both the header and the body were available for
consumption and everything sunshine, daisies and unicorn farts.

ENTER PACKET PIVOTING TO MESS THIS ALL UP!

As we're all fully aware (right?) encryption keys are per-session. So
this means that every session has its own set of keys, and hence to
decrypt a packet we need to make sure we've got the right session. This
was a no brainer before, because sessions read their own packets off
their own transports. But with pivots, that changed because packets
could appear on the transport that were intended for other sessions.

It appeared that the solution here was simple. When a packet is read off
the wire, just read the body in full without decrypting. Check the
session GUID to see if it matches the current session, or to see if it's
inteded for a pivoted session. If it's the latter, then use the pivot
session decryption key, if the former, use the current session's key.
Too easy, right?

Right?

There was an internal function that was invoked to dispatch packets
after the came off the wire, called `dispatch_inbound_packet`. It seemed
to make sense to decrypt the packet here because that was invoked across
the various transports. So code was added at this point to decrypt the
packets based on the appropriate session. Testing was done, things
seemed to work.

Fast forward to last night, where I lost a bunch of hours while working
on something that shouldn't be related. I have been changing the
mechanism used for methods so that we don't use strings, we instead use
identifiers (makes the noise on the wire smaller/less obvious, and
allows us to remove method strings from our payloads). Rather than
attempt to locate all the spots where the method IDs are either
hard-coded or generated, it made more sense to start with functionality
in the `Packet` class that would map between method strings and command
identifiers. In order to catch the case where we had a method string
that we didn't expect, I raise an exception when the method string
doesn't exist in the map of known strings.

This exception was a blessing and a curse. To cut this story a little
shorter, we ended up with the following situation:

* Packets would start coming in and the reader would read the header and
  then decode it so that we could find the size of the packet and read
  the packet body.
* The packet header was then in the clear, but the packet body was yet
  to be decrypted.
* The "prioritisation" hack would run, checking the packet type (which
  is fine because it's in the clear), then the method (which is not
  fine, because it hasn't been decrypted).

Prior to the work I was doing, the method id check would _always fail_
because the method string would come out blank.

After including my work, the exception literally killed the packet
dispatching, resulting in all kinds of horrid woes (such as having all
channels failing).

What this means is that since packet pivots came about, we have not been
correctly pushing channel close messages to the back of the queue before
processing. The result? I don't know! I know that we've had issues
raised against the code saying that packets are coming out of order in
certain cases when channels are in use, but I don't think that's
related. What's clear is that I broke it when I did the packet pivots,
and I've only just realised it now.

So this code is intended to fix the problem and make sure that channel
close messages are pushed to the back like they were before.

At this point, people should be well aware of how easy it is for me to
break things, and therefore revoke my access to anything with
a keyboard.

---
## [Skyrat-SS13/Skyrat13](https://github.com/Skyrat-SS13/Skyrat13)@[609ffe3011...](https://github.com/Skyrat-SS13/Skyrat13/commit/609ffe3011a5acdcc6ecfab8c89a49bdf774790e)
#### Friday 2020-05-08 18:45:20 by Useroth

Merge pull request #1771 from BobJoga/ididntaskforthis

You know the legion core nerf Citadel did? Yeah i fucking hate it.

---
## [DerFlammenwerfer/sojourn-station](https://github.com/DerFlammenwerfer/sojourn-station)@[4d49c15723...](https://github.com/DerFlammenwerfer/sojourn-station/commit/4d49c15723757ce924a8142c9b239167c7d0fa43)
#### Friday 2020-05-08 23:13:29 by DerFlammenwerfer

Hard nerf to render HP

Buffing renders to ridiculous levels solely because of ONE PLAYER doing ridiculous shit after equally ridiculous levels of preparation is not good gameplay balance, as it punishes everyone for the actions of one or two autists. These things are so goddamn fast already that you will be forced into a melee encounter with them and they hit so hard that melee is entirely unviable in the first place. THese things should not be taking out a party of three well-prepared people with hard-hitting guns. Vigil MacLeod is not a valid reason for over-buffing renders. It still has 500 HP with this change, which is plenty enough to make soloing them very fucking difficult if not impossible with all their other advantages.

---

# [<](2020-05-07.md) 2020-05-08 [>](2020-05-09.md)

