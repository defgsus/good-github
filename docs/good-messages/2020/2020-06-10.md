# [<](2020-06-09.md) 2020-06-10 [>](2020-06-11.md)

101,578 events, 53,429 push events, 86,942 commit messages, 9,053,783 characters


## [NeatNerdPrime/pyre-check](https://github.com/NeatNerdPrime/pyre-check)@[da54214da9...](https://github.com/NeatNerdPrime/pyre-check/commit/da54214da93f0923d40fbd4ed1b57a8267383911)
#### Wednesday 2020-06-10 23:12:38 by Mark Mendoza

RFC: Support nominally named parameters preceding parameter spec components

Summary:
While writing up the spec for `Concatenate`, I realized that I have been enforcing a requirement that doesn't actually need to exist.

It is correct that we can't concatenate named parameters onto parameter specifications.  However: this need only apply to the external interface of the function.  As long as we require that the "named" parameters being prepended are being called positionally, the function itself can be written to have named parameters.

Concretely, both
```
   def f(x: X, *args: _TParams.args, **kwargs: _TParams.kwargs) -> R: ...
 ```
and
```
   def g(x: X, \, *args: _TParams.args, **kwargs: _TParams.kwargs) -> R: ...
```
are valid instances of `Callable[Concatenate[X, TParams], R]`

The question is: what is the better user experience:

```
    def q(x: int) -> None: ...

    def f(
        fn: Callable[_TParams, _TReturn], # Option A:  error here and complain that prepended arguments must be positional-only
        *args: _TParams.args,
        **kwargs: _TParams.kwargs,
    ) -> _TReturn: ...

   f(fn=q, x=1) # Option B: Potentially confusingly error here complaining about missing first argument while it seems like you're correctly passing in a fn
```

This diff opts for B, as I feel like it's natural to add named parameters, especially when that named parameter is `self`.  Requiring `__` or `\` seems too painful.

I think we can avoid the confusion in option B with some best-effort special casing on the error to explicitly mention this policy.

Reviewed By: shannonzhu

Differential Revision: D21945652

fbshipit-source-id: 8f215c52f7a4fbbbe821c942e1a88a3e25bdf867

---
## [Mirantis/magma](https://github.com/Mirantis/magma)@[50d76bac9f...](https://github.com/Mirantis/magma/commit/50d76bac9fa1b747ed4f538f15a8fcdac2994733)
#### Wednesday 2020-06-10 23:37:20 by Tudor Cirstea

Bump flow to 0.126.1 (#1793)

Summary:
Pull Request resolved: https://github.com/facebookincubator/magma/pull/1793
Pull Request resolved: https://github.com/facebookincubator/symphony/pull/714
Pull Request resolved: https://github.com/facebookexternal/fbc/pull/2606
Pull Request resolved: https://github.com/facebookexternal/terragraph-nms/pull/277

Bump flow version, fix logging package to comply.
- convert package to es6 imports/exports to support flow types (otherwise you get "No default export found in imported module "fbcnms/logging"  import/default")
- typed StreamOptions explicitly because implicit wasn't working
- added return type for getHttpLogger since it was missing and assumed any

I'd love to hear thoughts on `es6` import vs `require` imports, and if we should update our codebase. There's quite a few calls that do `const logger = require('fbcnms/logging').getLogger(module);` in es6 import code.

Reviewed By: rckclmbr, aclave1

Differential Revision: D21965860

fbshipit-source-id: 0c205c16986bb5db10484ecafaa18ee509dec203

---
## [pytorch/pytorch](https://github.com/pytorch/pytorch)@[0aecbbb762...](https://github.com/pytorch/pytorch/commit/0aecbbb7625f8c5809b0a6ce7ca28c622404d8a2)
#### Wednesday 2020-06-10 23:42:10 by Mike Ruberry

Changes TensorIterator computation to not consider out kwarg, lets UnaryOps safe cast to out (#39655)

Summary:
**BC breaking note:**

In PyTorch 1.5 passing the out= kwarg to some functions, like torch.add, could affect the computation. That is,

```
out = torch.add(a, b)
```

could produce a different tensor than

```
torch.add(a, b, out=out)
```

This is because previously the out argument participated in the type promotion rules. For greater consistency with NumPy, Python, and C++, in PyTorch 1.6 the out argument no longer participates in type promotion, and has no effect on the computation performed.

**ORIGINAL PR NOTE**

This PR effectively rewrites Tensor Iterator's "compute_types" function to both clarify its behavior and change how our type promotion works to never consider the out argument when determining the iterator's "common dtype," AKA its "computation type." That is,

```
a = op(b, c)
```

should always produce the same result as

```
op(b, c, out=a)
```

This is consistent with NumPy and programming languages like Python and C++.

The conceptual model for this change is that a TensorIterator may have a "common computation type" that all inputs are cast to and its computation performed in. This common computation type, if it exists, is determined by applying our type promotion rules to the inputs.

A common computation type is natural for some classes of functions, like many binary elementwise functions (e.g. add, sub, mul, div...). (NumPy describes these as "universal functions.") Many functions, however, like indexing operations, don't have a natural common computation type. In the future we'll likely want to support setting the TensorIterator's common computation type explicitly to enable "floating ufuncs" like the sin function that promote integer types to the default scalar type. Logic like that is beyond the type promotion system, which can only review inputs.

Implementing this change in a readable and maintainable manner was challenging because compute_types() has had many small modifications from many authors over ~2 year period, and the existing logic was in some places outdated and in other places unnecessarily complicated. The existing "strategies" approach also painted with a broad brush, and two of them no longer made conceptual sense after this change. As a result, the new version of this function has a small set of flags to control its behavior. This has the positive effect of disentangling checks like all operands having the same device and their having the same dtype.

Additional changes in this PR:

- Unary operations now support out arguments with different dtypes. Like binary ops they check canCast(computation type, out dtype).
- The dtype checking for lerp was outdated and its error message included the wrong variable. It has been fixed.
- The check for whether all tensors are on the same device has been separated from other checks. TensorIterators used by copy disable this check.
- As a result of this change, the output dtype can be computed if only the input types are available.
- The "fast path" for checking if a common dtype computation is necessary has been updated and simplified to also handle zero-dim tensors.
- A couple helper functions for compute_types() have been inlined to improve readability.
- The confusingly named and no longer used promote_gpu_output_dtypes_ has been removed. This variable was intended to support casting fp16 reductions on GPU, but it has become a nullop. That logic is now implemented here: https://github.com/pytorch/pytorch/blob/856215509d89c935cd1768ce4b496d4fc0e919a6/aten/src/ATen/native/ReduceOpsUtils.h#L207.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39655

Differential Revision: D21970878

Pulled By: mruberry

fbshipit-source-id: 5e6354c78240877ab5d6b1f7cfb351bd89049012

---
## [judev1/FBot](https://github.com/judev1/FBot)@[f76ed2c52c...](https://github.com/judev1/FBot/commit/f76ed2c52c4cbfc17677a45559949c2809a8e06b)
#### Wednesday 2020-06-10 23:47:26 by Zynox

Commit v1.5

What's new in v1.5?
- Added new commands: Notice board, Events, Status, Ping, Commands
- Fixed and added to previous commands: Help and info
- New trigger words: Smiley faces, Hello, Hi, Welcome, Thanks, Thank you, Tysm, Ooo, Stfu
- Improved Hehe and Haha but a shit ton, can guess it most of the time
- Improved the call on FBot feature
- Improved Fuck you added F u
- Added a plain fuck trigger word
- Improved Oof
- Simplified some of the code
- Fixed Gayscale, again
- Fixed some of the wording
- Fixed a minor error
495 lines -> 637 lines

---

# [<](2020-06-09.md) 2020-06-10 [>](2020-06-11.md)

