# [<](2020-11-13.md) 2020-11-14 [>](2020-11-15.md)

1,885,173 events, 1,110,837 push events, 1,553,397 commit messages, 96,531,979 characters


## [binex-dsk/passmanpp](https://github.com/binex-dsk/passmanpp)@[f787f21133...](https://github.com/binex-dsk/passmanpp/commit/f787f2113372d8d24638da3644f54a4d36d0ba11)
#### Saturday 2020-11-14 00:17:07 by binex-dsk

1.3.0 update

- significantly changed entry interaction, edit, view, and delete are all combined into one "view" command, which lists each entry and provides an easy way to edit, delete, or view data
- updated the entry editor and creator, now the password can be directly input next to the others and generated with just a popup, as well as showing/hiding it
- added a lot of icons
- changed most connects to be newer format
- i changed some things with deleting but i fucking forgot what it was
- added -march option to the .pro file, also updated it because i forgot to lol
- updated readme and removed that part about legal shit yeah no im not going to jail

---
## [AuroraEngine/retard-code](https://github.com/AuroraEngine/retard-code)@[a662562bcf...](https://github.com/AuroraEngine/retard-code/commit/a662562bcf92f739d9b70690944f8ef866183859)
#### Saturday 2020-11-14 00:24:43 by Reece

FUCK YOU
reason ten thousand two hundred and fifty six to hate the dumbass boomers at who guessed it... unicode!!! go back to breaking pipe/virtcal bar or some other bullshit you fucking dumb ass emoji loving soy wojaks. why yes, i too dynamically link myself against a variable in another module only a tool-chain that has myself as a dependency can compile!! wait, you mean, we should at least initialize the variable somewhere because otherwise we can't self-host our ecosystem?! me no understandy. fucking retards

---
## [EbolaSpreadn247/endless-war](https://github.com/EbolaSpreadn247/endless-war)@[280e4d2244...](https://github.com/EbolaSpreadn247/endless-war/commit/280e4d224465b353d7042fcbac9cf904cebea471)
#### Saturday 2020-11-14 00:33:12 by EbolaSpreadn247

fuck trains

dont add this cus idk game design but is the code right? and did i forget to alter anywhere.
The lore idea is that the trains arent maintained very well and they just drive themselves using slime from the district. Like, both powering the engine and slimeing up the tracks.
I think itd be funny to require killing sacrificial lambs above ground for the dang trains to even work.
This'd probably suck rly bad though because it would DESTROY scavenging slime if not tweaked.

---
## [EbookFoundation/free-programming-books](https://github.com/EbookFoundation/free-programming-books)@[d692e894a6...](https://github.com/EbookFoundation/free-programming-books/commit/d692e894a67555aff442e99c6acf0873213684dd)
#### Saturday 2020-11-14 00:53:24 by Ramona Saintandre

added Python cheat sheets (#5011)

* added Python cheat sheets

Thanks for hosting this.
Just getting back into coding, and since I have been looking for resources for Python. 
I thought I would add what I have found. 

Happy hacking

* removed the spaces 

Hi sorry, it took so long to resolve this conflict. 
Actually took me a while to figure out how to do.

Thanks again so much for hosting this.

* alphabetize

* linespacing

* alphabetize

Co-authored-by: Eric Hellman <eric@hellman.net>

---
## [freebsd/freebsd](https://github.com/freebsd/freebsd)@[08af032b54...](https://github.com/freebsd/freebsd/commit/08af032b540e652928366c14cde38c73d1db924f)
#### Saturday 2020-11-14 01:31:40 by kevans

MFC imgact_binmisc housecleaning: r367361, r367439, r367441-r367442,
r367444, r367452, r367456, r367477

r367361:
imgact_binmisc: fix up some minor nits

- Removed a bunch of redundant headers
- Don't explicitly initialize to 0
- The !error check prior to setting imgp->interpreter_name is redundant, all
  error paths should and do return or go to 'done'. We have larger problems
  otherwise.

r367439:
imgact_binmisc: minor re-organization of imgact_binmisc_exec exits

Notably, streamline error paths through the existing 'done' label, making it
easier to quickly verify correct cleanup.

Future work might add a kernel-only flag to indicate that a interpreter uses
#a. Currently, all executions via imgact_binmisc pay the penalty of
constructing sname/fname, even if they will not use it. qemu-user-static
doesn't need it, the stock rc script for qemu-user-static certainly doesn't
use it, and I suspect these are the vast majority of (if not the only)
current users.

r367441:
binmiscctl(8): miscellaneous cleanup

- Bad whitespace in Makefile.
- Reordered headers, sys/ first.
- Annotated fatal/usage __dead2 to help `make analyze` out a little bit.
- Spell a couple of sizeof constructs as "nitems" and "howmany" instead.

r367442:
imgact_binmisc: validate flags coming from userland

We may want to reserve bits in the future for kernel-only use, so start
rejecting any that aren't the two that we're currently expecting from
userland.

r367444:
imgact_binmisc: abstract away the list lock (NFC)

This module handles relatively few execs (initial qemu-user-static, then
qemu-user-static handles exec'ing itself for binaries it's already running),
but all execs pay the price of at least taking the relatively expensive
sx/slock to check for a match when this module is loaded. Future work will
almost certainly swap this out for another lock, perhaps an rmslock.

The RLOCK/WLOCK phrasing was chosen based on what the callers are really
wanting, rather than using the verbiage typically appropriate for an sx.

r367452:
imgact_binmisc: reorder members of struct imgact_binmisc_entry (NFC)

This doesn't change anything at the moment since the out-of-order elements
were a pair of uint32_t, but future additions may have caused unnecessary
padding by following the existing precedent.

r367456:
imgact_binmisc: move some calculations out of the exec path

The offset we need to account for in the interpreter string comes in two
variants:

1. Fixed - macros other than #a that will not vary from invocation to
   invocation
2. Variable - #a, which is substitued with the argv0 that we're replacing

Note that we don't have a mechanism to modify an existing entry.  By
recording both of these offset requirements when the interpreter is added,
we can avoid some unnecessary calculations in the exec path.

Most importantly, we can know up-front whether we need to grab
calculate/grab the the filename for this interpreter. We also get to avoid
walking the string a first time looking for macros. For most invocations,
it's a swift exit as they won't have any, but there's no point entering a
loop and searching for the macro indicator if we already know there will not
be one.

While we're here, go ahead and only calculate the argv0 name length once per
invocation. While it's unlikely that we'll have more than one #a, there's no
reason to recalculate it every time we encounter an #a when it will not
change.

I have not bothered trying to benchmark this at all, because it's arguably a
minor and straightforward/obvious improvement.

r367477:
imgact_binmisc: limit the extent of match on incoming entries

imgact_binmisc matches magic/mask from imgp->image_header, which is only a
single page in size mapped from the first page of an image. One can specify
an interpreter that matches on, e.g., --offset 4096 --size 256 to read up to
256 bytes past the mapped first page.

The limitation is that we cannot specify a magic string that exceeds a
single page, and we can't allow offset + size to exceed a single page
either.  A static assert has been added in case someone finds it useful to
try and expand the size, but it does seem a little unlikely.

While this looks kind of exploitable at a sideways squinty-glance, there are
a couple of mitigating factors:

1.) imgact_binmisc is not enabled by default,
2.) entries may only be added by the superuser,
3.) trying to exploit this information to read what's mapped past the end
  would be worse than a root canal or some other relatably painful
  experience, and
4.) there's no way one could pull this off without it being completely
  obvious.

The first page is mapped out of an sf_buf, the implementation of which (or
lack thereof) depends on your platform.

---
## [MurilloM92/datasciencecoursera](https://github.com/MurilloM92/datasciencecoursera)@[b81341671a...](https://github.com/MurilloM92/datasciencecoursera/commit/b81341671a3beeb7cef0ec8b652c27b95c4d276b)
#### Saturday 2020-11-14 04:13:57 by Fernando Murillo

Update README.md

How to share data with a statistician
This is a guide for anyone who needs to share data with a statistician or data scientist. The target audiences I have in mind are:

Collaborators who need statisticians or data scientists to analyze data for them
Students or postdocs in various disciplines looking for consulting advice
Junior statistics students whose job it is to collate/clean/wrangle data sets
The goals of this guide are to provide some instruction on the best way to share data to avoid the most common pitfalls and sources of delay in the transition from data collection to data analysis. The Leek group works with a large number of collaborators and the number one source of variation in the speed to results is the status of the data when they arrive at the Leek group. Based on my conversations with other statisticians this is true nearly universally.

My strong feeling is that statisticians should be able to handle the data in whatever state they arrive. It is important to see the raw data, understand the steps in the processing pipeline, and be able to incorporate hidden sources of variability in one's data analysis. On the other hand, for many data types, the processing steps are well documented and standardized. So the work of converting the data from raw form to directly analyzable form can be performed before calling on a statistician. This can dramatically speed the turnaround time, since the statistician doesn't have to work through all the pre-processing steps first.

What you should deliver to the statistician
To facilitate the most efficient and timely analysis this is the information you should pass to a statistician:

The raw data.
A tidy data set
A code book describing each variable and its values in the tidy data set.
An explicit and exact recipe you used to go from 1 -> 2,3
Let's look at each part of the data package you will transfer.

The raw data
It is critical that you include the rawest form of the data that you have access to. This ensures that data provenance can be maintained throughout the workflow. Here are some examples of the raw form of data:

The strange binary file your measurement machine spits out
The unformatted Excel file with 10 worksheets the company you contracted with sent you
The complicated JSON data you got from scraping the Twitter API
The hand-entered numbers you collected looking through a microscope
You know the raw data are in the right format if you:

Ran no software on the data
Did not modify any of the data values
You did not remove any data from the data set
You did not summarize the data in any way
If you made any modifications of the raw data it is not the raw form of the data. Reporting modified data as raw data is a very common way to slow down the analysis process, since the analyst will often have to do a forensic study of your data to figure out why the raw data looks weird. (Also imagine what would happen if new data arrived?)

The tidy data set
The general principles of tidy data are laid out by Hadley Wickham in this paper and this video. While both the paper and the video describe tidy data using R, the principles are more generally applicable:

Each variable you measure should be in one column
Each different observation of that variable should be in a different row
There should be one table for each "kind" of variable
If you have multiple tables, they should include a column in the table that allows them to be joined or merged
While these are the hard and fast rules, there are a number of other things that will make your data set much easier to handle. First is to include a row at the top of each data table/spreadsheet that contains full row names. So if you measured age at diagnosis for patients, you would head that column with the name AgeAtDiagnosis instead of something like ADx or another abbreviation that may be hard for another person to understand.

Here is an example of how this would work from genomics. Suppose that for 20 people you have collected gene expression measurements with RNA-sequencing. You have also collected demographic and clinical information about the patients including their age, treatment, and diagnosis. You would have one table/spreadsheet that contains the clinical/demographic information. It would have four columns (patient id, age, treatment, diagnosis) and 21 rows (a row with variable names, then one row for every patient). You would also have one spreadsheet for the summarized genomic data. Usually this type of data is summarized at the level of the number of counts per exon. Suppose you have 100,000 exons, then you would have a table/spreadsheet that had 21 rows (a row for gene names, and one row for each patient) and 100,001 columns (one row for patient ids and one row for each data type).

If you are sharing your data with the collaborator in Excel, the tidy data should be in one Excel file per table. They should not have multiple worksheets, no macros should be applied to the data, and no columns/cells should be highlighted. Alternatively share the data in a CSV or TAB-delimited text file. (Beware however that reading CSV files into Excel can sometimes lead to non-reproducible handling of date and time variables.)

The code book
For almost any data set, the measurements you calculate will need to be described in more detail than you can or should sneak into the spreadsheet. The code book contains this information. At minimum it should contain:

Information about the variables (including units!) in the data set not contained in the tidy data
Information about the summary choices you made
Information about the experimental study design you used
In our genomics example, the analyst would want to know what the unit of measurement for each clinical/demographic variable is (age in years, treatment by name/dose, level of diagnosis and how heterogeneous). They would also want to know how you picked the exons you used for summarizing the genomic data (UCSC/Ensembl, etc.). They would also want to know any other information about how you did the data collection/study design. For example, are these the first 20 patients that walked into the clinic? Are they 20 highly selected patients by some characteristic like age? Are they randomized to treatments?

A common format for this document is a Word file. There should be a section called "Study design" that has a thorough description of how you collected the data. There is a section called "Code book" that describes each variable and its units.

How to code variables
When you put variables into a spreadsheet there are several main categories you will run into depending on their data type:

Continuous
Ordinal
Categorical
Missing
Censored
Continuous variables are anything measured on a quantitative scale that could be any fractional number. An example would be something like weight measured in kg. Ordinal data are data that have a fixed, small (< 100) number of levels but are ordered. This could be for example survey responses where the choices are: poor, fair, good. Categorical data are data where there are multiple categories, but they aren't ordered. One example would be sex: male or female. This coding is attractive because it is self-documenting. Missing data are data that are unobserved and you don't know the mechanism. You should code missing values as NA. Censored data are data where you know the missingness mechanism on some level. Common examples are a measurement being below a detection limit or a patient being lost to follow-up. They should also be coded as NA when you don't have the data. But you should also add a new column to your tidy data called, "VariableNameCensored" which should have values of TRUE if censored and FALSE if not. In the code book you should explain why those values are missing. It is absolutely critical to report to the analyst if there is a reason you know about that some of the data are missing. You should also not impute/make up/ throw away missing observations.

In general, try to avoid coding categorical or ordinal variables as numbers. When you enter the value for sex in the tidy data, it should be "male" or "female". The ordinal values in the data set should be "poor", "fair", and "good" not 1, 2 ,3. This will avoid potential mixups about which direction effects go and will help identify coding errors.

Always encode every piece of information about your observations using text. For example, if you are storing data in Excel and use a form of colored text or cell background formatting to indicate information about an observation ("red variable entries were observed in experiment 1.") then this information will not be exported (and will be lost!) when the data is exported as raw text. Every piece of data should be encoded as actual text that can be exported.

The instruction list/script
You may have heard this before, but reproducibility is a big deal in computational science. That means, when you submit your paper, the reviewers and the rest of the world should be able to exactly replicate the analyses from raw data all the way to final results. If you are trying to be efficient, you will likely perform some summarization/data analysis steps before the data can be considered tidy.

The ideal thing for you to do when performing summarization is to create a computer script (in R, Python, or something else) that takes the raw data as input and produces the tidy data you are sharing as output. You can try running your script a couple of times and see if the code produces the same output.

In many cases, the person who collected the data has incentive to make it tidy for a statistician to speed the process of collaboration. They may not know how to code in a scripting language. In that case, what you should provide the statistician is something called pseudocode. It should look something like:

Step 1 - take the raw file, run version 3.1.2 of summarize software with parameters a=1, b=2, c=3
Step 2 - run the software separately for each sample
Step 3 - take column three of outputfile.out for each sample and that is the corresponding row in the output data set
You should also include information about which system (Mac/Windows/Linux) you used the software on and whether you tried it more than once to confirm it gave the same results. Ideally, you will run this by a fellow student/labmate to confirm that they can obtain the same output file you did.

What you should expect from the analyst
When you turn over a properly tidied data set it dramatically decreases the workload on the statistician. So hopefully they will get back to you much sooner. But most careful statisticians will check your recipe, ask questions about steps you performed, and try to confirm that they can obtain the same tidy data that you did with, at minimum, spot checks.

You should then expect from the statistician:

An analysis script that performs each of the analyses (not just instructions)
The exact computer code they used to run the analysis
All output files/figures they generated.
This is the information you will use in the supplement to establish reproducibility and precision of your results. Each of the steps in the analysis should be clearly explained and you should ask questions when you don't understand what the analyst did. It is the responsibility of both the statistician and the scientist to understand the statistical analysis. You may not be able to perform the exact analyses without the statistician's code, but you should be able to explain why the statistician performed each step to a labmate/your principal investigator.

Contributors
Jeff Leek - Wrote the initial version.
L. Collado-Torres - Fixed typos, added links.
Nick Reich - Added tips on storing data as text.
Nick Horton - Minor wording suggestions.

---
## [kehull/VBA-challenge](https://github.com/kehull/VBA-challenge)@[f7943ee41e...](https://github.com/kehull/VBA-challenge/commit/f7943ee41e4522313ed6e197e074550ff07a7f3c)
#### Saturday 2020-11-14 04:48:32 by kehull

Add files via upload

This assignment was quite a challenge! I ended up talking to my tutor to figure out why my code wouldn't loop through all the changes- it turns out it was a scope issue, and I just needed to set a worksheet variable and deploy it on my Cells, Range, and Columns properties to get the code to iterate through all worksheets, even though the rest of the For Each loop to do so was already in place. In all honesty, I don't understand why the worksheets variable has to be attached to these properties, but I do understand that it needs to be there. In my research, I stumbled over the concept of Offset and CurrentRegion properties, which I wish I'd known before I started writing the code as it would have prevented some of my simple formatting errors early on in the project.

Perhaps the biggest idiosyncrasy of this project is found in lines 53-55. I know there's a way to retrieve the value I was seeking by using the If Cells(x,y) <> Cells(x,y+1) sort of thing, but for the life of me i couldn't get it to work on the first line of code in the project. When I tried to use my Google skills to find a solution, I stumbled over a few sites where former students of this class had uploaded the entire assignment to a help site and asked for input. I stayed clear of these pages, but had a hard time finding a solution anywhere else, so I rubbed all six of my brain cells together until I came up with the concept of the "nope"/"yup" checker. I'm certain I'm neither the first nor last person to use this method, so I make no claims of discovery, but I'm proud of myself for coming to that solution on my own.

Another topic I discussed with my tutor was how to use GitHub at all, as this was my first experience doing so. Since I didn't talk to him until my homework was basically finished, I only ended up doing two commits for this project. In the future, I'll try to do more commits on my projects.

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[d9dd626beb...](https://github.com/mrakgr/The-Spiral-Language/commit/d9dd626bebbb0b0fddf71b455ec6f98b3ea823a0)
#### Saturday 2020-11-14 11:40:21 by Marko Grdinić

"8:15am. I keep looking for an answer to my past life, but maybe the fact that I've made my determination to do programming is answer enough. There is no way of going any higher than this. Having a purpose and chasing after it, as opposed to the old me who was only living to buy time in order to find such a purpose would have made things go a lot differently.

There was no point in dragging my loserdom past 2014.

In the past, it was that when I was playing a game that I was great.

The only thing that has changed since then is that the way to my personal greatness is to do programming.

While I am in that state I am great. When I am trying to solve problems and surmount my obstacles, I am great. When I am resting and lounging around, not so.

8:20am. If the past me was great at anything, it would have been how determined I was to figure out how to break this prison called reality. Even if the omnipotence itself is the highest purpose, just that understanding is not anything to go by. Finding a connector - the way to get closer to such a power is something that would have been a great achievement.

And I've managed to find it. I could not sell such an idea even for a cent in this clown world, but in a sane world this would have brought me wide acclaim.

It is such a demonic method. It really has opened my eyes to what the real world is about.

8:25am. I won't pity myself. The fact that I am willing to work is something that gives me a lot of leverage.

The reality is that nobody can help me with this, and a lot of people can do the opposite, but I won't bemoan that fact.

8:30am. I figured out the self improvement loop, but I need an even better method to make the last step. This last step is not something I'll be able to get by navel gazing like before. The machines themselves will give me an answer. My own skill at programming will give me an answer.

I cannot win at life without their help. A programmer is nothing without his computer.

8:35am. Ultimately, necromutation might be transcendence, but it is not a big deal. The really fun parts lie in the future. There will be things I cannot even imagine right now. I anticipate that the future methods will be even more demonic at their core. Suicidal self iteration is just the start, not the end of things.

8:35am. Let me slack just for a little and then I will start. Today I will finally deal with the multi file server.

8:40am. Let me start.

Whatever the future holds, I need to get through this file server in order to get to it.

In order to get to the point where it makes sense to ask whether I am my own programs, I need to past the current obstacles. Right now, I am just way too green to even say this out loud and not make it look laughable. The devil is in the details and I need to find it.

8:50am. Focus me, close the browser, focus on the diff.

9:55am.

```fs
let union a b = a >>=* fun a -> b >>- fun b -> Infer.union a b
let multi_file package_id top_env =
    let rec create streams files' =
        {new MultiFileStream with
            member _.Run(results_parser,files) =
                let rec changed module_id results_infer streams (top_env : _ Promise) = function
                    | File(uri,name) ->
                        let r,tc = (typechecker package_id module_id top_env).Run(results_parser.[uri])
                        let top_env_additions = Stream.foldFromFun top_env_empty (fun a (b : InferResult) -> Infer.union a b.top_env_additions) r >>-* Infer.in_module name
                        {module_id=module_id+1; results_infer=Map.add uri r results_infer; top_env_additions=top_env_additions; streams=Map.add uri tc streams}
                    | Directory(name,l) ->
                        let state = {module_id=module_id; results_infer=results_infer; top_env_additions=Promise top_env_empty; streams=streams}
                        let r = changed_list (state, top_env) l
                        {r with top_env_additions=r.top_env_additions >>-* Infer.in_module name}
                and changed_list s l =
                    List.fold (fun (r : MultiFileLoopState, top_env) x ->
                        let r' = changed r.module_id r.results_infer r.streams top_env x
                        {r' with top_env_additions=union r'.top_env_additions r.top_env_additions}, union r'.top_env_additions top_env
                        ) s l |> fst

                let state = {module_id=0; results_infer=Map.empty; top_env_additions=Promise top_env_empty; streams=streams}
                let r = changed_list (state,top_env) files
                r.results_infer, r.top_env_additions, create r.streams files
                }
    create Map.empty []
```

I am not making any progress on this at all. I am supposed to do the diff here, but I am unwiling to wreck the current structure. I know that if I do that, things will get complicated to an uncomfortable degree.

10:20am. Ok, after thinking about it for 1.5 hours, I think I have something that satisfies me. Let me take a break.

10:45am. Let me finish the chapter of Hero's Daughter and then I am going to resume. I'll go with the idea I have in mind right now. It is quite simple and will allow me to work around the concerns of reusing code.

10:50am. Ok, focus. Let me do this thing.

Once I separate things out properly in my mind, the task at hand should not be too difficult.

```fs
type MultiFileInState = {
    module_id : int
    results_infer : Map<string, InferResult Stream>
    top_env : TopEnv Promise
    streams : Map<string, TypecheckerStream>
    }
type MultiFileOutState = {
    module_id : int
    results_infer : Map<string, InferResult Stream>
    top_env_additions : TopEnv Promise
    streams : Map<string, TypecheckerStream>
    }
```

Let me make these two. And then...

```fs
type MultiFileInState = {
    module_id : int
    results_infer : Map<string, InferResult Stream>
    top_env : TopEnv Promise
    streams : Map<string, TypecheckerStream>
    }
type MultiFileOutState = {
    module_id : int
    results_infer : Map<string, InferResult Stream>
    top_env_additions : TopEnv Promise
    streams : Map<string, TypecheckerStream>
    }
type MultiFileState = MultiFileInState * MultiFileOutState
type FileHierarchy =
    | File of uri: string * name: string * meta: MultiFileState option
    | Directory of name: string * FileHierarchy list * meta: MultiFileState option
```

Here is where I am going with this. Now let me redo the function again. As for diffing, I'll be able to do it separately from the rest.

11:10am.

```fs
let file_hierarchy_meta = function File(_,_,meta) | Directory(_,_,meta) -> meta
let multi_file package_id top_env'' =
    let rec create streams'' files' =
        {new MultiFileStream with
            member _.Run(results_parser,files) =
                let rec changed (i : MultiFileInState) x =
                    match file_hierarchy_meta x with
                    | Some(i,o) -> x,o
                    | None ->
                        match x with
                        | File(uri,name,_) ->
                            let r,tc = (typechecker package_id i.module_id i.top_env).Run(results_parser.[uri])
                            let top_env_additions = Stream.foldFromFun top_env_empty (fun a (b : InferResult) -> Infer.union a b.top_env_additions) r >>-* Infer.in_module name
                            let o = {module_id=i.module_id+1; results_infer=Map.add uri r i.results_infer; top_env_additions=top_env_additions; streams=Map.add uri tc i.streams}
                            File(uri,name,Some(i,o)),o
                        | Directory(name,l,_) ->
                            let l,o = changed_list i l
                            let o = {o with top_env_additions=o.top_env_additions >>-* Infer.in_module name}
                            Directory(name,l,Some(i,o)),o
```

This feels really good. unlike what I had before, this form has a lot of potential for reuse.

Let me do `changed_list`.

11:35am.

```fs
let multi_file package_id top_env =
    let rec create streams files' =
        {new MultiFileStream with
            member _.Run(results_parser,files) =
                let rec changed (i : MultiFileInState) x =
                    match file_hierarchy_meta x with
                    | Some(i,o) -> x,o
                    | None ->
                        match x with
                        | File(uri,name,_) ->
                            let r,tc = (typechecker package_id i.module_id i.top_env).Run(results_parser.[uri])
                            let top_env_additions = Stream.foldFromFun top_env_empty (fun a (b : InferResult) -> Infer.union a b.top_env_additions) r >>-* Infer.in_module name
                            let o = {module_id=i.module_id+1; results_infer=Map.add uri r i.results_infer; top_env_additions=top_env_additions; streams=Map.add uri tc i.streams}
                            File(uri,name,Some(i,o)),o
                        | Directory(name,l,_) ->
                            let l,o = changed_list i l
                            let o : MultiFileOutState = {o with top_env_additions=o.top_env_additions >>-* Infer.in_module name}
                            Directory(name,l,Some(i,o)),o
                and changed_list i l =
                    let o = {module_id=i.module_id; results_infer=i.results_infer; top_env_additions=Promise(top_env_empty); streams=i.streams}
                    let l,(_,o) =
                        List.mapFold (fun (top_env, o : MultiFileOutState) x ->
                            let i = {module_id=o.module_id; results_infer=o.results_infer; streams=o.streams; top_env=top_env}
                            let x,o' = changed i x
                            let top_env = union o'.top_env_additions top_env
                            let o = {o with top_env_additions=union o'.top_env_additions o.top_env_additions}
                            x, (top_env, o)
                            ) (i.top_env,o) l
                    l,o
                let i = {module_id=0; results_infer=Map.empty; top_env=top_env; streams=streams}
                let l,o = changed_list i (diff files' files) // TODO: Implement diff
                o.results_infer, o.top_env_additions, create i.streams l
                }
    create Map.empty []
```

This is really good.

12:25pm. I've been thinking all this time. Let me stop here.

I think I know how to do it.

```fs
let typechecker package_id module_id top_env =
    let rec run old_results env i (bs : Bundle list) =
        match bs with
        | b :: bs ->
            match PersistentVector.tryNth i old_results with
            | Some (b', _, env as s) when b = b' -> Cons(s,Promise(run old_results env (i+1) bs))
            | _ ->
                let x = Infer.infer package_id module_id env (bundle_top b)
                let _,_,env as s = b,x,Infer.union x.top_env_additions env
                Cons(s,promise_thunk (fun () -> run old_results env (i+1) bs))
        | [] -> Nil
    let rec loop old_results =
        {new TypecheckerStream with
            member _.Run(res) =
                let r =
                    res >>=* fun res ->
                    // Doing the memoization like this has the disadvantage of always focing the evaluation of the previous
                    // streams' first item, but on the plus side will reuse old state.
                    old_results >>- fun old_results ->
                    run (cons_fulfilled old_results) top_env 0 res.bundles
                let a = Stream.mapFun (fun (_,x,_) -> x) r
                a, loop r
            }
    loop Stream.nil
```

Let me go back to the previous version of the TC. I won't be passing in old parser states after all. I want to do this so I do not forget it.

```fs
type FileHierarchy =
    | File of uri: string * name: string * meta: MultiFileState option
    | Directory of name: string * FileHierarchy list * meta: MultiFileState option
```

As it turns out this was a good first step, but I am going to extend the parser state so...hell, let me do it now.

```fs
type FileHierarchy =
    | File of uri: string * name: string * (MultiFileInState * MultiFileOutState) option * ParserRes Promise * TypecheckerStream option
    | Directory of name: string * FileHierarchy list
```

Let me do it like this.

I'll also allow two kinds of inputs - order changes and file changes. Trying to fill the hole with reference equality over parser results is too hard.

...Actually, now the idea comes to me how it could be possible, but having two kinds of inputs is still better...

But doing it all at once would be more roboust.

Maybe I'll do it all at once...

12:40pm. Let me stop here. I should eat instead of trying to burn a hole through the fabric of reality with my mind."

---
## [peternowee/pydot](https://github.com/peternowee/pydot)@[682edb7be6...](https://github.com/peternowee/pydot/commit/682edb7be6d7d82bc1addbab901cce98c97ef7aa)
#### Saturday 2020-11-14 11:49:50 by Peter Nowee

Add ppc64le and bump Python versions in Travis CI

Considering that:

- IBM, through its supplier Datamatics, requested many Python projects
  to [add the `ppc64le` (Power) architecture][1] to their Travis CI
  test matrices. Related links:
  - ibm.com: [Build your open source projects on IBM Power Systems with
    Travis CI][2].
  - youtube.com: [OpenPOWER Summit NA 2019: Developing Applications
    using Acceleration Hardware on Power][3].
  - Gerrit Huizenga of IBM (@gerrith3 on GitHub) provided more details
    on the [motives and open source contributions of IBM][4] in answer
    to questions from Ludovic Fernandez (@ldez) of nrdcg/goinwx.
- The request to add `ppc64` testing was also made to pydot in PR
  [pydot/pydot#243][5] by @asellappen of Datamatics. The proposed
  implementation extended the test job configuration by [matrix
  expansion][6].
- [Matrix expansion][7] means adding a new dimension to the build
  configuration (e.g. `arch`, `os`, `env`), after which the number of
  test jobs is automatically multiplied by the number of choices given
  in the newly added dimension. In this case, it would add a dimension
  of two architectures, expanding the matrix of test jobs by a factor
  of 2: 5 Python versions (at that time) * 2 architectures = 10 test
  runs of 2 to 3 minutes each.
- Gerrit Huizenga of IBM said he thinks the adding of `ppc64le` test
  jobs to Travis will not lead to additional billing from Travis CI to
  the OSS community, because [he provides Travis CI with the Power
  hardware][8].
- The [Travis CI documentation on CPU architectures][9] currently says
  IBM architecture support is in beta stage and only available for Open
  Source repositories.
- There are many abstraction layers between pydot and the hardware
  architecture, including other libraries, Python and the operating
  system. Pydot itself does not need to be compiled for a specific
  architecture. In a perfect world, pydot is architecture-independent
  and testing pydot on a single architecture should be sufficient.
- However, the world is not perfect and [abstraction layers can be
  leaky][10]. According to Gerrit Huizenga of IBM, [corner cases do
  come up][11], which is why they do so much testing up front and
  provide free access to testing.
- Still, even if an automated testing service is provided for free, it
  has an [environmental footprint][12], so there are limits to how much
  we should make use of that as well.
- Travis CI also requests open-source users to remember that Travis CI
  provides its service free of charge to the community and only specify
  the matrix we [actually need][13].
- Even if we ever find an architecture-specific bug in pydot, it seems
  highly likely that it will come out on several Python versions for
  that architecture.
- All of this was further discussed between pydot maintainers Sebastian
  Kalinowski (@prmtl) and Peter Nowee (@peternowee) and Gerrit Huizenga
  of IBM in PR [pydot/pydot#245][14].

The following was decided:

- Baseline testing is kept at `amd64`, with testing of the lowest and
  the highest Python versions supported by the Python community, as
  well as the lowest end-of-life (EOL) Python version that we still
  choose to support pydot on. Currently, that means Python 3.6, 3.9 and
  2.7 respectively.
- For other architectures, such as `ppc64le`, we do limited testing to
  catch reasonably expectable corner cases and validate architecture
  independence in a resource conserving way. For now, we start with
  Python 2.7 and 3.9.

We can always reconsider this policy depending on actual findings.

[1]: https://github.com/pulls?q=author%3Aasellappen+ppc64le+travis
[2]: https://developer.ibm.com/tutorials/travis-ci-on-power/
[3]: https://www.youtube.com/watch?v=qPZtf3hoMSs
[4]: https://github.com/nrdcg/goinwx/pull/5
[5]: https://github.com/pydot/pydot/pull/243
[6]: https://docs.travis-ci.com/user/build-matrix/#matrix-expansion
[7]: https://config.travis-ci.com/matrix_expansion
[8]: https://github.com/nrdcg/goinwx/pull/5#issuecomment-721422838
[9]: https://docs.travis-ci.com/user/multi-cpu-architectures/
[10]: https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/
[11]: https://github.com/nrdcg/goinwx/pull/5#issuecomment-719084047
[12]: https://en.wikipedia.org/wiki/Data_center#Energy_use
[13]: https://docs.travis-ci.com/user/build-matrix/#listing-individual-jobs
[14]: https://github.com/pydot/pydot/pull/245

---
## [ProjectIgnis/BabelCDB](https://github.com/ProjectIgnis/BabelCDB)@[dbdc2206bd...](https://github.com/ProjectIgnis/BabelCDB/commit/dbdc2206bdfb5dfcd05b8a6c866f9b0593ea151f)
#### Saturday 2020-11-14 15:23:55 by ClementLouis

added new cards

Selection 10
- Radiant Seiyaryu
- Holy Night Rayel
- Holy Night Astel
- Holy Night Flamel
- Holy Night Bellfire
- Holy Night Nativity
- Holy Night Advent
- Holy Night Blast

Prismatic God Box
- Thunderforce Attack
- Fist of Fate
- Exchanging Souls
- Golden Idol

Lightning Overdrive
- ZW - Pegasus Twin Saber
- ZS - Arms Sage
- ZS - Ascend Sage
- ZW - Dragonic Halberd
- Rank-Up-Magic - Zexal Force
- Zexal Construction
- Zexal Entrust
- Zexal Alliance

---
## [TLWNL/ActualSP](https://github.com/TLWNL/ActualSP)@[2dc16ed8cd...](https://github.com/TLWNL/ActualSP/commit/2dc16ed8cd5c6beef18f3459b91dba68c92918b8)
#### Saturday 2020-11-14 15:57:00 by tlwnl

Receiving pub message kinda works, still wonky due to non implementation of the fucking notification function

---
## [newstools/2020-the-guardian-uk](https://github.com/newstools/2020-the-guardian-uk)@[46e9adb660...](https://github.com/newstools/2020-the-guardian-uk/commit/46e9adb6601f5f66b1eae28da23f08f2c6a1bbbf)
#### Saturday 2020-11-14 18:22:56 by NewsTools

Created Text For URL [www.theguardian.com/music/2020/nov/14/james-blunt-life-on-a-plate-body-temple-painful-experiences]

---
## [AngryJKirk/twitch_steam_registration](https://github.com/AngryJKirk/twitch_steam_registration)@[17252d04bc...](https://github.com/AngryJKirk/twitch_steam_registration/commit/17252d04bc154d0d3c7875faf096169f1b3b45df)
#### Saturday 2020-11-14 19:50:14 by Yaroslav

removed secrets from code (I have changed them, so fuck you HACKERS)

---
## [DavidColson/Athena](https://github.com/DavidColson/Athena)@[3e75ba8e82...](https://github.com/DavidColson/Athena/commit/3e75ba8e820eb57cb561b12ddbb72367b7ca62ca)
#### Saturday 2020-11-14 22:00:54 by David Colson

Polylines kinda working? Ends are a bit fucky, but we'll fix that tomorrow

---
## [ferba93/NotThatGameEngine](https://github.com/ferba93/NotThatGameEngine)@[8574c236c7...](https://github.com/ferba93/NotThatGameEngine/commit/8574c236c7ce4674c0edf51a54074da82310393a)
#### Saturday 2020-11-14 22:16:55 by Ferran-Roger Basart i Bosch

Okay, I think I know how to approach JSON. Also, Craftsdwarf talking about his tirade on Fairy Tail and his anime is unrealistic video broke my fucking mind and now I can't stop myself from thinking about art and the meaning of it all, and I better go play some fighters and go rest, because I CAN'T enter this cycle of overthinking shit while doing Engine. I just can't

---

# [<](2020-11-13.md) 2020-11-14 [>](2020-11-15.md)

