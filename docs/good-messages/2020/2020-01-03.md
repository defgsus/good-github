# [<](2020-01-02.md) 2020-01-03 [>](2020-01-04.md)

1,699,764 events, 879,764 push events, 1,357,764 commit messages, 97,183,835 characters


## [jkunimune15/world-hammer@92eadd18e4...](https://github.com/jkunimune15/world-hammer/commit/92eadd18e44e80aca943b7754cfa70a9a08ceaf9)
##### 2020-01-03 02:04:55 by Justin Kunimune

_Actually_, that was awful

Yeah the new queuing function was making my continents boring and weird looking, I'm sad to say. Whatever. This terrain looks great.

---
## [Nicklas373/kernel_xiaomi_lavender@8ce9202a5c...](https://github.com/Nicklas373/kernel_xiaomi_lavender/commit/8ce9202a5ccf47ad460f86757a96cf0ba44d033f)
##### 2020-01-03 02:58:47 by George Spelvin

lib/sort: make swap functions more generic

Patch series "lib/sort & lib/list_sort: faster and smaller", v2.

Because CONFIG_RETPOLINE has made indirect calls much more expensive, I
thought I'd try to reduce the number made by the library sort functions.

The first three patches apply to lib/sort.c.

Patch #1 is a simple optimization.  The built-in swap has special cases
for aligned 4- and 8-byte objects.  But those are almost never used;
most calls to sort() work on larger structures, which fall back to the
byte-at-a-time loop.  This generalizes them to aligned *multiples* of 4
and 8 bytes.  (If nothing else, it saves an awful lot of energy by not
thrashing the store buffers as much.)

Patch #2 grabs a juicy piece of low-hanging fruit.  I agree that nice
simple solid heapsort is preferable to more complex algorithms (sorry,
Andrey), but it's possible to implement heapsort with far fewer
comparisons (50% asymptotically, 25-40% reduction for realistic sizes)
than the way it's been done up to now.  And with some care, the code
ends up smaller, as well.  This is the "big win" patch.

Patch #3 adds the same sort of indirect call bypass that has been added
to the net code of late.  The great majority of the callers use the
builtin swap functions, so replace the indirect call to sort_func with a
(highly preditable) series of if() statements.  Rather surprisingly,
this decreased code size, as the swap functions were inlined and their
prologue & epilogue code eliminated.

lib/list_sort.c is a bit trickier, as merge sort is already close to
optimal, and we don't want to introduce triumphs of theory over
practicality like the Ford-Johnson merge-insertion sort.

Patch #4, without changing the algorithm, chops 32% off the code size
and removes the part[MAX_LIST_LENGTH+1] pointer array (and the
corresponding upper limit on efficiently sortable input size).

Patch #5 improves the algorithm.  The previous code is already optimal
for power-of-two (or slightly smaller) size inputs, but when the input
size is just over a power of 2, there's a very unbalanced final merge.

There are, in the literature, several algorithms which solve this, but
they all depend on the "breadth-first" merge order which was replaced by
commit 835cc0c8477f with a more cache-friendly "depth-first" order.
Some hard thinking came up with a depth-first algorithm which defers
merges as little as possible while avoiding bad merges.  This saves
0.2*n compares, averaged over all sizes.

The code size increase is minimal (64 bytes on x86-64, reducing the net
savings to 26%), but the comments expanded significantly to document the
clever algorithm.

TESTING NOTES: I have some ugly user-space benchmarking code which I
used for testing before moving this code into the kernel.  Shout if you
want a copy.

I'm running this code right now, with CONFIG_TEST_SORT and
CONFIG_TEST_LIST_SORT, but I confess I haven't rebooted since the last
round of minor edits to quell checkpatch.  I figure there will be at
least one round of comments and final testing.

This patch (of 5):

Rather than having special-case swap functions for 4- and 8-byte
objects, special-case aligned multiples of 4 or 8 bytes.  This speeds up
most users of sort() by avoiding fallback to the byte copy loop.

Despite what ca96ab859ab4 ("lib/sort: Add 64 bit swap function") claims,
very few users of sort() sort pointers (or pointer-sized objects); most
sort structures containing at least two words.  (E.g.
drivers/acpi/fan.c:acpi_fan_get_fps() sorts an array of 40-byte struct
acpi_fan_fps.)

The functions also got renamed to reflect the fact that they support
multiple words.  In the great tradition of bikeshedding, the names were
by far the most contentious issue during review of this patch series.

x86-64 code size 872 -> 886 bytes (+14)

With feedback from Andy Shevchenko, Rasmus Villemoes and Geert
Uytterhoeven.

Link: http://lkml.kernel.org/r/f24f932df3a7fa1973c1084154f1cea596bcf341.1552704200.git.lkml@sdf.org
Signed-off-by: George Spelvin <lkml@sdf.org>
Acked-by: Andrey Abramov <st5pub@yandex.ru>
Acked-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Reviewed-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Cc: Geert Uytterhoeven <geert@linux-m68k.org>
Cc: Daniel Wagner <daniel.wagner@siemens.com>
Cc: Don Mullis <don.mullis@gmail.com>
Cc: Dave Chinner <dchinner@redhat.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Dede Dindin Qudsy <xtrymind@gmail.com>

---
## [ShayanJavadi/tonal@aff7db5dee...](https://github.com/ShayanJavadi/tonal/commit/aff7db5deeccf9e10b72ea6f05ce7c56a7b7a04d)
##### 2020-01-03 05:13:05 by Shayan Javadi

Added showcase section in readme

Hi!

First thank you so much for creating Tonal. It's been an absolute lifesaver for my app.

I'm making this PR to create a showcase section under the project's README. 

I'd be absolutely honored to have my app Solfej showcased on there.

Solfej would quite frankly not be possible without Tonal. There's some very complex music theory stuff going on behind the scenes throughout the ~80 lessons/exercises, and Tonal plays a key part in all of them.

A bit about the app:

Solfej is a music theory and ear training app (iOS, Android, Web).

As a self taught musician and guitarist I've always struggled with music theory. This has been due to the lack of true user friendly resources (the amount of nights I've spent down youtube/wikipedia rabbit holes...)

In the last couple of months I've set out to fix this gap for myself and everyone by creating Solfej.

The iOS version of the app is in beta with ~1000 users, and the android and web versions are being released soon.

Website for Solfej: https://www.solfej.io/
Source code for Solfej: https://github.com/ShayanJavadi/solfej

---
## [JappaWakka/PkMnExp3d-Quartz@8baa79b140...](https://github.com/JappaWakka/PkMnExp3d-Quartz/commit/8baa79b140d750529018654fd70132d8062ef5b8)
##### 2020-01-03 11:52:12 by JappaWakka

L O D S of F E T U R S! Whas da spell? LOADS OF FEATURES! probably

- Updated the music for the Breeze Town map
- Added two variations of the MainFont: MainFontWhite and MainFontBlack.
- Started working on replacing the hideous and out of place MiniFont with either of the MainFont variations.
- Removed the font Ingame (might remove chatFont too later)
- Brought back the old (and in my opinion better) Main Menu, Bag, Game Menu, Pokémon Party and Pokémon Summary interfaces. I still need to find a way to add the bag interface to Battle menus in some way, as well as some other interfaces. But I'm getting there!
- Renamed files containing "PokéGear" (or pokegear. or Pokegear, or whatever) to contain the word "Phone" instead so it's not limited to Johto's phone thingamagic.
- Added desert/sand route graphics to the map_objects texture file, though they're not implemented yet.
- Reskinned the Menu interface textures to look more GBA styled. Also added Bag textures to that texture file.
- Added the Pokémon Quartz Party icons to the game, I modified a couple of sprites already, mainly recolors and small edits. I haven't made shiny alternatives yet, except for Sever and Squirritti.
- Added a placeholder sprite for the item Shoal Salt/Shell Dust.
- Added all item names to the English localization file, as well as some other data.
- Added/edited Pokémon data for Firegg, Aquegg and Squirritti.
- Edited every vanilla Pokémon follower sprite to have 4 frames of animation.
- Renamed NPC overworld sprite 63.png to GateKeeper.png
- Removed NPC overworld sprite 68.png because it was a duplicate of Gentleman.png
- Put texture Apricorn into the TextureSheets folder and made it possible for the sprites to have any aspect ratio.
- Changed the function of TextureSheet FloorOutside.png from grass variations to different types of backdrops (Snow, Grass, Sand and Rock)
- IMPORTANT! You can dynamically switch pronouns by adding <he> or <He>, <his> or <His>, <ofhis> or <ofHis> (for the "other" gender: theirs/Theirs), <him> or <Him> to your text messages and the game will replace it with the correct pronoun based on the player's gender.
- Fixed a bug where you couldn't start a new game using a GameMode that uses the 3D intro instead of the 2D intro.

---
## [mrakgr/The-Spiral-Language@fdcc4607de...](https://github.com/mrakgr/The-Spiral-Language/commit/fdcc4607def5e00d8138ec077121b721cd5eebb2)
##### 2020-01-03 13:10:52 by Marko Grdinić

"9:50am. Ah, I did not notice the random being in quotes. So that is exactly what he meant then.

```
let arr_cons next = sepBy1 next arr_fun |>> function [x] -> x | x -> pair_from_end RawTConstraint x
```

Ok, so I am going to make the constraints exactly this.

Let me also do dependent constraints.

10am.

```
let arr_depcon next = next .>>. (opt (arr_depcon >>. next)) |>> function (a,Some b) -> RawTDepConstraint(a,b) | (a,None) -> a
```

Let me do the forall next.

```
let arr_depcon next = pipe2 next (opt (arr_depcon >>. next)) (function a (Some b) -> RawTDepConstraint(a,b) | a None -> a)
```

Hmmm, actually this might be the first time ever that I've used this `function` pattern in F#. Interesting that it works.

...I think it should work in Spiral.

But if I go with the current union type syntax, it will stop working.

Well, I can make it work on the outer level, no problem.

```
let arr_depcon next = pipe2 next (opt (arr_depcon >>. next)) (function a (Some b) -> RawTDepConstraint(a,b) | a None -> a)
```

Ah, actually this does not work in F#. Nevermind.

10:15am.

```
let forall next = pipe2 (forall >>. many1 next .>> dot) next (fun l x -> RawTForall(List.toArray l,x))
```

What I need next are records and keywords. Then I will tie all of this together.

10:30am.

```
    let forall next (d : ParserEnv) =
        if d.typeforall_allowed then pipe2 (forall >>. many1 next .>> dot) next (fun l x -> RawTForall(List.toArray l,x)) d
        else d.FailWith ForallNotAllowed
    let record next = curlies (many ((var' .>> colon) >>. next)) |>> (Map.ofList >> RawTRecord)
    let keyword next =
        (keyword_unary |>> fun x -> RawTKeyword(fst x,[||]))
        <|> (many (keyword .>>. next) |>> fun l -> let a,b = List.unzip l in RawTKeyword(keyword_concat a, List.toArray b))
```

I need to bring in keyword_concat. As expected, it is really annoying to have to handle positional information explicitly, but I am going to have to deal with it.

```
    let arr_funs next = sepBy1 next arr_fun |>> function [x] -> x | x -> pair_from_end RawTFun x
    let arr_cons next = sepBy1 next arr_cons |>> function [x] -> x | x -> pair_from_end RawTConstraint x
```

Also since these two have the same rightwards precedence, and I am not parsing them using the precedence parser, I am going to have to turn them into one.

10:45am.

```
    let arr_funs_cons next =
        pipe2 next (many (((arr_fun >>% Funs) <|> (arr_cons >>% Cons)) .>>. next))
            (fun a l ->
                let rec loop a = function
                    | [] -> a
                    | (Funs, b) :: l -> RawTFun(a,loop b l)
                    | (Cons, b) :: l -> RawTConstraint(a,loop b l)
                loop a l)
```

10:55am. Ok, I took a little break. Let me get back on track.

`keyword_concat`. That is what I need.

11:15am.

```
    let keyword next =
        (keyword_unary |>> fun x -> RawTKeyword(fst x,[||]))
        <|> (many (keyword .>>. next) |>> (concat_keyword'' (fun _ t -> t) >> fun (a, b) -> RawTKeyword(a, List.toArray b)))
```

Ok, enough. Let me stop here for a while. I am already wasting time on social media as it is.

```
type FunsOrCons = Funs | Cons

let type_ (d : ParserEnv) =
    let unit_ = bracket_round_open >>. bracket_round_close >>% RawTUnit
    let pairs next = sepBy1 next product |>> function [x] -> x | x -> pair_from_end RawTPair x
    let arr_funs_cons next =
        pipe2 next (many (((arr_fun >>% Funs) <|> (arr_cons >>% Cons)) .>>. next))
            (fun a l ->
                let rec loop a = function
                    | [] -> a
                    | (Funs, b) :: l -> RawTFun(a,loop b l)
                    | (Cons, b) :: l -> RawTConstraint(a,loop b l)
                loop a l)
    let arr_depcon next = pipe2 next (opt (arr_depcon >>. next)) (fun a -> function Some b -> RawTDepConstraint(a,b) | None -> a)
    let forall next (d : ParserEnv) =
        if d.typeforall_allowed then pipe2 (forall >>. many1 next .>> dot) next (fun l x -> RawTForall(List.toArray l,x)) d
        else d.FailWith ForallNotAllowed
    let record next = curlies (many ((var' .>> colon) >>. next)) |>> (Map.ofList >> RawTRecord)
    let keyword next =
        (keyword_unary |>> fun x -> RawTKeyword(fst x,[||]))
        <|> (many (keyword .>>. next) |>> (concat_keyword'' (fun _ t -> t) >> fun (a, b) -> RawTKeyword(a, List.toArray b)))

    ()
```

Here is the whole thing as it is.

11:55am. Did, I forget to commit. At any rate, let me deal with those replies, preferably swiftly, then I'll do the chores and slack a little.

Despite it being my mission, I have little motivation to actually do the parser itself, which is a problem. I am going to have to coax and cajole myself until I get through it.

It is easy to decide to do something big, the hard part is following through with it.

1:15pm. "My frustration is that mainstream mathematics is really not giving me what I want. In neural nets for example, there have been efforts to go from 32-bit floats, to 16 and even binarize them. A year or two ago when I last looked into this, this has not been successful.

When you learn programming you very quickly learn about 32-bit, 16-bit, 8-bit and so on integers. They have a range, they have a capacity. The same goes for floats. You can only once a fixed amount of information in a data structure.

Neural nets themselves have a certain capacity, yet today what that capacity is and how to characterize it is a complete mystery. In my eyes it would be a huge accomplishment if somebody could unravel that, along with what exactly their optimization is doing.

With that idea, I step in, learn formal theorem proving and what do I find now that I actually know what proofs are? These novels kinds of 'proofs by divergence' apparently.

I mean how could anybody trust mathematicians from my vantage point? How could people who cannot give a proper treatment of basic numbers be expected to come up with the next great breakthroughs in AI? Why did I even respect them for one moment? I read many papers in ML where I had to skip the proof parts because they were too complex for me at the time, and now that I actually have some skill in theorem proving, I come to these conclusions.

I am beyond pissed off - certainly not all of the fields of mathematics suffer from this affliction (Wildberger gives a list in one of the videos), but certainly all the fields of ML that I am actually interested in are.

What is even the point of studying the theoretical work that went into them?

These questions are mostly rhetorical. I do not think I'll be able to resolve this inner conflict, so I do really want to go back to the things I can actually reason about and trust in my own way. I want to clean up the remains of my earlier work and wrap up the low-style before I do anything else.

Let me just say one more thing - so far I think your arguments have been quite level headed (unlike my own.) You didn't say anything wrong, but as a matter of attitude, I do not think these kinds of arguments will do anything to alleviate any of my concerns. Set theory, reals, current strands of thought are a Gordian knot problem that I have no idea where to begin cutting.

The measured, level headed attitude you are displaying in this debate is not going to be a catalyst for doing the work that needs to be done. And unfortunately, you are definitely channeling the zeitgeist of the community at large.

Am I wrong to feel that this work should be done by pro mathematicians rather than semi-anonymous randos such as myself?"

That one guy gave up and essentially just called me a loony, and I probably am being one right now. I certainly feel like that. This other guy still wants to keep going. I am not sure if /u/be42rin deserves this treatment, but he made an attempt and so I will tell him what is on my mind.

I wanted to be swift, but I am mad. Mad at math.

1:20pm. It is not really a stable state. I look around me and I see only the things I cannot accept.

But deep down I want a break that never comes.

I want a break, but what I need is effort.

I need to spark the flame and keep going with Spiral. I need to get into it.

The way to relax while one is awake is not to lie still. Boredom is hardly restful.

The way to relax is to play a fun game.

I need to start having fun programming. I need to remind myself of the spark that I had.

1:25pm. So I've had to endure many failures.

Spiral v0.1 was not enough, and theorem proving is not enough.

1:30pm. It does not mean that my efforts were a waste.

I just haven't been patient enough.

1:35pm. I mean, isn't that the root of all my ills?

For Spiral v0.09, it has never been enough to just make a language, I had to do something great with it.

It was never enough for me to make some great code. I have a need to control everything.

It is a blessing and a curse.

It is a blessing as staying true to that need is what is making me grow.

It is a curse as it is making me unhappy with the present state of affairs.

1:40pm. The way I am going with Spiral v0.2 is a mistake.

The way I did the previous versions was with my entire mind and body on the line. I hesitated not even a little.

Compared to the past me, right now I am weak.

Still this math thing will blow over. At the very least in the future when I resume reading ML papers, I will not have pangs of insecurity from skipping the proofs. I will just look at whether they use reals or infinite sets and if they do, chuck them where they belong - in the trash pile.

This will strengthen me.

I will find confidence from my 2019 efforts.

If there is something I do not regret, that would be doing that first proof from the CFR paper using randomized testing. Now that was a truly wonderful experience. I guess my current disappointment is partly because I haven't been able to follow through with that for the other proofs. That is definitely the catalyst for my current loss of stability.

Deep down, I am in grief that this world is not giving the technique the proper respect it deserves.

1:50pm. Still...I've been having the idea that machine learning is an evolutionary step forward from randomized testing.

So if I really want to pay my respects to randomized testing, it might make sense to do more ML.

...Hmmm...looking back at 2018, back when I was doing ML my testing protocols were trully atrocious. They really were horrid.

But as I said, I did botch the allocator by not using weak arrays.

And Spiral was still in development for much of that time.

1:55pm. Right now, one of the big things troubling me is that I am not exactly sure what should I do once I have Spiral v0.2.

I mean, I did the ML library back in the previous version. So what do I want to do differently this time.

Deep CFR? Remember that proof that I just mentioned I did using randomized testing?

It gave me an understanding of the perfect recall property. Though the stuff in the paper works, NNs would introduce state aliasing which would break it.

So I am not really enthused at trying random techniques like I was before.

Not in the same manner.

2pm. I understand what is going on.

21st century is going to be part two of the battle between finitists and infinitists. The old finitists were soundly defeated and cast to the fringes of history. They weren't prepared and they were too lightly armed to win.

I should not be a finitist.

I already have a perfectly good label to describe myself - a programmer.

2:05pm. Let me do the chores here.

Before I resume the parser, I am going to just stop and think for a while. What should be the first thing I am going to do in the new language?

Since I do not want agents nor ML libraries anymore, what will the language be for.

Could I take that proof that I did as a guiding light?

I cannot lose hope, it is always like I said it was - given any point in time, there is always a step to the next level.

Could I take all my experiences of the past 5 years and somehow mix them together?

A fast language - for randomized testing?

Would it be possible to envision a goal of going beyond simply making ML libraries and agents in them and doing those things at a meta-level?

2:10pm. Let stop here so I can do the chores.

I should really take the time to think about this.

If I can establish my motivation, then the language will go smoothly. Right now I am just fiddling around with the parser."

---
## [mrakgr/The-Spiral-Language@db1ec4cff8...](https://github.com/mrakgr/The-Spiral-Language/commit/db1ec4cff87b5027d1a014fa0eb36c8c8480215c)
##### 2020-01-03 14:17:01 by Marko Grdinić

"2:50pm. I think I will just step away from the screen for a while like I said I would yesterday. It is thinking time. Last month I spent quite a value thinking about the design for the language.

But now, I really do need to think about what would eventually use it for.

Clearly I am starting to develop feelings for mathematics.

I just do not know how to express them in a positive manner.

After the experiences of the last year, I am not really going to be able to just abandon mathematics. But I am not going to be able to just go through the ML papers like I wanted to either.

Really, as you go from infinitism to programming, the proofs do get harder, but it is not like you get nothing in return for exiling yourself from paradise.

[Randomized testing.](https://fscheck.github.io/FsCheck/)

When I did that first CFR proof, I could really feel the power of this.

At all really rests on these lightweight methods. I really do think that regular theorem proving should be kept for special things. I wish it were otherwise, but DTLs really are quite hard deal with.

Randomized testing is rarely talked about, but it really might be the killer application that finitists needed, but did not quite have in the early 20th century, along with computers.

3pm. Also, I just realized that I never even pushed the reply to the guy. I must be even madder than I thought to miss this. Still, I think I will just leave it at that since the flow is going against me anyway.

I am not really the kind of person to fish for negative votes.

Even if I made the post it is not like it would have added anything new to the conversation. That guy is just acting like a damage controller for the consensus. Calling constructive mathematics a flavor (therefore normalizing the deviance), and saying rejecting classical logic is unreasonable, I can only spit on this.

3:10pm. I was really fucking tooled like nothing else by classical mathematicians. I really need to think of a suitable reply to that, but whatever that reply is it cannot take the shape of arguing on the Internet.

There should be something better I can do to pay respects to my new principles.

3:15pm. Rejecting faulty assumptions is not unreasonable at all. It will really pare down the search space."

---
## [alanndz/kernel_xiaomi_lavender@fa6a3d4bc4...](https://github.com/alanndz/kernel_xiaomi_lavender/commit/fa6a3d4bc482d3e157c216f4482815dfa8e4a15c)
##### 2020-01-03 15:45:29 by Linus Torvalds

mm: remove unused variable in memory hotplug

When I removed the per-zone bitlock hashed waitqueues in commit
9dcb8b685fc3 ("mm: remove per-zone hashtable of bitlock waitqueues"), I
removed all the magic hotplug memory initialization of said waitqueues
too.

But when I actually _tested_ the resulting build, I stupidly assumed
that "allmodconfig" would enable memory hotplug.  And it doesn't,
because it enables KASAN instead, which then disables hotplug memory
support.

As a result, my build test of the per-zone waitqueues was totally
broken, and I didn't notice that the compiler warns about the now unused
iterator variable 'i'.

I guess I should be happy that that seems to be the worst breakage from
my clearly horribly failed test coverage.

Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: celtare21 <celtare21@gmail.com>

---
## [mrakgr/The-Spiral-Language@ef6d6c2ca7...](https://github.com/mrakgr/The-Spiral-Language/commit/ef6d6c2ca74a51a8409f0b20a8851d22d78c66be)
##### 2020-01-03 17:13:32 by Marko Grdinić

"5:10pm. While us idiots are having this argument on Reddit, maybe the truth is that the world has already moved on.

Maybe it is time that I come to an acceptance that the CFR algorithm is a proof in itself and that I cannot really reduce the strategy averaging aspect to anything more than it already is.

I mean, shouldn't at some point the emergent properties of algorithms be irreducible?

That /sci/ anon is right.

"Only repeatedly testing "random" things can lead to new knowledge. Science is obsessed with people and theories who could predict things correctly, but that is just glorified survivorship bias. Only experiments that cannot be reasonably predicted can provide new insights, only repeatedly forming new theories and discarding those that do not fit can lead to legitimate new knowledge."

Let me paste his post again.

Maybe rather than AI being alchemy or chemistry, its true nature is closer to cooking?

The issue are the damn floats!

Moving from operations on integers to floats makes things 100 times more difficult.

Classical mathematicians smartly skirted the issue with 'reals', but...

Sure, actual formal constructive proofs in Agda/Coq are the pinnacle of rigor. It is not just a flavor. It annoys me that somebody would compare it to picking between vanilla and chocolate ice cream.

But...

> Now the reason that mathematicians are not working with theorem provers is quite simple. Too much of mathematics is not formalized yet for us to build new research on it and it will take millions of hours of work until that has happened. There are people who work on that. However most of us deem our time better spent researching new mathematics. In addition one needs to be a true expert in theorem proving in order to archieve any sort of acceptable productivity (even then I'm not sure that is possible).

Fuck, if I can deny this part. I cannot deny the (im)practicalities of theorem proving.

I've never ever proven anything in Coq or Agda that I was not sure of beforehand! I've never once despite all my effort used it to generate novel insight about the programs I was making!

That randomized testing proof of CFR is actually the exception to this. In that the process of making it really did give me insight that I did not have before.

Fuck!

5:20pm. I did those converge proofs for sqrt over the rationals in Agda, but once I start dealing with imprecise floats, and actual algorithms, just how well will my plans go?

I can't see myself winning arguing for this at all.

Sure, in terms of rigor: Agda/Coq proofs > randomized testing >>> set theory.

Type theory might be the king of the hill, and randomized testing might be its good friend. They are high enough that they deserve their position being able to piss on classical mathematics from above. But type theory is really fat and slow, and if I was randomized testing, I would really be tempted - after the piss was done to give TT a nudge over the ledge in order to move into the huge mansion it was living in.

5:25pm. Back in 2016 I dumped Idris in favor of working on my own language. My impression was that the thing won't scale.

Now I know it and several other DTLs besides. My impression of this has not changed at all.

Now CFR is simple. No doubt future algorithms will just continue to get harder and harder.

I am betting that if you cut classical mathematics and constructive mathematics, and replace them with randomized testing, you'd really not lose anything. That guy mentioned that classical mathematics had successes, but for any success to qualify, it really needs to be applied and randomized testing can cover all of that.

It was not really an option to consider this seriously before now though.

Randomized testing came in without fanfare, and now it just exists. It silently conquered the world of machine learning.

5:30pm. Maybe the right thing to do is to simply start taking it seriously.

I had these thoughts last year, before I got sidetracked with the papers by Blackwell and Hart.

I might have disparaged those ML benchmarks due to the low value of their information, but maybe my problem is simply a matter of scale.

Maybe the right answer is that I need to repeat 2018 except at a far bigger scale.

5:35pm. What I need right now is a lot of energy. I need a firm belief.

And what about Spiral?

Spiral is the low hanging fruit.

The kind of reasoning that I do while making Spiral is definitely rigorous. I could only dream of achieving the degree of accuracy while doing ML that I do while doing PL.

5:40pm. Scale and speed. I can get both development speed and runtime performance with v0.2.

But what about scale? If my aim with v0.2 will not just be to do ML library and an agent, but generalize the concept of randomize testing as a part of doing ML, then what about scale?

I've heard disparaging remarks about languages that do whole program optimization, but in truth Spiral's scaling is no worse than C++.

...Which I am not sure is that good especially compared to langs like C# and F#.

6pm. Still, scale really depends on what you want to do.

Done with lunch.

Basically, there is no way I will ever need to handle more than 100k lines of code in total in one blast. Whatever Spiral v0.2's speed turns out to be, it should be enough to carry me through. I do not need to consider lesser languages like Julia.

Functional programming will rule the world.

6:05pm. Can I stomach doing this work in a weak language? Then pick something else. Otherwise do Spiral.

It really is not a weakness. Would I really be rather working on a PL or do ML experiments? I really can leave the later those research gamblers.

If I am going to program, it is important that I do it with pride.

I think that more than anything else is what makes me special compared to other people. Looking back at my school days, my schoolmates never had it.

6:10pm. I am really quite tired right now, so let me call it a day here.

Hopefully by tomorrow, I will manage to gather some real motivation.

I need to dream bigger. How do I abstract things like ML experiments - rather than just doing them. That is what I need to think about.

Once I have that the parser, the partial evaluator and the typechecker will take care of themselves. They will literally just come to me and out through my fingers."

---
## [newstools/2020-the-guardian-uk@4ab0e32676...](https://github.com/newstools/2020-the-guardian-uk/commit/4ab0e3267657466302b94d29369ec256fa2e4de0)
##### 2020-01-03 17:21:35 by NewsTools

Created Text For URL [www.theguardian.com/lifeandstyle/2020/jan/03/before-meeting-my-girlfriend-i-saw-escorts-how-do-i-get-over-my-past]

---
## [Anas-coder/My-Projects@fd203fdd4a...](https://github.com/Anas-coder/My-Projects/commit/fd203fdd4ad88695eb13064324dc1247f6508fc9)
##### 2020-01-03 18:15:17 by Md. Anas Ansari

Baby Names

Analysing Naming Trends
--Splitting the Top 1,000 names into the boy and girl portions is easy to do first
--number of birth per year
Measuring the increase in naming diversity
--Sum of table1000.prop by year and sex
--Number of popular names in top 50%
Boy names that became girl names (and vice versa)
--lesley_like name as male and female transition

---
## [rawatmanoj/Rainbow-Poem@8bae8491e9...](https://github.com/rawatmanoj/Rainbow-Poem/commit/8bae8491e9c66ab351fb018d49f9006ad1bf33ce)
##### 2020-01-03 21:33:51 by rawatmanoj

Poet: Kamala Das

This is an autobiographical poem which throws light on the life and work of Kamala Das. Das begins the poem by saying that she doesn’t understand politics but she knows the name of politicians probably referring to the fact that power is in the hands of a few elites and that it is usually males who run the country. She then gives a brief introduction of herself before she focuses on English being the medium she uses to expresses herself; how people criticize her for that; and why it is no one’s business other than herself. The poem then moves to her early and unsuccessful marriage and how the society she lives in is male dominated. The primary focus of the poem is the situation of women in a patriarchal society and the unjust burdens women have to go through in this male dominated world. Kamala Das or Kamala Surayya is one of the best known Indian female poets and this is her most famous work.

---
## [rawatmanoj/Rainbow-Poem@705fb96f90...](https://github.com/rawatmanoj/Rainbow-Poem/commit/705fb96f904c6764cecd824d4335a2006a0b53ff)
##### 2020-01-03 21:42:46 by rawatmanoj

RASHMIRATHI

In the great Indian epic Mahabharata, Karna was the first-born son of Kunti. However, she abandoned him at birth as he was conceived before her marriage. Karna then grows up in a lowly family but becomes one of the best warriors of his time. He becomes friends with Duryodhana and ultimately fights on his side against his own brothers, the Pandavas. Rashmirathi brilliantly captures the tale of Karna capturing all hues of human emotions he is trapped in due to the various dilemmas he faces. Ramdhari Singh Dinkar is considered as one of the most important modern Hindi poets and Rashmirathi is his most famous as well as his most critically acclaimed work.

---
## [vawser/Cinders-DS3@2d377391d3...](https://github.com/vawser/Cinders-DS3/commit/2d377391d344d0aeb0552e1a86fcad97afeecfac)
##### 2020-01-03 23:20:29 by Vawser

Update

- Champion's Pact: Boosts soul gain by 100%, no longer boosts Item Discovery.
- Lucidity: reduces FP reductiont to 15% (from 20%)
- Holy Moonlight Greatsword is now just part of the base Moonlight Greatsword when infused to Holy (it switches to that model). Removed the separate weapon.
- Reduced the spell FP consumption effects to 20% (from 30%).
- Shira's Set now reduces miracle FP consumption.
- Soul Arrow: damage to 200
- Great Soul Arrow: damage to 300
- Soul Spear: damage to 400
- Crystal Soul Spear: damage to 500
- Soul Greatsword: damage to 400
- Old Moonlight: damage to 400 (500)
- Soul Wave: damage to 300
- Soul Stream: damage to 200, FP to 100
- Unleash Magic: now boosts magic/dark damage by 20% for 30 seconds, but reduces absorption by 100% whilst active.
- Pestilent Mist: now inflicts Curse rather than dealing damage. Will inflict 240 Curse over its whole duration.
- Fire Surge: damage to 150
- Great Chaos Fire Orb: damage to 400
- Chaos Storm: damage to 400 (350)
- Profaned Flame: damage to 350
- Chaos Bed Vestiges: damage to 500
- Cataclysm: FP to 100
- Bursting Fireball: FP to 35, damage to 75
- Boulder Heave: FP to 60, damage to 500, 300
- Sacred Flame: reduced damage multipliers.
- Earthfall: FP to 100
- Acid Surge: deals damage and reduces durability rather than reducing absorption.
- Warmth: deals 30 HP per tick
- Power Within: boosts damage by 35%
- Force: no longer deals damage.
- Emit Force: damage to 250 (100)
- Wrath of the Gods: FP to 50, damage to 500
- Great Lightning Spear: damage to 350
- Sunlight Spear: FP to 75, damage to 500
- Heavenly Thunder: FP to 100
- Atonement: boosts absorption by 25% and grants 1% HP and FP recovery, but slows the caster. FP to 50.
- Sacred Oath: boosts damage and absorption by 20%.
- Seek Guidance renamed to Heavenly Relief
- Heavenly Relief: boosts equipment load by 25% and reveals signs without ember
- Perseverance: boosts Poise by 30 for 60 seconds.
All Dark spells no longer cost HP.
- Deep Barb: FP to 10, damage to 100
- Dark Edge: FP to 20, damage to 250
- Deep Soul: FP to 25, damage to 200
- Great Deep Soul: FP to 30, damage to 300
- Great Soul Dregs: FP to 50, damage to 500
- Climax: FP to 50, damage to 500, removes 1000 souls upon use. Doesn't affect damage.
- Abyssal Edge: FP to 30, damage to 400
- Affinity: FP to 75, damage to 100
- Whisper of Despair: FP to 60, now inflicts 100 Curse.
- Yearning Dregs: FP to 6, damage to 250
- Writhing Deep: FP to 40, damage to 250 [500]
- Dark Bead: FP to 50, damage to 75
- Cascading Deep: FP to 75, damage to 75
- Surging Deep: FP to 5, damage to 100
- Dreg Torrent: FP to 75, damage to 100
- Repel: FP to 25
- Numbness: FP to 100
- Black Fire Orb: FP to 40, damage to 400 (300)
- Black Flame: FP to 30, damage to 350 (200)
- Black Serpent: FP to 40, damage to 300
- Dark Dance: FP to 6, damage to 250
- Recollection: FP to 75, damage to 100
- Mournful Flames: FP to 40, damage to 300
- Nibble: FP to 10
- Gnaw: FP to 25
- Dorhys' Gnawing: FP to 30
- Lifehunt Scythe: FP to 30, damage to 400
- Devouring Swarm: FP to 75
- Dead Again: FP to 30
- Deep Protection: FP to 60
- Dark Blade: FP to 100
- Vow of Silence: FP to 30
- Added the covenant phantom rings to the Primordial shop
- Renamed Ring of the Fingers to Ring of Rosaria
- Renamed Ring of Champions to Ring of the Exalted
- Renamed Beastial Band to Bestial Band
- Dusk Crown Ring: reduces sorcery FP reduction to 25% (from 30%)
- Charred Bone: reduces pyromancy FP reduction to 25% (from 30%)
- Tome of Sunlight: reduces miracle FP reduction to 25% (from 30%)
- Increased the damage of the Soul of Cinder.
- Bosses are slightly tougher to inflict Curse on than before.
- Fixed various localization issues.
- Fixed Darkdrift instantly cursing you.
- Fixed Lightning Moonlight Greatsword not having the correct scaling.
- Fixed various irregularities with the animation cancel changes.

---

# [<](2020-01-02.md) 2020-01-03 [>](2020-01-04.md)

