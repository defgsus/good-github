# [<](2020-06-05.md) 2020-06-06 [>](2020-06-07.md)

2,985,692 events, 990,158 push events, 1,455,781 commit messages, 92,441,899 characters


## [ylh/tiddlymarker](https://github.com/ylh/tiddlymarker)@[87a4bd5985...](https://github.com/ylh/tiddlymarker/commit/87a4bd5985af489e704efb2bdd9810ce73d89ea0)
#### Saturday 2020-06-06 05:14:53 by Yestin L. Harrison

put tab-over mode out of its misery

this commit started its life with much more green.

having spent an entire day steadily implementing the blasted thing, it turns out
that the sort of access content scripts have to page scopes is just not enough.
perhaps some day in the future we can reconsider this after writing a TW plugin
that can pick up on postMessage calls or something, but as it stands, there just
isn't enough power in executeScript(). webserver mode is so good that the only
part of this that honestly hurts is the wasted time.

don't write browser code, people

---
## [Kaz205/kang_kernel_davinci](https://github.com/Kaz205/kang_kernel_davinci)@[a1d6928ccc...](https://github.com/Kaz205/kang_kernel_davinci/commit/a1d6928cccbd2a8ec9657be5e5e4b9054c3bcfec)
#### Saturday 2020-06-06 08:59:54 by Kazuki Hashimoto

qcacld-3.0: Disable Werror

FUCK YOU

Signed-off-by: Kazuki Hashimoto <kaz205@tuta.io>

---
## [ghc/ghc](https://github.com/ghc/ghc)@[42bdb3a879...](https://github.com/ghc/ghc/commit/42bdb3a879d3df6f8fd808e2bcbd33dccca6fa08)
#### Saturday 2020-06-06 16:44:44 by Simon Peyton Jones

Reduce result discount in conSize

Ticket #18282 showed that the result discount given by conSize
was massively too large.  This patch reduces that discount to
a constant 10, which just balances the cost of the constructor
application itself.

Note [Constructor size and result discount] elaborates, as
does the ticket #18282.

Reducing result discount reduces inlining, which affects perf.  I
found that I could increase the unfoldingUseThrehold from 80 to 90 in
compensation; in combination with the result discount change I get
these overall nofib numbers:

        Program           Size    Allocs   Runtime   Elapsed  TotalMem
--------------------------------------------------------------------------------
          boyer          -0.3%     +5.4%     +0.7%     +1.0%      0.0%
       cichelli          -0.3%     +5.9%     -9.9%     -9.5%      0.0%
      compress2          -0.4%     +9.6%     +7.2%     +6.4%      0.0%
    constraints          -0.3%     +0.2%     -3.0%     -3.4%      0.0%
   cryptarithm2          -0.3%     -3.9%     -2.2%     -2.4%      0.0%
         gamteb          -0.4%     +2.5%     +2.8%     +2.8%      0.0%
           life          -0.3%     -2.2%     -4.7%     -4.9%      0.0%
           lift          -0.3%     -0.3%     -0.8%     -0.5%      0.0%
         linear          -0.3%     -0.1%     -4.1%     -4.5%      0.0%
           mate          -0.2%     +1.4%     -2.2%     -1.9%    -14.3%
         parser          -0.3%     -2.1%     -5.4%     -4.6%      0.0%
         puzzle          -0.3%     +2.1%     -6.6%     -6.3%      0.0%
         simple          -0.4%     +2.8%     -3.4%     -3.3%     -2.2%
        veritas          -0.1%     +0.7%     -0.6%     -1.1%      0.0%
   wheel-sieve2          -0.3%    -19.2%    -24.9%    -24.5%    -42.9%
--------------------------------------------------------------------------------
            Min          -0.4%    -19.2%    -24.9%    -24.5%    -42.9%
            Max          +0.1%     +9.6%     +7.2%     +6.4%    +33.3%
 Geometric Mean          -0.3%     -0.0%     -3.0%     -2.9%     -0.3%

I'm ok with these numbers, remembering that this change removes
an *exponential* increase in code size in some in-the-wild cases.

I investigated compress2.  The difference is entirely caused by this
function no longer inlining

WriteRoutines.$woutputCodes
  = \ (w :: [CodeEvent]) ->
      let result_s1Sr
            = case WriteRoutines.outputCodes_$s$woutput w 0# 0# 8# 9# of
                (# ww1, ww2 #) -> (ww1, ww2)
      in (# case result_s1Sr of (x, _) ->
              map @Int @Char WriteRoutines.outputCodes1 x
         , case result_s1Sr of { (_, y) -> y } #)

It was right on the cusp before, driven by the excessive result
discount.  Too bad!

Metric Decrease:
    T12227
    T12545
    T15263
    T1969
    T5030
    T9872a
    T9872c
Metric Increase:
    T13701
    T9872d

---
## [amuricys/lispy-rusty](https://github.com/amuricys/lispy-rusty)@[8af1e2b5a7...](https://github.com/amuricys/lispy-rusty/commit/8af1e2b5a73f5b71af4052e5d8af13506240e928)
#### Saturday 2020-06-06 17:17:47 by Deco

Use fns instead of relying on named! macro (#1)

This will help make our life easier with recursion and such. Most of the struggle is figuring out the return types of the different constructs `nom` gives us. My understanding is that, when you have a macro named `my_macro`, say, and it takes a single argument and returns a function, doing `my_macro!(some_arg)` both _expands_ the macro _and_ calls the resulting function it produces. However, doing `my_macro(some_arg)` (without the `bang!`) simply returns the function it produces, meaning something like this `my_macro(some_arg)(some_other_arg)` could make sense (and that's how this PR is using `take_while` among others).

EDIT: This style is very reminiscent of `do` notation in Haskell. The `?` operator suggested by the God @gabaconrado gives us a short circuiting way of piping the the result of parsers to the next ones.

The next step is to implement variadic arguments and expressions as operators (the first argument to a list doesn't have to be a `char`, it can be an entire expression). Also, not every atom is a Num, so we'll make that into a rich enum as well.

---
## [igbanam/butler](https://github.com/igbanam/butler)@[430f1744fb...](https://github.com/igbanam/butler/commit/430f1744fb367c2b385b0093e3253976785e7a6f)
#### Saturday 2020-06-06 17:34:44 by Owajigbanam Ogbuluijah

Add the ability to add tasks to the Butler (#3)

Here we add some tasks to the Butler. Man's gotta know what to keep
track of. This is a simple "create a card on Trello"; so nothing fancy.
But there are some design decisions I am slowly realizing needs to be
made.

- Storage: currently, this is in a flat file as JSON, but it could be
something more elaborate as time goes on and the entity space grows.
- TDD: TBH, IDGAF. I'd rather build this out first... then create a test
harness later. That's just how my brain works. â€” although, I had to
write tests for the Store at some point cos mans didn't understand what
Crystal was doing behind the scenes. Turns out all the test you see for
the Store module was to prefix the expression in `Store#unique?` with !
- Sugar: `task add` is a backward way of speaking. I realize this is how
programmers speak, but Butlers are more polished. I would have to
introduce some sugar to this. What I currently don't know is how this
plays with the help text.
- Exceptions: I was just an ass here. I don't know why I tried to
maintain "errors" and "exceptions"

Writing to the datastore is an atomic task. This is could become a
problem if the number of tasks become ginormous cos I'm handling this in
one process. But when we get to that bridge, we'd cross it. i cannot
come and go and kill myself. Here's the note that I am aware of it.

At this point, I can add tasks. But there's nothing I can do with this
ability because Butler neither knows how to show me these tasks, nor how
to interact with them.

Worklog:

* Put tests in modules; don't litter the global scope
* Better routing + Task instruction
* Fixed logger write bug
* Add the Task entity
* Add a minimal store
* Add "CreateTask"
* Fixed tests setup and teardown

---
## [last-resort-gaming/LRG-Mission-Files](https://github.com/last-resort-gaming/LRG-Mission-Files)@[3dc07995bf...](https://github.com/last-resort-gaming/LRG-Mission-Files/commit/3dc07995bf41b4f3284bf7d274e3db64a952ec20)
#### Saturday 2020-06-06 17:53:47 by Mokka

Change to a different TFAR channel

Fuck you Mitch c:

---
## [govind123456789/Data-Analysis-Visulalization-of-Different-Dataset-Crimes-Covid19-Market-predictions-Housing-Data-](https://github.com/govind123456789/Data-Analysis-Visulalization-of-Different-Dataset-Crimes-Covid19-Market-predictions-Housing-Data-)@[7b7032544a...](https://github.com/govind123456789/Data-Analysis-Visulalization-of-Different-Dataset-Crimes-Covid19-Market-predictions-Housing-Data-/commit/7b7032544aea6bb6047e16aa3f8b5a5caff19924)
#### Saturday 2020-06-06 18:48:48 by govind123456789

Add files via upload

Context

This dataset contains complete information about various aspects of crimes happened in India from 2001. There are many factors that can be analysed from this dataset. Hope this dataset helps us to understand better about India.



Inspiration:

There could be many things one can understand by analyzing this dataset. Few inspirations for you to start with.

What is the major reason people being kidnapped in each and every state?
Offenders relation to the rape victim
Juveniles family background, education and economic setup.
Which state has more crime against children and women?
Age group wise murder victim
Crime by place of occurrence.
Anti corruption cases vs arrests.
Which state has more number of complaints against police?
Which state is the safest for foreigners?




Coronavirus disease 2019 (COVID-19) time series listing confirmed cases, reported deaths and reported recoveries. Data is disaggregated by country (and sometimes subregion). Coronavirus disease (COVID-19) is caused by the Severe acute respiratory syndrome Coronavirus 2 (SARS-CoV-2) and has had a worldwide effect. On March 11 2020, the World Health Organization (WHO) declared it a pandemic, pointing to the over 118,000 cases of the coronavirus illness in over 110 countries and territories around the world at the time.

This dataset includes time series data tracking the number of people affected by COVID-19 worldwide, including:

confirmed tested cases of Coronavirus infection
the number of people who have reportedly died while sick with Coronavirus
the number of people who have reportedly recovered from it


Big Data Mart Sales Problem

The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.

Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.

---
## [KeinR/cppdepends](https://github.com/KeinR/cppdepends)@[b4aaa0598b...](https://github.com/KeinR/cppdepends/commit/b4aaa0598b5ac4eb4ea1aa1e132ad51d6fb3842e)
#### Saturday 2020-06-06 19:42:02 by Kein

That one fucking like

Honestly, I'm gonna' start commenting my code now. It took me way too long to remember the convoluted way in which I programmed this...

---
## [AndyJYoung/Hazard_Dice_Game](https://github.com/AndyJYoung/Hazard_Dice_Game)@[6dd97fef06...](https://github.com/AndyJYoung/Hazard_Dice_Game/commit/6dd97fef06d6be2066ae5f4bb00d75af337f6d80)
#### Saturday 2020-06-06 20:35:08 by Unknown

Do not rely on this. It's kinda bad but we're working on it

Yeah I know this commit message sucks :P

---
## [starlight5234/android_kernel_vince_unitrix](https://github.com/starlight5234/android_kernel_vince_unitrix)@[163ab38b29...](https://github.com/starlight5234/android_kernel_vince_unitrix/commit/163ab38b2933e82e4f81b3d9187b6d98a66e59b0)
#### Saturday 2020-06-06 23:34:48 by Peter Zijlstra

sched/core: Implement new approach to scale select_idle_cpu()

Hackbench recently suffered a bunch of pain, first by commit:

  4c77b18cf8b7 ("sched/fair: Make select_idle_cpu() more aggressive")

and then by commit:

  c743f0a5c50f ("sched/fair, cpumask: Export for_each_cpu_wrap()")

which fixed a bug in the initial for_each_cpu_wrap() implementation
that made select_idle_cpu() even more expensive. The bug was that it
would skip over CPUs when bits were consequtive in the bitmask.

This however gave me an idea to fix select_idle_cpu(); where the old
scheme was a cliff-edge throttle on idle scanning, this introduces a
more gradual approach. Instead of stopping to scan entirely, we limit
how many CPUs we scan.

Initial benchmarks show that it mostly recovers hackbench while not
hurting anything else, except Mason's schbench, but not as bad as the
old thing.

It also appears to recover the tbench high-end, which also suffered like
hackbench.

Tested-by: Matt Fleming <matt@codeblueprint.co.uk>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Cc: Chris Mason <clm@fb.com>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Mike Galbraith <efault@gmx.de>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: hpa@zytor.com
Cc: kitsunyan <kitsunyan@inbox.ru>
Cc: linux-kernel@vger.kernel.org
Cc: lvenanci@redhat.com
Cc: riel@redhat.com
Cc: xiaolong.ye@intel.com
Link: http://lkml.kernel.org/r/20170517105350.hk5m4h4jb6dfr65a@hirez.programming.kicks-ass.net
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Signed-off-by: Raphiel Rollerscaperers <rapherion@raphielgang.org>
Signed-off-by: prorooter007 <shreyashwasnik112@gmail.com>
Signed-off-by: starlight5234 <ifist2834@gmail.com>

---

# [<](2020-06-05.md) 2020-06-06 [>](2020-06-07.md)

