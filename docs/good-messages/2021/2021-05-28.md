# [<](2021-05-27.md) 2021-05-28 [>](2021-05-29.md)

4,446,805 events, 1,312,072 push events, 2,158,531 commit messages, 161,834,395 characters


## [ZeraGmbH/vf-qmllibs@cac6a8178d...](https://github.com/ZeraGmbH/vf-qmllibs/commit/cac6a8178d289ac4d2ce0d229a999a1142919b8a)
##### 2021-05-28 00:26:54 by Andreas Müller

Cleanup CMake files

* remove annoying line feeds: Why scrolling more than necessary
* remove leftovers
* remove extra quirks: They are not necessary and from my experience as
  oe-packer: Every non standard option tends to break throughout the lifetime
  of a project
* use CMAKE_BUILD_TYPE correctly without forcing oe-recipe to preset it

Signed-off-by: Andreas Müller <schnitzeltony@gmail.com>

---
## [rdparker/nix-doom-emacs@c22250f74a...](https://github.com/rdparker/nix-doom-emacs/commit/c22250f74ae31e5918eeeabd1a30369bb049116b)
##### 2021-05-28 00:40:51 by github-actions[bot]

test/doom.d/init.el: Updating from hlissner/doom-emacs - 005a71e4

### Changes for test/doom.d/init.el

```diff
--- 
+++ 
@@ -111,7 +111,8 @@
 
        :lang
        ;;agda              ; types of types of types of types...
-       ;;cc                ; C/C++/Obj-C madness
+       ;;beancount         ; mind the GAAP
+       ;;cc                ; C > C++ == 1
        ;;clojure           ; java with a lisp
        ;;common-lisp       ; if you've seen one lisp, you've seen them all
        ;;coq               ; proofs-as-programs
@@ -124,6 +125,7 @@
        emacs-lisp        ; drown in parentheses
        ;;erlang            ; an elegant language for a more civilized age
        ;;ess               ; emacs speaks statistics
+       ;;factor
        ;;faust             ; dsp, but you get to keep your soul
        ;;fsharp            ; ML stands for Microsoft's Language
        ;;fstar             ; (dependent) types and (monadic) effects and Z3
@@ -138,9 +140,8 @@
        ;;julia             ; a better, faster MATLAB
        ;;kotlin            ; a better, slicker Java(Script)
        ;;latex             ; writing papers in Emacs has never been so fun
-       ;;lean
-       ;;factor
-       ;;ledger            ; an accounting system in Emacs
+       ;;lean              ; for folks with too much to prove
+       ;;ledger            ; be audit you can be
        ;;lua               ; one-based indices? one-based indices
        markdown          ; writing docs for people to ignore
        ;;nim               ; python + lisp at the speed of c
@@ -159,7 +160,7 @@
        ;;(ruby +rails)     ; 1.step {|i| p "Ruby is #{i.even? ? 'love' : 'life'}"}
        ;;rust              ; Fe2O3.unwrap().unwrap().unwrap().unwrap()
        ;;scala             ; java, but good
-       ;;scheme            ; a fully conniving family of lisps
+       ;;(scheme +guile)   ; a fully conniving family of lisps
        sh                ; she sells {ba,z,fi}sh shells on the C xor
        ;;sml
        ;;solidity          ; do you need a blockchain? No.
@@ -167,6 +168,7 @@
        ;;terra             ; Earth and Moon in alignment for performance.
        ;;web               ; the tubes
        ;;yaml              ; JSON, but readable
+       ;;zig               ; C, but simpler
 
        :email
        ;;(mu4e +gmail)

```

---
## [RavenholmZombie/Immersibrook@25fd486fbe...](https://github.com/RavenholmZombie/Immersibrook/commit/25fd486fbe55a4b588de429151a1fc9737a4c063)
##### 2021-05-28 00:58:01 by RavenholmZombie

Ye

What the fuck did you just fucking say about me, you little bitch? I'll have you know I graduated top of my class in the Navy Seals, and I've been involved in numerous secret raids on Al-Quaeda, and I have over 300 confirmed kills. I am trained in gorilla warfare and I'm the top sniper in the entire US armed forces. You are nothing to me but just another target. I will wipe you the fuck out with precision the likes of which has never been seen before on this Earth, mark my fucking words. You think you can get away with saying that shit to me over the Internet? Think again, fucker. As we speak I am contacting my secret network of spies across the USA and your IP is being traced right now so you better prepare for the storm, maggot. The storm that wipes out the pathetic little thing you call your life. You're fucking dead, kid. I can be anywhere, anytime, and I can kill you in over seven hundred ways, and that's just with my bare hands. Not only am I extensively trained in unarmed combat, but I have access to the entire arsenal of the United States Marine Corps and I will use it to its full extent to wipe your miserable ass off the face of the continent, you little shit. If only you could have known what unholy retribution your little "clever" comment was about to bring down upon you, maybe you would have held your fucking tongue. But you couldn't, you didn't, and now you're paying the price, you goddamn idiot. I will shit fury all over you and you will drown in it. You're fucking dead, kiddo.

---
## [codekane/stonkk@5baaa516a0...](https://github.com/codekane/stonkk/commit/5baaa516a0858d485da2e874298b65c85768af3d)
##### 2021-05-28 02:09:34 by codekane

I guess I did a bunch of stuff. I tried to do a bunch of stuff, that is. I mean. I got it sorta-working. I tried to make it actually do some cool stuff, right. I was going to set up a page for each stock with additional information, but, if I'm honest, there's not much additional information that I Have right now. I've revealed glaring architectural flaws in how I collect and distribute my data. And that's fine. That's cool. It's fun, even, and I'm glad to have learned such huge lessons on how to go forward. But continuing to dick around with front-end work right now? I don't think that's in the cards. What I really ought to be doing is going back to the back-end, and figuring out how to collect every single post for every single day for every single month of every single year. Then I'm going to analyze them. Then I'm going to get the real data. Then I might have something really cool going on. But right now? Ok. I've made the Stonk Ticker. It doesn't tick, because I fail to see the point in configuring that when I'm going to have to re-do the whole damned thing when I shift the architecture. That's fine. ITS FINE. ANd, hell, I got further in React. There's still stupid bugs like every other programming venture, but I got it far enough that it doesn't really matter. What does matter is the data. The data is going to tell a story. There's something important in the data I've been collecting, and, the beauty of it is, that someone else has already been collecting it for me. So I'm gonna see them about it, and get them to give it to me, and then I'm gonna dig through it. I'm gonna play with it like a child in a sandbox, except this isnt' just any sandbox. This is a meticulous sandbox, where no cat comes to shit. My sandbox.

---
## [mrakgr/The-Spiral-Language@5652663271...](https://github.com/mrakgr/The-Spiral-Language/commit/565266327188767eddc9daf753a4736637922702)
##### 2021-05-28 16:29:34 by Marko Grdinić

"9:20am. I am up. I slept well and even went to bed a bit earlier than usual. Let me chill a bit and then I'll begin. No replies to the question from yesterday. I guess to hack LN, I'd have to do a C++ extension. That is the only way I'll get my hands on the backwards primitive.

10am. Let me start.

I've pried myself off from the thing I was reading, but I am yawning here and do not feel like programming. Today my goal will be to deal with the replay buffer.

Since my orignal plan was not to do variance matching, but instead to use a moving average in both fast and slow variants, the algo I implemented is out of my expectations. Why did I do this?

I keep thinking about long sequences which will have small batch sizes. No matter what, I want to make sure that weight scaling in the upper layers is factored out. Moving averages are no good as they are reactive. I need a predictive measure instead. So I've been obsessing whether I did the right thing.

I think I did. Yesterday the algorithm just sprang to my mind as if it was calling out to me. It composes particularly well too. And things which compose well are usually right.

The weight scale elimination would be an effect of Adam and the moving average scheme anyway once the batch sizes get large enough. So doing it like this will have no disadvantage.

The LR of 2 ** -13 and 2 ** -14 might seem low, but yesterday while I was printing out the reward signals, I saw that they summed up into the 100s and even a 1000. This is because of the large batch size. Had I been doing online learning, it would be equivalent of 2 ** -4 or 2 ** -5.

It is not low at all.

10:25am. I am thinking whether I should do the moving average rebalancing that I described in the PL update.

That kind of rebalancing only makes sense when the estimates are good. When they are bad, they are more likely to destabilize training. I learned this while working on KFAC. It is not smart to move more that 10x in any direction. I needed to put in a large identity so the training would not blow up.

Yes it could be benefit. But unlike with the predictive scheme for the gradients that I implemented yesterday, having to do the rebalancing means I have to mess with moving averages. Suddenly I have an extra hyperparameter in need of optimization for every moving average.

10:30am. No just forget that. You can make everything work with more hyperparam tuning. But it will be brittle. The way I am doing things now will result in consistent credit assignment.

If I am missing something that would be wrapping around the log softmax and the LN ops. I am going to have to deal with that. If I want work to do, that is what I should focus on.

10:35am. So with that decided, let me think about the replay buffer.

10:40am. Actually for the log_softmax I won't need the variance matching trick. The rule is `grad - prob * sum grad`. But the way scaling works will ensure that the sum of the gradients...well, no. When calculating the means, I am multiplying by the probabilities rather than taking the average. So the sum won't necessarily be 0.

I'll have to do a wrapping after all.

How much work this is. What a bother. I'll have to jack the transformer too.

Ok, the replay buffer. Let me focus on that again.

I can think of different ways of doing it.

11:35am. No.

All the replay buffer amounts to is a complicated gradient clipping scheme. I do not feel like filling it up at all now that the moment of truth is here. Checkpointing + learning rate tuning should do the trick. I should just not worry about the outliers that could wreck training. I should depend on checkpointing.

Being able to generate perfectly accurate synthetic data is better than any kind of replay buffer scheme. The brain needs complicated memory mechanisms because it does not have direct access to the game, but in poker I should simplify things to the utmost.

12:10pm. I am still thinking about it. Yes, ditching the replay buffer will make things a lot simpler. For the transformer, I really will have to make my own extensions that do gradient variance matching. But that is fine. That is just a part and parcel of attaining skill in ML. Of course at some point I'd need to get dirty with the mainstream framework. This is also an ideal ramp to build up Spiral as well.

12:15pm. Replay buffer + Leduc. The problem with the replay buffer, is that it will push training towards stale data. That is the disadvantage of it. And fundamentally, even though I could turn things into +1 and -1, that in the limit is still the same as training normally with a finely tuned learning rate. Forget that.

12:20pm. I spent an immense amount of time thinking about this, but in the end it turned out to be nothing more than mental play. And the side idea that I had is the one which rose to the top.

2:10pm. Let me chill for a while longer.

3:25pm. I am still thinking about it.

3:30pm. GGGGGGGGGGGGGGGGGGGGGGGGGGGAAAAAAAHHHHHH!!!

Damn it! The replay buffer idea that I was so proud of seems like nothing right now. Is it really that different from simply lowering the learning rate while reweighting the gradients using the CFR formula? I don't think so.

3:35pm. Do I start? I have no choice really.

At some point, I swear I will figure out how proper long term reinforcement learning should work.

3:50pm. I am still thinking about it. Right now I am just as obsessed as I was during my studies. What was even the point of last month.

5:10pm. I guess it is one of those days where I am laid out in thought. The decision to drop the replay buffer was a heavy blow to me. Last month I was in idea generation mode, but now that I am actually programming, it gives me a different perspective on what is right and what is not. Even if I think of making a bigger replay buffer and sampling from that, it is still the same thing and gradient reweighting.

There is no point to the replay buffer at all when you have a perfect simulator. The brain needs it because it has to do a lot of processing, but the poker simulator I have here is as close to the essence of the game as it could be.

5:20pm. Let me read some papers by Lanctot and I will call it a day. I want to see what came of those actor critics he worked on.

Tomorrow I will just calculate the regret probabilities and reweight the gradients by them.

https://arxiv.org/abs/1810.09026
Actor-Critic Policy Optimization in Partially Observable Multiagent Environments

I remember reading this paper 2.5 years ago. Here they don't use the log probability, but the regular one. This might be the reason for the underperformance of QPG in some cases. But the way they just threeshold the gradients for RPG is interesting too.

It might be worth going for that. It would free me of the need to do both log_softmax and then an exp of it. Plug given that I am doing variance normalization this would fit better into the overall scheme.

6:10pm. I doesn't seem like the algorithms in these papers even divide by the opponent reach probability. I think that is what I will do too if I start having trouble with stability. I should get ready for that. But I do not think it will come to it.

6:15pm. I should be able to grasp it.

Complicated plans were never necessary. I have the responsibility to think, but going so much in depth was not needed. Going as far as spending years making a ML library and a language was not needed for the sake of simply making a player.

What was needed now and in the past was to show my determination

Let me stop here. Tomorrow I will just do the weighting and then move forward from there. Hell, once I do this, I am already at the threeshold of moving to Holdem.

It is a piece of cake to make progress when all you are doing is removing complicated parts. No emails yet.

Yeah, if I want transformers, I am going to have to hack softmax and layer norm so I can get variance matching activations.

But that gives me work worth doing.

6:20pm. Today I just could not do any programming. I wanted to, but the decision to ommit the replay buffer was just so huge that I could pivot to it easily. I spent the entire day deep in thought. Either way, tomorrow I am going to complete the first elite neural player and have it engage in self play.

I am going to get rid of the shared key dropout idea in favor of stochastic weight averaging as it will be simpler to deal with. No matter how much work I did, I am going to get rid of it all in favor of doing the simplest possible thing. So much thought and foresight, only to make a short step forward.

This is too easy. It won't take long at all until I am done."

---
## [Wildwestmars/sojourn-station@0dca421e69...](https://github.com/Wildwestmars/sojourn-station/commit/0dca421e697851f3701e7be5a087994980cb6bc2)
##### 2021-05-28 19:51:52 by Kazkin

Summary Changes

-Blackshield synths have been added. These synths focus towards combat, with models ranging from melee, ballistic, and non-lethal. They are faster than soteria synths, but not as insulated as AG synths.
-Blackshield synthetic limbs are now available, they come with the same armor as soteria synthetics but a less chance to malfunction. However, unlike sot limbs, you can't obtain them in round.
-Defibs now log and message admins with the rez sickness levels and stat losses for those who get revived so we can keep track.
-Modified the vision range and aggro vision of hell divers, cerberus, and chimeras to make their AI breaking alot less likely until we get a full patch through, making them far more reliable.
-Fixed an issue with the myrmidon rail pistol so it correctly fires kurtz rail rounds, buffing its power.
-All guild rail guns (myr and reductor) have been massively buffed.
--Reductor rail rifle, +20% damage, higher credit value, more recoil, charge cost per shot reduced by 75%. This makes the rail rifle more like a highly modular high impact rifle with somewhat slow fire speed, much better for frontline fighting.
--Myrmidon rail pistol, recoil increased, charged cost per shot reduced by 50%, fire delay reduced by 45%. Damage mildly buffed due fixing a pathing issue with bullet assignment.
-The conselour stun gun now has its fire modes set to the hand menu, making it easier to switch between taster, stunbolt, and stunshot.
-Implanted arm smg buffed to make it on par with a wirbel wind, giving it good 1 handed suppression ability, moderate damage, amazing recoil control, but shit armor penetration.
-Syndicate storm trooper gunslinger damage buffed due to kurtz ammo fix.

---
## [henricaodopao/kernel_asus_sdm660@3685c4493f...](https://github.com/henricaodopao/kernel_asus_sdm660/commit/3685c4493f1adada93c1d2e02060d6bd0df8fb4f)
##### 2021-05-28 20:31:17 by Michal Hocko

proc: oom: drop bogus task_lock and mm check

Series "Handle oom bypass more gracefully", V5

The following 10 patches should put some order to very rare cases of mm
shared between processes and make the paths which bypass the oom killer
oom reapable and therefore much more reliable finally.  Even though mm
shared outside of thread group is rare (either vforked tasks for a short
period, use_mm by kernel threads or exotic thread model of
clone(CLONE_VM) without CLONE_SIGHAND) it is better to cover them.  Not
only it makes the current oom killer logic quite hard to follow and
reason about it can lead to weird corner cases.  E.g.  it is possible to
select an oom victim which shares the mm with unkillable process or
bypass the oom killer even when other processes sharing the mm are still
alive and other weird cases.

Patch 1 drops bogus task_lock and mm check from oom_{score_}adj_write.
This can be considered a bug fix with a low impact as nobody has noticed
for years.

Patch 2 drops sighand lock because it is not needed anymore as pointed
by Oleg.

Patch 3 is a clean up of oom_score_adj handling and a preparatory work
for later patches.

Patch 4 enforces oom_adj_score to be consistent between processes
sharing the mm to behave consistently with the regular thread groups.
This can be considered a user visible behavior change because one thread
group updating oom_score_adj will affect others which share the same mm
via clone(CLONE_VM).  I argue that this should be acceptable because we
already have the same behavior for threads in the same thread group and
sharing the mm without signal struct is just a different model of
threading.  This is probably the most controversial part of the series,
I would like to find some consensus here.  There were some suggestions
to hook some counter/oom_score_adj into the mm_struct but I feel that
this is not necessary right now and we can rely on proc handler +
oom_kill_process to DTRT.  I can be convinced otherwise but I strongly
think that whatever we do the userspace has to have a way to see the
current oom priority as consistently as possible.

Patch 5 makes sure that no vforked task is selected if it is sharing the
mm with oom unkillable task.

Patch 6 ensures that all user tasks sharing the mm are killed which in
turn makes sure that all oom victims are oom reapable.

Patch 7 guarantees that task_will_free_mem will always imply reapable
bypass of the oom killer.

Patch 8 is new in this version and it addresses an issue pointed out by
0-day OOM report where an oom victim was reaped several times.

Patch 9 puts an upper bound on how many times oom_reaper tries to reap a
task and hides it from the oom killer to move on when no progress can be
made.  This will give an upper bound to how long an oom_reapable task
can block the oom killer from selecting another victim if the oom_reaper
is not able to reap the victim.

Patch 10 tries to plug the (hopefully) last hole when we can still lock
up when the oom victim is shared with oom unkillable tasks (kthreads and
global init).  We just try to be best effort in that case and rather
fallback to kill something else than risk a lockup.

This patch (of 10):

Both oom_adj_write and oom_score_adj_write are using task_lock, check for
task->mm and fail if it is NULL.  This is not needed because the
oom_score_adj is per signal struct so we do not need mm at all.  The code
has been introduced by 3d5992d2ac7d ("oom: add per-mm oom disable count")
but we do not do per-mm oom disable since c9f01245b6a7 ("oom: remove
oom_disable_count").

The task->mm check is even not correct because the current thread might
have exited but the thread group might be still alive - e.g.  thread group
leader would lead that echo $VAL > /proc/pid/oom_score_adj would always
fail with EINVAL while /proc/pid/task/$other_tid/oom_score_adj would
succeed.  This is unexpected at best.

Remove the lock along with the check to fix the unexpected behavior and
also because there is not real need for the lock in the first place.

Link: http://lkml.kernel.org/r/1466426628-15074-2-git-send-email-mhocko@kernel.org
Signed-off-by: Michal Hocko <mhocko@suse.com>
Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
Acked-by: Oleg Nesterov <oleg@redhat.com>
Cc: David Rientjes <rientjes@google.com>
Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Change-Id: Id5b29755e8dbc70fc104263e207bb54de5eeb6ee

---
## [nin66/CompGraphics@0b7394023e...](https://github.com/nin66/CompGraphics/commit/0b7394023e11ee0906e1eaab13d5e43aea3ff90f)
##### 2021-05-28 22:53:18 by WichaelMu

Minor Fish Update

"Minor"? this movement thing literally took me 12 hours and i still couldn't figure it out.

for some dumb reason, only BrownFish wants to move towards a coral, nothing else. Even though there's a fishies array that holds every single fish in the scene, it still doesnt want to move. However, 1 in 100 times, everything works properly.

Still, the fishies will still collide into terrain and go through them because three js and javascript are hilariously slow at processing anything and it takes too long to load Reef Exploration.

Three JS and javascript are both the two worst things ive ever worked with, same goes for python. interpreted languages are very stupid and should never be used.

i started working on this at 10am and its now 10pm. i literally wasted my whole day trying to get something to work on an inferior language.

I also love this subject, ive learnt more about how three js and javascript suck ass than actually learning content because no one teaches it anyway.

who even reads this?

---

# [<](2021-05-27.md) 2021-05-28 [>](2021-05-29.md)

