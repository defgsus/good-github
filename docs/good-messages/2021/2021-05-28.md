# [<](2021-05-27.md) 2021-05-28 [>](2021-05-29.md)

4,446,805 events, 1,312,072 push events, 2,158,531 commit messages, 161,834,395 characters


## [rdparker/nix-doom-emacs](https://github.com/rdparker/nix-doom-emacs)@[c22250f74a...](https://github.com/rdparker/nix-doom-emacs/commit/c22250f74ae31e5918eeeabd1a30369bb049116b)
#### Friday 2021-05-28 00:40:51 by github-actions[bot]

test/doom.d/init.el: Updating from hlissner/doom-emacs - 005a71e4

### Changes for test/doom.d/init.el

```diff
--- 
+++ 
@@ -111,7 +111,8 @@
 
        :lang
        ;;agda              ; types of types of types of types...
-       ;;cc                ; C/C++/Obj-C madness
+       ;;beancount         ; mind the GAAP
+       ;;cc                ; C > C++ == 1
        ;;clojure           ; java with a lisp
        ;;common-lisp       ; if you've seen one lisp, you've seen them all
        ;;coq               ; proofs-as-programs
@@ -124,6 +125,7 @@
        emacs-lisp        ; drown in parentheses
        ;;erlang            ; an elegant language for a more civilized age
        ;;ess               ; emacs speaks statistics
+       ;;factor
        ;;faust             ; dsp, but you get to keep your soul
        ;;fsharp            ; ML stands for Microsoft's Language
        ;;fstar             ; (dependent) types and (monadic) effects and Z3
@@ -138,9 +140,8 @@
        ;;julia             ; a better, faster MATLAB
        ;;kotlin            ; a better, slicker Java(Script)
        ;;latex             ; writing papers in Emacs has never been so fun
-       ;;lean
-       ;;factor
-       ;;ledger            ; an accounting system in Emacs
+       ;;lean              ; for folks with too much to prove
+       ;;ledger            ; be audit you can be
        ;;lua               ; one-based indices? one-based indices
        markdown          ; writing docs for people to ignore
        ;;nim               ; python + lisp at the speed of c
@@ -159,7 +160,7 @@
        ;;(ruby +rails)     ; 1.step {|i| p "Ruby is #{i.even? ? 'love' : 'life'}"}
        ;;rust              ; Fe2O3.unwrap().unwrap().unwrap().unwrap()
        ;;scala             ; java, but good
-       ;;scheme            ; a fully conniving family of lisps
+       ;;(scheme +guile)   ; a fully conniving family of lisps
        sh                ; she sells {ba,z,fi}sh shells on the C xor
        ;;sml
        ;;solidity          ; do you need a blockchain? No.
@@ -167,6 +168,7 @@
        ;;terra             ; Earth and Moon in alignment for performance.
        ;;web               ; the tubes
        ;;yaml              ; JSON, but readable
+       ;;zig               ; C, but simpler
 
        :email
        ;;(mu4e +gmail)

```

---
## [RavenholmZombie/Immersibrook](https://github.com/RavenholmZombie/Immersibrook)@[25fd486fbe...](https://github.com/RavenholmZombie/Immersibrook/commit/25fd486fbe55a4b588de429151a1fc9737a4c063)
#### Friday 2021-05-28 00:58:01 by RavenholmZombie

Ye

What the fuck did you just fucking say about me, you little bitch? I'll have you know I graduated top of my class in the Navy Seals, and I've been involved in numerous secret raids on Al-Quaeda, and I have over 300 confirmed kills. I am trained in gorilla warfare and I'm the top sniper in the entire US armed forces. You are nothing to me but just another target. I will wipe you the fuck out with precision the likes of which has never been seen before on this Earth, mark my fucking words. You think you can get away with saying that shit to me over the Internet? Think again, fucker. As we speak I am contacting my secret network of spies across the USA and your IP is being traced right now so you better prepare for the storm, maggot. The storm that wipes out the pathetic little thing you call your life. You're fucking dead, kid. I can be anywhere, anytime, and I can kill you in over seven hundred ways, and that's just with my bare hands. Not only am I extensively trained in unarmed combat, but I have access to the entire arsenal of the United States Marine Corps and I will use it to its full extent to wipe your miserable ass off the face of the continent, you little shit. If only you could have known what unholy retribution your little "clever" comment was about to bring down upon you, maybe you would have held your fucking tongue. But you couldn't, you didn't, and now you're paying the price, you goddamn idiot. I will shit fury all over you and you will drown in it. You're fucking dead, kiddo.

---
## [codekane/stonkk](https://github.com/codekane/stonkk)@[5baaa516a0...](https://github.com/codekane/stonkk/commit/5baaa516a0858d485da2e874298b65c85768af3d)
#### Friday 2021-05-28 02:09:34 by codekane

I guess I did a bunch of stuff. I tried to do a bunch of stuff, that is. I mean. I got it sorta-working. I tried to make it actually do some cool stuff, right. I was going to set up a page for each stock with additional information, but, if I'm honest, there's not much additional information that I Have right now. I've revealed glaring architectural flaws in how I collect and distribute my data. And that's fine. That's cool. It's fun, even, and I'm glad to have learned such huge lessons on how to go forward. But continuing to dick around with front-end work right now? I don't think that's in the cards. What I really ought to be doing is going back to the back-end, and figuring out how to collect every single post for every single day for every single month of every single year. Then I'm going to analyze them. Then I'm going to get the real data. Then I might have something really cool going on. But right now? Ok. I've made the Stonk Ticker. It doesn't tick, because I fail to see the point in configuring that when I'm going to have to re-do the whole damned thing when I shift the architecture. That's fine. ITS FINE. ANd, hell, I got further in React. There's still stupid bugs like every other programming venture, but I got it far enough that it doesn't really matter. What does matter is the data. The data is going to tell a story. There's something important in the data I've been collecting, and, the beauty of it is, that someone else has already been collecting it for me. So I'm gonna see them about it, and get them to give it to me, and then I'm gonna dig through it. I'm gonna play with it like a child in a sandbox, except this isnt' just any sandbox. This is a meticulous sandbox, where no cat comes to shit. My sandbox.

---
## [happy-software/shuffle_youtube_playlist](https://github.com/happy-software/shuffle_youtube_playlist)@[e3d3d30644...](https://github.com/happy-software/shuffle_youtube_playlist/commit/e3d3d30644a56d34821dab9834e4501501b59dfa)
#### Friday 2021-05-28 04:00:53 by Hebron George

idk I give up, I hate my life, I'm a terrible programmer; just go back to working at least half way

---
## [XNXXGANG/kernel_realme_trinket](https://github.com/XNXXGANG/kernel_realme_trinket)@[426cc4e27a...](https://github.com/XNXXGANG/kernel_realme_trinket/commit/426cc4e27aa639acf05756094dfb10bafe3eea3d)
#### Friday 2021-05-28 05:20:58 by Jan Alexander Steffens (heftig)

ZEN: Implement zen-tune v4.20 over v4.14-arm64

4.9:
In a surprising turn of events, while benchmarking and testing
hierarchical scheduling with BFQ + writeback throttling, it turns out
that raising the number of requests in queue _actually_ improves
responsiveness and completely eliminates the random stalls that would
normally occur without hierarchical scheduling.

To make this test more intense, I used the following test:

Rotational disk1: rsync -a /source/of/data /target/to/disk1
Rotational disk2: rsync -a /source/of/data /target/to/disk2

And periodically attempted to write super fast with:
dd if=/dev/zero of=/target/to/disk1/block bs=4096

This wrote 10gb incredibly fast to writeback and I encountered zero
stalls through this entire test of 10-15 minutes.

My suspicion is that with cgroups, BFQ is more able to properly sort
among multiple drives, reducing the chance of a starved process.  This
plus writeback throttling completely eliminate any outstanding bugs with
high writeback ratios, letting the user enjoy low latency writes
(application thinks they're already done), and super high throughput due
to batched writes in writeback.

Please note however, without the following configuration, I cannot
guarantee you will not get stalls:

CONFIG_BLK_CGROUP=y
CONFIG_CGROUP_WRITEBACK=y
CONFIG_IOSCHED_CFQ=y
CONFIG_CFQ_GROUP_IOSCHED=y
CONFIG_IOSCHED_BFQ=y
CONFIG_BFQ_GROUP_IOSCHED=y
CONFIG_DEFAULT_BFQ=y
CONFIG_SCSI_MQ_DEFAULT=n

Special thanks to h2, author of smxi and inxi, for providing evidence
that a configuration specific to Debian did not cause stalls found the
Liquorix kernels under heavy IO load.  This specific configuration
turned out to be hierarchical scheduling on CFQ (thus, BFQ as well).

4.10:
During some personal testing with the Dolphin emulator, MuQSS has
serious problems scaling its frequencies causing poor performance where
boosting the CPU frequencies would have fixed them.  Reducing the
up_threshold to 45 with MuQSS appears to fix the issue, letting the
introduction to "Star Wars: Rogue Leader" run at 100% speed versus about
80% on my test system.

Also, lets refactor the definitions and include some indentation to help
the reader discern what the scope of all the macros are.

4.11:
Increase MICRO_FREQUENCY_UP_THRESHOLD from 95 to 85
Increase MIN_FREQUENCY_UP_THRESHOLD from 11 to 6

These changes should help make using CFS feel a bit more responsive when
working with mostly idle workloads, browsing the web, scrolling through
text, etc.

Increasing the minimum frequency up threshold to 6% may be too
aggressive though.  Will revert this setting if it causes significant
battery drain.

4.12:
Make bfq the default MQ scheduler

Reduce default sampling down factor from 10 to 1

With the world eventually moving to a more laptop heavy configuration,
it's getting more important that we can reduce our frequencies quickly
after performing work.  This is normal with a ton of background
processes that need to perform burst work then sleep.

Since this doesn't really impact performance too much, lets not keep it
part of ZEN_INTERACTIVE.

Some time ago, the minimum frequency up threshold was set to 1 by
default, but the zen configuration was never updated to take advantage
of it.

Remove custom MIN_FREQUENCY_UP_THRESHOLD for MuQSS / ZEN_INTERACTIVE
configurations and make 1 the default for all choices.

4.18:
Prefer bfq-mq when available if zen interactive is enabled

The bfq-mq elevator is typically one major kernel version ahead in
optimizations and bug fixes due to early access patches in the
algodev/bfq-mq github repository.  Since these patches are typically low
risk and almost always improve performance and/or increase stability,
prefer bfq-mq over bfq when available.

Switch from MuQSS to PDS-mq.

4.19:
Switch from PDS-mq back to MuQSS

4.20:
During some experimentation to influence MuQSS into consolidating strong
single threaded workloads to single cores, I found that the up_threshold
just ends up forcing all cores to run at a higher frequency.

Instead, raising up_threshold back to defaults (95 with micro sampling),
and raising the sampling down factor to 5, the individual cores MuQSS
selects (typically the first few), tend to properly stick to their max
speed and because they complete their tasks faster, MuQSS selects them
again to for the earliest eligible deadline, causing a reciprocal cycle
that improves single thread performance.

Completely fair scheduler (CFS), never really had this issue, but we'll
leave sampling down factor high with CONFIG_ZEN_INTERACTIVE since it'll
benefit CFS users that want higher performance anyway.

Raise minimum CFS latency to 4ms to match 250hz configs.
Raise minimum MuQSS latency to 4ms to match 250hz configs.

Use [defer+madvise] as default khugepaged defrag strategy:

For some reason, the default strategy to respond to THP fault fallbacks
is still just madvise, meaning stall if the program wants transparent
hugepages, but don't trigger a background reclaim / compaction if THP
begins to fail allocations.  This creates a snowball affect where we
still use the THP code paths, but we almost always fail once a system
has been active and busy for a while.

The option "defer" was created for interactive systems where THP can
still improve performance.  If we have to fallback to a regular page due
to an allocation failure or anything else, we will trigger a background
reclaim and compaction so future THP attempts succeed and previous
attempts eventually have their smaller pages combined without stalling
running applications.

We still want madvise to stall applications that explicitely want THP,
so defer+madvise _does_ make a ton of sense.  Make it the default for
interactive systems, especially if the kernel maintainer left
transparent hugepages on "always".

Reasoning and details in the original patch: https://lwn.net/Articles/711248/

Add a scheduler even to multi-queue block devices:
We prefer interactivity to throughput and want BFQ if possible.

Signed-off-by: Albert I <kras@raphielgang.org>
Signed-off-by: Udit Karode <udit.karode@gmail.com>
Signed-off-by: ZyCromerZ <neetroid97@gmail.com>

---
## [youpiwaza/ansible-install-web-server](https://github.com/youpiwaza/ansible-install-web-server)@[01e17488cd...](https://github.com/youpiwaza/ansible-install-web-server/commit/01e17488cd6dd341b16ca3723cfb78ab3eb3a789)
#### Friday 2021-05-28 06:39:57 by Max

ðŸ³ðŸ‘·MOD: Containers > Enable bitnami (wp & mariadb) debug by default

More log in syslog / interactive mod, no more blank -1 fuck you error codes

---
## [FiestaLake/klte_intelli_kernel](https://github.com/FiestaLake/klte_intelli_kernel)@[3147cbfb74...](https://github.com/FiestaLake/klte_intelli_kernel/commit/3147cbfb7458f5f64fa67abf4d8f8fa3a457522c)
#### Friday 2021-05-28 07:49:07 by Masahiro Yamada

modpost: file2alias: go back to simple devtable lookup

commit ec91e78d378cc5d4b43805a1227d8e04e5dfa17d upstream.

Commit e49ce14150c6 ("modpost: use linker section to generate table.")
was not so cool as we had expected first; it ended up with ugly section
hacks when commit dd2a3acaecd7 ("mod/file2alias: make modpost compile
on darwin again") came in.

Given a certain degree of unknowledge about the link stage of host
programs, I really want to see simple, stupid table lookup so that
this works in the same way regardless of the underlying executable
format.

Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
Acked-by: Mathieu Malaterre <malat@debian.org>
[nc: Omit rpmsg, sdw, fslmc, tbsvc, and typec as they don't exist here
     Add of to avoid backporting two larger patches]
Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
Signed-off-by: Sasha Levin <sashal@kernel.org>
[haggertk: Backport to 3.4. Omit cpu, hdaudio, ipack, mei, mipscdmm,
 rapidio, ulpi]
Signed-off-by: Kevin F. Haggerty <haggertk@lineageos.org>
Change-Id: Ic632eaa7777338109f80c76535e67917f5b9761c
Signed-off-by: Sung Mingi <FiestaLake@protonmail.com>

---
## [SEAPUNK/PennyTS](https://github.com/SEAPUNK/PennyTS)@[fc6db5341d...](https://github.com/SEAPUNK/PennyTS/commit/fc6db5341d548a6dd1170c9f9696c61e3bb8be4e)
#### Friday 2021-05-28 08:11:04 by Lilwiggy

Lol I really do be stupid eh?

One character can fuck your whole ass level system. Check your code!!

---
## [robcore/mn3-lite](https://github.com/robcore/mn3-lite)@[ec458c6e1b...](https://github.com/robcore/mn3-lite/commit/ec458c6e1b4bb68a40b5d146b9cca7a49f9ca5d9)
#### Friday 2021-05-28 08:30:32 by robcore

ps i noticed when checkin out debug fs that pc debug counters is wrapping and completely broken.  upstream is broken too.  then i looked at this platform file and holy shit what a bunch of wasted time and energy for a fucking stat nobody reads and really doesnt affect a thing.

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[5652663271...](https://github.com/mrakgr/The-Spiral-Language/commit/565266327188767eddc9daf753a4736637922702)
#### Friday 2021-05-28 16:29:34 by Marko GrdiniÄ‡

"9:20am. I am up. I slept well and even went to bed a bit earlier than usual. Let me chill a bit and then I'll begin. No replies to the question from yesterday. I guess to hack LN, I'd have to do a C++ extension. That is the only way I'll get my hands on the backwards primitive.

10am. Let me start.

I've pried myself off from the thing I was reading, but I am yawning here and do not feel like programming. Today my goal will be to deal with the replay buffer.

Since my orignal plan was not to do variance matching, but instead to use a moving average in both fast and slow variants, the algo I implemented is out of my expectations. Why did I do this?

I keep thinking about long sequences which will have small batch sizes. No matter what, I want to make sure that weight scaling in the upper layers is factored out. Moving averages are no good as they are reactive. I need a predictive measure instead. So I've been obsessing whether I did the right thing.

I think I did. Yesterday the algorithm just sprang to my mind as if it was calling out to me. It composes particularly well too. And things which compose well are usually right.

The weight scale elimination would be an effect of Adam and the moving average scheme anyway once the batch sizes get large enough. So doing it like this will have no disadvantage.

The LR of 2 ** -13 and 2 ** -14 might seem low, but yesterday while I was printing out the reward signals, I saw that they summed up into the 100s and even a 1000. This is because of the large batch size. Had I been doing online learning, it would be equivalent of 2 ** -4 or 2 ** -5.

It is not low at all.

10:25am. I am thinking whether I should do the moving average rebalancing that I described in the PL update.

That kind of rebalancing only makes sense when the estimates are good. When they are bad, they are more likely to destabilize training. I learned this while working on KFAC. It is not smart to move more that 10x in any direction. I needed to put in a large identity so the training would not blow up.

Yes it could be benefit. But unlike with the predictive scheme for the gradients that I implemented yesterday, having to do the rebalancing means I have to mess with moving averages. Suddenly I have an extra hyperparameter in need of optimization for every moving average.

10:30am. No just forget that. You can make everything work with more hyperparam tuning. But it will be brittle. The way I am doing things now will result in consistent credit assignment.

If I am missing something that would be wrapping around the log softmax and the LN ops. I am going to have to deal with that. If I want work to do, that is what I should focus on.

10:35am. So with that decided, let me think about the replay buffer.

10:40am. Actually for the log_softmax I won't need the variance matching trick. The rule is `grad - prob * sum grad`. But the way scaling works will ensure that the sum of the gradients...well, no. When calculating the means, I am multiplying by the probabilities rather than taking the average. So the sum won't necessarily be 0.

I'll have to do a wrapping after all.

How much work this is. What a bother. I'll have to jack the transformer too.

Ok, the replay buffer. Let me focus on that again.

I can think of different ways of doing it.

11:35am. No.

All the replay buffer amounts to is a complicated gradient clipping scheme. I do not feel like filling it up at all now that the moment of truth is here. Checkpointing + learning rate tuning should do the trick. I should just not worry about the outliers that could wreck training. I should depend on checkpointing.

Being able to generate perfectly accurate synthetic data is better than any kind of replay buffer scheme. The brain needs complicated memory mechanisms because it does not have direct access to the game, but in poker I should simplify things to the utmost.

12:10pm. I am still thinking about it. Yes, ditching the replay buffer will make things a lot simpler. For the transformer, I really will have to make my own extensions that do gradient variance matching. But that is fine. That is just a part and parcel of attaining skill in ML. Of course at some point I'd need to get dirty with the mainstream framework. This is also an ideal ramp to build up Spiral as well.

12:15pm. Replay buffer + Leduc. The problem with the replay buffer, is that it will push training towards stale data. That is the disadvantage of it. And fundamentally, even though I could turn things into +1 and -1, that in the limit is still the same as training normally with a finely tuned learning rate. Forget that.

12:20pm. I spent an immense amount of time thinking about this, but in the end it turned out to be nothing more than mental play. And the side idea that I had is the one which rose to the top.

2:10pm. Let me chill for a while longer.

3:25pm. I am still thinking about it.

3:30pm. GGGGGGGGGGGGGGGGGGGGGGGGGGGAAAAAAAHHHHHH!!!

Damn it! The replay buffer idea that I was so proud of seems like nothing right now. Is it really that different from simply lowering the learning rate while reweighting the gradients using the CFR formula? I don't think so.

3:35pm. Do I start? I have no choice really.

At some point, I swear I will figure out how proper long term reinforcement learning should work.

3:50pm. I am still thinking about it. Right now I am just as obsessed as I was during my studies. What was even the point of last month.

5:10pm. I guess it is one of those days where I am laid out in thought. The decision to drop the replay buffer was a heavy blow to me. Last month I was in idea generation mode, but now that I am actually programming, it gives me a different perspective on what is right and what is not. Even if I think of making a bigger replay buffer and sampling from that, it is still the same thing and gradient reweighting.

There is no point to the replay buffer at all when you have a perfect simulator. The brain needs it because it has to do a lot of processing, but the poker simulator I have here is as close to the essence of the game as it could be.

5:20pm. Let me read some papers by Lanctot and I will call it a day. I want to see what came of those actor critics he worked on.

Tomorrow I will just calculate the regret probabilities and reweight the gradients by them.

https://arxiv.org/abs/1810.09026
Actor-Critic Policy Optimization in Partially Observable Multiagent Environments

I remember reading this paper 2.5 years ago. Here they don't use the log probability, but the regular one. This might be the reason for the underperformance of QPG in some cases. But the way they just threeshold the gradients for RPG is interesting too.

It might be worth going for that. It would free me of the need to do both log_softmax and then an exp of it. Plug given that I am doing variance normalization this would fit better into the overall scheme.

6:10pm. I doesn't seem like the algorithms in these papers even divide by the opponent reach probability. I think that is what I will do too if I start having trouble with stability. I should get ready for that. But I do not think it will come to it.

6:15pm. I should be able to grasp it.

Complicated plans were never necessary. I have the responsibility to think, but going so much in depth was not needed. Going as far as spending years making a ML library and a language was not needed for the sake of simply making a player.

What was needed now and in the past was to show my determination

Let me stop here. Tomorrow I will just do the weighting and then move forward from there. Hell, once I do this, I am already at the threeshold of moving to Holdem.

It is a piece of cake to make progress when all you are doing is removing complicated parts. No emails yet.

Yeah, if I want transformers, I am going to have to hack softmax and layer norm so I can get variance matching activations.

But that gives me work worth doing.

6:20pm. Today I just could not do any programming. I wanted to, but the decision to ommit the replay buffer was just so huge that I could pivot to it easily. I spent the entire day deep in thought. Either way, tomorrow I am going to complete the first elite neural player and have it engage in self play.

I am going to get rid of the shared key dropout idea in favor of stochastic weight averaging as it will be simpler to deal with. No matter how much work I did, I am going to get rid of it all in favor of doing the simplest possible thing. So much thought and foresight, only to make a short step forward.

This is too easy. It won't take long at all until I am done."

---
## [matheustavares/git](https://github.com/matheustavares/git)@[c35b21dfd0...](https://github.com/matheustavares/git/commit/c35b21dfd025f5860b1b524f82a79973358b7002)
#### Friday 2021-05-28 17:33:46 by Jeff King

ci: allow per-branch config for GitHub Actions

Depending on the workflows of individual developers, it can either be
convenient or annoying that our GitHub Actions CI jobs are run on every
branch. As an example of annoying: if you carry many half-finished
work-in-progress branches and rebase them frequently against master,
you'd get tons of failure reports that aren't interesting (not to
mention the wasted CPU).

This commit adds a new job which checks a special branch within the
repository for CI config, and then runs a shell script it finds there to
decide whether to skip the rest of the tests. The default will continue
to run tests for all refs if that branch or script is missing.

There have been a few alternatives discussed:

One option is to carry information in the commit itself about whether it
should be tested, either in the tree itself (changing the workflow YAML
file) or in the commit message (a "[skip ci]" flag or similar). But
these are frustrating and error-prone to use:

  - you have to manually apply them to each branch that you want to mark

  - it's easy for them to leak into other workflows, like emailing patches

We could likewise try to get some information from the branch name. But
that leads to debates about whether the default should be "off" or "on",
and overriding still ends up somewhat awkward. If we default to "on",
you have to remember to name your branches appropriately to skip CI. And
if "off", you end up having to contort your branch names or duplicate
your pushes with an extra refspec.

By comparison, this commit's solution lets you specify your config once
and forget about it, and all of the data is off in its own ref, where it
can be changed by individual forks without touching the main tree.

There were a few design decisions that came out of on-list discussion.
I'll summarize here:

 - we could use GitHub's API to retrieve the config ref, rather than a
   real checkout (and then just operate on it via some javascript). We
   still have to spin up a VM and contact GitHub over the network from
   it either way, so it ends up not being much faster. I opted to go
   with shell to keep things similar to our other tools (and really
   could implement allow-refs in any language you want). This also makes
   it easy to test your script locally, and to modify it within the
   context of a normal git.git tree.

 - we could keep the well-known refname out of refs/heads/ to avoid
   cluttering the branch namespace. But that makes it awkward to
   manipulate. By contrast, you can just "git checkout ci-config" to
   make changes.

 - we could assume the ci-config ref has nothing in it except config
   (i.e., a branch unrelated to the rest of git.git). But dealing with
   orphan branches is awkward. Instead, we'll do our best to efficiently
   check out only the ci/config directory using a shallow partial clone,
   which allows your ci-config branch to be just a normal branch, with
   your config changes on top.

 - we could provide a simpler interface, like a static list of ref
   patterns. But we can't get out of spinning up a whole VM anyway, so
   we might as well use that feature to make the config as flexible as
   possible. If we add more config, we should be able to reuse our
   partial-clone to set more outputs.

Signed-off-by: Jeff King <peff@peff.net>
Signed-off-by: Junio C Hamano <gitster@pobox.com>

---
## [anna-albright/TinDog](https://github.com/anna-albright/TinDog)@[5066df925b...](https://github.com/anna-albright/TinDog/commit/5066df925bbb97051a1ff4da9507ff44a2a495a2)
#### Friday 2021-05-28 21:04:17 by anna-albright

Update README.md

Find the True Love of Your Dog's Life Today.

---

# [<](2021-05-27.md) 2021-05-28 [>](2021-05-29.md)

