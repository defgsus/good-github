# [<](2021-02-19.md) 2021-02-20 [>](2021-02-21.md)

2,130,677 events, 1,188,870 push events, 1,720,905 commit messages, 109,760,770 characters


## [mimiottawa/Week-4-](https://github.com/mimiottawa/Week-4-)@[e7affbf321...](https://github.com/mimiottawa/Week-4-/commit/e7affbf32117eab8dea7671a7ed822470cc93084)
#### Saturday 2021-02-20 00:31:44 by mimiottawa

Week 4   Choose my own DH Museum Adventure

Choose your own DH Museum Adventure
Miria Munoz

I remember when I was 7 years old, my mother took my brother and I to see ‘The National Museum of Rio de Janeiro’ (Brazil). I remember coming home and being unable to fall asleep at night with all that overload of information in my head. I also remember feeling sorry we went there for just one day! It was simply not enough. I wanted more time to see the objects from close and observe each detail. After that day, every time we would visit my grand-parents in Rio de Janeiro, we would drive my mother nuts, trying to convince her to take us to the museum again. 
Fast forwarding 30 years, one day I started to get a bunch of ‘Whatsapp’ messages from my relatives in Brazil, containing videos of a horrible fire… and the fire… was at my favourite museum ever. I felt a mixture of feelings, from sadness to anger as I just could not accept that we had lost an entire museum due to administrative negligence. Apparently, the fire safety certificate was expired since 2014.
Thus, when I started to ponder upon the possibility of digitalizing museum collections, the first thought that came to my mind was: my favourite museum could have been saved at least from a ‘digital’ perspective. It had a collection of almost 20 million artifacts that had been collected in a period of 200 years. 
Information technology comes as a tool to preserve, at least digitally, so many precious confections from the past. It also facilitates the collection, analysis and visualization of artifacts; and helps in terms of public participation and even training methods.
Moreover, it is valid to remember how technology facilitates the democratization of archaeology as a discipline among communities, where people who would never have had a chance to be exposed to such field, end up being able to do so. Furthermore, digital collections can be an excellent tool for educators, specially now under this post-Covid era where ‘in person field trips’ are becoming extremely challenging to be arranged. 
Therefore, I would like to explore the 3 following readings:
Bivens, Joy, and Ben Garcia, Porchia Moore, nikhil trivedi, Aletheia Wittman. 2019. ‘Collections: How We Hold the Stuff We Hold in Trust’ in MASSAction, Museums As Site for Social Action, toolkit, https://static1.squarespace.com/static/58fa685dff7c50f78be5f2b2/t/59dcdd27e5dd5b5a1b51d9d8/1507646780650/TOOLKIT_10_2017.pdf

Houghton, Bernadette. 2016. Preservation Challenges in the Digital Age. D-Lib Magazine July/August 2016. 22:7/8 http://www.dlib.org/dlib/july16/houghton/07houghton.html

Trofanko, Brenda. 2014.“Reconsidering the Educational Promise of Public History Exhibits” in K. Kee, ed. Pastplay. https://www.jstor.org/stable/j.ctv65swr0.16

The first reading will be addressing how we store collections. The second reading will be dealing with challenges we face when it comes to digitalizing collections. And the third one will be covering the benefits of museum digitalization for the educational arena. Please note, I am opened to suggestions!


To watch:
https://www.youtube.com/watch?v=vT8dNfsaBsA
To explore: 
https://artsandculture.google.com/exhibit/descubra-o-museu-nacional/5gJywQA_-ABfJw?hl=pt

---
## [darkhomme/ryo-currency](https://github.com/darkhomme/ryo-currency)@[47a56abf7d...](https://github.com/darkhomme/ryo-currency/commit/47a56abf7db1c12e6f92dc053c9bec01fa1c9b6d)
#### Saturday 2021-02-20 00:43:22 by darkhomme

fix monero DOS funny incident

they were being fucked by someone creating huge bin strings as fluffy args and sending them around with a new fluffy block header with a not seen before ID. Funny.
shitty epee binary storage serialisation reserves all the memory in the world before the validity of the block, tx whatever is checked. now it doesnt

---
## [Kazkin/sojourn-station](https://github.com/Kazkin/sojourn-station)@[c6c50cd232...](https://github.com/Kazkin/sojourn-station/commit/c6c50cd2326eeb13027610b7aba624d73d8ddb86)
#### Saturday 2021-02-20 00:44:02 by Kazkin

Glock runtime fix plus cigs

-Hacking a vendor now reveals ComRed packets, communist brand cigaretts.
-Cigar cases now have a nice tiny sprite instead of the giant ugly ass sprite it had before.
-Lonestar brand cigarettes are now available in smoke vendors, due to not being an import it is now the cheapest smoke available.
-Church brand cigerettes are now available in the church vendors. Slightly expensive, but each pack has 6 uniquely flavored cigarettes.
-Fixes a runtime in glocks and an issue with sprites for said glocks.

---
## [stamp-protocol/core](https://github.com/stamp-protocol/core)@[53cc083c12...](https://github.com/stamp-protocol/core/commit/53cc083c12f8bbfd84c4e78de268987a456c69ed)
#### Saturday 2021-02-20 00:48:39 by Andrew Danger Lyon

Keychain and DAG stuff

- rewriting a bunch of keychain crap. remove KeyID as a signature ID
  from subkeys, and adding a new KeyID object which is effectively a
  wrapper around a public key
- subkeys are now referenced by their name field when
  revoking/deleting/etc (they no longer have ids, remember?)
- implementing all of the outer transaction functions for the DAG and
  some of the inner (ie, `apply_transaction()`) implementations. making
  huge progress here
- fleshing out a lot of the recovery system (kind of a rewrite). the DAG
  has made it almost impossibly simple to work with. there's still a lot
  to do, but it comes down to actual verification of the
  policy/requests, as opposed to having some weird, shitty, half-baked
  recovery transaction log like i was trying to do before.

overall, the DAG has simplified a bunch of stuff and i've been going
through the project with a medium-tooth comb rewriting things that no
longer make sense. i think we're at the point where like 90% of it is in
a good spot, and the rest will come moving forwrd. i want to really sit
down and focus on the unit tests for the identity/dag system, then work
on the recovery policy junk. there's a lot of hidden complexity in the
recovery...it's much simpler now than it was, but there is much to do.

---
## [mimiottawa/Week-5-](https://github.com/mimiottawa/Week-5-)@[f238473bda...](https://github.com/mimiottawa/Week-5-/commit/f238473bda17ff7916502a58e729860df487759b)
#### Saturday 2021-02-20 02:12:20 by mimiottawa

Week 5

Week 5 
Collaboration reading

Jenna – Cultural heritage and its institutions
My reflections

Museum institutions should consider the participation of local communities when managing their collections; in this way, their projects have a higher chance of being culturally sensitive. 
When museums try to understand and adapt to the needs of local communities, a spirit of cooperation is developed among community members and stakeholders. Participatory mapping is another way for museums to promote culture inclusiveness. It gives the community a chance to express their perceptions and social constructions; and also to demonstrate what cultural/historic pieces they consider significant or not. 


Jacob – The political rationality of the museum
My reflections
Are museums supposed to only entertain? Inspire? Educate?
Or…
Do museums have the political duty to change the world?
For instance…
During Trump’s Inauguration, the Queen’s Museum closed its doors in protest. 
Unfortunately, many museums only serve the purpose to keep collections from the ‘culture capital’ that belongs to elites only. 
Alternatively, the museum can become a way for the middle class to appreciate art, since they cannot buy art.
Therefore, the museum might become a tool of inequality where everything is too white or too masculine.
Many museums are managed by intolerant scholars who promote a political monoculture of the humanities. 
Thus, activists argue how museums should focus on groups that are traditionally neglected. 
Museums should promote social justice; they are not supposed to be condescending with their visitors. 

Kieran  - The whole story, and then some: ‘digital storytelling’ in evolving museum practice
My reflections

Nowadays, museums are having to face a lot of competition with technology. Museums entertain… Technology entertains… Museums are educational… Technology is educational… Museums are engaging… Technology is engaging… 
Moreover, storytelling is one of the main mandates of museums. Their collections exhibit stories related to arts, music, literature, science and etcetera.  Nevertheless, technology can also be a powerful tool for storytelling; being even more interactive and adaptive to the individual. 
So… what could museums do to compete with technology?
IF YOU CANNOT BEAT THEM – JOIN THEM.
Museums are using technology, so the visitor is not only a passive observer, but also an active participant. 
In addition, Covid restrictions have caused people to explore virtual alternatives as well. 
For instance, the National Gallery of Canada is presenting online access to some of the Gallery’s Canadian, Indigenous and international collections. Plus, kids and adults can get creative with online activities.

Please have a look:
https://www.gallery.ca/virtual-ngc
https://www.gallery.ca/virtual-ngc#get-creative

---
## [mimiottawa/Week-5-](https://github.com/mimiottawa/Week-5-)@[8d5021978d...](https://github.com/mimiottawa/Week-5-/commit/8d5021978d3ae4ec880c68284b5980305e4acbbb)
#### Saturday 2021-02-20 02:25:30 by mimiottawa

Week 5  Jenna Jacob Kieran

Week 5 
Collaborative  reading

Jenna – The origins of the public museum 

My reflections

The real meaning of 'public' ... 

Museum institutions should consider the participation of local communities when managing their collections; in this way, their projects have a higher chance of being culturally sensitive. 
When museums try to understand and adapt to the needs of local communities, a spirit of cooperation is developed among community members and stakeholders. Participatory mapping is another way for museums to promote culture inclusiveness. It gives the community a chance to express their perceptions and social constructions; and also to demonstrate what cultural/historic pieces they consider significant or not. 



Jacob – The political rationality of the museum
My reflections

Are museums supposed to only entertain? Inspire? Educate?
Or…
Do museums have the political duty to change the world?
For instance…
During Trump’s Inauguration, the Queen’s Museum closed its doors in protest. 
Unfortunately, many museums only serve the purpose to keep collections from the ‘culture capital’ that belongs to elites only. 
Alternatively, the museum can become a way for the middle class to appreciate art, since they cannot buy art.
Therefore, the museum might become a tool of inequality where everything is too white or too masculine.
Many museums are managed by intolerant scholars who promote a political monoculture of the humanities. 
Thus, activists argue how museums should focus on groups that are traditionally neglected. 
Museums should promote social justice; they are not supposed to be condescending with their visitors. 

Kieran  - The whole story, and then some: ‘digital storytelling’ in evolving museum practice
My reflections

Nowadays, museums are having to face a lot of competition with technology. Museums entertain… Technology entertains… Museums are educational… Technology is educational… Museums are engaging… Technology is engaging… 
Moreover, storytelling is one of the main mandates of museums. Their collections exhibit stories related to arts, music, literature, science and etcetera.  Nevertheless, technology can also be a powerful tool for storytelling; being even more interactive and adaptive to the individual. 
So… what could museums do to compete with technology?
IF YOU CANNOT BEAT THEM – JOIN THEM.
Museums are using technology, so the visitor is not only a passive observer, but also an active participant. 
In addition, Covid restrictions have caused people to explore virtual alternatives as well. 
For instance, the National Gallery of Canada is presenting online access to some of the Gallery’s Canadian, Indigenous and international collections. Plus, kids and adults can get creative with online activities.

Please have a look:
https://www.gallery.ca/virtual-ngc
https://www.gallery.ca/virtual-ngc#get-creative

---
## [ccodwg/Covid19Canada](https://github.com/ccodwg/Covid19Canada)@[831cdb3895...](https://github.com/ccodwg/Covid19Canada/commit/831cdb3895cb54960a71fe43fe6a6f32bd1ba96e)
#### Saturday 2021-02-20 02:38:29 by Jean-Paul R. Soucy

New data: 2021-02-19: See data notes.

Revise historical data: cases (BC, MB, ON, SK); mortality (MB, ON).

Note regarding deaths added in QC today: “14 new deaths, for a total of deaths of 10,278: 1 death in the last 24 hours, 12 deaths between February 12 and February 17, 1 death before February 12.” We report deaths such that our cumulative regional totals match today’s values. This sometimes results in extra deaths with today’s date when older deaths are removed.

Recent changes:

2021-01-27: Due to the limit on file sizes in GitHub, we implemented some changes to the datasets today, mostly impacting individual-level data (cases and mortality). Changes below:

1) Individual-level data (cases.csv and mortality.csv) have been moved to a new directory in the root directory entitled “individual_level”. These files have been split by calendar year and named as follows: cases_2020.csv, cases_2021.csv, mortality_2020.csv, mortality_2021.csv. The directories “other/cases_extra” and “other/mortality_extra” have been moved into the “individual_level” directory.
2) Redundant datasets have been removed from the root directory. These files include: recovered_cumulative.csv, testing_cumulative.csv, vaccine_administration_cumulative.csv, vaccine_distribution_cumulative.csv, vaccine_completion_cumulative.csv. All of these datasets are currently available as time series in the directory “timeseries_prov”.
3) The file codebook.csv has been moved to the directory “other”.

We appreciate your patience and hope these changes cause minimal disruption. We do not anticipate making any other breaking changes to the datasets in the near future. If you have any further questions, please open an issue on GitHub or reach out to us by email at ccodwg [at] gmail [dot] com. Thank you for using the COVID-19 Canada Open Data Working Group datasets.

- 2021-01-24: The columns "additional_info" and "additional_source" in cases.csv and mortality.csv have been abbreviated similar to "case_source" and "death_source". See note in README.md from 2021-11-27 and 2021-01-08.

Vaccine datasets:

- 2021-01-19: Fully vaccinated data have been added (vaccine_completion_cumulative.csv, timeseries_prov/vaccine_completion_timeseries_prov.csv, timeseries_canada/vaccine_completion_timeseries_canada.csv). Note that this value is not currently reported by all provinces (some provinces have all 0s).
- 2021-01-11: Our Ontario vaccine dataset has changed. Previously, we used two datasets: the MoH Daily Situation Report (https://www.oha.com/news/updates-on-the-novel-coronavirus), which is released weekdays in the evenings, and the “COVID-19 Vaccine Data in Ontario” dataset (https://data.ontario.ca/dataset/covid-19-vaccine-data-in-ontario), which is released every day in the mornings. Because the Daily Situation Report is released later in the day, it has more up-to-date numbers. However, since it is not available on weekends, this leads to an artificial “dip” in numbers on Saturday and “jump” on Monday due to the transition between data sources. We will now exclusively use the daily “COVID-19 Vaccine Data in Ontario” dataset. Although our numbers will be slightly less timely, the daily values will be consistent. We have replaced our historical dataset with “COVID-19 Vaccine Data in Ontario” as far back as they are available.
- 2020-12-17: Vaccination data have been added as time series in timeseries_prov and timeseries_hr.
- 2020-12-15: We have added two vaccine datasets to the repository, vaccine_administration_cumulative.csv and vaccine_distribution_cumulative.csv. These data should be considered preliminary and are subject to change and revision. The format of these new datasets may also change at any time as the data situation evolves.

https://www.quebec.ca/en/health/health-issues/a-z/2019-coronavirus/situation-coronavirus-in-quebec/#c47900

Note about SK data: As of 2020-12-14, we are providing a daily version of the official SK dataset that is compatible with the rest of our dataset in the folder official_datasets/sk. See below for information about our regular updates.

SK transitioned to reporting according to a new, expanded set of health regions on 2020-09-14. Unfortunately, the new health regions do not correspond exactly to the old health regions. Additionally, the provided case time series using the new boundaries do not exist for dates earlier than August 4, making providing a time series using the new boundaries impossible.

For now, we are adding new cases according to the list of new cases given in the “highlights” section of the SK government website (https://dashboard.saskatchewan.ca/health-wellness/covid-19/cases). These new cases are roughly grouped according to the old boundaries. However, health region totals were redistributed when the new boundaries were instituted on 2020-09-14, so while our daily case numbers match the numbers given in this section, our cumulative totals do not. We have reached out to the SK government to determine how this issue can be resolved. We will rectify our SK health region time series as soon it becomes possible to do so.

---
## [InfoTeddy/VVVVVV](https://github.com/InfoTeddy/VVVVVV)@[79893e2484...](https://github.com/InfoTeddy/VVVVVV/commit/79893e2484d4170eeb61df42fed5c4521b2d8735)
#### Saturday 2021-02-20 04:11:59 by Misa

Move all editor loop functions to separate file

This moves editorrenderfixed(), editorrender(), editorinput(),
editorlogic(), and their associated functions to a new file named
editorloop.cpp - which is exactly what it says on the tin; it stores all
the functions related to the actual in-game editor loop.

editor.cpp now just contains custom level support, rather than the
actual editor logic. Yeah, it's kind of stupid, the game put both custom
level functions and the in-game editor into one file named editor.cpp -
but, now they're two files, and one is clearly the editor and the other
is clearly just custom level support.

editor.cpp is now no longer massive - it's been reduced from 5000 lines
to 2000 lines, with the 3000 lines going to editorloop.cpp.

By the way, I also pulled some Git magic off to preserve Git-blame
history when moving all the functions to the new file. I created two
different branches - one where I renamed editor.cpp to editortemp.cpp;
one where I deleted editor.cpp and added editorloop.cpp - then created a
third branch and octopus-merged them together (that's when you merge
with more than two branches at once; three branches in this case); then
I renamed editortemp.cpp back to editor.cpp, and removed the editor loop
functions from editor.cpp (which is this commit).

Unfortunately, if you squash everything together, then the magic gets
lost and Git will blame me for the entirety of editorloop.cpp (which is
not what I want).

---
## [InfoTeddy/VVVVVV](https://github.com/InfoTeddy/VVVVVV)@[132314ae93...](https://github.com/InfoTeddy/VVVVVV/commit/132314ae9384561d4cc1af2eb2c9f96b4a55c1b1)
#### Saturday 2021-02-20 04:54:15 by Misa

Move all editor loop functions to separate file

This moves editorrenderfixed(), editorrender(), editorinput(),
editorlogic(), and their associated functions to a new file named
editorloop.cpp - which is exactly what it says on the tin; it stores all
the functions related to the actual in-game editor loop.

editor.cpp now just contains custom level support, rather than the
actual editor logic. Yeah, it's kind of stupid, the game put both custom
level functions and the in-game editor into one file named editor.cpp -
but, now they're two files, and one is clearly the editor and the other
is clearly just custom level support.

editor.cpp is now no longer massive - it's been reduced from 5000 lines
to 2000 lines, with the 3000 lines going to editorloop.cpp.

Unfortunately, this loses all of the blame history in editorloop.cpp. I
tried to preserve it using some Git hacks; but unfortunately, they are
just merely Git hacks, and they also make rebasing impossible. So I just
decided to drop the whole effort. At least you can do `git blame -C` if
you want the actual history.

---
## [mfrancis2337/CPlusPlus-Learning](https://github.com/mfrancis2337/CPlusPlus-Learning)@[fea3a4e690...](https://github.com/mfrancis2337/CPlusPlus-Learning/commit/fea3a4e69064f71de84d1f89f378780e2e8b0f96)
#### Saturday 2021-02-20 05:21:37 by M. Francis

Import 647 files as dependencies yay

Imported SDL2 and SSDL. I also edited .gitignore to include Visual Studio files and added the next C++ file. This totals to 649 changes total. It's midnight and I'm still struggling with errors I can't seem to find fixes for online or in my book. And so I sit here on the uncomfortable 10-year-old office chair I have in my room and type here on my laptop sipping a cup of water wondering how I got here and if I should go to bed. The answer is I am. Good night.

---
## [Sekomer/hermes](https://github.com/Sekomer/hermes)@[7139152571...](https://github.com/Sekomer/hermes/commit/7139152571fd04b8c0954af2ca0f9669794dbb6d)
#### Saturday 2021-02-20 10:50:39 by Alperen Serkan Aksöz

update README

Hi Sebastian! 
I watched nearly all of your videos and am a huge fan of yours, I love C language as you do and I want to contribute on open projects but literally have no experience and don't know where to start :( 
I changed README files markdown a little bit, If you allow me to contribute, I'd be honored. Later I will also check other files too. Thanks!

---
## [deszip/themis](https://github.com/deszip/themis)@[1905a77aeb...](https://github.com/deszip/themis/commit/1905a77aebb93b88a0de0972b9181f751ca45eb1)
#### Saturday 2021-02-20 14:38:30 by Alexei Lozovsky

Fuzzing with american fuzzy lop (#364)

* Fuzzing with american fuzzy lop

This one has been on my mind for quite a while and finally I've managed
to get it going. Let's throw a security fuzzer into the breach and see
what it finds for us. (Oh boy, some crashes does it find! I've got 7 of
them reported in 5 minutes.)

Here we add "american fuzzy lop" because I like how little tweaking and
configuration it requires. Basically, you feed it an example input data
and it uses -- high technology from 1960s -- artificial intelligence (!)
and machine learning (!!) to work through big data queue (!!!) of tests
trying to invent ones that crash your application.

You can read user manual (of a sort) in the README. In order to run the
fuzzer you need to install it and then do "make fuzz FUZZ_BIN=something"
to build and run the tools.

Implementation-wise, we make a custom build of Themis (by recursively
calling make because that's the easiest way) which is instrumented by
a special compiler. This allows the fuzzer to monitor the behavior of
Themis and the tools and see how the input influences the control flow
in the program.

Only two tools are implemented for starters:

  - Round-trip through a Secure Cell in sealing mode. This makes sure
    that user input cannot be used to produce a secure container which
    crashes the application during processing.

  - Decrypting a presumable container with Secure Cell in sealing mode.
    This makes sure that data corruption cannot cause the application
    to crash when receiving messages encrypted with Secure Cell.

It should be easy to add more tools in the future. For example, Secure
Message could use a fuzzer for key files. Other components use the same
containers as Secure Cell so fuzzing the encrypted data may not be so
fruitful, but it may still be worth a shot.

I admit that error handling and memory management in the tools is a bit
sloppy but that should be fine unless it crashes in unexpected places.
It's just too verbose to do *everything* right.

Finally, we add a "make fuzz" step to the CI build in order to keep up
with API changes. We don't run the fuzzing automatically but let's at
least ensure that the tools can be compiled. (We can also check that
they can handle the input data but it's not *that* important.)

* Add a symlink to "tools" for libthemis-src crate

The makefile unconditionally includes "tools/afl/fuzzy.mk" so it has
to be available during Themis builds. Rust wrapper's "libthemis-src"
embeds Themis source code, but tries to minimize the footprint by
including only bare necessities. This new file is one of them.

(And here I started wondering whether this setup is a good one...
Maybe we should simply symlink the whole Themis repo directory and
deal with it by adding a special script for publishing crates.)

---
## [deszip/themis](https://github.com/deszip/themis)@[37e168d655...](https://github.com/deszip/themis/commit/37e168d655121a087c3e46c5ed9f69af8a8a4ae3)
#### Saturday 2021-02-20 14:38:30 by Alexei Lozovsky

Teach CircleCI to build Rust wrapper (#353)

* Install Rust on CircleCI

We install Rust using rustup which is the most common way to do this.
This gets us the current stable version (updated if necessary). I would
like to use the Debian package for this, but Rust is not stable enough
for that yet. The language is evolving, the libraries adopt new compiler
features, and the Rust team supports only the latest stable version.
Therefore we go for downloading stuff from the Internet via rustup.rs.

In addition to core Rust toolchain we install a couple of tools that we
use for quality assurance: code formatter, static analyzer, and dead
link checker. "deadlinks" take a while to compile and install, sadly.

In general, Rust tooling *looooves* to pull lots of stuff from the net
so we cache the downloaded packages and installed binaries if possible.
This cuts down compilation time considerably, but there's still way to
go. We do a clean build of Themis so we take our time to compile all
the dependencies of dependencies...

Aside from rustc we install a couple of native dependencies. pkg-config
is required to locate Themis after installation. Clang is a dependency
of bindgen crate which is used to generate bindings for Themis.

* Run rust-themis tests during CI builds

Add "test_rust" target to Makefile which runs rust-themis tests similar
to other language wrappers. We skip the tests if Rust does not seem to
be installed on the system.

The test suite may be a bit large. We'll see how much time it takes
to run on the CI. It replicates the current setup used by the original
rust-themis on TravisCI [1]

- Disallow any warning by any tool (the compiler and alike)
- Ensure consistent code formatting with rustfmt
- Avoid common mistakes by running static analysis with Clippy
- Check both dynamic and static linkage to core Themis library
- Compile and run automated test suite for all packages
- Check that API documentation is likely to build on docs.rs
- Check that documentation does not contain dead URLs

[1]: https://github.com/ilammy/rust-themis/blob/master/.travis.yml

* Drop "themis" symlink in libthemis-src

We change the build system in the next commit. This removal is done
separately because some older versions of git do not handle symlinks
well during rebases and I got tired of having to manually resolve
nonexistent merge conflicts every time I update the branch.

* Fix vendoring of Themis source code

libthemis-src needs to embed the source code of core Themis library.
Previously, we did this by using git submodules when rust-themis was
developed in a separate repository. It was... satisfactory. (Not great
because "cargo package" refused to package "themis" directory as it
contained a Cargo.toml file.)

git submodule has been converted into a symlink to the root directory
when rust-themis was merged into the main repository. This should have
worked well, but it didn't. First of all, the "copy_dir" crate does not
copy symlinks and this breaks the build. Second, the root directory now
contains Cargo's target directory which is huge and copying it as well
significantly increases build times.

In order to reduce build time let's copy only the essential files.
Replace the quick-and-dirty "themis" symlink with a more refined
selection of files necessary to compile core Themis library. We don't
really need all the wrappers, documentation, etc. Just copy the source
code, the build system, and some other files which are unconditionally
included by the top-level makefile. (This should also significantly
reduce the size of the crate.)

Next, drop the "copy_dir" crate and replace it with our ad-hoc utility
for copying files that handles symlinks the way we need it. It's not
a general-purpose utility but it does its job well enough.

And while we're here, redirect output of "make install" to /dev/null.
It's annoying to look at when we run tests. Ideally we would like to
redirect it into stderr, but that's not possible with Command's API.
(We keep the original stderr though, so we'll see compilation errors
just fine.)

Unfortunately, running "cargo package" for libthemis-src is still kinda
broken because Cargo does not handle symlinks. This is a known issue.
Currently we will have to manually copy the source directory to resolve
all the symlinks and just get a distributable files. Annoying, yes.

* Persist Cargo PATH for later build steps

CircleCI runs each build step in a separate shell. Therefore we lose
environment variables set by "source ~/.cargo/env" during rustup
installation.

Append the environment configuration file to $BASH_ENV in order to
set the PATH correctly for next commands. This is a special file
read by CircleCI's bash.

* Drop secure zeroing test

It's actually not alright.

This test is flaky and is not really safe to run. It happens to behave
itself in single-threaded environment, but in reality the memory freed
from the keys is instantly reused by the allocator and does not contain
zeros anymore. It's not even guaranteed to be actually allocated.

I guess it's fine to trust zeroize to do its thing and do not verify
that memory is indeed zeroed out. Let's drop this test because flaky
CI builds are bad.

* More robust tests for data corruption

Writing some random value at arbitrary offset sometimes is not enough
to break Secure Cell. Internally, Secure Cell uses randomized keys
so the ciphertext may actually happen to contain "42" at exactly that
offset. Or so it seems to be when the tests are run by CircleCI.

Try flipping the bits instead of writing a constant. We do so in
documentation tests and this should be a better way to simulate
corrupted data.

* Clarify the source of "curl | sh" instructions

I'm not a fan of this kind of installation that downloads random stuff
from the Internet, but we'll have to live with it for a while, until
Rust is stable enough for disto-packaged versions not becoming outdated
after half a year.

Let's at least add a comment about the source of the spell below so
that it can be properly audited.

* Avoid copying source code during libthemis-src build

It turns out that we can avoid copying Themis source code. The build
system supports out-of-source builds with BUILD_PATH environment
variable. Just point it into OUT_DIR and Cargo should be fine.

Remove all the copying utilities and blatant lies about Themis from
the comments.

We still keep the "optimized" symlinks instead of reverting back to
a single one pointing to the root directory of the repo. This way
it's more friendly towards crate packaging: "themis" directory won't
have a Cargo.toml inside it and the whole crate size is smaller.

---
## [deszip/themis](https://github.com/deszip/themis)@[3436c392cc...](https://github.com/deszip/themis/commit/3436c392cc61208e2546455c50c17ad35bd35df1)
#### Saturday 2021-02-20 14:38:30 by Alexei Lozovsky

Rust integration tests (#356)

* Integration tools for Rust wrapper

Add tools written in Rust which are usable by integration tests.

These are simply command-line utilities with a particular interface.
They are somewhat similar to the examples from docs/rust/examples
(keygen in particular) and may also serve as examples of Rust-Themis
usage. The tools are even added to Cargo.toml as examples so that they
are at least compile-checked during every build.

Writing these tools turned out to be quite useful exercise for the API.
For example, they have revealed that Rust-Themis API for Secure Cell is
a bit different from other language wrappers. Currently we do nothing
about it and just leave TODOs in the code. We'll improve the API later.

* Install Rust toolchain for integration tests

We're going to need Rust for integration tests so install it. We use
the same rustup.rs approach as for the main test suite. We also have
to install "clang" and "pkg-config" as they are dependencies of the
Rust-Themis build.

* Add Rust-Themis to integration tests

Well, this is a bit tricky... Currently the integration tests work only
for scripting languages like Python, or for Go which has extremely fast
compilation times and right tooling. Basically, the current setup runs
tests like this: "$interpreter tools/$language/$tool.$extension $args".
Rust tools can't support this without additional help.

To work around this we introduce a whole bunch of wrapper Shell scripts
which act as testing tools and then forward their arguments to actual
tool binaries. This way we don't have to break assumptions like that
every testing tool has a filename extension. (Also, note the quote
usage in the scripts. That's how it must be because some arguments may
contain spaces and empty strings.)

Rust is a compiled language and its compilation times are quite long.
Simply using "cargo run --example" for each tool adds about a second
to the startup time (for Cargo to check that the files did not change),
assuming that all tools have been already compiled. In order to have
a more streamlined testing process we precompile the tools and install
them to Cargo's binary directory (~/.cargo/bin) which is in PATH.

We check for these installed binaries in wrapper shell scripts and use
them if they are not available. Otherwise we call Cargo to compile and
run the tools for us, in case you don't want to install them (and you
really don't want to do this outside of a temporary Docker container).

* Improve invocation of Rust integration tools

It's not really nice to install binaries into user's home directly
so let's try to do better. This is still a hack, but it's cleaner.

Originally we had this setup:

- "rustthemis_examples_install" target defined in top-level Makefile
  called Cargo to install integration testing tools to ~/.cargo/bin

- integration tests were running "bash tools/rust/*.sh" scripts
  when asked to run Rust tools

- the shell script were running integration tools from ~/.cargo/bin

This is bad because the developers can accidentally install tools
into their home directory.

Now we have the following hack:

- "rustthemis_integration_tools" target defined in test.mk file
  calls Cargo to install integration testing tools into Themis
  build directory ("build" at repository root by default)

- the tools are then copied to tools/rust with "*.rust" extension

- integration tests were run Rust tools as "env tools/rust/*.rust"
  pretending that they are actually scripts

This is still kinda bad because we pollute the tools/rust directory
with throwaway binaries. However, integration tests already do this
to tests/_integration directory with integration driver scripts, so
adding some more binaries should be acceptable.

(Oh, and the keygen tool has been officially renamed into keygen_tool
to avoid conflicts with docs/examples/rust/keygen.rs)

* Clean Rust build files on "make clean"

Cargo keeps its build directory in "target". This can be changed, but
it is very inconvenient for developers.

Nevertheless, running "make clean" should ensure a clean build so run
"cargo clean" after removing regular build files. Also remove the
temporary binaries which may have been copied into tools/rust.
Do this only if Rust toolchain is installed in the system.

---
## [mimiottawa/Week-6-Hist-4916-My-reflections](https://github.com/mimiottawa/Week-6-Hist-4916-My-reflections)@[8c1dc9ada2...](https://github.com/mimiottawa/Week-6-Hist-4916-My-reflections/commit/8c1dc9ada210238fc3dabc951ec68c6d02f94bbb)
#### Saturday 2021-02-20 15:03:15 by mimiottawa

Week 6 Hist 4916  My reflections

Week 6 
Collaborative  reading

Please note
I am not being able to find Niall’s and Sian’s readings

Sara
https://www.jstor.org/stable/24587086?seq=1#metadata_info_tab_contents


My reflections

It is interesting to see how technology is transforming communication patterns among museums and their visitors; the communication road is no longer one way only.
Anyone can disseminate information over the internet about a museum or a collection. 
Visitors can also talk about their personal experiences when visiting museums. 
We have seen how such institutions exercise their presence over the web and also over social media. The internet allows museums to interact with their audiences in a very dynamic manner.  
So many museums have joined Facebook and have ‘conquered’ millions of ‘friends’ who follow them and leave comments.  
This communication reciprocity allows visitors to affect the way museums address their audiences. 
Now…
How can museums control public comments?
How can they keep their online reputation?
How can they manage negative comments?

Some comments can be really hard to orchestrate.
For instance, if you go to the ‘trip advisor website’, you can find really negative comments/reviews about the Museum of Nature here in Ottawa.
Please see below a complaint from a visitor and the museum’s response. 

https://www.tripadvisor.ca/Attraction_Review-g155004-d282693-Reviews-Canadian_Museum_of_Nature-Ottawa_Ontario.html


Kelli O wrote a review Jan. 2017
48 contributions 35 helpful votes

“Worst experience!
So I brought my 2 adult daughters to see the reptile exhibit (my older one loves reptiles and has one herself) Biggest disappointment. Hardly any reptiles, no one giving information about them, and if you are going to advertise about reptiles, you should have something very unique (most were just snakes) You are better off going to Little Ray's, and I am being honest.

Secondly the volunteer's are VERY rude. My Daughter's sat down to look at the leaf table (no one was there) they were genuinely interested and started to design some leaves on the paper provided. A volunteer named "Christianne" came over and said "this is for children" and gave them the dirtiest look and kept hovering over them rudely. My older Daughter stood up, she grabbed her paper with leaves on it, and crumpled it up in front of her. Some children came to the table finally, they wanted to colour, but she grabbed the leaves from my younger Daughter and told them they were to play with the leaves.

We paid over $65 for admission for 3 people, (plus 12 for parking!) and after that promptly left. Was not worth even close to that.

you would think a Museum would want people to learn and explore, this is not the case here. the volunteer's/staff were neither helpful or polite.

I really hope they read this and take action. your volunteer made my Daughter's feel awful and in turn I felt horrible for even bringing them as no one deserves to be treated like that. We will not be returning and take our complaint higher. If you do not want young adults to learn or participate you should make that clear before you charge us for admission!”

Date of experience: January 2017

Response from museumofnature, Communications Officer at Canadian 

Museum of Nature
Responded Jan. 29, 2017

“Thank you for your comments. As a national museum and major attraction in the National Capital Region, we want to ensure the best possible experience for our visitors. Feedback such as yours is very, very important.    We’re sorry that you were disappointed with our current Reptiles exhibition. It is a travelling, educational exhibition from an accredited zoo in the United States and has been received quite well by our visitors. It’s unfortunate that it didn’t meet your expectations.   We deeply apologize for the experience you had at the hands-on botany station. The intention there is to have the plant-mounting activity for adults and a paper leaf craft for children. We’re very sorry that your daughters had a negative experience with a volunteer at that station. We are looking into the situation and will re-examine our current volunteer training. A positive museum experience for visitors of all ages is our utmost priority. Thank you so much for drawing this to our attention.    We hope you will visit us again. On June 21, we are opening our new Arctic Gallery (included in museum admission) which will offer genuine specimens, artifacts, multimedia and unique perspectives on this vast, mysterious and little-understood region.   Again, thank you, and please accept our apologies.”

---
## [pool-party/bubblifier-bot](https://github.com/pool-party/bubblifier-bot)@[3aaa2f662d...](https://github.com/pool-party/bubblifier-bot/commit/3aaa2f662d0788bd0a1d25731a007b2afaa0eeb7)
#### Saturday 2021-02-20 17:03:39 by Simon Naumov

:sparkles: selenium start, i already hate this language

a lot of refactoring TODO and so fucking on, damn

---
## [4ad/mgk.ro](https://github.com/4ad/mgk.ro)@[656dc73d7b...](https://github.com/4ad/mgk.ro/commit/656dc73d7b8cfe87c550f3352857111f0ebf2982)
#### Saturday 2021-02-20 17:55:11 by Aram Hăvărneanu

COPYING: rename COPYRIGHT to COPYING to appease pkg.go.dev

Fuck you pkg.go.dev!

---
## [Retro8it/hello-world](https://github.com/Retro8it/hello-world)@[a56828f116...](https://github.com/Retro8it/hello-world/commit/a56828f116acab4efc531f8d6af329652fd72376)
#### Saturday 2021-02-20 18:44:45 by Retro8it

Merge pull request #1 from Retro8it/readme-edits

I want to update my god damn shit already

---
## [AaronSchmaus/Aaron-Schmaus_Assignment6](https://github.com/AaronSchmaus/Aaron-Schmaus_Assignment6)@[a4f7ea89b8...](https://github.com/AaronSchmaus/Aaron-Schmaus_Assignment6/commit/a4f7ea89b859ebc00762a6a8e07543b98e090cfd)
#### Saturday 2021-02-20 18:50:01 by AaronSchmaus

Page 2.html

<html>
    <title>Story Time</title>
    <body>
        <h1>True Conservation</h1>
      
<p>To want to be part of something more than ourselves is always on our mind as we walk through our 
day-to-day lives.  We join teams because we want to live for our brothers and sisters beside us. Some 
of us think we can enhance and elevate what we are working for as the collective. Others want to live 
for another.  There are countless examples of brave men and women that have done, and will continue to 
do, acts to protect others. Some they have met and are very fond of, others they may never know and 
will never have the chance of knowing.But, they know one thing for certain the collective is stronger, 
better, and more appealing than what we can ever have ourselveson a singular level.  </p>

<p>It is this natural born sense of wanting to be part of something better, and create something better, 
that makes me want to be a hunter.  The hunter has lived through the ages as the hero. Hunting parties 
would come home and the villages would gather around to greet them, thankful for their bountiful harvests. 
When I think about the hunter, I think of an ancient or even prehistoric being that provided. They saw 
past their current position at the time.  They could never have envisioned they would be the people that 
would set me on the trail towards conservation of sustainable resources.  All they were doing was trying 
to survive.  Did they know that if they over-harvested, or over-utilized a resource that they would not 
have it anymore for future generations? Did the thought even cross their mind? Or were they living in 
the moment? I believe that they learnedthrough trial and error that if they over-utilized a species of 
animal, or over-harvested a type of plant, that they would no longer have it. </p>

<p>There is the school of thought that ancient humans would have never thought about conservation.  If you 
look at the historical evidence, it is plausible that there was no thought about conservation of utilizable 
resources.  Native Americans ran large amounts of buffalo off of cliffs and hundreds would die at a time.  
There are historical accounts of colonial explorers crossing the lands encountering the wretched stench 
of stained air for miles from the carcasses of unused buffalo meat.  We have this iconic idea that everything 
was utilized, unfortunately, this iconic idea doesn’t make any logical sense.  They did what they had to 
do to survive.  They did what they could to cultivate and harvest the most calorie-efficient diet.  Is it 
their fault? How could they possibly have known that they would have driven one of their main resources, 
the buffalo, to a point that they were on a slide to what would eventually be near extinction? Of course, 
they were not the only cause for the drive of the buffalo to their near extinction.  The market hunters 
had no regard or thought that they would not rebound and have a devastating part in the buffalo’s near extinction. 
​We as a people have learned since that time.  We still have a lot to learn, but we as hunters are on the 
right path towards conserving our natural resources. </p>  

<p> Conservationists are not just the ones that sustainably harvest.  Conservationists are all of us in the 
collective that believe that we need our animal, plant, and other national resources.  You do not have to be a 
hunter to want to conserve the beautiful things that this world has to offer, but you do have to be willing to 
acknowledge that we are the only thing that can wreck wildlife’s existence if we do not do something. We need to 
stop arguing over whether the sustainable harvest of a wild animal based on biological and empirical evidence is 
going to send us crashing into a life of suffering with nothing on our planet.  The true conservation that we need 
to take heed to is the conservation of our lands, waterways, and atmosphere. </p>

<p> On an equal but separate note, there is a lot to be said about the clean air and water that we utilize.  When our 
oceans are filled with garbage, our lands are covered in asphalt, and our air is filled with smoke to the extent 
that cancer spreads to a point of seemingly no return, then it is time that we rethink how we are living our lives.  
We as a collective live no better than those Native Americans or market hunters that decimated the buffalo.  In fact, 
it could be said that we are actually worse because of our separated ideals we believe simply because of an idiotic 
longing to be right.  True conservation concentrates on what we are doing to poison the habitat of the animals we 
love and not to mention ourselves.  </p>

<p> As a hunter, I have learned this.  I have learned thatconservation is not as simple as just sustainable utilization 
of an animal.  It is sustainable utilization of all of our resources that we as a collective take in part.  We must 
stop with the dividing lines.  If we are to continue life for future generations then, we,the non-hunters and hunters, 
have to come together in this true metric that is undoubtedly the true meaning of conservation.</p>
    </body>
   
    <img src="Images/IMG_0105.jpeg"/>

    <p><a href="webpage.html" target="_new">Page 1</a></p>
    <p><a href="Page 3.html" target="_new">Page 3</a></p>
    </html>

---
## [Blokyk/Parsex](https://github.com/Blokyk/Parsex)@[30e5eeebed...](https://github.com/Blokyk/Parsex/commit/30e5eeebed0e2f7f31b4a76f64ff4e97db76513b)
#### Saturday 2021-02-20 20:27:01 by Blokyk

Resolved a long-standing FIXME kinda annoying !

Basically, while we expected for consumers to never give any null value
(tokens have Token.NULL, nodes all have their own NULLs, etc...), the
Consumer<T> class would return default(T) as the default value, which is
just a literal null for most types, so... that's not very good :/

In fact, in old versions it would cause a NullPointerException in some
cases. Therefore, *specifically* for the Consumer<T> edge case,
I had to add a check to [Expression]Parser.ConsumeValue() that would
return `left` (i.e. the expression built thus far) if the token was
null, but honestly that just didn't feel right. So instead, I made
Consumer<T>.Default setable, and not nullable (I mean, in principle
at least, nullability is only a warning away bla bla bla), so that
now it just generally checks if it's the consumer's default (whether
consumer or tokenizer or whatever else, because default is an
IConsumer property), and if so returns `left`.

---
## [Danny-Support/github-slideshow](https://github.com/Danny-Support/github-slideshow)@[7b67090ff4...](https://github.com/Danny-Support/github-slideshow/commit/7b67090ff45b8b4a38dab493f3db38d0e09de0f6)
#### Saturday 2021-02-20 20:48:43 by Danny-Support

Create 0000-01-02-Danny-Support.md

This site is so fucking stupid, if I wasn’t forced to be here I would have never come to this dogshit site.  Fuck you for making it “commit” and “commit messages” because my brain screams that it needs to be comment, cause you comment in code

---

# [<](2021-02-19.md) 2021-02-20 [>](2021-02-21.md)

