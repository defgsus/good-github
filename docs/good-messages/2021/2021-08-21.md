# [<](2021-08-20.md) 2021-08-21 [>](2021-08-22.md)

1,938,667 events, 1,125,517 push events, 1,612,790 commit messages, 102,093,924 characters


## [torque/fpdf2](https://github.com/torque/fpdf2)@[3e4bc57715...](https://github.com/torque/fpdf2/commit/3e4bc57715f14273cbf5f7d6d36762f6ce5d27d1)
#### Saturday 2021-08-21 00:51:17 by torque

drawing: rewrite to support alpha blending and style inheriting

To start with, the rendering pass has been updated to pass more
information from one stage to the next. This will allow better support
for some SVG drawing operators that we might as well try to support
because it doesn't hurt anything. In addition, a lot of rework has gone
into how the styling is structured (theoretically with little change to
the API) to allow for automatic determination of painting style from
the style attributes.

Some of the style items are being offloaded into the graphics state
parameter dictionaries which notably allows us to support the PDF
transparency model---this was completely missing from the initial
implementation. Unfortunately, the use of these dicts is a mess, and I
wish they didn't exist. Drawing transparency is a PDF 1.4+ feature, so
we will need to bump the version flag above 1.3 (the default) if using
any of the drawing features, to be sure we don't emit an out-of-spec
PDF.

There are still a number of features we don't support (patterns etc.)
that are not going to be added right now, though the state dict support
should theoretically make it easier to add support in the future,
maybe.

reimplement remaining style attributes

this includes blend modes.

Also, I've just determined that the graphics state parameter dictionary
ordering is nondeterministic thanks to my very skillful use of sets,
which do not preserve order. I will need to swap that implementation
out for an OrderedDict, I think.

commit message quality going downhill

Reintroduce the render tree output, renamed as debug output. This is
now generated during the rendering process, so it has access to the
contextual information used during the rendering process, which allows
even more useful output to be produced, such as the resolved absolute
output of relative path components. I tried doing this with separate
render paths for debug and not debug to avoid adding a bunch of debug
branching, but this ended up kind of falling apart at the
GraphicsContext level, as there's sufficiently entangled logic in the
rendering process that I didn't feel comfortable copy pasting it in.
At the end of the day, I haven't measured the performance of this at
all anyway, so it's kind of silly to try to worry about such
optimizations. However, the real motivation was to avoid polluting the
normal code with a bunch of debug branches, and I'm sad to say I've
failed in this regard. I may come back to this, maybe not.

The graphics state dictionary output should be deterministic now, as
it's been switched over to using OrderedDicts instead of sets.

Adding docstrings is an ongoing process, but the next step will
probably be rewrite the existing unit tests and start adding new ones.

The features are now in place to produce accurate output of the svg
logo test, including properly omitting the stroke and including
transparency.

also add arc support

Now including prettier debug information as well.

---
## [vsoch/hardware_introduction](https://github.com/vsoch/hardware_introduction)@[11b4798859...](https://github.com/vsoch/hardware_introduction/commit/11b47988598a093a14f9b86ee2ebebf3d08db4f9)
#### Saturday 2021-08-21 01:47:33 by Vanessasaurus

Small typos in pluto.jl

I absolutely loved reading this post! It seems like this is a missing part of a researcher / programmer's education that should be more common to teach - because (at most) even in CS departments we learn about, for example, Big O Notation and it's only in rare niche courses that we learn all these details.

Anyway I really enjoyed reading this, and I'll be sharing it with my colleagues! It has me thinking that (along with my point about about needing more teaching here) it would be neat to actually analyze some code in the wild, compare techniques to do calculations, and maybe have an automated way to then make those suggestions.

---
## [squid-cache/squid](https://github.com/squid-cache/squid)@[b5f72ed36a...](https://github.com/squid-cache/squid/commit/b5f72ed36ab7d4e11710a11978c0e4f41068d53d)
#### Saturday 2021-08-21 03:01:10 by Alex Rousskov

Bug 5055: FATAL FwdState::noteDestinationsEnd exception: opening (#877)

The bug was caused by commit 25b0ce4. Other known symptoms are:

    assertion failed: store.cc:1793: "isEmpty()"
    assertion failed: FwdState.cc:501: "serverConnection() == conn"
    assertion failed: FwdState.cc:1037: "!opening()"

This change has several overlapping parts. Unfortunately, merging
individual parts is both difficult and likely to cause crashes.

## Part 1: Bug 5055.

FwdState used to check serverConn to decide whether to open a connection
to forward the request. Since commit 25b0ce4, a nil serverConn pointer
no longer implies that a new connection should be opened: FwdState
helper jobs may already be working on preparing an existing open
connection (e.g., sending a CONNECT request or negotiating encryption).

Bad serverConn checks in both FwdState::noteDestination() and
FwdState::noteDestinationsEnd() methods led to extra connectStart()
calls creating two conflicting concurrent helper jobs.

To fix this, we replaced direct serverConn inspection with a
usingDestination() call which also checks whether we are waiting for a
helper job. Testing that fix exposed another set of bugs: The helper job
pointers or in-job connections were left stale/set after forwarding
failures. The changes described below addressed most of those problems.


## Part 2: Connection establishing helper jobs and their callbacks

A proper fix for Bug 5055 required answering a difficult question: When
should a dying job call its callbacks? We only found one answer which
required cooperation from the job creator and led to the following
rules:

* AsyncJob destructors must not call any callbacks.

* AsyncJob::swanSong() is responsible for async-calling any remaining
  (i.e. set, uncalled, and uncancelled) callbacks.

* AsyncJob::swanSong() is called (only) for started jobs.

* AsyncJob destructing sequence should validate that no callbacks remain
  uncalled for started jobs.

... where an AsyncJob x is considered "started" if AsyncJob::Start(x)
has returned without throwing.

A new JobWait class helps job creators follow these rules while keeping
track on in-progress helper jobs and killing no-longer-needed helpers.

Also fixed very similar bugs in tunnel.cc code.


## Part 3: ConnOpener fixes

1. Many ConnOpener users are written to keep a ConnectionPointer to the
   destination given to ConnOpener. This means that their connection
   magically opens when ConnOpener successfully connects, before
   ConnOpener has a chance to notify the user about the changes. Having
   multiple concurrent connection owners is always dangerous, and the
   user cannot even have a close handler registered for its now-open
   connection. When something happens to ConnOpener or its answer, the
   user job may be in trouble. Now, ConnOpener callers no longer pass
   Connection objects they own, cloning them as needed. That adjustment
   required adjustment 2:

2. Refactored ConnOpener users to stop assuming that the answer contains
   a pointer to their connection object. After adjustment 1 above, it
   does not. HappyConnOpener relied on that assumption quite a bit so we
   had to refactor to use two custom callback methods instead of one
   with a complicated if-statement distinguishing prime from spare
   attempts. This refactoring is an overall improvement because it
   simplifies the code. Other ConnOpener users just needed to remove a
   few no longer valid paranoid assertions/Musts.

3. ConnOpener users were forced to remember to close params.conn when
   processing negative answers. Some, naturally, forgot, triggering
   warnings about orphaned connections (e.g., Ident and TcpLogger).
   ConnOpener now closes its open connection before sending a negative
   answer.

4. ConnOpener would trigger orphan connection warnings if the job ended
   after opening the connection but without supplying the connection to
   the requestor (e.g., because the requestor has gone away). Now,
   ConnOpener explicitly closes its open connection if it has not been
   sent to the requestor.

Also fixed Comm::ConnOpener::cleanFd() debugging that was incorrectly
saying that the method closes the temporary descriptor.

Also fixed ConnOpener callback's syncWithComm(): The stale
CommConnectCbParams override was testing unused (i.e. always negative)
CommConnectCbParams::fd and was trying to cancel the callback that most
(possibly all) recipients rely on: ConnOpener users expect a negative
answer rather than no answer at all.

Also, after comparing the needs of two old/existing and a temporary
added ("clone everything") Connection cloning method callers, we decided
there is no need to maintain three different methods. All existing
callers should be fine with a single method because none of them suffers
from "extra" copying of members that others need. Right now, the new
cloneProfile() method copies everything except FD and a few
special-purpose members (with documented reasons for not copying).

Also added Comm::Connection::cloneDestinationDetails() debugging to
simplify tracking dependencies between half-baked Connection objects
carrying destination/flags/other metadata and open Connection objects
created by ConnOpener using that metadata (which are later delivered to
ConnOpener users and, in some cases, replace those half-baked
connections mentioned earlier. Long-term, we need to find a better way
to express these and other Connection states/stages than comments and
debugging messages.


## Part 4: Comm::Connection closure callbacks

We improved many closure callbacks to make sure (to the extent possible)
that Connection and other objects are in sync with Comm. There are lots
of small bugs, inconsistencies, and other problems in Connection closure
handlers. It is not clear whether any of those problems could result in
serious runtime errors or leaks. In theory, the rest of the code could
neutralize their negative side effects. However, even in that case, it
would only be a matter of time before the next bug bites us due to stale
Connection::fd and such. These changes themselves carry elevated risk,
but they get us closer to reliable code as far as Connection maintenance
is concerned. Without them, we will keep chasing deadly side effects of
poorly implemented closure callbacks.

Long-term, all these manual efforts to keep things in sync should become
unnecessary with the introduction of appropriate Connection ownership
APIs that automatically maintain the corresponding environments (TODO).


## Part 5: Other notable improvements in the adjusted code

Improved Security::PeerConnector::serverConn and
Http::Tunneler::connection management, especially when sending negative
answers. When sending a negative answer, we would set answer().conn to
an open connection, async-send that answer, and then hurry to close the
connection using our pointer to the shared Connection object. If
everything went according to the plan, the recipient would get a non-nil
but closed Connection object. Now, a negative answer simply means no
connection at all. Same for a tunneled answer.

Refactored ICAP connection-establishing code to to delay Connection
ownership until the ICAP connection is fully ready. This change
addresses primary Connection ownership concerns (as they apply to this
ICAP code) except orphaning of the temporary Connection object by helper
job start exceptions (now an explicit XXX). For example, the transaction
no longer shares a Connection object with ConnOpener and
IcapPeerConnector jobs.

Probably fixed a bug where PeerConnector::negotiate() assumed that
sslFinalized() does not return true after callBack(). It may (e.g., when
CertValidationHelper::Submit() throws). Same for
PeekingPeerConnector::checkForPeekAndSpliceMatched().
 
Fixed FwdState::advanceDestination() bug that did not save
ERR_GATEWAY_FAILURE details and "lost" the address of that failed
destination, making it unavailable to future retries (if any).

Polished PeerPoolMgr, Ident, and Gopher code to be able to fix similar
job callback and connection management issues.

Polished AsyncJob::Start() API. Start() used to return a job pointer,
but that was a bad idea:
    
* It implies that a failed Start() will return a nil pointer, and that
  the caller should check the result. Neither is true.

* It encourages callers to dereference the returned pointer to further
  adjust the job. That technically works (today) but violates the rules
  of communicating with an async job. The Start() method is the boundary
  after which the job is deemed asynchronous.
    
Also removed old "and wait for..." post-Start() comments because the
code itself became clear enough, and those comments were becoming
increasingly stale (because they duplicated the callback names above
them).

Fix Tunneler and PeerConnector handling of last-resort callback
requirements. Events like handleStopRequest() and callException() stop
the job but should not be reported as a BUG (e.g., it would be up to the
callException() to decide how to report the caught exception). There
might (or there will) be other, similar cases where the job is stopped
prematurely for some non-BUG reason beyond swanSong() knowledge. The
existence of non-bug cases does not mean there could be no bugs worth
reporting here, but until they can be identified more reliably than all
these benign/irrelevant cases, reporting no BUGs is a (much) lesser
evil.

TODO: Revise AsyncJob::doneAll(). Many of its overrides are written to
check for both positive (i.e. mission accomplished) and negative (i.e.
mission cancelled or cannot be accomplished) conditions, but the latter
is usually unnecessary, especially after we added handleStopRequest()
API to properly support external job cancellation events. Many doneAll()
overrides can probably be greatly simplified.

---
## [Noteperson/mlp_aet](https://github.com/Noteperson/mlp_aet)@[9de789c5d2...](https://github.com/Noteperson/mlp_aet/commit/9de789c5d2328e3b814b48bb520b3c85048b38e9)
#### Saturday 2021-08-21 04:32:26 by Noteperson

Summer 21 Final

-Updated to All-Time Hits Squad
 -Replaced Horsefucker -> Fuck Your Marker (3)
 -Replaced Its Still Shit -> Its Already Shit (6)
 -Replaced Space Pony -> Bomb Ass Tea (13)
 -Replaced wAIfu -> Jackie Chan Tulpa (15)
 -Replaced Belongs on v -> Choose Your Own Autism (17)
 -Replaced Kirin Beer -> The Burdened (19)
 -Replaced no hooves -> DYEWTS (21)
 -Replaced Boop -> Molestia (23)
-Updated kit configs to shirt name size 10 (to fit better)
-Updated >rape and Larson to remove occular occlusion models (were blocking eyes in match)
-Updated kit number textures for Crotchtits, WOW Glimmer, Tracy, Lyra Plushie, HI ANON
-Updated Tracy boots to use outfield player kit texture
-Updated IWTCIRD cutie mark texture (CM.dds)
-Added Common textures: dummy_kit_srm.dds, normal.dds (kit), metal.dds

---
## [Hexenstag-Team/WH-Hexenstag-Dev](https://github.com/Hexenstag-Team/WH-Hexenstag-Dev)@[f8be713ebd...](https://github.com/Hexenstag-Team/WH-Hexenstag-Dev/commit/f8be713ebd79a004e2917ed7d377caaf5b5516b7)
#### Saturday 2021-08-21 04:45:40 by HobbitDerrick

Chrome

Strapped up the Morr's Garden situation by excluding it for chaos_gods_group and ormazd_group. They should replace but this is adequate.

Added a raft of extra wonder upgrades. Reduced cost impact for multiple wonders of the same type

Added high end society powers:
* Engineers and Inventors can now spend huge amounts of society currency to improve the new Great Works.
* Shallyans can self-sacrifice to set back the Doom counter. (They need a full rework but this gives the society a bit more flair.)

Did some reworking of Myrmidian Religion so it's different from Geheimnisnacht. Some of these changes are strict improvements.
* Moved Cult of Myrmidia to default capital in the Blighted Marshes (Tylos). Not right, not wrong either. (The setup with the Cult capitaled in Magritta is based on the specific situation in the Karl Franz era and we're told that's unusual.)
* Moved Myrmidian holy site from Istrabul (why?) to Couronne (center of Shallya's faith - the sister of Myrmidia!)
* Moved Myrmidian pentarchy site from Thakos to Couronne (this and the Altdorf site should probably should get a fancier treatment but this will do for now), removed Bilbali pentarchy site.
* Deactivated the High Temple titles. They don't really work as intended right now so we're better off with default Pentarchy behavior until a comprehensive rework

Caught some typos in previous work

Tried bugfixing the rare issue with the legendary journeys events. It looks like the problem (inherited from vanilla) is re-calling HF.12000 as a character event even though it's a province event.

Used error log to patch up some issues with events. (Still had some no MTTH/no trigger events, including one of mine :( )

---
## [HarshGupta-DS/House-Price-Prediction_Advanced-Regression-Techniques](https://github.com/HarshGupta-DS/House-Price-Prediction_Advanced-Regression-Techniques)@[5051cb967e...](https://github.com/HarshGupta-DS/House-Price-Prediction_Advanced-Regression-Techniques/commit/5051cb967e4df886e10617f72ca27f4479bf307e)
#### Saturday 2021-08-21 05:02:01 by Harsh Gupta

House Price Prediction

Description
Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.

With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.

Practice Skills
Creative feature engineering 
Advanced regression techniques like random forest and gradient boosting

Acknowledgments
The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often-cited Boston Housing dataset.

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[7312c5ec42...](https://github.com/mrakgr/The-Spiral-Language/commit/7312c5ec42dbb0d7e445608ecb9e57ff5144a8e0)
#### Saturday 2021-08-21 05:16:34 by Marko Grdinić

"6:30am. I am up. To think that I both had time to lounge in bed and do the chores by this point. Let me check out the mail and then I'll put the v2.1 docs through the grammar checker.

No, nothing. Yeah, my working estimate that nothing is going to come from this. There are too many stories of people looking for jobs for half a year. I am not going to make it this way.

6:35am. Let me also push out the new Spiral.

6:40am. Interesting how many errors are in the docs. The Google Docs tool is catching a bunch of them, and it is not from the newer segments either!

7:10am. Fixed a bunch of errors. I am surprised how many they were even in sections that should have been checked previously.

Let me commit here so the readme shows up on the main page. After that I'll chill a bit and practice reciting.

I'll also take the time to study the /r/cscareerquestions sub. I noticed yesterday that there is a resume thread, and it gave me inspiration how to redo my own. I should rewrite it and put my work experience on top.

7:15pm. This I'll leave for later. Today my main priority should be to learn to speak better. I need to level up that skill. Then I will be able to pass that Toptal test. After that will come the actual programming tests that I need to pass in order to be eligible for work.

I'll have to give that my utmost. That AI chip will not land itself in my lap on its own."

---
## [veridisquot/alice](https://github.com/veridisquot/alice)@[eec0ba3c1b...](https://github.com/veridisquot/alice/commit/eec0ba3c1baef4735dd07e25a751174088d632c1)
#### Saturday 2021-08-21 10:50:58 by George Lamb

Shadowmapping for point lights (almost)

Kind of works, but not really. I will fix it tomorrow.
Only committing this because I'm paranoid my computer
will die and I will lose 40 minutes of work. Stupid? Yes.

---
## [dwarakanath88/Love-caluclator](https://github.com/dwarakanath88/Love-caluclator)@[d8b68457d8...](https://github.com/dwarakanath88/Love-caluclator/commit/d8b68457d818e51985a49bfbc65c59add215218b)
#### Saturday 2021-08-21 14:01:12 by dwarakanath88

Add files via upload

A funny program to check once score in love by entering their names, as we used to do when we are kids.

---
## [0x07CB/Arch-Linux-PPC](https://github.com/0x07CB/Arch-Linux-PPC)@[d3afeca2eb...](https://github.com/0x07CB/Arch-Linux-PPC/commit/d3afeca2eb900bb74a518d63cf23730563626214)
#### Saturday 2021-08-21 14:06:16 by 0x07CB

I propose to deploy the mirror server, and more...

@alicela1n , Alice? Listen, fully please.

I start now to get and keep the PPC Port of Arch Linux , all.
Create an server, an server web everything next.
Pay billing and register.
Testing security, do all needs and after finish all.
@alicela1n I give full access of the server.
I have an older Mac PPC and ...I feel that I will never regret doing to you what. :)


[1 - I Do maximum - Support me and Don't Reject anycase please ] 
_
I look what you have do and I follow news from you, just I precise one thing : ( I over-focus sometime, I have ADHD diagnosed and meds to that )  Okay ? AND I NOT DO BAD THINGS(I'm not an stalker or weird guy) ; So please don't block me if I have do wrong any things just tell me okay.
Sorry I'm feeling in obligation to precise that because I have so much difficulty's for establish and keep any socials relationships, my ADHD too late diagnosed have made other troubles, with time and a lot of traumatic events ... painful commorbidity has developed in addition. And yeah I tell here that in public. 
_

[2 - You have an positive impact on my life]
You have touch my heart so deeply Alice because your work... you are amazing... I am so anxious because I see other projects from you, I have made an anxiety huge crisis and that continue increase I take an sedative of more okay... 
...not your fault and trust me I do maximum to found force inside myself because I feel, I want help you and I can  
I just be clear I feel passion in your work and interests, and that is very good for me you understand.
Look you use Arch too and you know what you are inside the organization of maintainers of my favorite OS on my own PinePhone (your two friends inside, if I forgot thanks for me please, I am so happy with this ... yeah it's my only one phone of every days, I have drop my old huawei, really so many thanks.)

So you help me at an level so high I feel that, so hard work from me next to you and let's me time to do okay, and now the time has come for me to surprise you 

[ - Specifically My Engagement To You in 3 Points Important :  ]
-I Do for you now and continuously all I Can, my heart guide me , I am not in waiting of money or your love to me, I wait nothing in obligation.( I hope and only hope we can be good friend a day )
-All efforts to you: Never I cheat on you, Never Bad actions to you, I made public promise of that, and that is true forever in any case.
-Never I revenge me to you: If you hurt me or do any bad actions to made me pain or any abuse, and only if it's your goal, if this case true at moment you lost just my respect and my following.

Okay I have start to do related task's and check you email next hours, I start sending just in 40 minutes.
Let's me the time, I believe in you.
Peace.

---
## [venky328/road-lane-detection-for-autonomous-vehicles](https://github.com/venky328/road-lane-detection-for-autonomous-vehicles)@[11495cbb86...](https://github.com/venky328/road-lane-detection-for-autonomous-vehicles/commit/11495cbb86a9f32f1f9e41e843729c2e9d7b1730)
#### Saturday 2021-08-21 19:29:29 by venky328

re

ROAD LANE DETECTION FOR AUTONOMOUS VEHICLES USING OPENCV

A project report submitted in partial fulfilment of the requirements for the award of degree in                                                 

Bachelor of Computer Applications (BCA)

By

VENKATESH CHALIBINDI
(Regd. No: 121812501022)

Under the esteemed guidance of 

Dr. M. Sri Venkatesh
Assistant Professor
 
	Department of Computer Science

GITAM Institute of Science

GITAM (Deemed to be University)

Visakhapatnam -530045, A.P

(2020-2021)





  


CERTIFICATE



This is to certify that the project entitled “ROAD LANE DETECTION FOR AUTONOMOUS VEHICLES USING OPENCV” is a Bonafede work done by VENKATESH CHALIBINDI, Regd. No: 121812501022 during February 2020 to May 2021 in partial fulfilment of the requirement for the award of degree of Bachelor of Computer Applications (BCA) in the Department of Computer Science, GITAM Institute of Science, GITAM (Deemed to be University), Visakhapatnam.









Internal Guide				           Head of the Department

Dr. M. Sri Venkatesh                                                   Ms.Vedavathi Katneni
Assistant Professor                                                       Professor 
Dept of Computer Science, GIS                            	   Dept of Computer Science, GIS          
GITAM                                                                     	    GITAM 
 









DECLARATION


I VENKATESH CHALIBINDI, Regd. No: 121712501042 hereby declare that the project entitled “ROAD LANE DETECTION FOR AUTONOMOUS VEHICLES USING OPENCV” is an original work done in the partial fulfilment of the requirements for the award of degree of Bachelor of Computer Applications (BCA) in GITAM Institute of Science, GITAM (Deemed to be University), Visakhapatnam. I assure that this project work has not been submitted towards any other degree or diploma in any other colleges or universities.





									VENKATESH CHALIBINDI
                                                                                            (Regd. No:121812501022)  














                       
                                                     ACKNOWLEDGEMENT


The satisfaction that accompanies the successful completion of any task would be incomplete without the mention of people who made it possible and whose constant guidance and encouragement crown all the efforts with success. 

I would like to express my deep sense of gratitude and sincere thanks to Assistant Professor                                                Dr. M. Sri Venkatesh , Department of Computer Science, for his constant guidance, supervision and motivation in completing the project. 

I would like to express my deep sense of gratitude and sincere thanks to my reviewer and Assistant Professor Ms.B. SATYA SAI VANI , Department of Computer Science, for her constant guidance, supervision and motivation in completing the project. 

I extremely elated to extend my gratitude to Ms.Vedavathi Katneni, Professor and Head of the Department of Computer Science, for her encouragement all the way during this project. Her annotations and insinuation are the key behind the successful completion of the project. 







									VENKATESH CHALIBINDI
    									   (Regd. No:121812501022)  




							INDEX
1.Introdction
 	1.1 Types of lanes
  	1.2 Lane detection 
  	1.3 Benefits of lane detection
     1.4 Problem domain and motivation
2.Related work	
3. Literature review
4.Overview Of The Proposed System 
5. Proposed method and requirements
	 5.1 video acquisition
 	5.2 pre-processing
		 5.2.1 capturing and decoding video file
		5.2.2 gray scale conversion of image
		5.2.3 gaussiann blur
	5.3 ROI selection 
	5.4 lane detection from Hough transform 
 		5.4.1 Hough transform 
	5.5 block diagram of proposed system
	5.6 system requirements
6.Methodology
	6.1 pre-processing
	6.2 colour transform
	6.3 basic pre-processing
	6.4 ROI selection
	6.5 adding Edge detection in pre-processing
	6.6 output frame
	6.7 visualization of the methodology
7.Road lane detection though Hough transform 
 	7.1 introduction 
	7.2 the Hough space
 	7.3 mapping of points to Hough space
 	7.4 basic algorithm
 	7.5 transformation to Hough space
    7.6 detection of infinite line
8.Experiments with code 
9.Limitations
10.References

 

                                                    ABSTRACT

Today, unmanned vehicle technologies are developing in parallel with increasing interest in technological developments. These developments aim to improve people's quality of life. Transportation, which is a part of human life, has taken its share from this developing technology.  With the development of artificial intelligence, it is aimed to provide the necessary assistance to the driver in transportation and to provide ease of driving. Fuelled by Deep Learning algorithms, they are continuously driving our society forward and creating new opportunities in the mobility sector. For vehicles to be able to drive by themselves, they need to understand their surrounding world like human drivers, so they can navigate their way in streets, pause at stop signs and traffic lights, and avoid hitting obstacles such as other cars and pedestrians. Based on the problems encountered in detecting objects by autonomous vehicles an effort has been made to demonstrate lane detection using OpenCV library.

















1.	INTRODUCTION
Traffic accidents have become one of the most serious problems in today's world. Roads are the mostly chosen modes of transportation and provide the finest connections among all modes. Most frequently occurring traffic problem is the negligence of the drivers and it has become more and more serious with the increase of vehicles.
Increasing the safety and saving lives of human beings is one of the basic functions of Intelligent Transportation System (ITS). Intelligent transportation systems are advanced applications which aim to provide innovative services relating to different modes of transport and traffic management. This system enables various users to be better informed and make safer, more coordinated, and smarter use of transport networks.
These road accidents can be reduced with the help of road lanes or white markers that assist the driver to identify the road area and non-road area. A lane is a part of the road marked which can be used by a single line of vehicles as to control and guide drivers so that the traffic conflicts can be reduced.

 
	      	Fig 1 Road scene image
Most roads such as highways have at least two lanes, one for traffic in each direction, separated by lane markings. Major highways often have two roadways separated by a median, each with multiple lanes. To detect these road lanes some system must be employed that can help the driver to drive safely.

Lane detection is an area of computer vision with applications in autonomous vehicles and driver support systems.
 
Fig 2: Lane detection

Despite the perceived simplicity of finding white markings on a simple road, it can be very difficult to determine lane markings on various types of road. These difficulties can be shadows, occlusion by other vehicles, changes in the road surfaces itself, and different types of lane markings. A lane detection system must be able to detect all manner of markings from roadways and filter them to produce a reliable estimate of the vehicle position relative to the lane.

1.1.1 Types of Lanes
•	Traffic Lane: Lane for the vehicles moving from one destination to another
•	Express Lane: Used by faster moving traffic and has less access to exits/off ramps
•	Reversible Lane: To match the peak flow direction of vehicles is changed. Periods of high traffic flow are accommodated by this lane.
•	 Auxiliary Lane: Used for separating entering, exiting or turning traffic from the through    traffic.
•	 In some areas, for non-moving vehicles lane adjacent to curb is reserved.
1.1.2 Lane Detection 
Lane detection is one significant method in the visualization-based driver support structure and capable to be used for vehicle routing, cross power, crash avoidance, or lane departure warning system. Different road condition that creates this difficulty more complex include dissimilar variety of lanes (straight or rounded), occlusions cause by obstacle, fog, darkness, illumination change (like night-time), and so on. Therefore, it is the method to locate lane in the picture and is a significant enable or attractive skill in different automobile application, include lane departure recognition and warning, travel control, cross-control, and self-directed driving.
A lane departure warning system (LDWS) is a technology designed for warning a driver when the vehicle begins to depart from its lane. An effective lane detection system will navigate autonomously or assist driver in all types of lanes like straight and curved, white and yellow, single and double, solid and broken and pavement or highway lane boundaries. The system should be able to detect lane even under noisy conditions such as fog, shadow, and stain.


1.1.3 Benefits of Lane Detection:
•	Gives assistance and details to pedestrians and drivers 
•	Uniformity of the markings is an important factor in minimizing confusion and uncertainty about their meaning 
•	 Allows vehicular drivers to drive safely
•	Gives assistance and information to drivers. 
•	 Uniformity of the markings is a very important aspect in minimizing confusion and uncertainty approximately about their meaning. 
•	 Allows drivers to drive vehicles safely and protect the passengers from any mis happenings.  
•	The warnings signal will alert the passengers also









1.2	PROBLEM DOMAIN AND MOTIVATION	
As mentioned previously, the most appealing feature of self-driving cars is the ability to transport passengers and/or cargo, from point A to point B, autonomously in a safe manner. In order to achieve this goal, the vehicle should have some form of road following system, traversing through both rural and busy urban streets while abiding all the existing traffic laws.
 Two common ways for a car to follow a road, without modifying existing roads or attaching various sensors on other cars, is to either use GPS tracking or machine vision. According to the U.S. government, the accuracy of a GPS tracking unit is approximately 7.8m at 95% confidence level. Lanes on average are between 2-4m width, so the accuracy of a GPS is not good enough to keep a car within its lane reliably. 
The other approach, machine vision, would rely on lane markings and other road features extracted from the footage of an on-board camera to enable lane detection and subsequent following. However, lane markings may not be always clearly defined, the camera view may get obstructed by on-road traffic or other obstacles and various weather and light conditions could affect the visibility of the camera. Nevertheless, this approach provides the most accurate horizontal position of a car within a road and with well-thought algorithms and hardware addons, the aforementioned problems can be alleviated or downright eliminated.
 Ideally, you would use both methods to get to set point B, machine vision and other sensors to keep the car on its lane and avoid obstacles, and GPS tracking to identify the vehicle’s position in relation to the set final destination and derive a path to it.
In Carolo -Cup (an international student competition of the Technical University of Braunschweig and takes place annually. The aim of the competition is to develop an autonomous 1:10 scale model vehicle) one of the main requirements is to be able to follow the lanes of a previously unmet and unmapped road, the participating teams are not informed of the layout of the road beforehand. This outright eliminates GPS tracking. As the participating vehicles are 1/10 size of real cars, the course track is miniaturised accordingly so the inaccuracy of GPS is magnified by factor of 10. Therefore, machine vision remains the sole solution. The competition track is arranged as a circuit and the aim of the lane following discipline is to complete as many laps as possible in the allotted time, there is no need for a car to go to a specific location in the track. The lane following discipline also includes intersection scenarios, missing lane markings and obstacles on the road. The parking discipline may also require lane following for the purpose of keeping the car in the middle of its lane.
				TABLE I 

Lane Detection	
Extract and identify lane markings correctly
Lane Following	Derive trajectory based on the extracted lane markings or other approximations. The trajectory should lead to the middle of the lane the car is located in.


2.RELATED WORK 
        Safety is the main objective of all the road lane detection systems due to the reason is that most of the vehicle road accident happens because of the driver miss leading of the vehicle path. Therefore, currently many different vision-based roads detection algorithms have been developed to avoid vehicle crash on the road. Among these algorithms the GOLD system developed by Broggi, it uses an edge-based lane boundary detection algorithm. 
The acquired image is remapped in a new image representing a bird’s eye view of the road where the lane markings are nearly vertical bright lines on a darker background. Specific adaptive filtering is used to extract quasi vertical bright lines that concatenated into specific larger segments. Kreucher C. propose in the LOIS algorithm as a deformable template approach. A parametric family of shapes describes the set of all possible ways that the lane edges could appear in the image.
 A function is defined whose value is proportional to how well a particular set of lane shape parameters matches the pixel data in a specified image. Lane detection is performed by finding the lane shape that maximizes the function for the current image. The Carnegie Mellon University proposes the RALPH system, used to control the lateral position of an autonomous vehicle. It uses a matching technique that adaptively adjusts and aligns a template to the averaged scan line intensity profile in order to determine the lane’s curvature and lateral offsets. The same university developed another system called AURORA which tracks the lane markers present on structured road using a colour camera mounted on the side of a car pointed downwards toward the road.
 A single scan line is applied in each image to detect the lane markers. An algorithm destined to painted or unpainted road is described in. Some colour cues were used to conduct image segmentation and remove the shadow. Assuming that the lanes are normally long with smooth curves then their boundaries can be detected using Hough transformation applied to the edge image. A temporal correlation assumed between successive images is used in the following phase. Three-feature based automatic lane detection algorithm (TFALDA) is primarily intended for automatic extraction of the lane boundaries without manual initialization or a priori information under different road environments and real-time processing. 
It is based upon similarity match in a three-dimensional (3-D) space spanned by the three features of a lane boundary starting position direction (or orientation), and its Gray-level intensity features comprising a lane vector are obtained via simple image processing. LANA algorithm [9] was based on novel set of frequency domain features that captures relevant information concerning the magnitude and orientation of spatial edges extracted by 8*8(DCT). The hyperbola-pair model is deformable template, and is developed on the base of Kluge’s work. The origin model is suitable for single-side lane markings firstly, but in the work by Wang and Chen. It was emphasized that some parameters of the model were the same, if the lanes were parallel on the same road. The considerable feature is also used in our algorithms, to form an extended equation to fit the road shape





3.Literature Review:
Lane Detection for Autonomous Car via Video Segmentation There are many steps in detecting lanes on a road, first comes the camera calibration. Cameras uses curved lenses to form an image, and light rays often bend a little too much or too little at the edges of these lenses. Images can be undistorted mapping distorted points to undistorted points such as a chessboard. This distortion correction is then applied to raw images, convert images to grayscale, apply gradients and finally apply deep leaning. 
Then perspective transform is applied to the binary image from bird’s eye view. A Global Convolution Networks (GCN) model is used to rectify the classification and localization issues for semantic segmentation of lane. The model is evaluated using colour-based segmentation. For classification task the conical CNN have proved to be better. For object segmentation, the size of the kernel matters a lot. But a larger size results in increase in weights and the number of parameters also increases. In the model 1 D kernels were used to increase performance with less weights. 
The dataset was collected from Carla Simulator which contained 3000 images of road. The sky portion and car hood part from the images was cropped to increase the performance. International Research Journal of Engineering and Technology] Prof A. B. Deshmukh e t a in their paper that Lane detection is an essential component of Advance Driver Assistance system (ADAS). Many different approaches have been proposed till today by researchers but still it is a challenging task to correctly detect the road lanes in various environmental conditions. 
The main purpose of the system is to detect the lane departure to avoid road accidents and to provide safety for pedestrians. The proposed method detects the road edges using the canny edges detector whereas the feature extraction technique like Hough transform is used in image analysis and digital signal processing. The main input to the system is camera captured images in order to detect and track the road boundaries. 
This concept of image processing is implemented using OpenCV library function on Raspberry pi hardware. This method can correctly detect the roads in various challenging situations. Results shows that the proposed method can detect both the straight and curves lanes correctly. Recently, many studies are conducted based on advanced driver assistance system (ADAS) to avoid car accidents. Mostly, the lane departure warning system (LDWS) system warns the driver when the vehicles tend to depart of its lane, which is the most basic and important part of the ADAS.
 
4. Overview of the Proposed System
This project is an advanced lane detection technology to improve the efficiency and accuracy of real-time lane detection to get curved lanes. The lane detection module is usually divided into two steps: (1) image pre-processing and (2) the establishment and matching of line lane detection model.

 



Figure shows the overall diagram of our proposed system where lane detection blocks are the main contributions of this paper. The first step is to read the frames in the video stream. The second step is to enter the image pre-processing module. What is different from others is that in the pre-processing stage we not only process the image itself but also do colour feature extraction and edge feature extraction. In order to reduce the influence of noise in the process of motion and tracking, after extracting the colour features of the image, we need to use Gaussian filter to smooth the image. Then, the image is obtained by binary threshold processing and morphological closure. These are the pre-processing methods mentioned in this paper.
Next, we select the adaptive area of interest (ROI) in the pre-processed image. The last step is lane detection. Firstly, Canny operator is used to detect the edge of lane line; then Hough transform is used to detect line.

 
5.Proposed method and Requirements 

    The main purpose of this project is to demonstrate a curved lane detection using opencv and following algorithm developed to overcome the challenge of detecting the curved lanes from Hough transform. Straight line detection gives maximum regulations and increase the problem of accidents if there is any curve present in the next road and that challenge is accomplished here with Hough transform .it will help the computer to detect the lane with maximum accuracy and precision. 

 
Fig: block diagram of the proposed system


5.1 Video Acquisition:
There are many sources for the video acquisition in field of signal processing. The main important one is vision-based approach. Here camera is mounted on the vehicle which is capable of reaching real time performances in detection and tracking of structured road Boundaries (Painted or Unpainted Lane markings) with slight curvature, which is robust enough in presence of shadow conditions. Also, rear view camera based moving object detection algorithm which helps detection of moving object when the vehicle is passing it also very effectively used for backup aid and parking assist application.



5.2 Pre-processing:
In this section we are going to convert the video into the frames followed by gaussian blur, canny edge.
5.2.1 Capturing and decoding video file: 
We will capture the video using Video Capture object and after the capturing has been initialized every video frame is decoded (i.e., converting into a sequence of images).
5.2.2 Grayscale conversion of image: 
The video frames are in RGB format, RGB is converted to grayscale because processing a single channel image is faster than processing a three-channel coloured image.
5.2.3 Gaussian blur:
  	Noise can create false edges, therefore before going further, it’s imperative to perform image smoothening. Gaussian filter is used to perform this process.            5.2.4 Canny Edge:
It computes gradient in all directions of our blurred image and traces the edges with large changes in intensity. For more explanation, please go through this article: Canny Edge Detector
5.3 ROI Selection:
Region of interest (ROI) is the rough translation of “relevant measurement range”. The term is used to refer to the relevant section of a measurement curve. This area can then be regarded preferably statistically. The concept of a ROI is commonly used in many application areas. For example, in medical imaging, the boundaries of a tumour may be defined on an image or in a volume, for the purpose of measuring its size.
This step is to take into account only the region covered by the road lane. A mask is created here, which is of the same dimension as our road image. Furthermore, bitwise AND operation is performed between each pixel of our canny image and this mask. It ultimately masks the canny image and shows the region of interest traced by the polygonal contour of the mask.
 5.4 Lane detection:
Line detection is as important as edge detection in lane detection. With regard to line detection, we usually have two methods which include Hough transform and bitwise operator. 
5.4.1 Hough transform:
The Hough transform is a feature extraction technique used in image analysis, computer vision, and digital image processing. The Probabilistic Hough Line Transform is used here, which gives output as the extremes of the detected lines.

5.5 Block diagram:













5.6 System Requirements:
General:
•	Operating system: Windows 10/8/7
•	Scripting Language: Python
•	Software used: atom 
Phyton Library’s required:  
•	NumPy:
NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.
•	Matplotlib:
Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, python, Qt, or GTK
•	OpenCV:   
OpenCV is a library of programming functions mainly aimed at real-time computer vision. Originally developed by Intel, it was later supported by Willow Garage then Itseez.
6. Methodology 
6.1 Pre-processing
Pre-processing is an important part of image processing and an important part of lane detection. Pre-processing can help reduce the complexity of the algorithm, thereby reducing subsequent program processing time. The video input is an RGB-based colour image sequence obtained from the camera. In order to improve the accuracy of lane detection, many researchers employ different image pre-processing techniques.
Smoothing and filtering graphics is a common image pre-processing technique. The main purpose of filtering is to eliminate image noise and enhance the effect of the image. Low-pass or high-pass filtering operation can be performed for 2D images, low- threshold is advantageous for denoising, and image blurring and high-threshold are used to find image boundaries. In order to perform the smoothing operation, an average, median, or Gaussian filter could be used in order to preserve detail and remove unwanted noise. 

6.2. Colour Transform
Colour model transform is an important part of computer vision, and it is also an indispensable part of lane detection. The actual road traffic environment and light intensity all produce noise that interferes with the identification of colour. We cannot detect the separation of white lines, yellow lines, and vehicles from the background. The RGB colour space used in the video stream is extremely sensitive to light intensity, and the effect of processing light at different times is not ideal. In this paper, the RGB sequence frames in the video stream are colour-converted into Gray colour space images. Following figures are images of RGB colour space and Gray colour space, respectively. And as we can see in the Gray scale image the values of black and white colours are very bright in the V-component compared to other colours and are easily extracted, providing a good basis for the next colour extraction. Experiments show that the colour processing performed in Gray scale is more robust to detecting specific targets.

		

 			Fig: RGB frame 					Fig: Gray scale frame 

6.3 Basic Pre-processing
a large number of frames in the video will be pre-processed. The images are individually Gray scaled, blurred, threshold of frame. In order to cater for different lighting conditions, an adaptive threshold is implemented during the pre-processing phase. Then, we remove the spots in the image obtained from the binary conversion and perform the morphological closing operation. The basic pre-processed frames cannot be very good at removing noise, there is still a large amount of noise. So we convert it into binary imaging inorder to remove the noise and the unwanted edges.


		 
Fig: Gray scale image					Fig: Binary Image 

6.4 ROI Selection
In order to lower image redundancy and reduce algorithm complexity, we can set an adaptive area of interest (ROI) on the image. We only set the input image on the ROI area and this method can increase the speed and accuracy of the system. In this paper, we use the standard KITTI road database [26]. We divide the image of each frame in the running video of the vehicle into two parts, and one-half of the lower part of the image frame serves as the ROI area. Figure 10 shows the ROI selection of sample frames (a), (b), (c), and (d) which are processed by the proposed pre-processing. The images of the four different sample frames have been able to substantially display the lane information after being processed by the proposed pre-processing method, but not only the lane information but also a lot of nonane noise is present in the upper half of the image. So, we cut out the lower half of the image (one-half) as the ROI area.
	




		
				   Fig: Roi selection from the frame
6.5Adding Edge Detection in Pre-processing:
Canny edge detection is done successively performed on a wide range of edge detection extraction in the entire frame image. In the second, the edge detection is performed again after the lane detection after ROI selection. This detection further improves the accuracy of lane detection. This section mainly performs the overall edge detection on the frame image, using the improved Canny edge detection algorithm. The concrete steps of Canny operator edge detection are as follows: First, we use a Gaussian filter to smooth the image (pre-processed image), and then we use the Sobel operator to calculate the gradient magnitude and direction. Next step is to suppress the nonmaximal value of the gradient amplitude. Finally, we need to use a double-threshold algorithm to detect and connect edges. Figure shows the image after extraction with Canny edge detection.

 
       Fig: canny edge
An edge relates to a locale in a picture where there is a sharp change in the force/shading between nearby pixels in the picture.  A solid slope is a lofty change and the other way around is a shallow change.
 
        Fig: plot to cut the ROI from the canny edge
  So, in a manner we can say a picture is a pile of lattice with lines and sections of forces. This implies that we can likewise speak to a picture in 2D facilitate space, x hub crosses the width (segments) and y hub comes the picture stature (columns).  Vigilant capacity plays out a subordinate on the x and y pivot subsequently estimating the adjustment in forces concerning contiguous pixels.   At   the   end   of   the day, we   are   processing   the inclination (which is change in brilliance) every which way. It at that point follows the most grounded inclinations with a progression of white pixels.  
 

The low threshold, high threshold permits us to disconnect the nearby pixels that follow the most grounded angle.  On the off chance that the slope is bigger than the upper edge then it is acknowledged as an edge pixel, on the off chance that it's underneath the low limit, at that point it is dismissed. On the off chance that the inclination is between the edges then it is acknowledged just if it’s associated with a solid edge. 

Canny edge detection basically uses gradient vector of an intensity image. Lane boundaries have high contrast in the image, and this feature yields high values of gradient vector by which we can find the edge direction, which is orthogonal to gradient vector. Many edge detection methods are based on this principle, but the efficiency levels are different. One of the best and efficient methods is canny edge detection. The most important characteristics of canny method are that the error rate of this method is low because this algorithm uses double thresholding, hysteresis thresholding Hysteresis threshold, double thresholding, suppresses the pixels that are not related to edges. Therefore, the detected edge is really close to true place. We should also mention that canny edge detector is very sensitive to noise; therefore, we smooth the image by a low pass filter to reduce the effect of noise.
6.6 Output frame
All the above methodologies will be done in sequence and to every frame that comes into the stream and give the best out come as possible and as a final step we use BITWISE AND to imply the canny edge to the original RGB frame so that the lines are seen on the stream of frames that are running in the screen.






                                      		Fig: result
6.7. visualization of the methodology  































        7.Road lane detection through Hough transform

7.1 introduction:
 Over the years range from 1992 to 2020 there were many researches done on this Hough transform based lane detection and they have performed good at some scenarios but while coming to the curved lane they failed to follow the path and that gave the idea of detecting the curved lane from the Hough transform with parabolic method.  When images are to be used in different areas of image analysis such as object recognition, it is important to reduce the amount of data in the image while preserving the important, characteristic, structural information. Edge detection makes it possible to reduce the amount of data in an image considerably. However, the output from an edge detector is still an image described by its pixels. If lines, ellipses and so forth could be defined by their characteristic equations, the amount of data would be reduced even more. The Hough transform was originally developed to recognize lines, and has later been generalized to cover arbitrary shapes. This worksheet explains how the Hough transform is able to detect (imperfect) straight lines.
7.2 the Hough space:
Lines can be represented uniquely by two parameters. Often the form in Equation 1 is used
with parameters a and b.

y = a · x + b (1)

  This form is, however, not able to represent vertical lines. Therefore, the Hough transform uses the form in Equation 2, which can be rewritten to Equation 3 to be similar to Equation 1.
The parameters θ and r is the angle of the line and the distance from the line to the origin
respectively.

r = x · cos θ + y · sin θ ⇔ (2)
y = − cos θ / sin θ · x + r /sin θ (3)
All lines can be represented in this form when θ ∈ [0, 180[ and r ∈ R (or θ ∈ [0, 360[ and r ≥ 0). The Hough space for lines has therefore these two dimensions; θ and r, and a line is represented by a single point, corresponding to a unique set of parameters (θ0, r0). The line-to-point mapping is illustrated in Figure below.
 
7.3 Mapping of Points to Hough Space
 An important concept for the Hough transform is the mapping of single points. The idea is, that a point is mapped to all lines, that can pass through that point. This yields a sine-like line in the Hough space. The principle is illustrated for a point p0 = (40, 30) in Figure below.


 
Fig: Transformation of a single point (p0) to a line in the Hough space. The Hough space line represents all possible lines through p0.
7.4	Algorithm
 	The algorithm for detecting straight lines can be divided into the following steps:

 1. Edge detection, e.g., using the Canny edge detector. 
 2. Mapping of edge points to the Hough space and storage in an accumulator. 
3. Interpretation of the accumulator to yield lines of infinite length. The interpretation is done by thresholding and possibly other constraints. 
4. Conversion of infinite lines to finite lines.
7.5 Transformation to Hough Space
 The Hough transform takes a binary edge map as input and attempts to locate edges placed as straight lines. The idea of the Hough transform is, that every edge point in the edge map is transformed to all possible lines that could pass through that point. Figure above illustrates this for a single point, and Figure below illustrates this for two points.
A typical edge map includes many points, but the principle for line detection is the same as illustrated in Figure below for two points. Each edge point is transformed to a line in the Hough space, and the areas where most Hough space lines intersect is interpreted as true lines in the edge map.
	
 
Fig: Transformation of two points (p0 and p1) to two lines in the Hough space. The intersection of the Hough space lines indicates the line that pass through both p0 and p1.
7.6 Detection of Infinite Lines
 Infinite lines are detected by interpretation of the accumulator when all edge points have been transformed. An example of the entire line detection process is shown in Figure below. The most basic way the detect lines is to set some threshold for the accumulator, and interpret all values above the threshold as a line. The threshold could for instance be 50% of the largest.
 
Fig: Line detection using the Hough transformation. The lines detected in the source image (Figure a) are marked with white boxes in the Hough transform

value in the accumulator. This approach may occasionally suffice, but for many cases additional constraints must be applied. As it is obvious from Figure e, several entrances in the accumulator around one true line in the edge map will have large values. Therefore, a simple threshold has a tendency to detect several (almost identical) lines for each true line. To avoid this, a suppression neighbourhood can be defined, so that two lines must be significantly different before both are detected.





8.Experiments with code:

Step 0: Introduction
Before we work with videos, let’s work with static images since it is much easier to debug with. Here is the image we will be working with
 .
                      Fig: input image
---------------------------------
#importing some useful packages
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
import cv2
import math
import sys
%matplotlib inline
---------------------------------
The lane detection pipeline follows these steps:
1.	Pre-process image using grayscale and gaussian blur
2.	Apply canny edge detection to the image
3.	Apply masking region to the image
4.	Apply Hough transform to the image
5.	Extrapolate the lines found in the Hough transform to construct the left and right lane lines
6.	Add the extrapolated lines to the input image


Step 1: Pre-processing of image
We grayscale the input image which is needed for canny edge detection.
--------------------------------------------------------------
def grayscale(img):
    """Applies the Grayscale transform
    This will return an image with only one color channel
    but NOTE: to see the returned image as grayscale
    you should call plt.imshow(gray, cmap='gray')"""
    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

image = mpimg.imread('test_images/solidYellowCurve2.jpg')
# grayscale the image
grayscaled = grayscale(image)
plt.imshow(grayscaled, cmap='gray')
--------------------------------------------------------------

 
        		  Fig: Gray scale image
We then apply a gaussian smoothing function to the image. Again, this is needed for the canny edge detection to average out anomalous gradients in the image.
--------------------------------------------------------------
def gaussian_blur(img, kernel_size):
    """Applies a Gaussian Noise kernel"""
    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)
  
# apply gaussian blur
kernelSize = 5
gaussianBlur = gaussian_blur(grayscaled, kernelSize)
--------------------------------------------------------------


Step 2: Canny Edge Detection
We need to detect edges for lane detection since the contrast between the lane and the surrounding road surface provides us with useful information on detecting the lane lines.
Canny edge detection is an operator that uses the horizontal and vertical gradients of the pixel values of an image to detect edges.
 
--------------------------------------------------------------
def canny(img, low_threshold, high_threshold):
    """Applies the Canny transform"""
    return cv2.Canny(img, low_threshold, high_threshold)

# canny
minThreshold = 100
maxThreshold = 200
edgeDetectedImage = canny(gaussianBlur, minThreshold, maxThreshold)
--------------------------------------------------------------
 
Fig: canny edge 

Step 3: Mask out points that are not in the region of interest
The region of interest for the car’s camera is only the two lanes immediately in its field of view and not anything extraneous. We can filter out the extraneous pixels by making a polygon region of interest and removing all other pixels that are not in the polygon.
--------------------------------------------------------------
def region_of_interest(img, vertices):
    """
    Applies an image mask.
    
    Only keeps the region of the image defined by the polygon
    formed from `vertices`. The rest of the image is set to black.
    """
    #defining a blank mask to start with
    mask = np.zeros_like(img)   
    
    #defining a 3 channel or 1 channel color to fill the mask with 
    #depending on the input image
    if len(img.shape) > 2:
        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image
        ignore_mask_color = (255,) * channel_count
    else:
        ignore_mask_color = 255
        
    #filling pixels inside the polygon defined by "vertices" with the fill color    
    cv2.fillPoly(mask, vertices, ignore_mask_color)
    
    #returning the image only where mask pixels are nonzero
    masked_image = cv2.bitwise_and(img, mask)
    return masked_image
  
#apply mask
lowerLeftPoint = [130, 540]
upperLeftPoint = [410, 350]
upperRightPoint = [570, 350]
lowerRightPoint = [915, 540]

pts = np.array([[lowerLeftPoint, upperLeftPoint, upperRightPoint, 
lowerRightPoint]], dtype=np.int32)
masked_image = region_of_interest(edgeDetectedImage, pts)
--------------------------------------------------------------

 
			Fig. Removed all pixels not in the region of interest


Step 4: Hough Transform
Now that we have detected edges in the region of interest, we want to identify lines which indicate lane lines. This is where the Hough transform comes in handy.
The Hough transformation converts a “x vs. y” line to a point in “gradient vs. intercept” space. Points in the image will correspond to lines in Hough space. An intersection of lines in Hough space will thus correspond to a line in Cartesian space. Using this technique, we can find lines from the pixel outputs of the canny edge detection output. A detailed explanation of the Hough transformation can be found here.
--------------------------------------------------------------
def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):
    """
    `img` should be the output of a Canny transform.
        
    Returns an image with hough lines drawn.
    """
    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), 
              minLineLength=min_line_len, maxLineGap=max_line_gap)
    line_img = np.zeros((*img.shape, 3), dtype=np.uint8)
    
    draw_lines(line_img, lines)
    return line_img

def draw_lines(img, lines, color=[255, 0, 0], thickness=2):
    """
    This function draws `lines` with `color` and `thickness`.    
    """
    for line in lines:
        for x1,y1,x2,y2 in line:
            cv2.line(img, (x1, y1), (x2, y2), color, thickness)
    
    
#hough lines
rho = 1
theta = np.pi/180
threshold = 30
min_line_len = 20 
max_line_gap = 20

houged = hough_lines(masked_image, rho, theta, 
                  threshold, min_line_len, max_line_gap)
--------------------------------------------------------------

 
Fig. The hough transform identifies the lane lines in the image
Step 5: Extrapolate the individual, small lines and construct the left and right lane lines

The Hough transform gives us small lines based on the intersections in Hough space. Now, we can take this information and construct a global left lane line and a right lane line.
We do this by separating the small lines into two groups, one with a positive gradient and the other with a negative gradient. Due to the depth of the camera’s view, the lane lines slant towards each other as we go further in depth, so that should have the opposite gradients.
We then take the average gradient and intercept values for each group and construct our global lane lines from there. The lane lines are then extrapolated to the edge detected pixel with the minimum y-axis coordinate and the pixel with the maximum y-axis coordinate.
To implement this algorithm, we simply change the draw line’s function stated before to extrapolate the lines.
---------------------------------------------------------------------------
def draw_lines(img, lines, color=[255, 0, 0], thickness=2):
    """
    This function draws `lines` with `color` and `thickness`.    
    """
    imshape = img.shape
    
    # these variables represent the y-axis coordinates to which 
    # the line will be extrapolated to
    ymin_global = img.shape[0]
    ymax_global = img.shape[0]
    
    # left lane line variables
    all_left_grad = []
    all_left_y = []
    all_left_x = []
    
    # right lane line variables
    all_right_grad = []
    all_right_y = []
    all_right_x = []
    
    for line in lines:
        for x1,y1,x2,y2 in line:
            gradient, intercept = np.polyfit((x1,x2), (y1,y2), 1)
            ymin_global = min(min(y1, y2), ymin_global)
            
            if (gradient > 0):
                all_left_grad += [gradient]
                all_left_y += [y1, y2]
                all_left_x += [x1, x2]
            else:
                all_right_grad += [gradient]
                all_right_y += [y1, y2]
                all_right_x += [x1, x2]
    
    left_mean_grad = np.mean(all_left_grad)
    left_y_mean = np.mean(all_left_y)
    left_x_mean = np.mean(all_left_x)
    left_intercept = left_y_mean - (left_mean_grad * left_x_mean)
    
    right_mean_grad = np.mean(all_right_grad)
    right_y_mean = np.mean(all_right_y)
    right_x_mean = np.mean(all_right_x)
    right_intercept = right_y_mean - (right_mean_grad * right_x_mean)
    
    # Make sure we have some points in each lane line category
    if ((len(all_left_grad) > 0) and (len(all_right_grad) > 0)):
        upper_left_x = int((ymin_global - left_intercept) / left_mean_grad)
        lower_left_x = int((ymax_global - left_intercept) / left_mean_grad)
        upper_right_x = int((ymin_global - right_intercept) / right_mean_grad)
        lower_right_x = int((ymax_global - right_intercept) / right_mean_grad)

        cv2.line(img, (upper_left_x, ymin_global), 
                      (lower_left_x, ymax_global), color, thickness)
        cv2.line(img, (upper_right_x, ymin_global), 
                      (lower_right_x, ymax_global), color, thickness)
---------------------------------------------------------------------------

 
Fig. Extrapolated lane lines
Step 6: Add the extrapolated lines to the input image
We then overlay the extrapolated lines to the input image. We do this by adding a weight value to the original image based on the detected lane line coordinate.
--------------------------------------------------------------
def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):
    """
    `img` is the output of the hough_lines(), An image with lines drawn on it.
    Should be a blank image (all black) with lines drawn on it.
    
    `initial_img` should be the image before any processing.
    
    The result image is computed as follows:
    
    initial_img * α + img * β + λ
    NOTE: initial_img and img must be the same shape!
    """
    return cv2.addWeighted(initial_img, α, img, β, λ)
  
# outline the input image
colored_image = weighted_img(houged, image)
---------------------------------------------------------------------------

 
     Fig. Line lanes overlaid on the input image
Step 7: Add pipeline to video
Once we have designed and implemented the entire car lane detection pipeline, we apply the transformation frame by frame. We need a video of car driving on lane lines to do this.
---------------------------------------------------------------------------
from moviepy.editor import VideoFileClip
from IPython.display import HTML

def process_image(image):
    # grayscale the image
    grayscaled = grayscale(image)

    # apply gaussian blur
    kernelSize = 5
    gaussianBlur = gaussian_blur(grayscaled, kernelSize)

    # canny
    minThreshold = 100
    maxThreshold = 200
    edgeDetectedImage = canny(gaussianBlur, minThreshold, maxThreshold)

    # apply mask
    lowerLeftPoint = [130, 540]
    upperLeftPoint = [410, 350]
    upperRightPoint = [570, 350]
    lowerRightPoint = [915, 540]

    pts = np.array([[lowerLeftPoint, upperLeftPoint, upperRightPoint, 
                    lowerRightPoint]], dtype=np.int32)
    masked_image = region_of_interest(edgeDetectedImage, pts)

    # hough lines
    rho = 1
    theta = np.pi/180
    threshold = 30
    min_line_len = 20 
    max_line_gap = 20

    houged = hough_lines(masked_image, rho, theta, threshold, min_line_len, 
                         max_line_gap)

    # outline the input image
    colored_image = weighted_img(houged, image)
    return colored_image

output = 'car_lane_detection.mp4'
clip1 = VideoFileClip("insert_car_lane_video.mp4")
white_clip = clip1.fl_image(process_image)
%time white_clip.write_videofile(output, audio=False)
---------------------------------------------------------------------------

 
                     		Fig: output of the project 

    9.Limitations 
Lane Departure Warning Systems and Lane Keeping Systems totally rely upon visible lane markings. They typically cannot work out on faded, absent, or inappropriate lane markings. Markings covered with snow or old lane markings left visible can hinder the ability of the system. The analysis of the literature and existing algorithms, project that majority of these algorithms assume the conditions and situations to be close to ideal and neglect the following: 
1) Environmental Conditions: The survey showed that the existing techniques provide good accuracy and efficiency for high-quality images but sometimes, it additionally provides negative outcomes, for environmental conditions such as fog, haze, noise, and dust.
 2) Curved Lanes Not Dealt Properly: Most of the existing techniques work best for straight lanes, but they offer poor and inaccurate results for curved roads which won't generate a warning at right time and may show to be fatal. 
3) Most of the lane detection techniques are based on standard Hough transforms which leaves a gap for improving accuracy.



ROAD LANE DETECTION FOR AUTONOMOUS VEHICLES USING OPENCV

A project report submitted in partial fulfilment of the requirements for the award of degree in                                                 

Bachelor of Computer Applications (BCA)

By

VENKATESH CHALIBINDI
(Regd. No: 121812501022)

Under the esteemed guidance of 

Dr. M. Sri Venkatesh
Assistant Professor
 
	Department of Computer Science

GITAM Institute of Science

GITAM (Deemed to be University)

Visakhapatnam -530045, A.P

(2020-2021)





  


CERTIFICATE



This is to certify that the project entitled “ROAD LANE DETECTION FOR AUTONOMOUS VEHICLES USING OPENCV” is a Bonafede work done by VENKATESH CHALIBINDI, Regd. No: 121812501022 during February 2020 to May 2021 in partial fulfilment of the requirement for the award of degree of Bachelor of Computer Applications (BCA) in the Department of Computer Science, GITAM Institute of Science, GITAM (Deemed to be University), Visakhapatnam.









Internal Guide				           Head of the Department

Dr. M. Sri Venkatesh                                                   Ms.Vedavathi Katneni
Assistant Professor                                                       Professor 
Dept of Computer Science, GIS                            	   Dept of Computer Science, GIS          
GITAM                                                                     	    GITAM 
 









DECLARATION


I VENKATESH CHALIBINDI, Regd. No: 121712501042 hereby declare that the project entitled “ROAD LANE DETECTION FOR AUTONOMOUS VEHICLES USING OPENCV” is an original work done in the partial fulfilment of the requirements for the award of degree of Bachelor of Computer Applications (BCA) in GITAM Institute of Science, GITAM (Deemed to be University), Visakhapatnam. I assure that this project work has not been submitted towards any other degree or diploma in any other colleges or universities.





									VENKATESH CHALIBINDI
                                                                                            (Regd. No:121812501022)  














                       
                                                     ACKNOWLEDGEMENT


The satisfaction that accompanies the successful completion of any task would be incomplete without the mention of people who made it possible and whose constant guidance and encouragement crown all the efforts with success. 

I would like to express my deep sense of gratitude and sincere thanks to Assistant Professor                                                Dr. M. Sri Venkatesh , Department of Computer Science, for his constant guidance, supervision and motivation in completing the project. 

I would like to express my deep sense of gratitude and sincere thanks to my reviewer and Assistant Professor Ms.B. SATYA SAI VANI , Department of Computer Science, for her constant guidance, supervision and motivation in completing the project. 

I extremely elated to extend my gratitude to Ms.Vedavathi Katneni, Professor and Head of the Department of Computer Science, for her encouragement all the way during this project. Her annotations and insinuation are the key behind the successful completion of the project. 







									VENKATESH CHALIBINDI
    									   (Regd. No:121812501022)  




							INDEX
1.Introdction
 	1.1 Types of lanes
  	1.2 Lane detection 
  	1.3 Benefits of lane detection
     1.4 Problem domain and motivation
2.Related work	
3. Literature review
4.Overview Of The Proposed System 
5. Proposed method and requirements
	 5.1 video acquisition
 	5.2 pre-processing
		 5.2.1 capturing and decoding video file
		5.2.2 gray scale conversion of image
		5.2.3 gaussiann blur
	5.3 ROI selection 
	5.4 lane detection from Hough transform 
 		5.4.1 Hough transform 
	5.5 block diagram of proposed system
	5.6 system requirements
6.Methodology
	6.1 pre-processing
	6.2 colour transform
	6.3 basic pre-processing
	6.4 ROI selection
	6.5 adding Edge detection in pre-processing
	6.6 output frame
	6.7 visualization of the methodology
7.Road lane detection though Hough transform 
 	7.1 introduction 
	7.2 the Hough space
 	7.3 mapping of points to Hough space
 	7.4 basic algorithm
 	7.5 transformation to Hough space
    7.6 detection of infinite line
8.Experiments with code 
9.Limitations
10.References

 

                                                    ABSTRACT

Today, unmanned vehicle technologies are developing in parallel with increasing interest in technological developments. These developments aim to improve people's quality of life. Transportation, which is a part of human life, has taken its share from this developing technology.  With the development of artificial intelligence, it is aimed to provide the necessary assistance to the driver in transportation and to provide ease of driving. Fuelled by Deep Learning algorithms, they are continuously driving our society forward and creating new opportunities in the mobility sector. For vehicles to be able to drive by themselves, they need to understand their surrounding world like human drivers, so they can navigate their way in streets, pause at stop signs and traffic lights, and avoid hitting obstacles such as other cars and pedestrians. Based on the problems encountered in detecting objects by autonomous vehicles an effort has been made to demonstrate lane detection using OpenCV library.

















1.	INTRODUCTION
Traffic accidents have become one of the most serious problems in today's world. Roads are the mostly chosen modes of transportation and provide the finest connections among all modes. Most frequently occurring traffic problem is the negligence of the drivers and it has become more and more serious with the increase of vehicles.
Increasing the safety and saving lives of human beings is one of the basic functions of Intelligent Transportation System (ITS). Intelligent transportation systems are advanced applications which aim to provide innovative services relating to different modes of transport and traffic management. This system enables various users to be better informed and make safer, more coordinated, and smarter use of transport networks.
These road accidents can be reduced with the help of road lanes or white markers that assist the driver to identify the road area and non-road area. A lane is a part of the road marked which can be used by a single line of vehicles as to control and guide drivers so that the traffic conflicts can be reduced.

 
	      	Fig 1 Road scene image
Most roads such as highways have at least two lanes, one for traffic in each direction, separated by lane markings. Major highways often have two roadways separated by a median, each with multiple lanes. To detect these road lanes some system must be employed that can help the driver to drive safely.

Lane detection is an area of computer vision with applications in autonomous vehicles and driver support systems.
 
Fig 2: Lane detection

Despite the perceived simplicity of finding white markings on a simple road, it can be very difficult to determine lane markings on various types of road. These difficulties can be shadows, occlusion by other vehicles, changes in the road surfaces itself, and different types of lane markings. A lane detection system must be able to detect all manner of markings from roadways and filter them to produce a reliable estimate of the vehicle position relative to the lane.

1.1.1 Types of Lanes
•	Traffic Lane: Lane for the vehicles moving from one destination to another
•	Express Lane: Used by faster moving traffic and has less access to exits/off ramps
•	Reversible Lane: To match the peak flow direction of vehicles is changed. Periods of high traffic flow are accommodated by this lane.
•	 Auxiliary Lane: Used for separating entering, exiting or turning traffic from the through    traffic.
•	 In some areas, for non-moving vehicles lane adjacent to curb is reserved.
1.1.2 Lane Detection 
Lane detection is one significant method in the visualization-based driver support structure and capable to be used for vehicle routing, cross power, crash avoidance, or lane departure warning system. Different road condition that creates this difficulty more complex include dissimilar variety of lanes (straight or rounded), occlusions cause by obstacle, fog, darkness, illumination change (like night-time), and so on. Therefore, it is the method to locate lane in the picture and is a significant enable or attractive skill in different automobile application, include lane departure recognition and warning, travel control, cross-control, and self-directed driving.
A lane departure warning system (LDWS) is a technology designed for warning a driver when the vehicle begins to depart from its lane. An effective lane detection system will navigate autonomously or assist driver in all types of lanes like straight and curved, white and yellow, single and double, solid and broken and pavement or highway lane boundaries. The system should be able to detect lane even under noisy conditions such as fog, shadow, and stain.


1.1.3 Benefits of Lane Detection:
•	Gives assistance and details to pedestrians and drivers 
•	Uniformity of the markings is an important factor in minimizing confusion and uncertainty about their meaning 
•	 Allows vehicular drivers to drive safely
•	Gives assistance and information to drivers. 
•	 Uniformity of the markings is a very important aspect in minimizing confusion and uncertainty approximately about their meaning. 
•	 Allows drivers to drive vehicles safely and protect the passengers from any mis happenings.  
•	The warnings signal will alert the passengers also









1.2	PROBLEM DOMAIN AND MOTIVATION	
As mentioned previously, the most appealing feature of self-driving cars is the ability to transport passengers and/or cargo, from point A to point B, autonomously in a safe manner. In order to achieve this goal, the vehicle should have some form of road following system, traversing through both rural and busy urban streets while abiding all the existing traffic laws.
 Two common ways for a car to follow a road, without modifying existing roads or attaching various sensors on other cars, is to either use GPS tracking or machine vision. According to the U.S. government, the accuracy of a GPS tracking unit is approximately 7.8m at 95% confidence level. Lanes on average are between 2-4m width, so the accuracy of a GPS is not good enough to keep a car within its lane reliably. 
The other approach, machine vision, would rely on lane markings and other road features extracted from the footage of an on-board camera to enable lane detection and subsequent following. However, lane markings may not be always clearly defined, the camera view may get obstructed by on-road traffic or other obstacles and various weather and light conditions could affect the visibility of the camera. Nevertheless, this approach provides the most accurate horizontal position of a car within a road and with well-thought algorithms and hardware addons, the aforementioned problems can be alleviated or downright eliminated.
 Ideally, you would use both methods to get to set point B, machine vision and other sensors to keep the car on its lane and avoid obstacles, and GPS tracking to identify the vehicle’s position in relation to the set final destination and derive a path to it.
In Carolo -Cup (an international student competition of the Technical University of Braunschweig and takes place annually. The aim of the competition is to develop an autonomous 1:10 scale model vehicle) one of the main requirements is to be able to follow the lanes of a previously unmet and unmapped road, the participating teams are not informed of the layout of the road beforehand. This outright eliminates GPS tracking. As the participating vehicles are 1/10 size of real cars, the course track is miniaturised accordingly so the inaccuracy of GPS is magnified by factor of 10. Therefore, machine vision remains the sole solution. The competition track is arranged as a circuit and the aim of the lane following discipline is to complete as many laps as possible in the allotted time, there is no need for a car to go to a specific location in the track. The lane following discipline also includes intersection scenarios, missing lane markings and obstacles on the road. The parking discipline may also require lane following for the purpose of keeping the car in the middle of its lane.
				TABLE I 

Lane Detection	
Extract and identify lane markings correctly
Lane Following	Derive trajectory based on the extracted lane markings or other approximations. The trajectory should lead to the middle of the lane the car is located in.


2.RELATED WORK 
        Safety is the main objective of all the road lane detection systems due to the reason is that most of the vehicle road accident happens because of the driver miss leading of the vehicle path. Therefore, currently many different vision-based roads detection algorithms have been developed to avoid vehicle crash on the road. Among these algorithms the GOLD system developed by Broggi, it uses an edge-based lane boundary detection algorithm. 
The acquired image is remapped in a new image representing a bird’s eye view of the road where the lane markings are nearly vertical bright lines on a darker background. Specific adaptive filtering is used to extract quasi vertical bright lines that concatenated into specific larger segments. Kreucher C. propose in the LOIS algorithm as a deformable template approach. A parametric family of shapes describes the set of all possible ways that the lane edges could appear in the image.
 A function is defined whose value is proportional to how well a particular set of lane shape parameters matches the pixel data in a specified image. Lane detection is performed by finding the lane shape that maximizes the function for the current image. The Carnegie Mellon University proposes the RALPH system, used to control the lateral position of an autonomous vehicle. It uses a matching technique that adaptively adjusts and aligns a template to the averaged scan line intensity profile in order to determine the lane’s curvature and lateral offsets. The same university developed another system called AURORA which tracks the lane markers present on structured road using a colour camera mounted on the side of a car pointed downwards toward the road.
 A single scan line is applied in each image to detect the lane markers. An algorithm destined to painted or unpainted road is described in. Some colour cues were used to conduct image segmentation and remove the shadow. Assuming that the lanes are normally long with smooth curves then their boundaries can be detected using Hough transformation applied to the edge image. A temporal correlation assumed between successive images is used in the following phase. Three-feature based automatic lane detection algorithm (TFALDA) is primarily intended for automatic extraction of the lane boun…

---
## [jappeace/unwitch](https://github.com/jappeace/unwitch)@[26128f2b7f...](https://github.com/jappeace/unwitch/commit/26128f2b7fbbc2414e99ff2928e065cb67260a19)
#### Saturday 2021-08-21 19:58:56 by Jappie Klooster

Initial template

Disable tests as they don't work for some reason

Initial commit

run is broken and sdist should update cabal

Some more warnings

Ship license

Shell regeneartes default.nix before doing any cabal stuff

Now detect native dependencies

yes this is the whole point of using nix..

Better project description

mtl is better for libs

Use default extensions

watch more files

Stylish haskell works now with default lang imports

Now we can handle dependnecies, but lost cabal as build input

Get rid of dependency that we can manage

todo items

fix sdist

don't set a default github, it's wrong and should be updated on

publishing

Figured out how to unfree

Remove pin to seperate file

Update pin

REMOTE BUILD FARM FRAMEWORK

Also watch pin

HAX for dir selection

Better pin syntax, upgrade pin

Add ghcid and a test suite, Add overriding shell cabalities

Better readme

Even better readme

Add cabal install too shell

Independent nix-shell for update, more robuust

Add etags to ghcid

Use a seperate stable hpack shell for generating default.nix

This breaks the dependency upon default.nix for generating
it in the first place.

The ability to run is nice as well

Add all badges so we can just remove whatever neccisary

(branding by default, which is good because I'll definitly forget).

Add ghci command using the cabal repl

Allow easy building with travis

Adding a badge

Move build status away from branding

Excplicitly mention file name

Move badge next to the other ones

Link badge to builds

Add discord link

Add cachix support for faster builds!

Add checks for formattign

Fixup hlint issues

Don't depend on pin for cachix, it won't get the env variable & slow

Maybe we need findutils to find stuff

Explain why these shells

Explain what this shell is doing

Better description of what this does

This should fix britanny issue

Add brittany management spells

Apply formatting everywehre

Add autoamtic hlint refactor

Bump year

Add haddock hackage

Add gitignoreSource

Fix library tool depends

Use callCabal2Nix and move nix expressions to subfolder

Make logos smaller

Simplify setup for testing, make ghic work reliably

Give readme some more love

Generate etags from hpack shell

Don't need that send script anymore

Haven't used this in ages

Add more rigor to sdist

Fix hercules CI sha

Add changelog

Add testing against various ghc versions for CI

Reduce the amount of supported compilers

Add rudementary description of how to use the template

Better describe templates in git idea

LInke to project example

Add github actions for more lazy building

Make template checklist

Add github actions badge

Fixup readme checklist

Add bundle support

Upgrade nix install blah

Upgrade pin

Fix ci build

Add stack based ci.

I can't test this locally so on the branch it goes.

---
## [hatkidchan/hatkidchan.github.io](https://github.com/hatkidchan/hatkidchan.github.io)@[1aa95c7d13...](https://github.com/hatkidchan/hatkidchan.github.io/commit/1aa95c7d135b57690f06adff6553b2351c051808)
#### Saturday 2021-08-21 21:47:06 by hkc

Absolutely no fucking idea what was wrong in here

Yeah. I hate web

---
## [uakci/toadua](https://github.com/uakci/toadua)@[ab589699e7...](https://github.com/uakci/toadua/commit/ab589699e7d9a528b9cff2d24da83ebe35a16a4d)
#### Saturday 2021-08-21 22:59:35 by uakci

my god idk any more

* removed snyk, added webpack + babel + vue and a buncha scripts
* database migrated from messagepack to gzipped json
* restructured frontend/, added a library for both ends at shared/
* made entries auto-normalize themselves, finally, and painstakingly
  implemented this in the webapp
* fixed error handling / shutdown logic
* there might be other things but i can't remember at this point

---
## [SylvainTran/V00717](https://github.com/SylvainTran/V00717)@[baf9faf4ae...](https://github.com/SylvainTran/V00717/commit/baf9faf4ae9afac7a5a509a57faf0b91c1c89ec3)
#### Saturday 2021-08-21 23:19:32 by Sylvain Tran

Added real game terrain; added base male (headless) with idle and walking animation; Added weather system; Adjusted audition page and various fixes; Setup camera lanes in quadrant at various angles

This commit packs in the art big time, and it's bringing much needed life. I should have dropped these art assets sooner. It really does change the perceived feelings (and that could even re-adjust priorities?). Anyways, we have a bunch of headless people walking around in a foggy, mystical landscape of four seasons. It somehow feels spiritual to watch headless beings walk in a barren landscape. This commit bears a few problems with the nav agent setup on the navmesh so sometimes the character bots get stuck in place (will fix that next).

---

# [<](2021-08-20.md) 2021-08-21 [>](2021-08-22.md)

