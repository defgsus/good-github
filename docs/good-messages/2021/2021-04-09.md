# [<](2021-04-08.md) 2021-04-09 [>](2021-04-10.md)

2,984,647 events, 1,532,749 push events, 2,621,912 commit messages, 192,274,485 characters


## [limograf/caldervalleyscouts@a4ecb3903f...](https://github.com/limograf/caldervalleyscouts/commit/a4ecb3903fceceda8049631ce595a46997f4216e)
##### 2021-04-09 13:20:33 by Pip

Changed boys and girls to young people or children.

This was a suggestion from my friend after she looked at the website.

---
## [InfoTeddy/VVVVVV@300f1b7919...](https://github.com/InfoTeddy/VVVVVV/commit/300f1b791956f864a6d4333c0c2edb6b56097ab5)
##### 2021-04-09 16:07:27 by Misa

Switch assets mounting to dedicated directory

This fixes an issue where you would be able to mount things other than
custom assets in per-level custom asset directories and zips.

To be fair, the effects of this issue were fairly limited - about the
only thing I could do with it was to override a user-made quicksave of a
custom level with one of my own. However, since the quicksave check
happens before assets are mounted, if the user didn't have an existing
quicksave then they wouldn't be able load my quicksave. Furthermore,
mounting things like settings.vvv simply doesn't work because assets
only get mounted when the level gets loaded, but the game only reads
from settings.vvv on startup.

Still, this is an issue, and just because it only has one effect doesn't
mean we should single-case patch that one effect only. So what can we
do?

I was thinking that we should (1) mount custom assets in a dedicated
directory, and then from there (2) mount each specific asset directly -
namely, mount the graphics/ and sounds/ folders, and mount the
vvvvvvmusic.vvv and mmmmmm.vvv files. For (1), assets are now mounted at
a (non-existent) location named .vvv-mnt/assets/. However, (2) doesn't
fully work due to how PhysFS works.

What DOES work is being able to mount the graphics/ and sounds/ folders,
but only if the custom assets directory is a directory. And, you
actually have to use the real directory where those graphics/ and
sounds/ folders are located, and not the mounted directory, because
PHYSFS_mount() only accepts real directories. (In which case why bother
mounting the directory in the first place if we have to use real
directories anyway?) So already this seems like having different
directory and zip mounting paths, which I don't want...

I tried to unify the directory and zip paths and get around the real
directory limitation. So for mounting each individual asset (i.e.
graphics/, sounds/, but especially vvvvvvmusic.vvv and mmmmmm.vvv), I
tried doing PHYSFS_openRead() followed by PHYSFS_mountHandle() with that
PHYSFS_File, but this simply doesn't work, because PHYSFS_mountHandle()
will always create a PHYSFS_Io object, and pass it to a PhysFS internal
helper function named openDirectory() which will only attempt to treat
it as a directory if the PHYSFS_Io* passed is NULL. Since
PHYSFS_mountHandle() always passes a non-NULL PHYSFS_Io*,
openDirectory() will always treat it like a zip file and never as a
directory - in contrast, PHYSFS_mount() will always pass a NULL
PHYSFS_Io* to openDirectory(), so PHYSFS_mount() is the only function
that works for mounting directories.

(And even if this did work, having to keep the file open (because of the
PHYSFS_openRead()) results in the user being unable to touch the file on
Windows until it gets closed, which I also don't want.)

As for zip files, PHYSFS_mount() works just fine on them, but then we
run into the issue of accessing the individual assets inside it. As
covered above, PHYSFS_mount() only accepts real directories, so we can't
use it to access the assets inside, but then if we do the
PHYSFS_openRead() and PHYSFS_mountHandle() approach,
PHYSFS_mountHandle() will treat the assets inside as zip files instead
of just mounting them normally!

So in short, PhysFS only seems to be able to mount directories and zip
files, and not any loose individual files (like vvvvvvmusic.vvv and
mmmmmm.vvv). Furthermore, directories inside directories works, but
directories inside zip files doesn't (only zip files inside zip files
work).

It seems like our asset paths don't really work well with PhysFS's
design. Currently, graphics/, sounds/, vvvvvvmusic.vvv, and mmmmmm.vvv
all live at the root directory of the VVVVVV folder. But what would work
better is if all of those items were organized into a subfolder, for
example, a folder named assets/. So the previous assets mounting system
before this patch would just have mounted assets/ and be done with it,
and there would be no risk of mounting extraneous files that could do
bad things. However, due to our unorganized asset paths, the previous
system has to mount assets at the root of the VVVVVV folder, which
invites the possibility of those extraneous bad files being mounted.

Well, we can't change the asset paths now, that would be a pretty big
API break (maybe it should be a 2.4 thing). So what can we do?

What I've done is, after mounting the assets at .vvv-mnt/assets/, when
the game loads an asset, it checks if there's an override available
inside .vvv-mnt/assets/, and if so, the game will load that asset
instead of the regular one. This is basically reimplementing what PhysFS
SHOULD be able to do for us, but can't. This fixes the issue of being
able to mount a quicksave for a custom level inside its asset directory.

I should also note, the unorganized asset paths issue also means that
for .zip files (which contain the level file), the level file itself is
also technically mounted at .vvv-mnt/assets/. This is harmless (because
when we load a level file, we never load it as an asset) but it's still
a bit ugly. Changing the asset paths now seems more and more like a good
thing to do...

---
## [mrakgr/The-Spiral-Language@c93dc9da02...](https://github.com/mrakgr/The-Spiral-Language/commit/c93dc9da02a482939067a74c264b02fd7774ac63)
##### 2021-04-09 18:56:32 by Marko GrdiniÄ‡

"2:50pm. Done with breakfast and chores.

Let me resume.

First let me write down my thoughts.

In the sampling version of CFR, I think I will ditch regret updates completely and derive the current policy off the Q values.

In the original CFR, the regret updates do act as a weighted moving average of regrets, so reifying what is actually being done in the sampling version is not too great of a step. When I move to the NN version, that will simplify things greatly as I won't need a policy and Q network.

2:55pm. What is giving me trouble are the average strategy updates. I can't straightforwardly translate them to the sampling version. This is because I need to divide the thing by the opponent probabilities. This is doable in a simulator, but it is a red flag that I could not do this in real life.

I do not understand why policy averaging works the way it does in CFR.

I think I now completely understand the regret updates, and I can see how diving by self action probs acts as an learning accelerant.

3pm. No actually I do understand it.

I thinking on Rock Paper Scissors (I said it was Tic Tac Toe in one of the previous entries by mistake), if the other player probability was a factor, the the strategy averaging definitely would not work. It would not be 'averaging' in any sense.

3:05pm. I am going to believe the paper when it says that simply keeping around old policies and sampling from them is the right thing to do.

It is really quite remarkable how profound adding a few things together can be. I am in awe.

Doing strategy averaging without eliminating the other player's propabilities is definitely wrong, I can see that much.

3:10pm. Ok, I can see it.

1) I can see how not ignoring the opponent reach prob when doing policy averaging is wrong.

2) What about the chance node probability?

Imagine if there were a bunch of chance nodes with zero probability. We already know how the regret updates work with respect to that - they get ignored. During sampling those nodes would never happen and would never get optimized.

If I ignored chance probability, that would break strategy averaging.

So chance nodes should be a factor.

3) What about the self action probabilities?

3:15pm. This one is a bit hairier.

Let me consider the case where probability is zero again.

3:20pm. It is interesting. In regular CFR, the updates would happen, but in sampling those cases would never occur.

7:05pm. Done with lunch. I got a lot of inspiration and spent all this time in bed.

I think I understand completely now how to do a tabular sampling CFR variant. Not neural one though.

I was wrong. I thought that ommiting the self probability in the regret update acts as an accelerant, but that is not the right way of thinking about it.

What that really does is cut the whole graph into a large number of pieces.

But I've realized as I thought that `Q / self_prob`s are not the equivalent of the full path `Q`s.

In fact, if I tried to sample from them directly, I would get biased results!

I've realized that the reason strategy averaging makes sense is because in a way it is in fact optimizing the full graph.

I remember how CFR oscilates between values in the current policy.

As a result of that, the strategy update does in fact do something similar to Monte Carlo updates.

This is the only way to see this.

If strategy averaging was not optimizing the full graph, if something like `self_prob` was ommited in its multiplication, then the solution could not be found in the first place. I do not know what exactly it should be called, but what CFR is doing is definitely not strategy averaging. It something closer to hierarchical RL.

Because you have those selfless `Q / prob_self`s which act as an invariant separate from end-to-end optimization. And they you have a separate process taking advantage of that to speed up learning.

This makes me realize, simply sampling the old policies would not work. Because I can easily think of an example where such a thing would give biased results. If it were regular `Q`s then maybe it would, but not with those selfless `Q`s.

In order for a sampling based algorithm in the spirit of the original CFR to work, what I am going to have to do are MC rollouts.

One thing that worried me, which I've realized is not really an issue is the opponent probability. I said that I could not get that in real life, but in strategy averaging it is not important that I get the opponent probability. I can just substitute in my own and then divide by it.

Also one more thing...

```
    action = dyn fun {chance_prob player player' actions next} =>
        inl num_actions = length actions
        inl op_prob = exp_log_prob (add_log_prob chance_prob player')
        inl self_prob = exp_log_prob (add_log_prob chance_prob player.prob)
        inl d = value d num_actions player.observations
        inl actions_prob = regret_match (if is_avg_policy then d.avg_policy else d.regret)
        inl reward,reward_wsum =
            inl state = state player
            am.mapFold2 (fun s prob a =>
                inl util =
                    if prob = 0 && op_prob = 0 then 0 // the pruning optimization
                    else next ((to_log_prob prob,a),state)
                util, s + prob * util
                ) 0 actions_prob actions
        if is_update then
            d.regret |> am.mapInplace (fun i x => x + op_prob * (index reward i - reward_wsum))
            d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
        reward_wsum
```

This implementation of CFR is actually incorrect.

```
d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
```

I have this problem in the F# version as well. I definitely should not be multiplying the current action probability in `self_prob * index actions_prob i`. Rather...

```
        if is_update then
            d.regret |> am.mapInplace (fun i x => x + op_prob * (index reward i - reward_wsum))
            inl actions_prob = regret_match d.regret
            d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
```

Definitely like this. In fact...

```
        inl reward,reward_wsum =
            inl state = state player
            am.mapFold2 (fun s prob a =>
                inl util =
                    if prob = 0 && op_prob = 0 then 0 // the pruning optimization
                    else next ((to_log_prob prob,a),state)
                util, s + prob * util
                ) 0 actions_prob actions
```

Instead of these rewards I should consider weighting them by the updated policy. But that is not as important as the strategy update.

I can see on a toy example, that using the stale action probs instead of the updated ones would completely break the optimization scheme.

```
    action = dyn fun {chance_prob player player' actions next} =>
        inl num_actions = length actions
        inl op_prob = exp_log_prob (add_log_prob chance_prob player')
        inl self_prob = exp_log_prob (add_log_prob chance_prob player.prob)
        inl d = value d num_actions player.observations
        let actions_prob () = regret_match d.regret
        // This one might give better results during training.
        // let actions_prob () = regret_match (if is_update then d.regret else d.avg_policy)
        inl reward,reward_wsum =
            inl state = state player
            am.mapFold2 (fun s prob a =>
                inl r =
                    if prob = 0 && op_prob = 0 then 0 // the pruning optimization
                    else next ((to_log_prob prob,a),state)
                r, s + prob * r
                ) 0 actions_prob() actions
        if is_update then
            d.regret |> am.mapInplace (fun i x => x + op_prob * (index reward i - reward_wsum))
            inl actions_prob = actions_prob()
            d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
            am.fold2 (fun s prob r => s + prob * r) 0 actions_prob reward
        else
            reward_wsum
```

Agh, why am I still thinking about this? So much about doing the UIs today.

7:45pm. I should read the single deep CFR paper, and look at the DREAM paper again, but literally nothing in those papers makes sense apart from the overall bootstraping scheme. I don't trust any of it.

This is the ML in a nutshell today. Will I really succeed at this? But I suppose today was good, as my understanding of CFR went from 50 to about 70%.

7:55pm. No, I am not there yet. Right now, I do not think I understand the chance nodes well enough.

There is a similarity between the opponent probabilities and the environment.

```
inl self_prob = exp_log_prob (add_log_prob chance_prob player.prob)
...
d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
```

Why does the average policy update need the chance probabilities? Why not the opponents' as well? Both of them can be considered a part of the environment as far as the optimization algorithm is concerned, and the selfless `Q`s already consider that part of the probability as well.

I am not doing this right. I am obsessing about the algorithm ahead of time and making grinding progress when I really should be doing real life testing in order to generate insight.

8pm. Let me play Rance here. I should have stopped at 6pm. There will be plenty of time to think about this in the future.

8:05pm. This is just so hard to understand. In the worst case, I could always just use policy gradients with my buffer tricks.

Yeah, maybe PG with selfless `Q`s for variance reduction would not be a bad idea. I can't actually constantly add values in a NN regime like in the tabular case because it would unhinge the lower layers. This is even with the trick I will use for training the `Q` nets.

8:15pm. Agh, forget Rance. Let me think about it for a while longer.

I am thinking abuot the PG net + selfless `Q`s. Suppose I ommitted opponent's probabilities from it. What sort of graph would I get. What sort of problem would be optimized for?

Yeah, it would not be good. The problem being solved would be wrong.

8:20pm. Yeah, that is exactly it. And on the same problem I could cause the strategy averaging CFR scheme to go wrong.

```
d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
```

Is the fact that it is only using `self_prob` just a hack that happened to work on the problem the authors were working on, but is actually broken when seen more broadly.

8:55pm. Though I am pushing it as hard as I can, my brain is just not working right now. Though the example is simple, my mental muscle is too slack to get to the bottom of it. I am relying too much on visualization and not enough on actual numbers.

I should open an F# script and try to code my idea up since I obviously can't hold everything in my head.

Forget it, I am done for the day."

---
## [thestubborn/Skyrat-tg@847b97e6d5...](https://github.com/thestubborn/Skyrat-tg/commit/847b97e6d583cf555b5ea71f03fcd0cc2196cdf7)
##### 2021-04-09 19:57:34 by grandwizmiller

Mazovian Socio-Economics

People think Communism was some crazy idea that had its comeuppance 40 years ago. A fever that shook the world, never to return again. They were right. Until *he* woke up today â€“ a spiritual corpse responsive only to the call of Commodore Red, prostitutes, and Kras Mazov. For him, Communism is still a *thing*. He will single-handedly raise the Commune of '02 from the oceanic trench where it has been resting, covered in ghosts and seaweed! He is the Big Communism Builder. Come, witness his attempt to rebuild Communism in the year '51!

0.000% of Communism has been built. Evil child-murdering billionaires still rule the world with a shit-eating grin. All he has managed to do is make himself *sad*. He is starting to suspect Kras Mazov *fucked him over* personally with his socio-economic theory. It has, however, made him into a very, very smart boy with something like a university degree in Truth. Instead of building Communism, he now builds a precise model of this grotesque, duplicitous world.

---
## [nectarboy/gameboy@6770401261...](https://github.com/nectarboy/gameboy/commit/677040126163f4974d77bc7664438a5e132a065e)
##### 2021-04-09 20:32:33 by nectarboy

i think im done wit pollen boy ~

i added the last stuff like a volume slider n a pitch shift -- and finally custom palletes ~
i tried to add the halt bug - nearly there but sumn with IF reg ~
i fixed sum goofy ass code in readByte and writeByte - that shit has been there since the beginning ~

maybe ill come back and add some more stuff, who knows
ofc, ill push fixes and stuff if i find bugs, whenever

buhbye - this was fun as fuck :D

---

# [<](2021-04-08.md) 2021-04-09 [>](2021-04-10.md)

