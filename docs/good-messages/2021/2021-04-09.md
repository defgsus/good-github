# [<](2021-04-08.md) 2021-04-09 [>](2021-04-10.md)

2,984,647 events, 1,532,749 push events, 2,621,912 commit messages, 192,274,485 characters


## [mitchburnett/mlib_devel](https://github.com/mitchburnett/mlib_devel)@[005d801b88...](https://github.com/mitchburnett/mlib_devel/commit/005d801b88e8211fb1d1373bfff23db7c7948511)
#### Friday 2021-04-09 03:02:15 by Mitch Burnett

working snapshot example design using all four tiles

Updates `toolflow.py` to create project in `init` stage and then the platform
yellow block (e.g., zcu216) has an `init` call that instances the board design.
This allows allows other yellow blocks to now access the platforms board design
and either add IP to the board design or update IP already placed in a sourced
board design. This seemed to be the quickest and least painful of all the
methods I looked at in building out and supporting zynq, the rfsoc and
downstream software flows as this allows for the ability to get the xilinx
shell archive (`.xsa`) to include IP like the MPSoC and rfdc for the full
supported xilinx software flow.

That being said, if using board design is not really where we want to go than
there are still things we can do to support instancing IP from the IP catalog
instead of a board design. It would just require our own hacking of device tree
generator stuff which I have demonstrated for the rfdc.

Will go back now to clean up code but needed to at least capture a working
design to go back to. Tested with casperfpga in py3.

No particular order of things still needing to do and by no means exhaustive,
just a brain dump...

 * rfdc yellow block
    - fix up any unique name port connection issues
    - implement mts and test adding related IP needed for that, this may require
      doing something in the rfsoc platform block (e.g., zcu216) to provide the
      clocking infrastructure to support and so there could be this back and for
      that needs to happen with the dependencies. At first glance the zcu216
      will need to provide the required clock that the rfdc can do a fabric copy
      for the mts/mcs algorithm
    - give board design instance a unique name
    - document/comment to make more transparent the board design stuf
    - rfdc axis reset is connected to axil reset, worth re-thinking, was just to
      make simple at the time, not sure what most other casper adcs do

 * rfdc simulink
    - still need to extend to dual-tile parts, but eveyrthing should work break
      well with any quad part
    - port placement disabling and enabling ports is wonky, better port drawing
      neededed

 * zcu216 yellow block
    - need better handling of clocking infrastructure and pass of of information
      between rfdc. Problem being that the lmk/lmx clocking and user clock
      selection in the platform block dependent on rfdc configuration
    - implement mts clocking support (see rfdc yellow block mts note)
    - I still have a goal to become board design agnostic... but this is a good
      first pass... will want to go back to this commit and previous to see what
      I was doing to get board design agnostic potentially leverage more board
      design tcl calls like rfdc instead of adding direct from ip catalog
      doesn't need to be that way for custom hdl though only xilinx specific and
      things we care about to be in software design (e.g., mpsoc, rfdc)

---
## [petre-symfony/symfony-stimulus](https://github.com/petre-symfony/symfony-stimulus)@[e916a4dc23...](https://github.com/petre-symfony/symfony-stimulus/commit/e916a4dc238c0ded872e21170f2d42f5fad1c1a9)
#### Friday 2021-04-09 06:34:21 by petrero

37.5. Ajax Form Response Status Codes

Listening to Bootstrap Modal Events
But before we do, search for "bootstrap 5 modal", click into its docs, and then go down to the "Events" section at the bottom. As the modal opens and closes, the modal itself dispatches some events. What if we needed to listen to those? Like, what if we need to run some custom code whenever a modal is closed. How can we do that? This is the beauty of Stimulus. We already know how! If something dispatches an event, all we need to do is add an action for that event.
Check it out: over in index.html.twig, I'll break this onto multiple lines and then add a data-action="". What we're going to do is listen to this hidden.bs.modal event, which happens after the modal is finished being hidden.
Use that event name, then ->, the name of our controller - modal-form - a # sign and then call the new method modalHidden.
Now, the hidden.bs.modal event won't be dispatched directly on this <div>. It will be dispatched on the modal element. But we already know that's okay! The event will bubble up to this <div>.
Copy modalHidden and head into our Stimulus controller. At the bottom add that method and let's just console.log('it was hidden').
Try it out! Go back to our site, refresh, open the modal and hit cancel. There's the log! Open it again, hit X and... another log. I love that.
Next: to make this a truly smooth user experience, after a successful form submit, what we should really do is make another Ajax call to reload the product list... so that the user can see the new product. Let's do that by, once again, making a reusable Stimulus controller. This controller will be able to reload the HTML for any element.

---
## [petre-symfony/symfony-stimulus](https://github.com/petre-symfony/symfony-stimulus)@[39316da7b0...](https://github.com/petre-symfony/symfony-stimulus/commit/39316da7b07da69cfdef60be1b328035bddf010c)
#### Friday 2021-04-09 06:34:21 by petrero

37.2. Ajax Form Response Status Codes

On Error: 422 Error Response
But there's still one problem. When the form has validation errors, we're currently returning a 200 success status code: that's what $this->render() uses by default.
In other words, whether the form submit is successful or not, both situations return a successful status code. That... doesn't make our life very easy in JavaScript where we need to figure out which situation we're in! Sure, we could look exactly for a 200 versus 204 status code... but... there's a better way.
What is it? Return an error status code when the form fails validation. This will not only make our life easier in JavaScript... it's also more correct! And it will still work fine if you're submitting a form normally without JavaScript: the error status code won't confuse old browsers or anything like that.
The third argument to render() is a Response object that the HTML from the template will be put into. When this is not passed, a Response object with a 200 status code is automatically created. Let's pass our own: new Response() - the same one from HttpFoundation - passing null for the for the content... because that's going to be replaced by the HTML from the template anyways.
The really important thing is the status code. But this render() method is called both when the form is originally loaded and when it's submitted with invalid data. So we need to figure out which situation we're in. Use the ternary syntax here to say: if $form->isSubmitted(), then use 422 else 200.
This works because - if the form was submitted and was successful - we would already be inside the first if statement... and we would never get down here. So if the form is submitted, we definitely know it's an invalid submit.
The 422 status code means "unprocessable entity". And it's a pretty standard status code for validation errors. As a bonus, doing this on your forms will play super nicely with Stimulus's sister technology Turbo. Oh, and by the way, in Symfony 5.3, there is a new renderForm() shortcut in your controller, which will automatically set the 422 status code for you on error. That'll make this much cleaner.

---
## [NusantaraProject-ROM/android_vendor_themes](https://github.com/NusantaraProject-ROM/android_vendor_themes)@[545977dba8...](https://github.com/NusantaraProject-ROM/android_vendor_themes/commit/545977dba83776d5da30c879bc1b7e8201e469e1)
#### Friday 2021-04-09 10:51:30 by Kshitij Gupta

vendor: notch-city: Add 3 mode display cutout handler [2/3]

[AgentFabulous - POSP]
- Introduces the HideCutout and StatusBarStock overlay used in the
  3 mode display cutout handler. The HideCutout overlay is necessary
  since we can't register a content observer in the display manager code.
  We only have access to resources during boot. Thus, leave this as an
  overlay and let the config and overlay change methods handle this.
  Though we can probably do statusbar stock height toggling in the
  SystemUI code without overlays, I kinda got lazy by the end, just
  live with it god damn it xD

Signed-off-by: Kshitij Gupta <kshitijgm@gmail.com>
Change-Id: I62f63f39bcb410cfbc68e0028b9cef3d748d7eb6

Signed-off-by: Genkzsz11 <genkzsz11@nusantaradevs.org>

---
## [limograf/caldervalleyscouts](https://github.com/limograf/caldervalleyscouts)@[a4ecb3903f...](https://github.com/limograf/caldervalleyscouts/commit/a4ecb3903fceceda8049631ce595a46997f4216e)
#### Friday 2021-04-09 13:20:33 by Pip

Changed boys and girls to young people or children.

This was a suggestion from my friend after she looked at the website.

---
## [awkwardartist/AwKernel](https://github.com/awkwardartist/AwKernel)@[c514b23f33...](https://github.com/awkwardartist/AwKernel/commit/c514b23f33057d1624da026a8e970a4363d23c12)
#### Friday 2021-04-09 18:03:42 by awkwardartist

how the fuck am i still working on this console driver holy shit

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[c93dc9da02...](https://github.com/mrakgr/The-Spiral-Language/commit/c93dc9da02a482939067a74c264b02fd7774ac63)
#### Friday 2021-04-09 18:56:32 by Marko Grdinić

"2:50pm. Done with breakfast and chores.

Let me resume.

First let me write down my thoughts.

In the sampling version of CFR, I think I will ditch regret updates completely and derive the current policy off the Q values.

In the original CFR, the regret updates do act as a weighted moving average of regrets, so reifying what is actually being done in the sampling version is not too great of a step. When I move to the NN version, that will simplify things greatly as I won't need a policy and Q network.

2:55pm. What is giving me trouble are the average strategy updates. I can't straightforwardly translate them to the sampling version. This is because I need to divide the thing by the opponent probabilities. This is doable in a simulator, but it is a red flag that I could not do this in real life.

I do not understand why policy averaging works the way it does in CFR.

I think I now completely understand the regret updates, and I can see how diving by self action probs acts as an learning accelerant.

3pm. No actually I do understand it.

I thinking on Rock Paper Scissors (I said it was Tic Tac Toe in one of the previous entries by mistake), if the other player probability was a factor, the the strategy averaging definitely would not work. It would not be 'averaging' in any sense.

3:05pm. I am going to believe the paper when it says that simply keeping around old policies and sampling from them is the right thing to do.

It is really quite remarkable how profound adding a few things together can be. I am in awe.

Doing strategy averaging without eliminating the other player's propabilities is definitely wrong, I can see that much.

3:10pm. Ok, I can see it.

1) I can see how not ignoring the opponent reach prob when doing policy averaging is wrong.

2) What about the chance node probability?

Imagine if there were a bunch of chance nodes with zero probability. We already know how the regret updates work with respect to that - they get ignored. During sampling those nodes would never happen and would never get optimized.

If I ignored chance probability, that would break strategy averaging.

So chance nodes should be a factor.

3) What about the self action probabilities?

3:15pm. This one is a bit hairier.

Let me consider the case where probability is zero again.

3:20pm. It is interesting. In regular CFR, the updates would happen, but in sampling those cases would never occur.

7:05pm. Done with lunch. I got a lot of inspiration and spent all this time in bed.

I think I understand completely now how to do a tabular sampling CFR variant. Not neural one though.

I was wrong. I thought that ommiting the self probability in the regret update acts as an accelerant, but that is not the right way of thinking about it.

What that really does is cut the whole graph into a large number of pieces.

But I've realized as I thought that `Q / self_prob`s are not the equivalent of the full path `Q`s.

In fact, if I tried to sample from them directly, I would get biased results!

I've realized that the reason strategy averaging makes sense is because in a way it is in fact optimizing the full graph.

I remember how CFR oscilates between values in the current policy.

As a result of that, the strategy update does in fact do something similar to Monte Carlo updates.

This is the only way to see this.

If strategy averaging was not optimizing the full graph, if something like `self_prob` was ommited in its multiplication, then the solution could not be found in the first place. I do not know what exactly it should be called, but what CFR is doing is definitely not strategy averaging. It something closer to hierarchical RL.

Because you have those selfless `Q / prob_self`s which act as an invariant separate from end-to-end optimization. And they you have a separate process taking advantage of that to speed up learning.

This makes me realize, simply sampling the old policies would not work. Because I can easily think of an example where such a thing would give biased results. If it were regular `Q`s then maybe it would, but not with those selfless `Q`s.

In order for a sampling based algorithm in the spirit of the original CFR to work, what I am going to have to do are MC rollouts.

One thing that worried me, which I've realized is not really an issue is the opponent probability. I said that I could not get that in real life, but in strategy averaging it is not important that I get the opponent probability. I can just substitute in my own and then divide by it.

Also one more thing...

```
    action = dyn fun {chance_prob player player' actions next} =>
        inl num_actions = length actions
        inl op_prob = exp_log_prob (add_log_prob chance_prob player')
        inl self_prob = exp_log_prob (add_log_prob chance_prob player.prob)
        inl d = value d num_actions player.observations
        inl actions_prob = regret_match (if is_avg_policy then d.avg_policy else d.regret)
        inl reward,reward_wsum =
            inl state = state player
            am.mapFold2 (fun s prob a =>
                inl util =
                    if prob = 0 && op_prob = 0 then 0 // the pruning optimization
                    else next ((to_log_prob prob,a),state)
                util, s + prob * util
                ) 0 actions_prob actions
        if is_update then
            d.regret |> am.mapInplace (fun i x => x + op_prob * (index reward i - reward_wsum))
            d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
        reward_wsum
```

This implementation of CFR is actually incorrect.

```
d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
```

I have this problem in the F# version as well. I definitely should not be multiplying the current action probability in `self_prob * index actions_prob i`. Rather...

```
        if is_update then
            d.regret |> am.mapInplace (fun i x => x + op_prob * (index reward i - reward_wsum))
            inl actions_prob = regret_match d.regret
            d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
```

Definitely like this. In fact...

```
        inl reward,reward_wsum =
            inl state = state player
            am.mapFold2 (fun s prob a =>
                inl util =
                    if prob = 0 && op_prob = 0 then 0 // the pruning optimization
                    else next ((to_log_prob prob,a),state)
                util, s + prob * util
                ) 0 actions_prob actions
```

Instead of these rewards I should consider weighting them by the updated policy. But that is not as important as the strategy update.

I can see on a toy example, that using the stale action probs instead of the updated ones would completely break the optimization scheme.

```
    action = dyn fun {chance_prob player player' actions next} =>
        inl num_actions = length actions
        inl op_prob = exp_log_prob (add_log_prob chance_prob player')
        inl self_prob = exp_log_prob (add_log_prob chance_prob player.prob)
        inl d = value d num_actions player.observations
        let actions_prob () = regret_match d.regret
        // This one might give better results during training.
        // let actions_prob () = regret_match (if is_update then d.regret else d.avg_policy)
        inl reward,reward_wsum =
            inl state = state player
            am.mapFold2 (fun s prob a =>
                inl r =
                    if prob = 0 && op_prob = 0 then 0 // the pruning optimization
                    else next ((to_log_prob prob,a),state)
                r, s + prob * r
                ) 0 actions_prob() actions
        if is_update then
            d.regret |> am.mapInplace (fun i x => x + op_prob * (index reward i - reward_wsum))
            inl actions_prob = actions_prob()
            d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
            am.fold2 (fun s prob r => s + prob * r) 0 actions_prob reward
        else
            reward_wsum
```

Agh, why am I still thinking about this? So much about doing the UIs today.

7:45pm. I should read the single deep CFR paper, and look at the DREAM paper again, but literally nothing in those papers makes sense apart from the overall bootstraping scheme. I don't trust any of it.

This is the ML in a nutshell today. Will I really succeed at this? But I suppose today was good, as my understanding of CFR went from 50 to about 70%.

7:55pm. No, I am not there yet. Right now, I do not think I understand the chance nodes well enough.

There is a similarity between the opponent probabilities and the environment.

```
inl self_prob = exp_log_prob (add_log_prob chance_prob player.prob)
...
d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
```

Why does the average policy update need the chance probabilities? Why not the opponents' as well? Both of them can be considered a part of the environment as far as the optimization algorithm is concerned, and the selfless `Q`s already consider that part of the probability as well.

I am not doing this right. I am obsessing about the algorithm ahead of time and making grinding progress when I really should be doing real life testing in order to generate insight.

8pm. Let me play Rance here. I should have stopped at 6pm. There will be plenty of time to think about this in the future.

8:05pm. This is just so hard to understand. In the worst case, I could always just use policy gradients with my buffer tricks.

Yeah, maybe PG with selfless `Q`s for variance reduction would not be a bad idea. I can't actually constantly add values in a NN regime like in the tabular case because it would unhinge the lower layers. This is even with the trick I will use for training the `Q` nets.

8:15pm. Agh, forget Rance. Let me think about it for a while longer.

I am thinking abuot the PG net + selfless `Q`s. Suppose I ommitted opponent's probabilities from it. What sort of graph would I get. What sort of problem would be optimized for?

Yeah, it would not be good. The problem being solved would be wrong.

8:20pm. Yeah, that is exactly it. And on the same problem I could cause the strategy averaging CFR scheme to go wrong.

```
d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
```

Is the fact that it is only using `self_prob` just a hack that happened to work on the problem the authors were working on, but is actually broken when seen more broadly.

8:55pm. Though I am pushing it as hard as I can, my brain is just not working right now. Though the example is simple, my mental muscle is too slack to get to the bottom of it. I am relying too much on visualization and not enough on actual numbers.

I should open an F# script and try to code my idea up since I obviously can't hold everything in my head.

Forget it, I am done for the day."

---
## [thestubborn/Skyrat-tg](https://github.com/thestubborn/Skyrat-tg)@[847b97e6d5...](https://github.com/thestubborn/Skyrat-tg/commit/847b97e6d583cf555b5ea71f03fcd0cc2196cdf7)
#### Friday 2021-04-09 19:57:34 by grandwizmiller

Mazovian Socio-Economics

People think Communism was some crazy idea that had its comeuppance 40 years ago. A fever that shook the world, never to return again. They were right. Until *he* woke up today – a spiritual corpse responsive only to the call of Commodore Red, prostitutes, and Kras Mazov. For him, Communism is still a *thing*. He will single-handedly raise the Commune of '02 from the oceanic trench where it has been resting, covered in ghosts and seaweed! He is the Big Communism Builder. Come, witness his attempt to rebuild Communism in the year '51!

0.000% of Communism has been built. Evil child-murdering billionaires still rule the world with a shit-eating grin. All he has managed to do is make himself *sad*. He is starting to suspect Kras Mazov *fucked him over* personally with his socio-economic theory. It has, however, made him into a very, very smart boy with something like a university degree in Truth. Instead of building Communism, he now builds a precise model of this grotesque, duplicitous world.

---
## [Rajat6697/Analyze-Data-With-Python-Codecademy-](https://github.com/Rajat6697/Analyze-Data-With-Python-Codecademy-)@[4af944321a...](https://github.com/Rajat6697/Analyze-Data-With-Python-Codecademy-/commit/4af944321ac13f8466f648dbc71746fb1935fa86)
#### Friday 2021-04-09 20:16:02 by Rajat6697

Create life_expectancy.txt

STATISTICS FOR DATA ANALYSIS
Life Expectancy By Country
Over the course of the past few centuries, technological and medical advancements have helped increase the life expectancy of humans. However, as of now, the average life expectancy of humans varies depending on what country you live in.

In this project, we will investigate a dataset containing information about the average life expectancy in 158 different countries. We will specifically look at how a country’s economic success might impact the life expectancy in that area.

Tasks
12/12 Complete
Mark the tasks as complete by checking them off
Access the Data
1.
We’ve imported a dataset containing the life expectancy in different countries. The data can be found in the variable named data.

To begin, let’s get a sense of what this data looks like. Print data.head() to see the first 5 rows of the dataset.

Look at the names of the columns. What other pieces of information does this dataset contain?

You may want to comment out this print statement after looking at the data.


Stuck? Get a hint
2.
Let’s isolate the column that contains the life expectancy and store it in a variable named life_expectancy. To get a single column from a Panda DataFrame, use this syntax:

single_column = dataFrameName["columnName"]
Make sure to pay attention to capitalization and spaces when using the column name!


Stuck? Get a hint
Find the Quantiles
3.
We can now use NumPy functions on that column! Let’s use the np.quantile() function to find the quartiles of life_expectancy. Store the result in a variable named life_expectancy_quartiles and print the results.


Stuck? Get a hint
4.
Nice work! By looking at those three values you can get a sense of the spread of the data. For example, it seems like some of the data is fairly close together — a quarter of the data is between 72.5 years and 75.4 years.

Could you predict what the histogram might look like from those three number? Plot the histogram by using the following two lines of code. Does it look how you expected?

plt.hist(life_expectancy)
plt.show()

Stuck? Get a hint
5.
Let’s take a moment to think about the meaning of these quartiles. If your country has a life expectancy of 70 years, does that fall in the first, second, third, or final quarter of the data?

Click on the hint to see the answer!


Stuck? Get a hint
Splitting the Data by GDP
6.
GDP is a mesaure of a country’s wealth. Let’s now use the GDP data to see if life expectancy is affected by this value.

Let’s split the data into two groups based on GDP. If we find the median GDP, we can create two datasets for “low GDP countries” and “high GDP countries.

To start, let’s isolate the GDP column and store it in a variable named gdp. This should be similar to how you isolated the life expectancy column.


Stuck? Get a hint
7.
We now want to find the median GDP. You can use NumPy’s np.median() function, but since the median is also a quantile, we can call np.quanitle() using 0.5 as the second parameter.

Store the median in a variable named median_gdp. Print that variable to see the median.


Stuck? Get a hint
8.
Let’s now grab all of the rows from our original dataset that have a GDP less than or equal to the median. The following code will do that for you

low_gdp = data[data['GDP'] <= median_gdp]
Do the same for all of the rows that have a GDP higher than the median. Store those rows in a variable named high_gdp.

The line of code should look almost identical to the one above, but you should change the <= to >.

Remember to change the name of the variable!

9.
Now that we’ve split the data based on the GDP, let’s see how the life expectancy of each group compares to each other.

Find the quartiles of the "Life Expectancy" column of low_gdp. Store the quartiles in a variable named low_gdp_quartiles. Print the results.


Stuck? Get a hint
10.
Find the quartiles of the high GDP countries and store them in a variable named high_gdp_quartiles. This should look very similar to the last line of code you wrote. Print the results.


Stuck? Get a hint
Histogram and Conclusions
11.
By looking at the quantiles, you should get a sense of the spread and central tendency of these two datasets. But let’s plot a histogram of each dataset to really compare them.

Remove the two lines of code that are currently plotting the histogram of the original dataset. At the bottom of your code, add these four lines:

plt.hist(high_gdp["Life Expectancy"], alpha = 0.5, label = "High GDP")
plt.hist(low_gdp["Life Expectancy"], alpha = 0.5, label = "Low GDP")
plt.legend()
plt.show()
12.
We can now truly see the impact GDP has on life expectancy.

Once again, consider a country that has a life expectancy of 70 years. If that country is in the top half of GDP countries, is it in the first, second, third, or fourth quarter of the data with respect to life expectancy? What if the country is in the bottom half of GDP countries? Check the hint to see our thoughts.

---
## [nectarboy/gameboy](https://github.com/nectarboy/gameboy)@[6770401261...](https://github.com/nectarboy/gameboy/commit/677040126163f4974d77bc7664438a5e132a065e)
#### Friday 2021-04-09 20:32:33 by nectarboy

i think im done wit pollen boy ~

i added the last stuff like a volume slider n a pitch shift -- and finally custom palletes ~
i tried to add the halt bug - nearly there but sumn with IF reg ~
i fixed sum goofy ass code in readByte and writeByte - that shit has been there since the beginning ~

maybe ill come back and add some more stuff, who knows
ofc, ill push fixes and stuff if i find bugs, whenever

buhbye - this was fun as fuck :D

---
## [silaspokemon/Super-Mario-64-HDS](https://github.com/silaspokemon/Super-Mario-64-HDS)@[7247933561...](https://github.com/silaspokemon/Super-Mario-64-HDS/commit/72479335619bfc97eb80f257111043ecaf81efae)
#### Friday 2021-04-09 20:34:35 by BuildTools

Bad commit

This texture is so horrible I made a commit dedicated to it so I could point out how horrible it is, it's so cursed and dumb and bad and screwed up, I hate this so much, but I can not say I completed the basement before doing this, this was the best that way too much time upscaling could bring me, I had to do a ton of bad editing, that also sucks, I hate this so much, this is so horrible, I should not have to do this, if I could choose I would just not do this and not suffer, knowing that I had not created this abomination.
I hate this texture so much, the original doesn't even look bad, it just takes so much advantage of its medium that it doesn't translate well to upscaled, I hate this all so much aaaaaaaaaaaaaaa

---
## [Ixarias/bedrock-backup](https://github.com/Ixarias/bedrock-backup)@[56ccf18942...](https://github.com/Ixarias/bedrock-backup/commit/56ccf18942c399ec4fdd436e6a718fa34b25985d)
#### Friday 2021-04-09 20:45:09 by Ixarias

ok actually for real this time fuck you gityhuadfj

---
## [dylanjaide/memes-on-this-day](https://github.com/dylanjaide/memes-on-this-day)@[2fe87a2e9b...](https://github.com/dylanjaide/memes-on-this-day/commit/2fe87a2e9bdeaba71756667e2e5f4be008e19396)
#### Friday 2021-04-09 20:56:22 by Dylan-JW

Added entries to database

Added entries for Bernie Sanders mittens, The Bielefeld Conspiracy, Get Stick Bugged lol, Baby Shark, He Scream At Own Ass, and Distracted Boyfriend

---
## [awseward/homebrew-tap](https://github.com/awseward/homebrew-tap)@[6407359297...](https://github.com/awseward/homebrew-tap/commit/6407359297f22ee33eeb7e6b17b96424c4fe681c)
#### Friday 2021-04-09 21:30:10 by Andrew Seward

Make sure alignment isn't terrible

As much as I don't love writing this, either, I think it beats ugly
alignment in the PLIST file.

---
## [grapl-security/grapl](https://github.com/grapl-security/grapl)@[d9e6dbd426...](https://github.com/grapl-security/grapl/commit/d9e6dbd4268f97abac3c7f1b7b7c8b72d05a226c)
#### Friday 2021-04-09 23:10:38 by wimax-grapl

Add Prettier as a JS code formatter (#734)

* Attempting to add JS formatting

* Added prettier formatter to codebase

* add to workflow

* debugging a bit

* god i love editing github workflows. jk it sucks

* oh ok fixed

* use sudo to install prettier maybe

---
## [silvasean/mlir-npcomp](https://github.com/silvasean/mlir-npcomp)@[46aa6d0a24...](https://github.com/silvasean/mlir-npcomp/commit/46aa6d0a245123c741ff8035b51ec897989f0201)
#### Friday 2021-04-09 23:57:14 by Sean Silva

[RefBackend] Fix leaks related to ABI boundaries.

Best as I can tell (e.g. from LeakSanitizer), this fixes all the leaks
except for those due to buffers created internally to the codegenned
code itself (up next I'll add the buffer deallocation pass to fix
those).

The main change is that instead of attempting to pass `refbackrt::Tensor`
to the codegenned function directly, we make all the ABI types be
UnrankedMemRef which gets passed awkwardly (but workably) as a
`{size_t rank, void *ptrToDescriptor}` on the ABI. The reason why
refbackrt::Tensor wasn't workable is that is that MLIR doesn't really
have a way to deal with the lifetime of unranked memref descriptors that
happen inside the function, which is inevitably what would happen in the
old code that would emit runtime calls to
`refbackrt.to_memref/refbackrt.from_memref` to convert back and forth to
`refbackrt::Tensor` inside the codegenned code.

So, instead of the `refbackrt.to_memref/refbackrt.from_memref` with no
real sound basis for valid lifetime management, we now have a lovely
piece of code in `refbackrt::invoke` in `Runtime.cpp` that just barely
seems to be sound. We rely on the codegenned code having these
properties, which it seems to have:

- it won't free memref descriptors or their backing buffer for arguments
  of UnrankedMemRef type.

- it will allocate a separate memref descriptor for each result
  UnrankedMemRef (which is ensured by having a separate memref_cast for
  each)

- we can sniff the `allocatedPtr`'s (i.e. the backing buffer pointers)
  to avoid double-freeing in the case of aliasing of the backing buffer
  (including backing buffers for arguments feeding into results)

- to catch the case of statically allocated data (which we need to avoid
  passing to `free`) , check if the `allocatedPtr` is (no joke) equal to
  `0xDEADBEEF`, because there is otherwise no way to distinguish
  statically allocated from malloc'ed data...  (std.global_memref lowering
  to LLVM by happenstance sets the allocatedPtr equal to `0xDEADBEEF`,
  presumably mainly as a debugging thing)

Even with all this, we *still* need to (internally to refbackrt::invoke)
make copies of all inputs/outputs! And the details of how the LLVM-level
ABI gets laid out for e.g. function arguments/returns is still super
tricky.

This really highlights how deficient memref is as the general runtime
type for our use case. It's stewing in my mind how best to improve the
situation. My general gut feeling is that IREE's abstractions for this
are "right", but I need to think more how to distill those aspects of
IREE's design in a "reference" way for RefBackend.

Some implementation notes:

- In terms of how this is implemented, this did catch a bug in our ABI
  wrapper functions in LowerToLLVM.cpp, which I had to fix (it happened to
  work before through some combination of npcomprt::Tensor being passed as
  a single pointer + probably me infinite-monkey-ing it until it worked)

- This actually removes 2 out of the 3 compiler runtime functions (the
  only one left is "abort_if". (most of the memref descriptor code moved
  from CopmilerRuntime.cpp to Runtime.cpp)

  - this also means deleting `refbackrt.from_memref` and
  `refbackrt.to_memref`

---

# [<](2021-04-08.md) 2021-04-09 [>](2021-04-10.md)

