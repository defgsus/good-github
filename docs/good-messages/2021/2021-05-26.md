# [<](2021-05-25.md) 2021-05-26 [>](2021-05-27.md)

3,912,189 events, 1,460,864 push events, 2,402,306 commit messages, 178,087,457 characters


## [KeefeOverby/Button-to-Second-Screen@fb431af0ec...](https://github.com/KeefeOverby/Button-to-Second-Screen/commit/fb431af0ec611fe4f152419cbd7713d2dad66649)
##### 2021-05-26 05:42:49 by KeefeOverby

Button to Second Screen App
v 0.12
Success!!!!!!!!!!
It was way to difficult to find a simple explanation for one button that navigates to a second screen. Developers that have a lot of experience make "basic" lessons way too complicated. It took me three days searching the internet to find little bits and pieces of information just to complete this simple app. It's a shame that experienced developers call their extremely complicated lessons "basic" it's dishonest to say that overly complicated lessons are "basic" when they know deep down that its clearly not basic. This app that I just created is basic, one button to a second screen. That is basic, clear and easy to comprehend. I am honest, but unfortunately some people are not, dishonest people are horrible.

---
## [mrakgr/The-Spiral-Language@2d2906731a...](https://github.com/mrakgr/The-Spiral-Language/commit/2d2906731af17b85d6424c584c7bf5eabf558063)
##### 2021-05-26 10:12:41 by Marko Grdinić

"9:10am. Let me chill for a while. 10m is not enough to get over my grogginess. I slept well tonight though.

I had time to think about it. Unless I got answers that solve my problems already, I will put some special effort into making the Python debugger work. Until I can figure out how to hack PyTorch I am stuck, so I should focus on overcoming that particular hurdle.

9:40am. Let me read a chapter of Takahane and then I will start the work day.

9:55am. Time to start. I had my bit of fun. Let me check my mail first to see if there are any updates.

> I'm afraid you won't be able to copy-paste these functions as they are implemented in c++. But you don't need that with a custom Function as described here: https://pytorch.org/docs/stable/notes/extending.html. You can simply write the formula you want for the backward.

No way. For the softmax it should definitely be possible to grab the backwards closure, though it might be simply exping the log outputs...

Agh, I might have to write the formula for that. But I am going to have to fire up the debugger to make sure. There is no way of doing it as separate ops if it makes things slower in the end.

10am. Regarding the SO question, there is no peep.

So my plan for the day is intact. I'll have to figure out how to start the debugger on my own.

```
PS C:\Users\Marko\Source\Repos\The Spiral Language\Spiral Compilation Tests> C:/Users/Marko/anaconda3/shell/condabin/conda-hook.ps1
PS C:\Users\Marko\Source\Repos\The Spiral Language\Spiral Compilation Tests> conda activate 'C:/Users/Marko/anaconda3'
(base) PS C:\Users\Marko\Source\Repos\The Spiral Language\Spiral Compilation Tests>
```

Oh, is just this enough to activate it?

10:10am. Crap. Debugging into these library calls does not work. It just goes over them.

https://stackoverflow.com/questions/53594900/visual-studio-code-python-debugging-step-into-the-code-of-external-functions

Oh it should be possible. Where do I put the option?

```
    "configurations": [
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false
        }
    ]
```

Intellisense suggested this, so it has to be right. Let me try running it.

10:15am.

```py
def _run_code(code, run_globals, init_globals=None,
              mod_name=None, mod_spec=None,
              pkg_name=None, script_name=None):
    """Helper to run code in nominated namespace"""
    if init_globals is not None:
        run_globals.update(init_globals)
    if mod_spec is None:
        loader = None
        fname = script_name
        cached = None
    else:
        loader = mod_spec.loader
        fname = mod_spec.origin
        cached = mod_spec.cached
        if pkg_name is None:
            pkg_name = mod_spec.parent
    run_globals.update(__name__ = mod_name,
                       __file__ = fname,
                       __cached__ = cached,
                       __doc__ = None,
                       __loader__ = loader,
                       __package__ = pkg_name,
                       __spec__ = mod_spec)
    exec(code, run_globals)
    return run_globals
```

Huh, why does it put me here as soon as I step into the softmax. Well regardless, let me continue with this. After the debugging session I should know here things stand.

10:20am. Ok, so the above segment is what runs after the script executes the last statement.

```py
import torch
import torch.optim
import torch.functional
import torch.nn
import torch.nn.functional
import torch.distributions
import numpy as np
import torch._C
import nets

q = torch.rand((3,3),requires_grad=True)
x = torch.softmax(q,dim=-1)
print('hello')
```

Even though I am placing the breakpoints here, it is skipping over the two statements I want to debug.

10:25am. Let me send out another question to SO.

10:40am. Now I am reading /g/. Ok, I hadn't expected this. What the hell. I thought my job would be done after I've managed to get the Python debugger to run.

11:05am. Had to take a break. Let me just try stepping into nn.linear. The reason why the debugger is failing might be because it straight up links to C code.

```
import torch.nn
w = torch.nn.Linear(3,4)
```

Damn it, this works. I guess the reason this does not work is because it straight up links to C code.

https://stackoverflow.com/questions/67690696/how-to-make-the-python-debug-task-use-the-default-powershell-profile/67701492#67701492

Oh, got a reply here.

No, the solution given here does not work for me.

11:15am. Ok, I am going to have to do what those guys suggest and write my own stuff. Forget the softmax. Though I am tempted to go with `torch.script.jit`, who knows what kind of results that will give me.

Instead I should focus on making my own linear layer. Right now, I won't even bother with moving averages. Instead let me just normalize the inputs and the gradients based on the batch information. Let me just focus on that.

11:25am. Now if I could only collect my bearings. Today and yesterday these issues caught me completely off guard.

Ok, let me make something.

https://pytorch.org/docs/stable/notes/extending.html
https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html

You know what, let me remove https://stackoverflow.com/questions/67701264/how-to-debug-external-library-python-code-in-visual-studio-code

The question is too generic and it got sidetracked into my own misunderstanding.

```
print(torch.nn.functional.softmax)
```

No softmax both in `torch` and `torch.nn.functional` is a plain function.

```py
# Inherit from Function
class LinearFunction(Function):

    # Note that both forward and backward are @staticmethods
    @staticmethod
    # bias is an optional argument
    def forward(ctx, input, weight, bias=None):
        ctx.save_for_backward(input, weight, bias)
        output = input.mm(weight.t())
        if bias is not None:
            output += bias.unsqueeze(0).expand_as(output)
        return output

    # This function has only a single output, so it gets only one gradient
    @staticmethod
    def backward(ctx, grad_output):
        # This is a pattern that is very convenient - at the top of backward
        # unpack saved_tensors and initialize all gradients w.r.t. inputs to
        # None. Thanks to the fact that additional trailing Nones are
        # ignored, the return statement is simple even when the function has
        # optional inputs.
        input, weight, bias = ctx.saved_tensors
        grad_input = grad_weight = grad_bias = None

        # These needs_input_grad checks are optional and there only to
        # improve efficiency. If you want to make your code simpler, you can
        # skip them. Returning gradients for inputs that don't require it is
        # not an error.
        if ctx.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)
        if ctx.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input)
        if bias is not None and ctx.needs_input_grad[2]:
            grad_bias = grad_output.sum(0)

        return grad_input, grad_weight, grad_bias
```

This is straight from the docs. I should just focus on hacking this.

Let me just say that I do not like PyTorch's way of doing things here. It is almost like in old Spiral, but what it does is allocate the adjoints in the backward function itself. That is inefficient when the function has shared inputs. It will allocate them in the backwards function and then the autograd will have to copy them into the actual one. A better way would be for the adjoints to get passed in and added into.

Does it do it like this for the sake of distributed processing perhaps? A particularly big problem of this way of doing things would be the weight matrices. Allocating a fresh matrix only to copy it over would be inefficient.

I am going to ask about this as it is a problem.

Should I zero out the weight grads by setting them to `None` instead of zeroes?

Let me take a look at that page again.

https://pytorch.org/docs/master/generated/torch.optim.Optimizer.zero_grad.html

> instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance.

Yeah, as expected. I should be setting it to `None`. That will take care of the problem of efficiency.

As for the rest, I just have to make sure not to share inputs unless I can't help it.

12:05pm. Sorry, I know I should have been writing code this morning, but this series of issues just threw me off my game so much.

Let me stop here for breakfast. After that I'll make my grad normalizing linear layer. That should stabilize things somewhat. I say somewhat because the full scheme with the replay buffer is needed to get me all the way. Apart from that, I might even try making the softmax.

I should just focus on this today and do what I have to. After that hopefully it should give me some familiarity with hacking PyTorch."

---
## [nin66/CompGraphics@0b7394023e...](https://github.com/nin66/CompGraphics/commit/0b7394023e11ee0906e1eaab13d5e43aea3ff90f)
##### 2021-05-26 12:27:29 by WichaelMu

Minor Fish Update

"Minor"? this movement thing literally took me 12 hours and i still couldn't figure it out.

for some dumb reason, only BrownFish wants to move towards a coral, nothing else. Even though there's a fishies array that holds every single fish in the scene, it still doesnt want to move. However, 1 in 100 times, everything works properly.

Still, the fishies will still collide into terrain and go through them because three js and javascript are hilariously slow at processing anything and it takes too long to load Reef Exploration.

Three JS and javascript are both the two worst things ive ever worked with, same goes for python. interpreted languages are very stupid and should never be used.

i started working on this at 10am and its now 10pm. i literally wasted my whole day trying to get something to work on an inferior language.

I also love this subject, ive learnt more about how three js and javascript suck ass than actually learning content because no one teaches it anyway.

who even reads this?

---
## [mrakgr/The-Spiral-Language@444b9db68e...](https://github.com/mrakgr/The-Spiral-Language/commit/444b9db68e4ed765629449c11677fc224ee3d4ff)
##### 2021-05-26 16:21:06 by Marko Grdinić

"1:25pm. Let me finish the chapter, deal with the chores and I will resume.

2pm. Done with chores. Let me resume.

https://stackoverflow.com/questions/67690696/how-to-make-the-python-debug-task-use-the-default-powershell-profile/67701492?noredirect=1#comment119667591_67701492

I got some good replies to the question. I think if I combine with what Steve and Molly told me, I can get rid of the need to have profiles.

2:10pm. I accepted the second answer. Now let me move forward. What is next?

Right, the hacked linear layer.

I talked before about how inefficient the backward pass is in allocating adjoints. What I've just realized is that it would be particularly bad for RNNs. Having to copy at every timestep would hit those architectures hard. With FF nets it would be fine if I set the weight adjoint matrices to None at every step. RNNs would not benefit much from that.

2:20pm. https://pytorch.org/

I am looking over the docs for a bit. This recent issues have got me wondering.

By all appearances PyTorch in fact just an AD library. I knew this, but thought there might be a small chance that there would be something more to it.

The people who are working on neurochips, just how will they fit PyTorch models given the hardware constraints? AD is very flexible and general. You can't fit neurochips in such a framework - they get their power by being restricted. If they have local memory and computation, you'd need to instruct the nodes specifically.

It is a different kind of compilation challenge.

Neurochips would be expected to need specific libraries that match the constraint of the hardware. Maybe I've gotten lucky by getting passed over during this first wave of ML hardware. If those people really think they can succeed in meshing with the PyTorch ecosystem, then their hardware might not be radical enough to be worth looking into.

Once those meristive chips come around, I'll be writing my own library in the end.

Right now, let me not make too much of a fuss over this.

2:30pm. Focus me. I finally got the Anaconda env init out of the way. I've been suffering over this for a long time.

Now let me do that gradient normalizing linear layer.

```
    if not torch.jit.is_scripting():
        if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):
            return handle_torch_function(linear, tens_ops, input, weight, bias=bias)
    if input.dim() == 2 and bias is not None:
        # fused op is marginally faster
        ret = torch.addmm(bias, input, weight.t())
    else:
        output = input.matmul(weight.t())
        if bias is not None:
            output += bias
        ret = output
    return ret
```

I should try to use addmm on the forward pass if possible.

2:40pm. There is just one thing that is bothering me. Whether to normalize the gradients to `var (b * a)` or just `var b` where `b` and `a` the outer and inner matrix dimensions specifically. I was surprised to find that layer norm is normalizing to `var (b * a)` which made me reconsider my original plan of normalizing to `var b`.

3:20pm. I am still thinking about it. Realizing the way long term credit assignment works broke me.

Backprop no longer makes sense to me. I think that the constrastive learning rules would be better. Also sparsity. When I was considering normalizing to just `var b` I was assuming the the activations should be sparse. But `var (b * a)` feels like it assumes the activations are dense.

4:15pm. I am still thinking about it. I think I am going to go for the old idea I had back in April and nearly forgot by now.

That idea was to make sure that the input and the output variance match up. I came up with this for RNNs, but now that I am completely perplexed over how to normalize, it might be a fitting time to revive the idea.

Normalizing to 1 only makes sense when the signals are all 1 or -1 and I still haven't gotten to the replay buffer. Therefore, for straightforward training, matching the variances seems like the best way to go.

I want to have something useful even for regular PG style training. What I start doing CFR style training through the replay buffer, things will be easier and I will normalize the rows to unit norm.

4:25pm. I say the rows, but my intention was to do that for the whole batch. Am I changing my mind.

Actually, even for the signals, where at the top they are 1 or -1, suppose I had sparse activations and a tree like graph. Then it would be better to do variance matching.

If I have something like a diamond graph.

```
 O
X Y
 I
```

Output `O` and input `I` should have equal credit, but `X` and `Y` should not. Their credit should just sum to 1.

That is why it might be better to stick with variance matching as a general principle.

4:30pm. This way I could completely ditch the need for tracking using moving averages.

Ah, right, now I have a problem. I am going to have to fuse the nonlinearity with the linear layer in order for this to work properly. Otherwise the gradient will get reduced.

Variance matching is the other side of the coin to layer norm. I should use LN instead of normalizing the inputs.

4:50pm.

```
q = torch.zeros(3,3)
w = q.requires_grad_()
w
q
```

This is wrong. This ends up setting the mult on the original tensor.

4:55pm.

```py
class MultFunction(torch.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input * 2

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        return grad_output / 2
```

The trouble with non being able to get the original primitives is that I am not sure how to deal with inputs here.

5:15pm.

```py
import torch
from torch.autograd import Function
import torch.nn
import torch.nn.functional

class Mult(Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input * 2

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        print(input)
        return (grad_output / 2)

i = torch.scalar_tensor(2,requires_grad=True)
x = Mult.apply(i)
x.backward(torch.scalar_tensor(1))
print(i.grad)
```

I am stuck no matter how much I think about it. I want to implement LN + linear + relu, but I do not know how to reuse the backward primitives from existing operations. For multiplication in this example, I do not think that runing the backward function on the result would give me what I want.

5:25pm. I have no idea. I am going to ask for advice on SO or on PyTorch forums.

5:30pm.

```py
import torch
from torch.autograd import Function
import torch.nn
import torch.nn.functional

class VarianceMatch(Function):
    @staticmethod
    def forward(ctx, f, x):
        y = f(x)
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, grad_y):
        y, = ctx.saved_tensors
        grad_x = y.backward(grad_y)
        grad_x *= torch.sqrt(torch.square(grad_y).sum(1) / torch.square(grad_x).sum(1))
        return grad_x.grad

i = torch.scalar_tensor(2,requires_grad=True)
x = VarianceMatch.apply((lambda x: x * 2), i)
x.backward(torch.scalar_tensor(1))
print(i.grad)
```

What happens when I try this? It goes into an infinite loop.

6:05pm. https://stackoverflow.com/questions/67708650/how-to-reuse-the-backward-pass-primitives

I am doing a lot of thinking, but I am not being very effective in getting to a solution. I am stuck. If this was the old Spiral ML library, I'd have no trouble hacking things so they work, but with PyTorch I feel lost. Maybe I could somehow save the temporary forward function, but whether it is that or putting the zeroes as the return, none of the solutions satisfy me. There should be a more elegant way of doing this.

Sigh, don't tell me I will have to do my own thing in the end? I doubt it. There should be a way of pushing through what I want here. I don't need that much from the PyTorch library.

This, then transformers, and that will last me until the end of the GPU era. After that I should be able to make Spiral really useful.

6:10pm. I am done for the day. Let me go get lunch. Hopefully tomorrow I will be able to make progress. Today I started off trying to make the debugger idea work, but that flamed out. After that I was not sure what to do. I know what I want to accomplish with PyTorch, but I still do not know how to go about it. Whatever the case, I need variance matching and the replay buffer after that. I won't get far without the stabilizers in my RL journey."

---
## [CapnMachaddish/Outpost-R505@39a838034b...](https://github.com/CapnMachaddish/Outpost-R505/commit/39a838034b2503abcec5c37ade996330ec85f4f0)
##### 2021-05-26 20:06:44 by thestubborn

single clothing item added to loadout to match a uniform (#4496)

* single clothing item added to loadout to match a uniform, as well as some sprites for future plans

i live in a cum

* FUCK IT, I'LL ADD THEM ALL

 I don't give a FLYING FUCK, that bitch can fuck off, I've divorced her ass three hours ago! I'm SO SICK, my body is doing THINGS - THAT THING! And you over there, SHUT UP. And you, take off my pants! YOU WANNA SEE SOME - WEIRD SHIT?

* i had some deadspace and it was annoying me

irtujrfffucking hate dogborgs

* i mixed something up

/obj/item/clothing/suit/yakuza not majimcoat

---
## [Perkedel/Kaded-fnf-mods@0f568e0c1d...](https://github.com/Perkedel/Kaded-fnf-mods/commit/0f568e0c1d764fde4de284f72d1daf460e3dcb81)
##### 2021-05-26 20:33:21 by Joel Robert Justiawan

FNALLY THE RULE THE WORLD COMPLETE

Left the Last song. We'll meet again. and Boyfriend wears masker

c'mon we can do this!

add Hookx dialogue appear. attempted fix bubble go here
pls encapsulate the portrait dialogue initialization as a function instead, like I did here in the `DialogueBox` class, function initialize portrait left and right
psst! I copied the xml from senpai appear lol!!! I'm lazy to edit Texture Atlas XML right now!

enhanced a bit the mouse support! now you can click the menu to navigate. when this menu is currently selected, it will choose the menu like press enter. and we don't know how to build Back button. where to put this?

whoops forgot to load save data variable `naughtiness`!

an add the same wait finish funny move for main menu mouse support

okay, why pause state is not really working? I just did the same, why?!

oh man. the allowed to headbang checks if song!!! now we got it

Rule the World now has the cheer `Cool and Good` meme man lol on each beat drop rule the woorld... lmao!

hehe click arrow to change difficulty on story mode menu

whoops! left this compiler check for discord rpc should've checked only for cpp build in TitleState!

---
## [CapnMachaddish/Outpost-R505@6b8b416881...](https://github.com/CapnMachaddish/Outpost-R505/commit/6b8b4168813ebd97ffc07de20048210ccabc433d)
##### 2021-05-26 21:02:18 by SkyratBot

[MIRROR] Empty graves can now be spawned (#4755)

* Empty graves can now be spawned (#58200)

* i walked along the no mans road

POV: you got fucked on a previous branch from something stupid

* Update code/modules/ruins/lavalandruin_code/elephantgraveyard.dm

Co-authored-by: Fikou <piotrbryla@ onet.pl>

* Update code/modules/ruins/lavalandruin_code/elephantgraveyard.dm

Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

Co-authored-by: Fikou <piotrbryla@ onet.pl>
Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

* Empty graves can now be spawned

Co-authored-by: ishitbyabullet <deathzombine@outlook.com>
Co-authored-by: Fikou <piotrbryla@ onet.pl>
Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

---
## [CapnMachaddish/Outpost-R505@45ba44d276...](https://github.com/CapnMachaddish/Outpost-R505/commit/45ba44d2765b9c3ffb1f603c5683bd245a45ecf1)
##### 2021-05-26 21:15:40 by death and coding

[modular][ready] adds a uniform in the loadout for the superstar cops (#4779)

* the expression

* its speaking again

* its still talking

* WORK

* Mazovian Socio-Economics

People think Communism was some crazy idea that had its comeuppance 40 years ago. A fever that shook the world, never to return again. They were right. Until *he* woke up today – a spiritual corpse responsive only to the call of Commodore Red, prostitutes, and Kras Mazov. For him, Communism is still a *thing*. He will single-handedly raise the Commune of '02 from the oceanic trench where it has been resting, covered in ghosts and seaweed! He is the Big Communism Builder. Come, witness his attempt to rebuild Communism in the year '51!

0.000% of Communism has been built. Evil child-murdering billionaires still rule the world with a shit-eating grin. All he has managed to do is make himself *sad*. He is starting to suspect Kras Mazov *fucked him over* personally with his socio-economic theory. It has, however, made him into a very, very smart boy with something like a university degree in Truth. Instead of building Communism, he now builds a precise model of this grotesque, duplicitous world.

* work

* Delete accessories.dm

there is nothing, only darkness

* FUCKFUCKFUCK

* RIGHT FIXEDA AAAA

---
## [definitely-nobody-is-here/BBmulti_Announcements@2fef560b21...](https://github.com/definitely-nobody-is-here/BBmulti_Announcements/commit/2fef560b21ceaadfdbc3b649f9ca35627bb8fcd2)
##### 2021-05-26 23:23:24 by Radioactive64

5/26/21 RELEASE 1.0.0

Woooooo!

The game finishes its developement stage and enters version 1.0.0! You can enjoy the full BattleBoxes Multiplayer experience now, with no Beta testing features. The game has come a long way since it's original creation on September 5th of 2020. It has had numerous clones and hopefully you have been enjoying it. We have enjoyed making this game so far and will continue updating the game until it dies. It's amazing what we've done so far.

---

# [<](2021-05-25.md) 2021-05-26 [>](2021-05-27.md)

