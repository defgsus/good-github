# [<](2021-05-25.md) 2021-05-26 [>](2021-05-27.md)

3,912,189 events, 1,460,864 push events, 2,402,306 commit messages, 178,087,457 characters


## [N0zza/MappaMundi](https://github.com/N0zza/MappaMundi)@[557e51f526...](https://github.com/N0zza/MappaMundi/commit/557e51f5266a25072d47d8cc941bc4085ac75836)
#### Wednesday 2021-05-26 01:07:01 by FaazNoushad

Extremely Important Change.

HOLY FUCKING SHIT. MAPPA MUNDI WILL NEVER BE THE SAME AGAIN. WE WILL RULE THE WORLD BECAUSE OF THI....
I fixed Mingrelia..

---
## [KeefeOverby/Button-to-Second-Screen](https://github.com/KeefeOverby/Button-to-Second-Screen)@[fb431af0ec...](https://github.com/KeefeOverby/Button-to-Second-Screen/commit/fb431af0ec611fe4f152419cbd7713d2dad66649)
#### Wednesday 2021-05-26 05:42:49 by KeefeOverby

Button to Second Screen App
v 0.12
Success!!!!!!!!!!
It was way to difficult to find a simple explanation for one button that navigates to a second screen. Developers that have a lot of experience make "basic" lessons way too complicated. It took me three days searching the internet to find little bits and pieces of information just to complete this simple app. It's a shame that experienced developers call their extremely complicated lessons "basic" it's dishonest to say that overly complicated lessons are "basic" when they know deep down that its clearly not basic. This app that I just created is basic, one button to a second screen. That is basic, clear and easy to comprehend. I am honest, but unfortunately some people are not, dishonest people are horrible.

---
## [kumailkhan1/fyp](https://github.com/kumailkhan1/fyp)@[a583f1acb2...](https://github.com/kumailkhan1/fyp/commit/a583f1acb2c9c5a49aee88102052fca7f1554e8b)
#### Wednesday 2021-05-26 09:46:01 by HafsaZubair17

Revert "FUCK YOU KUMAIL"

This reverts commit ae497d8c65e0779662ba968155c122d06e27984b.

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[2d2906731a...](https://github.com/mrakgr/The-Spiral-Language/commit/2d2906731af17b85d6424c584c7bf5eabf558063)
#### Wednesday 2021-05-26 10:12:41 by Marko GrdiniÄ‡

"9:10am. Let me chill for a while. 10m is not enough to get over my grogginess. I slept well tonight though.

I had time to think about it. Unless I got answers that solve my problems already, I will put some special effort into making the Python debugger work. Until I can figure out how to hack PyTorch I am stuck, so I should focus on overcoming that particular hurdle.

9:40am. Let me read a chapter of Takahane and then I will start the work day.

9:55am. Time to start. I had my bit of fun. Let me check my mail first to see if there are any updates.

> I'm afraid you won't be able to copy-paste these functions as they are implemented in c++. But you don't need that with a custom Function as described here: https://pytorch.org/docs/stable/notes/extending.html. You can simply write the formula you want for the backward.

No way. For the softmax it should definitely be possible to grab the backwards closure, though it might be simply exping the log outputs...

Agh, I might have to write the formula for that. But I am going to have to fire up the debugger to make sure. There is no way of doing it as separate ops if it makes things slower in the end.

10am. Regarding the SO question, there is no peep.

So my plan for the day is intact. I'll have to figure out how to start the debugger on my own.

```
PS C:\Users\Marko\Source\Repos\The Spiral Language\Spiral Compilation Tests> C:/Users/Marko/anaconda3/shell/condabin/conda-hook.ps1
PS C:\Users\Marko\Source\Repos\The Spiral Language\Spiral Compilation Tests> conda activate 'C:/Users/Marko/anaconda3'
(base) PS C:\Users\Marko\Source\Repos\The Spiral Language\Spiral Compilation Tests>
```

Oh, is just this enough to activate it?

10:10am. Crap. Debugging into these library calls does not work. It just goes over them.

https://stackoverflow.com/questions/53594900/visual-studio-code-python-debugging-step-into-the-code-of-external-functions

Oh it should be possible. Where do I put the option?

```
    "configurations": [
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false
        }
    ]
```

Intellisense suggested this, so it has to be right. Let me try running it.

10:15am.

```py
def _run_code(code, run_globals, init_globals=None,
              mod_name=None, mod_spec=None,
              pkg_name=None, script_name=None):
    """Helper to run code in nominated namespace"""
    if init_globals is not None:
        run_globals.update(init_globals)
    if mod_spec is None:
        loader = None
        fname = script_name
        cached = None
    else:
        loader = mod_spec.loader
        fname = mod_spec.origin
        cached = mod_spec.cached
        if pkg_name is None:
            pkg_name = mod_spec.parent
    run_globals.update(__name__ = mod_name,
                       __file__ = fname,
                       __cached__ = cached,
                       __doc__ = None,
                       __loader__ = loader,
                       __package__ = pkg_name,
                       __spec__ = mod_spec)
    exec(code, run_globals)
    return run_globals
```

Huh, why does it put me here as soon as I step into the softmax. Well regardless, let me continue with this. After the debugging session I should know here things stand.

10:20am. Ok, so the above segment is what runs after the script executes the last statement.

```py
import torch
import torch.optim
import torch.functional
import torch.nn
import torch.nn.functional
import torch.distributions
import numpy as np
import torch._C
import nets

q = torch.rand((3,3),requires_grad=True)
x = torch.softmax(q,dim=-1)
print('hello')
```

Even though I am placing the breakpoints here, it is skipping over the two statements I want to debug.

10:25am. Let me send out another question to SO.

10:40am. Now I am reading /g/. Ok, I hadn't expected this. What the hell. I thought my job would be done after I've managed to get the Python debugger to run.

11:05am. Had to take a break. Let me just try stepping into nn.linear. The reason why the debugger is failing might be because it straight up links to C code.

```
import torch.nn
w = torch.nn.Linear(3,4)
```

Damn it, this works. I guess the reason this does not work is because it straight up links to C code.

https://stackoverflow.com/questions/67690696/how-to-make-the-python-debug-task-use-the-default-powershell-profile/67701492#67701492

Oh, got a reply here.

No, the solution given here does not work for me.

11:15am. Ok, I am going to have to do what those guys suggest and write my own stuff. Forget the softmax. Though I am tempted to go with `torch.script.jit`, who knows what kind of results that will give me.

Instead I should focus on making my own linear layer. Right now, I won't even bother with moving averages. Instead let me just normalize the inputs and the gradients based on the batch information. Let me just focus on that.

11:25am. Now if I could only collect my bearings. Today and yesterday these issues caught me completely off guard.

Ok, let me make something.

https://pytorch.org/docs/stable/notes/extending.html
https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html

You know what, let me remove https://stackoverflow.com/questions/67701264/how-to-debug-external-library-python-code-in-visual-studio-code

The question is too generic and it got sidetracked into my own misunderstanding.

```
print(torch.nn.functional.softmax)
```

No softmax both in `torch` and `torch.nn.functional` is a plain function.

```py
# Inherit from Function
class LinearFunction(Function):

    # Note that both forward and backward are @staticmethods
    @staticmethod
    # bias is an optional argument
    def forward(ctx, input, weight, bias=None):
        ctx.save_for_backward(input, weight, bias)
        output = input.mm(weight.t())
        if bias is not None:
            output += bias.unsqueeze(0).expand_as(output)
        return output

    # This function has only a single output, so it gets only one gradient
    @staticmethod
    def backward(ctx, grad_output):
        # This is a pattern that is very convenient - at the top of backward
        # unpack saved_tensors and initialize all gradients w.r.t. inputs to
        # None. Thanks to the fact that additional trailing Nones are
        # ignored, the return statement is simple even when the function has
        # optional inputs.
        input, weight, bias = ctx.saved_tensors
        grad_input = grad_weight = grad_bias = None

        # These needs_input_grad checks are optional and there only to
        # improve efficiency. If you want to make your code simpler, you can
        # skip them. Returning gradients for inputs that don't require it is
        # not an error.
        if ctx.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)
        if ctx.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input)
        if bias is not None and ctx.needs_input_grad[2]:
            grad_bias = grad_output.sum(0)

        return grad_input, grad_weight, grad_bias
```

This is straight from the docs. I should just focus on hacking this.

Let me just say that I do not like PyTorch's way of doing things here. It is almost like in old Spiral, but what it does is allocate the adjoints in the backward function itself. That is inefficient when the function has shared inputs. It will allocate them in the backwards function and then the autograd will have to copy them into the actual one. A better way would be for the adjoints to get passed in and added into.

Does it do it like this for the sake of distributed processing perhaps? A particularly big problem of this way of doing things would be the weight matrices. Allocating a fresh matrix only to copy it over would be inefficient.

I am going to ask about this as it is a problem.

Should I zero out the weight grads by setting them to `None` instead of zeroes?

Let me take a look at that page again.

https://pytorch.org/docs/master/generated/torch.optim.Optimizer.zero_grad.html

> instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance.

Yeah, as expected. I should be setting it to `None`. That will take care of the problem of efficiency.

As for the rest, I just have to make sure not to share inputs unless I can't help it.

12:05pm. Sorry, I know I should have been writing code this morning, but this series of issues just threw me off my game so much.

Let me stop here for breakfast. After that I'll make my grad normalizing linear layer. That should stabilize things somewhat. I say somewhat because the full scheme with the replay buffer is needed to get me all the way. Apart from that, I might even try making the softmax.

I should just focus on this today and do what I have to. After that hopefully it should give me some familiarity with hacking PyTorch."

---
## [sobujupwork2021/fashioncad2021](https://github.com/sobujupwork2021/fashioncad2021)@[bfde877769...](https://github.com/sobujupwork2021/fashioncad2021/commit/bfde877769b890cf4542a6f88739789b34a083b3)
#### Wednesday 2021-05-26 11:47:20 by fashioncad2021

Add files via upload

Hello, I am a professional sewing pattern maker and Garments Technician. I Have 10 years of experience and still working in the apparel Industry. I have worked with Most of the best apparel buyers in the world. I can make any types of Garments Pattern that is ready for production,
MY SERVICES:
ïƒ˜ Pattern making and grading
ïƒ˜ Pattern converting
ïƒ˜ Pattern editing
ïƒ˜ Pattern Cutter, Plotting, Converting
ïƒ˜ Marker Making
ïƒ˜ Graded Measurements Chart
ïƒ˜ Sketch & Tech pack making
ïƒ˜ Consumption
100% satisfaction guaranteed

PRODUCTS FASHION DESIGN:
Womenâ€™s wear, men's wear, baby/toddler/kid's wear. â€¢ Sportswear: Skiwear & snowboard wear,
swimwear & surf wear, equestrian sportswear, activewear, yoga wear, running wear. â€¢ Special
design collections: Maternity & nursing wear, plus size womenâ€™s wear, modest fashion. â€¢ t-shirt
design â€¢ Underwear design â€¢ Full range fashion collections including, woven, jersey, heavy knit,
lightweight knit, denim, nightwear, swimwear, underwear, and jackets,

Delivery Format:
ïƒ˜ I give the soft copy a format that you can use any CAD software,
ïƒ˜ As like DXF,RUL PLT, AI, HPGL, PDD, MDL PSD, PSB PDF,
ïƒ˜ Pattern sketch that you will be able to print on this page
ïƒ˜ A4, A3, A5, etc.

What I need from you to get this Start:
ïƒ˜Garment Front + Back Clear Pictures/sketch / Design or hand drawing image & please give me your
choices any idea,
ïƒ˜ Size Chart for your pattern according to measurements spec,

I always believe in providing the best quality service, I love to do communicate in detail with my clients and give daily updates on works, are you looking for a person who is able to your job in the best at a certain time? Please donâ€™t hesitate to send me an invitation to your valuable interview,

---
## [HeroShotPhotography/I-Am-Not-Photogenic](https://github.com/HeroShotPhotography/I-Am-Not-Photogenic)@[a97be6ef65...](https://github.com/HeroShotPhotography/I-Am-Not-Photogenic/commit/a97be6ef6566652851e64556096d93c4f141a1da)
#### Wednesday 2021-05-26 11:52:15 by HeroShotPhotography

Update README.md

I'm not photogenic. Can you still help me?

You, my friend, are our favourite customer!

Over the last 10 years we have literally heard every story about why people think they have a special case of â€˜un-photogenic-nessâ€™.

It amazes us how bad people think they look until they experience a headshot session with us, so it really doesnâ€™t phase us in the slightest.

In-fact, it makes us even more excited to work with you!

The majority of the headshots that are displayed on our website and the reviews youâ€™ll read about us are from ordinary people who also felt they werenâ€™t photogenic.

We are highly skilled photographers, here to help pose and coach you throughout your entire session.

We have professional lighting and cutting edge technology and are 100% committed to make you look your best.

You just might be surprised at how photogenic you can be.

#photoshoot #photogenic #headshots #headshotphotographer #findyourheroshot

---
## [nin66/CompGraphics](https://github.com/nin66/CompGraphics)@[0b7394023e...](https://github.com/nin66/CompGraphics/commit/0b7394023e11ee0906e1eaab13d5e43aea3ff90f)
#### Wednesday 2021-05-26 12:27:29 by WichaelMu

Minor Fish Update

"Minor"? this movement thing literally took me 12 hours and i still couldn't figure it out.

for some dumb reason, only BrownFish wants to move towards a coral, nothing else. Even though there's a fishies array that holds every single fish in the scene, it still doesnt want to move. However, 1 in 100 times, everything works properly.

Still, the fishies will still collide into terrain and go through them because three js and javascript are hilariously slow at processing anything and it takes too long to load Reef Exploration.

Three JS and javascript are both the two worst things ive ever worked with, same goes for python. interpreted languages are very stupid and should never be used.

i started working on this at 10am and its now 10pm. i literally wasted my whole day trying to get something to work on an inferior language.

I also love this subject, ive learnt more about how three js and javascript suck ass than actually learning content because no one teaches it anyway.

who even reads this?

---
## [ash-chin/wtf](https://github.com/ash-chin/wtf)@[5d5a154079...](https://github.com/ash-chin/wtf/commit/5d5a15407976f61b9e4c968e4f0bc0a8a4fd24d8)
#### Wednesday 2021-05-26 12:30:02 by Ash

uuh I forgot what I've done

Oh!
* Redesigned menu
* Added option to select scenes in menu
* Tried to change camera reticle and failed
* Fixed BountyNetwork in scenes
* WORKING VOLUME SLIDER FINALLY FUCK YOU BRACKEY

---
## [jaspervdj/hakyll](https://github.com/jaspervdj/hakyll)@[8980133284...](https://github.com/jaspervdj/hakyll/commit/8980133284d7d5f0d7cd71580796150c74b22f2d)
#### Wednesday 2021-05-26 14:54:34 by Alexander Batischev

Docs: IRC channel moved from Freenode to Libera.Chat (#848)

There's no mention of this on the issue tracker, so here's my recap of
what led to this change.

There is some drama around Freenode. Their volunteer staff quit[1]. Then
the new-ish management started enacting strange policy changes[2] and
take over the channels[3]. On June 26th 2021 Freenode's bot tried to
take over #hakyll, but failed; however, it did succeed in #haskell and
many, many other channels.

For the preceding week, me and IRC user henk were trying to move the
channel off the Freenode, either to Libera.Chat or OFTC. Jasper, the
founder of Hakyll, was absent from IRC and didn't respond to my emails.
Me and henk are the most active IRC users on the channel. Also, I have
Collaborator access to the repo, and henk has chanop access to the IRC
channel. We believe that at this time, we're the ones who are the best
positioned to execute the move, so we're doing it.

We only considered Libera.Chat and OFTC. Libera.Chat was created a week
earlier by the same staff who quit Freenode; I personally consider them
to be a spiritual successor of Freenode. OFTC is a well-established
network with good governance documentation. Both networks are
FOSS-friendly. The choice wasn't obvious.

Libera.Chat was picked because 10 users moved there (and 1 more did
while I was typing this!), whereas only 2 joined OFTC (me and henk, and
those weren't votes for OFTC â€” we were just ensuring that the channel is
ours). Also, #haskell and #ghc are on Libera too, so it makes sense for
a Haskell project such as Hakyll to be present on Libera.

For posterity:

<Minoru> info #hakyll
-ChanServ(ChanServ@services.)- Information on #hakyll:
-ChanServ(ChanServ@services.)- Founder    : jaspervdj
-ChanServ(ChanServ@services.)- Registered : Mar 01 13:29:17 2011 (10y 12w 5d ago)
-ChanServ(ChanServ@services.)- Mode lock  : +ntc-slk
-ChanServ(ChanServ@services.)- *** End of Info ***

1. https://kline.sh
2. https://github.com/freenode/web-7.0/pull/513
3. https://www.devever.net/~hl/freenode_abuse

---
## [Musvage/The-Second-Sengoku](https://github.com/Musvage/The-Second-Sengoku)@[ac856d65a6...](https://github.com/Musvage/The-Second-Sengoku/commit/ac856d65a68fea879975a1f14b0c8dd77a26c9c9)
#### Wednesday 2021-05-26 15:04:23 by GottKroprinz

Some Okinawa Love cause I love this island

I fucking love this island so much

---
## [hashicorp/terraform-plugin-framework](https://github.com/hashicorp/terraform-plugin-framework)@[d914828a67...](https://github.com/hashicorp/terraform-plugin-framework/commit/d914828a677d3bedc0ee8858e6e59b609c18b108)
#### Wednesday 2021-05-26 15:04:55 by Paddy Carver

Add reflection package.

Add an internal reflect package, with one exported function, Into. This
lets us use common reflection rules to map a tftypes.Value onto a Go
type.

This doesn't handle unknown or null flags yet. My current thoughts are
that we'll have flags for different options re: handling those
situations:

* we can error if an unknown or null value is encountered
* we can treat unknown and null as empty values, flattening out some
  context
* we can see if there's an optional `SetKnown` and `SetNull` method on
  the Go type being targeted, and call it if there is, letting users opt
  into caring about these details if they want.

Also updated the List type to use this new package.

Committing to share, going to think a bit more on the design of the
thing:

* Are we sure we want to go from tftypes.Value instead of from
  AttributeValue?
* Let's get null and unknown values taken care of
* teeeeeeessssts
* the godoc could honestly be much more helpful

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[444b9db68e...](https://github.com/mrakgr/The-Spiral-Language/commit/444b9db68e4ed765629449c11677fc224ee3d4ff)
#### Wednesday 2021-05-26 16:21:06 by Marko GrdiniÄ‡

"1:25pm. Let me finish the chapter, deal with the chores and I will resume.

2pm. Done with chores. Let me resume.

https://stackoverflow.com/questions/67690696/how-to-make-the-python-debug-task-use-the-default-powershell-profile/67701492?noredirect=1#comment119667591_67701492

I got some good replies to the question. I think if I combine with what Steve and Molly told me, I can get rid of the need to have profiles.

2:10pm. I accepted the second answer. Now let me move forward. What is next?

Right, the hacked linear layer.

I talked before about how inefficient the backward pass is in allocating adjoints. What I've just realized is that it would be particularly bad for RNNs. Having to copy at every timestep would hit those architectures hard. With FF nets it would be fine if I set the weight adjoint matrices to None at every step. RNNs would not benefit much from that.

2:20pm. https://pytorch.org/

I am looking over the docs for a bit. This recent issues have got me wondering.

By all appearances PyTorch in fact just an AD library. I knew this, but thought there might be a small chance that there would be something more to it.

The people who are working on neurochips, just how will they fit PyTorch models given the hardware constraints? AD is very flexible and general. You can't fit neurochips in such a framework - they get their power by being restricted. If they have local memory and computation, you'd need to instruct the nodes specifically.

It is a different kind of compilation challenge.

Neurochips would be expected to need specific libraries that match the constraint of the hardware. Maybe I've gotten lucky by getting passed over during this first wave of ML hardware. If those people really think they can succeed in meshing with the PyTorch ecosystem, then their hardware might not be radical enough to be worth looking into.

Once those meristive chips come around, I'll be writing my own library in the end.

Right now, let me not make too much of a fuss over this.

2:30pm. Focus me. I finally got the Anaconda env init out of the way. I've been suffering over this for a long time.

Now let me do that gradient normalizing linear layer.

```
    if not torch.jit.is_scripting():
        if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):
            return handle_torch_function(linear, tens_ops, input, weight, bias=bias)
    if input.dim() == 2 and bias is not None:
        # fused op is marginally faster
        ret = torch.addmm(bias, input, weight.t())
    else:
        output = input.matmul(weight.t())
        if bias is not None:
            output += bias
        ret = output
    return ret
```

I should try to use addmm on the forward pass if possible.

2:40pm. There is just one thing that is bothering me. Whether to normalize the gradients to `var (b * a)` or just `var b` where `b` and `a` the outer and inner matrix dimensions specifically. I was surprised to find that layer norm is normalizing to `var (b * a)` which made me reconsider my original plan of normalizing to `var b`.

3:20pm. I am still thinking about it. Realizing the way long term credit assignment works broke me.

Backprop no longer makes sense to me. I think that the constrastive learning rules would be better. Also sparsity. When I was considering normalizing to just `var b` I was assuming the the activations should be sparse. But `var (b * a)` feels like it assumes the activations are dense.

4:15pm. I am still thinking about it. I think I am going to go for the old idea I had back in April and nearly forgot by now.

That idea was to make sure that the input and the output variance match up. I came up with this for RNNs, but now that I am completely perplexed over how to normalize, it might be a fitting time to revive the idea.

Normalizing to 1 only makes sense when the signals are all 1 or -1 and I still haven't gotten to the replay buffer. Therefore, for straightforward training, matching the variances seems like the best way to go.

I want to have something useful even for regular PG style training. What I start doing CFR style training through the replay buffer, things will be easier and I will normalize the rows to unit norm.

4:25pm. I say the rows, but my intention was to do that for the whole batch. Am I changing my mind.

Actually, even for the signals, where at the top they are 1 or -1, suppose I had sparse activations and a tree like graph. Then it would be better to do variance matching.

If I have something like a diamond graph.

```
 O
X Y
 I
```

Output `O` and input `I` should have equal credit, but `X` and `Y` should not. Their credit should just sum to 1.

That is why it might be better to stick with variance matching as a general principle.

4:30pm. This way I could completely ditch the need for tracking using moving averages.

Ah, right, now I have a problem. I am going to have to fuse the nonlinearity with the linear layer in order for this to work properly. Otherwise the gradient will get reduced.

Variance matching is the other side of the coin to layer norm. I should use LN instead of normalizing the inputs.

4:50pm.

```
q = torch.zeros(3,3)
w = q.requires_grad_()
w
q
```

This is wrong. This ends up setting the mult on the original tensor.

4:55pm.

```py
class MultFunction(torch.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input * 2

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        return grad_output / 2
```

The trouble with non being able to get the original primitives is that I am not sure how to deal with inputs here.

5:15pm.

```py
import torch
from torch.autograd import Function
import torch.nn
import torch.nn.functional

class Mult(Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input * 2

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        print(input)
        return (grad_output / 2)

i = torch.scalar_tensor(2,requires_grad=True)
x = Mult.apply(i)
x.backward(torch.scalar_tensor(1))
print(i.grad)
```

I am stuck no matter how much I think about it. I want to implement LN + linear + relu, but I do not know how to reuse the backward primitives from existing operations. For multiplication in this example, I do not think that runing the backward function on the result would give me what I want.

5:25pm. I have no idea. I am going to ask for advice on SO or on PyTorch forums.

5:30pm.

```py
import torch
from torch.autograd import Function
import torch.nn
import torch.nn.functional

class VarianceMatch(Function):
    @staticmethod
    def forward(ctx, f, x):
        y = f(x)
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, grad_y):
        y, = ctx.saved_tensors
        grad_x = y.backward(grad_y)
        grad_x *= torch.sqrt(torch.square(grad_y).sum(1) / torch.square(grad_x).sum(1))
        return grad_x.grad

i = torch.scalar_tensor(2,requires_grad=True)
x = VarianceMatch.apply((lambda x: x * 2), i)
x.backward(torch.scalar_tensor(1))
print(i.grad)
```

What happens when I try this? It goes into an infinite loop.

6:05pm. https://stackoverflow.com/questions/67708650/how-to-reuse-the-backward-pass-primitives

I am doing a lot of thinking, but I am not being very effective in getting to a solution. I am stuck. If this was the old Spiral ML library, I'd have no trouble hacking things so they work, but with PyTorch I feel lost. Maybe I could somehow save the temporary forward function, but whether it is that or putting the zeroes as the return, none of the solutions satisfy me. There should be a more elegant way of doing this.

Sigh, don't tell me I will have to do my own thing in the end? I doubt it. There should be a way of pushing through what I want here. I don't need that much from the PyTorch library.

This, then transformers, and that will last me until the end of the GPU era. After that I should be able to make Spiral really useful.

6:10pm. I am done for the day. Let me go get lunch. Hopefully tomorrow I will be able to make progress. Today I started off trying to make the debugger idea work, but that flamed out. After that I was not sure what to do. I know what I want to accomplish with PyTorch, but I still do not know how to go about it. Whatever the case, I need variance matching and the replay buffer after that. I won't get far without the stabilizers in my RL journey."

---
## [kindleszzw/cdplus](https://github.com/kindleszzw/cdplus)@[9325664440...](https://github.com/kindleszzw/cdplus/commit/9325664440b9f57b9ce5add446a9d04443735ef5)
#### Wednesday 2021-05-26 16:53:30 by kindleszzw

Merge remote-tracking branch 'origin/master'
fuck you

---
## [Speedyderat/Burgersheets](https://github.com/Speedyderat/Burgersheets)@[13c920fe44...](https://github.com/Speedyderat/Burgersheets/commit/13c920fe44bb0a3341c71bc1cb0b6f7c30a7770e)
#### Wednesday 2021-05-26 17:37:13 by Marina

had a fun day with you today but fuck you! now i gotta do programming...

---
## [takipsizad/discordbot](https://github.com/takipsizad/discordbot)@[c33ad67947...](https://github.com/takipsizad/discordbot/commit/c33ad67947486a45b7d24f0bf37da75e34a8207b)
#### Wednesday 2021-05-26 18:05:32 by takipsizad

so anyway  finally fucking done with this piece of shit

moving my bot to cogs

---
## [Awajsbro/utsushis-charm](https://github.com/Awajsbro/utsushis-charm)@[772fd315e3...](https://github.com/Awajsbro/utsushis-charm/commit/772fd315e32c955ff529dce6aaf17f4c76180ac0)
#### Wednesday 2021-05-26 19:12:04 by Awajsbro

=== charm_extraction.py Charm.py ===
I add the frame name at the charm to find it more easily on my chest

=== skillsRank.json ===
All skills with a ranking value base on god talisman checker at https://gamewith.net/monsterhunter-rise/article/show/28392

=== skillsToJewel.json ===
All skills we can craft as jewel with the jewel's lvl

=== trashSkills.txt ===
File for write skills name for ignore it during the ranking process if the user really don't care about some skills

=== rebirth.py ===
Get the charms.json make by your utsushis_charm for ranking all charms and give a liste of charm you can rebirth
Charms for rebirth are charms you can "remake" with another
ex: you can make any charm with 1 skill at 1 lvl with a 3slot charm
Also create a json file for the ranked charms and another for the charm to rebirth
run it with: "python rebirth.py"

=== More ===
I'm not a python dev so sorry if i maybe do some bad stuff
Feel free to integrete the rebirth.py if you want
my apologies for my english and thank you a lot for utsushi_charm you save to me a great amount of time =D

---
## [Perkedel/Kaded-fnf-mods](https://github.com/Perkedel/Kaded-fnf-mods)@[0f568e0c1d...](https://github.com/Perkedel/Kaded-fnf-mods/commit/0f568e0c1d764fde4de284f72d1daf460e3dcb81)
#### Wednesday 2021-05-26 20:33:21 by Joel Robert Justiawan

FNALLY THE RULE THE WORLD COMPLETE

Left the Last song. We'll meet again. and Boyfriend wears masker

c'mon we can do this!

add Hookx dialogue appear. attempted fix bubble go here
pls encapsulate the portrait dialogue initialization as a function instead, like I did here in the `DialogueBox` class, function initialize portrait left and right
psst! I copied the xml from senpai appear lol!!! I'm lazy to edit Texture Atlas XML right now!

enhanced a bit the mouse support! now you can click the menu to navigate. when this menu is currently selected, it will choose the menu like press enter. and we don't know how to build Back button. where to put this?

whoops forgot to load save data variable `naughtiness`!

an add the same wait finish funny move for main menu mouse support

okay, why pause state is not really working? I just did the same, why?!

oh man. the allowed to headbang checks if song!!! now we got it

Rule the World now has the cheer `Cool and Good` meme man lol on each beat drop rule the woorld... lmao!

hehe click arrow to change difficulty on story mode menu

whoops! left this compiler check for discord rpc should've checked only for cpp build in TitleState!

---

# [<](2021-05-25.md) 2021-05-26 [>](2021-05-27.md)

