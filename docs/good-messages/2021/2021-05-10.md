# [<](2021-05-09.md) 2021-05-10 [>](2021-05-11.md)

5,830,935 events, 1,216,040 push events, 1,940,693 commit messages, 152,130,341 characters


## [alexhb1/mapUON](https://github.com/alexhb1/mapUON)@[94ab043f5a...](https://github.com/alexhb1/mapUON/commit/94ab043f5a654d7506bafc8a3465db785590a302)
#### Monday 2021-05-10 02:48:15 by PennSylvannia

Well shit

totally fucking forgot to do derf's shit, but eh whatever
everything else should be good
maybe. RDA, Ultima, and WS are gonna get into a skirimish in the future, should be funny

---
## [ljvmiranda921/ljvmiranda921.github.io](https://github.com/ljvmiranda921/ljvmiranda921.github.io)@[7fd45997ed...](https://github.com/ljvmiranda921/ljvmiranda921.github.io/commit/7fd45997ed53a5f71ea4d6d5398bde5c63071eb9)
#### Monday 2021-05-10 03:06:01 by Lj V. Miranda

Improve organization of MLOps series

I'm a bit sick, but I guess my grogginess has allowed me to be more
objective and edit things in cold blood--not that it's a good thing,
mind you. Anyway, some updates:
- I added a horizontal rule at the end to separate the "What's next"
sections
- Also improved the wording on some parts, especially for Part 2.
- Added some teasers for Part 2, and a few more edits on grammar and
usage (I hate prepositions!)

---
## [VedantPol/CODEFORCES_ROUND_SOLUTIONS](https://github.com/VedantPol/CODEFORCES_ROUND_SOLUTIONS)@[5fd4e300a3...](https://github.com/VedantPol/CODEFORCES_ROUND_SOLUTIONS/commit/5fd4e300a3b22ca0c652f449d7f9cb6574e60a0d)
#### Monday 2021-05-10 06:18:27 by vedant pol

Hulk   Codeforces Round #366 (Div. 2)

Dr. Bruce Banner hates his enemies (like others don't). As we all know, he can barely talk when he turns into the incredible Hulk. That's why he asked you to help him to express his feelings.

Hulk likes the Inception so much, and like that his feelings are complicated. They have n layers. The first layer is hate, second one is love, third one is hate and so on...

For example if n = 1, then his feeling is "I hate it" or if n = 2 it's "I hate that I love it", and if n = 3 it's "I hate that I love that I hate it" and so on.

Please help Dr. Banner.

Input
The only line of the input contains a single integer n (1 ≤ n ≤ 100) — the number of layers of love and hate.

Output
Print Dr.Banner's feeling in one line.

Examples
inputCopy
1
outputCopy
I hate it
inputCopy
2
outputCopy
I hate that I love it
inputCopy
3
outputCopy
I hate that I love that I hate it

---
## [TwinkleInstituteAB/MBBS-abroad](https://github.com/TwinkleInstituteAB/MBBS-abroad)@[d939fc1e8c...](https://github.com/TwinkleInstituteAB/MBBS-abroad/commit/d939fc1e8c50bd138fff6177c22619a94fc4c1dc)
#### Monday 2021-05-10 07:27:26 by TwinkleInstituteAB

Create Become a Successful Doctor From Russia Medical College 2021-22 Twinkle InstituteAB

Among all the developed nations within the world, Russia stands to go in being an extremely educated country with a skilled rate of ninety-nine and the majority of its locals holding a university degree. This made tradition of Russian instructional establishments provides all the scholars with glorious education and living conditions. It is the best destination for international students to study medicine as every year Russia takes in the admission of many aspirants from over a hundred and seventy different countries across the globe and additionally provides free education to the meriting few students. Even otherwise, the price of education in Russia is incredibly reasonable as compared to alternative developed countries just like the U.S., UK, or alternative European countries.

There is an alternative you’ll be able to create from 741 universities in eighty-two regions providing over four hundred fields of study and quite 650 subjects across numerous authorizations. Further, their area unit several of the leading Russian medical universities have joint programs unitedly with European universities thereby giving the good thing about twin state degrees to the scholars. A student following MBBS in Russia area unit perpetually being benefitted by the superb sports facility.

Over concerning seventy-one of scholars in Russia medical college area unit concerned in sports activities and most of the schools have glorious sports facilities and also the students get the prospect to induce into several of the highest groups and radio-controlled by the most effective internationally noted coaches. Culturally, Russia may be an international country wherever students from each corner of the globe and of the foremost numerous countries and cultures will adapt simply.

The Russian constitution provides the liberty to the individuals to apply numerous religions from Christianity, Buddhism, Hinduism, Judaism, Islam, and several ethnic religions. in addition, students will get the good thing about several subsidies like free travel on transport and avail several alternative facilities.

The health care system in Russia is being provided by the govt furthermore as by personal hospitals running there. There area unit concerning 5 thousand 5 hundred hospitals in Russia at an instant. although the state provides a free medical facility to the scholars learning there, as in several alternative countries, the increasing bourgeois society prefers personal hospitals over any government hospitals thanks to fast service, shorter waiting times, and easier access.

However, the foremost vital issue to envision is that the standard of medical treatment in Russia is very good, and recently in a number of the foremost specialized medical fields Russia has several advanced achievements in most of its areas. Now students United Nations agency area unit dreaming of learning within the field of medication abroad, particularly MBBS abroad in Russia will glance through the few general facts concerning this country that area unit given below.

Advantages of pursuing study  abroad MBBS in Russia for Indian Medical students.

Their area unit most of the Indian medical aspirants once qualifying for NEET has an extended struggle obtaining a seat into Govt medical schools because the availableness is few and competition is hard. The demand for large donations and high fees leaves a student with little or no hope. Taking this stuff into thought Russia has become the proper place to check MBBS with no taxation fee or high fee structure.

There area unit various places wherever individuals dream concerning learning medication or doing their MBBS, however, once a student needs to induce full exposure and most perception into medication, Russia is that the right place with the most effective medical schools.
Once the place of destination is being set the next question that clicks into mind is that the university fee and price of living. learning anyplace abroad is an upscale journey however once it involves Russia it’s the foremost cheap destination to check MBBS compared to the opposite countries within the world.
All the Russian medical universities area unit standard and just in case a student finds it exhausting to finance himself he will get an education loan from any bank once it involves this country with no problem.
there’s additionally the scholarship take a look at in some medical universities that a student will pass to avail himself with the advantages of it.
The Top medical universities in Russia give the most effective facilities for all the foreign medical students and additionally permits students United Nations agency haven’t cleared the language tests IELTS or TOFEL.
Being abounding with these skills is one among the countries wherever doing MBBS in Russia assures a high paying job anyplace during this whole world.

---
## [Aditi-Karve/ML_Python](https://github.com/Aditi-Karve/ML_Python)@[b82d91c022...](https://github.com/Aditi-Karve/ML_Python/commit/b82d91c022fd8785500816203333ecc0db6fa25a)
#### Monday 2021-05-10 08:27:48 by Aditi Karve

Heart Disease Yes/No - Decision Tree

Heart Disease Prediction
Abstract: Heart disease is easier to treat when it is detected in the early stages. Machine learning techniques may aid a more efficient analysis in the prediction of the disease. Moreover, this prediction is one of the most central problems in medicine, as it is one of the leading diseases related to an unhealthy lifestyle. So, an early prediction of this disease will be useful for a cure or aversion. Problem Statement: Analyze the heart disease dataset to explore the machine learning algorithms and build decision tree model to predict the disease. Dataset Information: Each attribute in the heart disease dataset is a medical risk factor.
Variable Description:
Column
Description age Age of the patient gender Gender of the patient - (0,1) - (Male, Female) chest_pain It refers to the chest pain experienced by the patient -(0,1,2,3) rest_bps Blood pressure of the patient while resting(in mm/Hg) cholesterol Patient's cholesterol level (in mg/dl) fasting_blood_sugar The blood sugar of the patient while fasting rest_ecg Potassium level (0,1,2) thalach The patient’s maximum heart rate exer_angina It refers to exercise-induced angina - (1=Yes, 0=No)
PG Program in Analytics
Problem Statement – Decision Tree
old_peak It is the ST depression induced by exercise relative to rest(ST relates to the position on ECG plots) slope It refers to the slope of the peak of the exercise ST-Segment- (0,1,2) ca Number of major vessels - (0,1,2,3,4) thalassemia It refers to thalassemia which is a blood disorder - (0,1,2,3) target The patient has heart disease or not - (1=Yes, 0=No)
Scope: ● Understand data by performing exploratory data analysis ● Training and building classification algorithms to predict heart disease ● Understand various model performance metrics and measure the performance of each model
Learning Outcome:
The students should be able to predict heart disease from medical records with the help of classification models. They should also be able to perform EDA and re-build the model and check if there is any significant change in the predictive scores.

---
## [Aditi-Karve/ML_Python](https://github.com/Aditi-Karve/ML_Python)@[18cd9a710f...](https://github.com/Aditi-Karve/ML_Python/commit/18cd9a710f17f3f4bd349718f7886d4138912d3a)
#### Monday 2021-05-10 08:39:35 by Aditi Karve

HR Analytics Random Forest - RandomSearchCV

A company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company. Many people signup for their training. Company wants to know which of these candidates are really wants to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates. Information related to demographics, education, experience are in hands from candidates signup and enrollment.
This dataset designed to understand the factors that lead a person to leave current job for HR researches too. By model(s) that uses the current credentials,demographics,experience data you will predict the probability of a candidate to look for a new job or will work for the company, as well as interpreting affected factors on employee decision.
The whole data divided to train and test . Target isn't included in test but the test target values data file is in hands for related tasks. A sample submission correspond to enrollee_id of test set provided too with columns : enrollee _id , target
Note:
•	The dataset is imbalanced.
•	Most features are categorical (Nominal, Ordinal, Binary), some with high cardinality.
•	Missing imputation can be a part of your pipeline as well.
Features
•	enrollee_id : Unique ID for candidate
•	city: City code
•	city_ development _index : Developement index of the city (scaled)
•	gender: Gender of candidate
•	relevent_experience: Relevant experience of candidate
•	enrolled_university: Type of University course enrolled if any
•	education_level: Education level of candidate
•	major_discipline :Education major discipline of candidate
•	experience: Candidate total experience in years
•	company_size: No of employees in current employer's company
•	company_type : Type of current employer
•	lastnewjob: Difference in years between previous job and current job
•	training_hours: training hours completed
•	target: 0 – Not looking for job change, 1 – Looking for a job change
Inspiration
•	Predict the probability of a candidate will work for the company
•	Interpret model(s) such a way that illustrate which features affect candidate decision

---
## [Aditi-Karve/ML_Python](https://github.com/Aditi-Karve/ML_Python)@[93ceaa1522...](https://github.com/Aditi-Karve/ML_Python/commit/93ceaa152202fd2f97e08e65f377f4b93dacdba9)
#### Monday 2021-05-10 08:44:09 by Aditi Karve

Student Grades - Linear Regression/Random forest

This data approach student achievement in secondary education of two Portuguese schools. The data attributes include student grades, demographic, social and school-related features) and it was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). In [Cortez and Silva, 2008], the two datasets were modeled under binary/five-level classification and regression tasks. Important note: the target attribute G3 has a strong correlation with attributes G2 and G1. This occurs because G3 is the final year grade (issued at the 3rd period), while G1 and G2 correspond to the 1st and 2nd period grades. It is more difficult to predict G3 without G2 and G1, but such prediction is much more useful (see paper source for more details).
Attribute Information:
1.	school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)
2.	sex - student's sex (binary: 'F' - female or 'M' - male)
3.	age - student's age (numeric: from 15 to 22)
4.	address - student's home address type (binary: 'U' - urban or 'R' - rural)
5.	famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)
6.	Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)
7.	Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)
8.	Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)
9.	Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
10.	Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
11.	reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')
12.	guardian - student's guardian (nominal: 'mother', 'father' or 'other')
13.	traveltime - home to school travel time (numeric: 1 - 1 hour)
14.	studytime - weekly study time (numeric: 1 - 10 hours)
15.	failures - number of past class failures (numeric: n if 1<=n<3, else 4)
16.	schoolsup - extra educational support (binary: yes or no)
17.	famsup - family educational support (binary: yes or no)
18.	paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
19.	activities - extra-curricular activities (binary: yes or no)
20.	nursery - attended nursery school (binary: yes or no)
21.	higher - wants to take higher education (binary: yes or no)
22.	internet - Internet access at home (binary: yes or no)
23.	romantic - with a romantic relationship (binary: yes or no)
24.	famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
25.	freetime - free time after school (numeric: from 1 - very low to 5 - very high)
26.	goout - going out with friends (numeric: from 1 - very low to 5 - very high)
27.	Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
28.	Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
29.	health - current health status (numeric: from 1 - very bad to 5 - very good)
30.	absences - number of school absences (numeric: from 0 to 93)
these grades are related with the course subject, Math or Portuguese:
1.	G1 - first period grade (numeric: from 0 to 20)
2.	G2 - second period grade (numeric: from 0 to 20)
3.	G3 - final grade (numeric: from 0 to 20, output target)

---
## [raffycabrera/Drew](https://github.com/raffycabrera/Drew)@[e3bab10449...](https://github.com/raffycabrera/Drew/commit/e3bab10449a7cdef090e9212074c63bed7afef82)
#### Monday 2021-05-10 08:54:33 by jamesfeliciano

register now has send email verification, log in has reset password and users will not be able to log in unless they have verified by checking their email and clicking the link sent, fuck you kalvin i had to put all of the shit mentioned above again because u pushed without the changes

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[7844a6a511...](https://github.com/mrakgr/The-Spiral-Language/commit/7844a6a5112b322a71e5243ea33854aa212c2f0c)
#### Monday 2021-05-10 10:59:18 by Marko Grdinić

"9:35am. Yesterday I got off Fang Yuan's wild ride. Now that I've fully caught up to Reverend Insanity, I am finally free to enjoy other things.

9:45am. Right now I am chilling. I'll talk later about my ideas.

10:15am. Oh Tog's hiatus will end in a week.

10:20am. Forget that, let me focus for a bit. It is time to start ramping up the pace.

I've been continuing on the theme from yesterday and thinking about hierarchical RL and long term credit assignment.

In 2019 I tried to replace backprop, but now I am changing my mind. Even so, there is no doubt that backprop is not the right tool for very long term time horizons. The inputs and their traces simply cannot be kept in memory for too long. The answer is without a doubt to predict the gradients rather than react to them. There is nothing in the calculus derivation that says how this could be done. The right rules for doing that are a complete mystery.

In general, the issue of credit assignment over long term horizons is why hierarchical RL is necessary. You need systems that attend to different timescales. That way if you have a game that requires millions or billions of steps to get a reward, you can compress the time over multiple layers of hierarchy and push the internal simulation of the world towards the desired ends.

For the outside world which cannot be manipulated directly, the internal model must be warped so the good states become more likely.

10:40am. Look at what we have now - GANs for unsupervised learning, ??? for long term credit assignment. How could I bring that all together to make a system capable of achieving human level intelligence?

I have no idea how to turn vague notions of what is right into proper algorithms.

My programming skill might be peak human level, but it quickly becomes useless when rational quantities get involved. I cannot reason out optimization processes. I put in so much effort, but the level I want to be at is negligibly closer compared to when I started.

10:45am. Still, one thing I am going to do is stop putting RNNs on a pedestal.

RNNs have the allure of being Turing complete, but that does not matter. What I need is stability and ease of training. And fixed time horizon backprop is not going to be able to train them to the level that I need. In fact, the fixed time horizon is not the problem here, on toy problems we know that backprop cannot deal with RNNs properly over large number of timesteps. This is even with LSTMs. The only thing that works are skip connections through time.

So RNNs are probably not the reason why the brain has its powers. I should not expect that backprop can do real program search using them.

The secrets of intelligence are not really related to the particular architecture being used. Feedforward ones should do.

Rather my own inventions in the recent PL monthly review, and then the GMN paper point as to where the trend is. Ironically, my 2019 push was in the right direction. It is all in how the nets are optimized.

Cracking long term credit assignment is all in figuring out novel training methods.

Also GANs might not be the end of the unsupervised learning story. There might be other games the network can play internally.

11am. We barely probed the surface of that. Our understanding of learning is broad, but shallow.

But now that I've said this, it allows me to make a determination.

I should not be hesitant to hack the PyTorch library to implement the training scheme that I want. If I need to hack the backward pass for matmults and biases, so be it. If I could see a ML textbook for the future with the all the intelligence secrets unearthed, it is a 100% certainty that I would need to do that anyway.

Also I do not have to worry about recurrence and weight sharing.

MLP-Mixer is proof enough that feedforward nets are all that I need for any conceivable task.

It is all in the way they are trained.

But there is a trap in this way of thinking.

11:05am. Autoencoders do not really work for unsupervised learning. They work on Mnist, but result in blurry images on larger datasets as the network is primed to learn the average pixel intensities.

If I took an autoencoder and tried to solve the problem directly I could not do it. It is not possible to go from autoencoders to GANs conceptually. Even if I hypothesized that an attention mechanism that instructs the generator to create less blurry images is needed, I'd have no idea how to do that.

It is likely that I could look at autoencoders and say - oh they need better optimizers. The images are blurry because they are not optimizing well. So I'd go down the rabbit hole of trying to force them to work using second order methods.

Even if I could think of the GAN architecture, being too pure in my principles might be a disadvantage. Once I notice the instability of it, I could conclude that I am on the wrong track and discarded it. In truth, the methods to stabilize 2p min max games exist and it is the kind of training method I cannot anticipate or reason out.

11:15am. This is how ML was from the start. It is easy to imagine, but hard to penetrate. Nobody can do it. All they can do is be patient scientists and test out their ideas rigorously.

Sigh. The original GAN is simple enough. The GMN duality gap method is simple enough as well. There were all sort of stabilization tricks and complicated cost function like the Wasserstein one, but those aren't the truth.

11:20am. The way to propagate credit in CFR sampling methods turned out to be simple as well. Regarding policy averaging, it seemed like a huge secret initially, but all I have to do is use dropout and do key sharing between policy and value nets. There seems to be multiple ways of doing policy averaging depending how current policy is weighted, but that does not matter.

Optimizing an ensemble of current policies seems to work. I feared non-stationarity, but the solution is trivial.

So in the end, the real contribution of CFR is that ommitting the self probability is good.

11:25am. Actually it is not really appreciated at this juncture, but a policy net can serve as a metal model of the self. This is not really obvious if you are just doing value function learning, but once you start dividing the self probability, the story gains a deeper profoundity.

11:30am. I should not worry too much and just go forward. We are all stuck on autoencoders and trying to get to GANs. We want to go beyond, but are glue towards thinking in terms of current methods. I should just accept this and not be overly harsh on what exists today.

I was wrong to suggest that backprop needs to be replaced outright in 2019. Its rules will have its place in future systems.

The future rules will be simple much like it. They are just hidden by a shroud of uncertainty. I cannot infer them.

I cannot wait anymore. While the rest of the field figures things out, I must start cultivating RL agents. Feeding them data and using them is something I need to become proficient in. Right now my develpment in this is behind many people whose programming skills are much less than mine.

I pushed my talent to its limit and Spiral is my contribution to the world. If anybody asks, I'll point them to it.

11:40am. Now I am finally thinking about the work that I need to do today.

It seems that the right way to load data into these nets will take some work. Feedforward nets are one thing, but seems like transformers will be hard to deal with as well. Just how would I batch their inputs?

This is really hard. Do I have no choice, but to commit fully to fixed length feedforward nets. It seems so. Unlike with supervised learning, in transformers I cannot just batch the traces so they all have equal size.

11:50am. No...feedforward nets aren't the answer. For long sequences, I do not want every element to have its own embedding. It would slow down learning too much. If feedfoward nets are the answer, then they would be dominant on NLP tasks instead of transforms.

It is transformers that I need here. Leduc is one thing, I am going to have to use transformers for full NL Holdem. Nothing else will suffice.

I complained about batching inputs, but it is not like it can't be done. I'll have to endure a slowdown, and will have to pad the shorter traces in the sequence with zeroes, but it will go through.

12:05pm. Yeah, I have to go with transformer architectures for sequence learning. Feedforward nets will not cut it except on toy tasks like Leduc. Even if I had to pad them to do proper batching, I should just do that. It is not more work that it would take to shove all the data into a feedforward net.

This is really making me wonder about GNNs in general.

You know what, let me read that Geometric DL paper. The talk by Veličković really flared up my imagination. It is 160 pages so it will take me a while to cover.

12:10pm. I really should watch some videos on Neother's Theorem.

https://www.youtube.com/watch?v=B-dtMvEauiM
Noether's Theorem Explained (Part 1/6) - Introduction

Let me read the paper and then I'll watch these videos.

12:30pm. I am reading the paper, but I am not getting much out of it. The talk was interesting, but I feel like just skimming the paper itself.

https://petar-v.com/talks/Algo-WWW.pdf
Graph Representation Learning for Algorithmic Reasoning

Maybe I can find a talk of this. Forget the GDL paper. It is category theory stuff. I am not going to be able to digest it.

https://www.youtube.com/watch?v=IPQ6CPoluok
Graph Representation Learning for Algorithmic Reasoning

Here is the talk by Veličković. Let me go get breakfast."

---
## [Gigabitten/broomba](https://github.com/Gigabitten/broomba)@[1a0554771a...](https://github.com/Gigabitten/broomba/commit/1a0554771a1b362635fd57fdc049f90c52880ea1)
#### Monday 2021-05-10 12:11:05 by Caleb Spiess

Changed a ton honestly. Fixed a problem with the model. Fixed the fact that it was taking a frame difference, which was a relic of the example this is based off of and very very stupid. Probably changed a ton of random stuff I can't remember off the top of my head.

---
## [warman69/USI_Algo_Bible](https://github.com/warman69/USI_Algo_Bible)@[8a421639fa...](https://github.com/warman69/USI_Algo_Bible/commit/8a421639fa01ef057b82d36fa474a1e36c04dbad)
#### Monday 2021-05-10 13:39:01 by Martin Donati

Delete extremely dangerous file

Dear sir, for the past couple of nights I couldn't sleep well. I had nightmares. Many nightmares. I didn't know why, but I remember the content of the night mare very well: I was being chased by a mysterious minimal aluminium blob. I am running but the blob is always reaching me. No matter how fast and smooth I run. When the blob reaches me, it jumps and crushes its whopping 10 kilobytes onto my mortal frail body. I can't take it. It crushes every single atom of me and dissolves my soul into nothing. I finally find that you sir are the culprit of my nightmares. You are letting this disgrace running wild in people's dreams because you are hosting it on the servers of GitHub, owned by Microsoft, founded by Bill Doors. The minimal aluminium blob is the root of all evil, so it shall be removed from this oasis of knowledge of yours. Please sir do the needful and accept this humble commit, so that I may finally rest at night.

---
## [Skyrat-SS13/Skyrat-tg](https://github.com/Skyrat-SS13/Skyrat-tg)@[95444ca417...](https://github.com/Skyrat-SS13/Skyrat-tg/commit/95444ca41764a17bd95880cbd1fe00f9f14b7f07)
#### Monday 2021-05-10 13:47:23 by death and coding

[modular][ready]fuck it, more plushies from a hack shitty coder (#5561)

Co-authored-by: louiseedwardstuart <bonniefluff>

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[0eea385e9c...](https://github.com/mrakgr/The-Spiral-Language/commit/0eea385e9c4d3c31d3619b07bbadf98e3536a2cc)
#### Monday 2021-05-10 16:54:45 by Marko Grdinić

"1:55pm. The talk was quite interesting. I'll check out the NEE paper.

https://arxiv.org/abs/2006.08084
Neural Execution Engines - Learning to Execute Subroutines

The general theme really gives a strong story on what the secret of the brain's generalization performance really is. Algorithms generalize, but input output mappings do not.

> Second, to generalize to unseen data, we show that encoding numbers with a binary representation leads to embeddings with rich structure once trained on downstream tasks like addition or multiplication.

This really caught my attention. I am debating whether to encode poker pot and stack sizes as one hot or as binary embeddings and am not sure.

Let me read this paper and then I'll watch the Neother's Theorem ones.

> We do not use positional encodings (which we found to hurt performance) and use single-headed attention

Hmmm...

> Prior work has found that scalar numbers have difficulty representing large ranges [25] and that
binary is a useful representation that generalizes well [12, 21].

I am going to look into this.

2:25pm. Let me do the chores here. Then the Ayakashi chapter. Then Noether's theory lectures. Then I will try to do some programming.

I should forget about the difference between regular and transformer nets. In one I just shove everything into one huge matrix while in the other, I have a separate matrix for every timestep. It might be worth optimizing their initialization, but I should not concern myself with the specifics too much at the moment. I'll write the Cuda backend and optimize the crap out of everything when the time comes.

For now I should just focus on getting things off the ground. Getting that elite player will take development over time. I can make a player in a few weeks, but getting to the top of the world will take longer.

2:40pm. What this algorithmic reasoning stuff would useful for, more than just poker, is trading. Trading is all about extrapolation, and to be good the net should be weighting an ensemble of trading algos internally. Input-output mappings will flame out here.

3:15pm. Done with chores and Ayakashi. Let me watch the videos.

https://youtu.be/B-dtMvEauiM?t=131

Lagrangian mechanics...did I study that in high school? I don't remember, I got barely passing grades for all subjects during that time and I slept through class. My high school phyisics experience was pure trash. The prof was the most boring person imaginable, and every test required us to write an essay on the subject being covered.

This is in contrast to the elementary school where the prof was great and the tests were multiple choice questions and math problems. I liked that.

I guess I'll check out the videos. I said I would study physics at some point. Now seems to be as good time as any.

https://www.youtube.com/watch?v=JdFD55SnpKo&list=PLx5rbdJH2sWaTh4qL_XUpeX46Qpdr7VUt

They are two hours. I guess I'll spend the rest of the work day on this. I do not feel like programming.

If tragedy strikes (even more than it has) I'll just get a regular job.

After the experience of applying at AI companies my current plan would be to take the first offer whatever it is and search for a higher paying job after that.

3:55pm. https://youtu.be/JdFD55SnpKo?list=PLx5rbdJH2sWaTh4qL_XUpeX46Qpdr7VUt&t=1560

This is extremely boring.

Forget this, let me do some programming.

4:15pm. Just a moment. Can transformer's attention heads look into the future or just the past? I think they can look forward too otherwise I'd have to pass state like in a recurrent net.

https://youtu.be/S27pHKBEp30?t=861

Ah, it goes both ways right. I do remember it being mentioned that is is n^2. Yes, so it is a feedforward net and I can't keep around the previous inputs.

4:25pm. https://www.youtube.com/watch?v=ygInBb1fS9Y
Prof. Geoffrey Hinton | Part-whole hierarchies in neural networks | LIVE with Univ.AI

I do not feel like programming after all. Let me watch this. Hadn't heard from Hinton for a while.

https://youtu.be/ygInBb1fS9Y?t=243

Wow, I did end up imagining 4 corners.

5pm. Had to pause as I heard buzzing and saw that wasps are flying around my room. Swatted them. Maybe they were bees, I can't tell them apart.

5:15pm. Agh, again. Let me get back to the video. I'll just close the damn door.

https://youtu.be/ygInBb1fS9Y?t=3613
Geof: (on the importance of belief) Oh, it is huge. You can't do your best work on something you don't believe in.

https://youtu.be/ygInBb1fS9Y?t=4003
Geof: ...Now we have, we are getting to the machines that can do like an exaflop. And once you can do an exaflop, you have a comparable compute power to a human brain. And there may well be much better ways of using that much computer power than what we've evolved. So it is perfectly reasonable that we may be able to make intelligent systems that work in utterly different way. But that wasn't my research agenda. Because my research agenda started in the 70s when brains were much better than computers. My brain was to understand how brains did it. I think this issue comes up with backpropagation and the issue whether brains can really do backpropagation. If you look at language models, say GPT-3, that has about a fifth of a trillion connections. And if you ask how much of a brain has a fifth of a trillion connections, and the answer is about half a cubic millimeter. So when you look at a brain scan one of the voxels in a brain scan, has more weights in it that the whole of GPT-3. So that's interesting, because it is surprising that there is enough synapses in one voxel, that is one cubic millimeter of your brain, to be able to have this incredible model of huge amounts of structure and language. And it may well be because backpropagation is better at packing information into the parameters than whatever the brain uses.

Geof: The brain has a different problem which is we don't live for very long. That is, you only have about 10 billion fixations in your lifetime. But you have a 100 trillion connections. So the problem in the brain isn't how you pack a lot of knowledge into not many connections. It is the opposite problem of how you use not much experience to learn a lot, but have lots of connections. So it is quite conceivable that the brain doesn't use backpropagation and the brain isn't good as packing information into connections as backpropagation is.

6:25pm. He said a bit that the symbolic and the neural divide does not exist which I did not feel like transcribing. It is an ancient clash from decades ago and it is not really important. The QA is quite interesting and Hinton shows his usual dry British humor.

I think I've gotten over a lot of my inertia and even felt like doing programming for a moment. Tomorrow I am going to implement the feedforward parallel training scheme and the uniform player. I've rolled it around quite a bit in my mind.

I am still trying to overcome my depression of not being able to reason it all out with my power. I want pride to work, but the situation is forcing me to be humble. Programming problems I typically overcome by being prideful, but that won't happen here.

It is frustrating because intelligence is all around us. The methods are waiting to be found out.

It is like the GAN problem. I could have gone and invented GANs myself. But I could not have done that while being fixated on autoencoders.

So I am going to have to learn to live with the pain of being able to predict, but not being able to take the crucial step that would allow me meteoric success. Being optimistic in my predictions is not something I should give up, but I also cannot mistake the limit of my grasp.

Once I entered the field, I became a bear myself when I've should have stayed a bull. I've let the circumstances affect me.

6:35pm. I have to make up my mind to accept that mastering poker via ML will be an ongoing development. It will take me more than a month. Maybe it will take me years. I should resolve myself to do this and see it through to completion. Tomorrow, I'll do the parallel training and the unform player. The day after that I'll go beyond that and so on.

I like the Bill Gates quote that people overestimate what they can do in a month, but underestimate what the can do in a year.

6:40pm. Even once I train the agent, that does not mean I'll start printing money like crazy. It will be an ongoing battle against the online dens' defenses. I'll experience setbacks and asset forfeitures. I'll have to look around and play in many different places.

Maybe I won't make anything at all.

But poker is my best first bet to make something. Trading requires too much of a starting stake for me. Mainstream games are too big for the agents to handle. And any other uses of ML would require me to get a job at some company. I'd rather work for myself instead.

6:45pm. Let me stop here for the day. I won't what I should pick as my hobby. I have a huge backlog of manga, novels and anime, Rance Quest, Pathfinder Kingmaker. Since RI recommendations were so on point, maybe Lord Of Mysteries is something I should give a try? But it has 1.6k chapters and I am wary of picking up something like that when it took me so long to get through RI. I must have been reading it for months or even longer.

Either way, there is no wrong choice when it comes to how leisure time is spent.

6:50pm. I can't achieve greatness through the sheer weight of my programming skill. Even if it is exceptional, it does not matter when I can infer AI breakthroughs directly. And if others really want to, they can pick up Spiral and benefit from the functional programming features it provides.

But even if greatness is out of reach, if I pick a game as my target and cultivate it every day, I should be able to attain something. At the very least, it will be different than the last six years. The small steps made every day, will carry me a long distance in the end.

Since it is too hard, I should not think of myself as pursuing AI. Rather mastering the game itself should be my primary goal, and AI is just a means to an end."

---
## [Fargowilta/FargowiltasSouls](https://github.com/Fargowilta/FargowiltasSouls)@[d1674cb844...](https://github.com/Fargowilta/FargowiltasSouls/commit/d1674cb844c1ce0dfda848456afaf0ca02f896ab)
#### Monday 2021-05-10 18:29:05 by terrynmuse

eridanus
 p2 vortex rotation rate nerfed again
 faster rate of punches before uppercut and after timestop
 improved visual offset of nebula blazes spawning from his fist
 p3 solar fireballs stop closer to eri as life decreases (more space for you when arena is very small)
 p3 lightning is more per ring but slower
 p3 nebula blazes changed to a V spread
updated debuffs
 ml solar attacks, cultist fireballs, eri fireballs all inflict on fire and short burning
 eri nebula blazes inflict berserked+lethargic
fixed spider ench not working during mutant
fixed lihzahrd treasure box accidentally having slimy shield slime on landing
big sparkling love visuals tweaked (boss and weapon)
mutant blazing ray gives graze consistently while its disappearing
vampire knives nerf replacement
 -25% damage
 -25% speed
 between 33-66% life, lifesteal timer takes 2x as long as base (more delay between heals)
 above 66% life, lifesteal timer takes 3x as long as base
chaos elementals have increased chances of spawning in huge hordes
diffractor blaster now has spinup, needs time to reach max spin speed/damage
gemini glaives slightly buffed

---
## [nacrt/SkyblockClient-REPO](https://github.com/nacrt/SkyblockClient-REPO)@[9cbb114601...](https://github.com/nacrt/SkyblockClient-REPO/commit/9cbb11460150648e3e00651df69de338f1995143)
#### Monday 2021-05-10 18:46:44 by Feel65

Dungeons Guide superiority removed (#113)

* Dungeons Guide devalues a ton of other mods

not ok, as it feels like dungeons guide is "the" only good dungeon mod and we don't even need other mods like dsm, skytils, cowlection as it's **CLEARLY** superior! no.

* fuck you

and fuck sbe

---
## [rlucas585/webserv](https://github.com/rlucas585/webserv)@[24d113daa8...](https://github.com/rlucas585/webserv/commit/24d113daa8f89cf06571269c6b8ab67a19396d39)
#### Monday 2021-05-10 19:17:57 by Ryan Lucas

Client request (#41)

* Removed unnecessary data variable in Client

* FAILING TEST COMMIT - Client Parsing development

The HTTP Parser is effective, now currently development is underway to
implement it in the Client class.

Currently, there's a balancing act of trying to accept input from
multiple terminals at once using netcat (an excellent example of this
working is present in src/Examples/src/ClientParsing.cpp), and to
accept input in one chunk, even when this input doesn't end in a new
line character (as in the failing test, in tst/HttpParsing.cpp).

Next steps:

- Fix failing test
- Create Response class (No Parser required, but a Builder would be
  welcome)
- Add code to check whether a Request is valid beyond the error checking
  to see if it is well formed, e.g. check to see that a requested file
  actually exists.

Optional steps:

- Consider also creating a URI class, to better represent the separate
  components of a URI, instead of just storing it as a single
  std::string.

* Fixed failing test

I think this is a significant amount of changes for a PR now.

- Bug fix: The failing tests was relatively simple to fix, I added code
  in Client during the generate_request() call to read everything left
  from the client until content-length in the parser was 0 - this should
  ensure that the entire body of a client's message is always read, even
  when the body doesn't end in a CRLF.
  Any information after a CRLF is ignored.
- Bug fix: While fixing the aforementioned bug, I happened to run the
  tests through valgrind, and was met with an insane amount of error
  messages concerning "Uninitialized values". Long story short, after a
  couple of painful hours, I learned you can't pass a char* by reference
  into a lambda in C++. Errors vanished after passing by value.
- Bug fix: I found valgrind errors in the Socket connection test.
  Discovered it was due to an uninitialized socklen_t. Fixed now by
  initializing to 0.

Next goals remain the same as before. Next PR will be to do with a
Response class, and there should be testing for this too. As there is no
parsing required for this class however, it should be simpler than the
Request class.

Co-authored-by: Ryan Lucas <rlucas@student.codam.nl>
Co-authored-by: Ryan Lucas <rlucas@f1r4s12.codam.nl>

---
## [Total-RP/Total-RP-3](https://github.com/Total-RP/Total-RP-3)@[bf634280e8...](https://github.com/Total-RP/Total-RP-3/commit/bf634280e8cdc60c2c9531aaa7ebd26a9dfbc9a0)
#### Monday 2021-05-10 21:04:04 by Daniel Yates

Implement smart nameplate queue strategy (#566)

* Implement smart nameplate queue strategy

This reworks the nameplate request system to use a much smarter queue
mechanism than we currently have.

Our existing approach only applies a cooldown on a per-character level,
such that we only throttle requests sent to nameplates of units we've
already seen at most once per 90 seconds. Unfortunately it doesn't
account for a few real-world issues.

Firstly, nameplates often quickly appear on-screen and then go off.
These can be thought of as one-hit wonders for people you're probably
not going to interact with and whose data isn't quite so important.

Secondly, you can walk into a field of a hundred players and turn your
nameplates on and suddenly request every visible nameplates' profile
immediately all at the same time. This completely throttles comms
on both sender and receiver ends.

To resolve this a smarter queue system is implemented for nameplate
requests with the following characteristics.

To solve the first issue, an enqueued request for a nameplate must first
exceed a minimum eligibility time before the request is sent. This
is kept small - at around 2.5 seconds - and should solve the issue of
nameplates briefly popping onto the screen an disappearing issuing a
full request for their data.

For the second issue, the queue system has an internal semaphore
representing request "slots". Each attempt to send a request will
first attempt to obtain a slot and decrement an internal counter; if
no slots are available the request won't be dispatched immediately and
must wait until a "recharge period" has elapsed. Each tick of the period
regains a single request slot.

The idea behind this is to significantly hamstring the ability to burst
request nameplate information; if you walk into a field of a hundred
players who all stay on screen to exceed their eligibility period, you
will only send at-most five requests to people on-screen immediately and
then one additional request each time a slot is regained every ~1.25s
thereafter.

Finally, the existing system of applying a per-character cooldown is
kept around - so if you repeatedly see a nameplate for someone you've
already requested then they will gain a significantly increased
eligibility time. The timer for this has been increased from 90 seconds
to 5 minutes; the idea is if you're interacting with someone for that
long you'll probably have seen their tooltip accidentally and requested
updated data anyway, or that they won't have made any change to their
profile worth querying.

The changes described significantly reduce the amount of requests that
can be sent in any short period of time, however for the user experience
this will come at a significantly increased latency as far as seeing an
undecorated nameplate and actually receiving their profile data.

To minimize this, there also exists a basic system of prioritized
requests. Prioritized requests will always be placed at the front of
the queue and apply to the nameplates of units that are either in your
friends list - both character and Battle.net, units that you are
presently grouped with, and members of your own guild.

* Minor changes to tweakable variable names

Primarily this just renames a few things around the cooldowns for
requests for people who haven't yet been submitted and for those
that have had requests submitted.

* Nerf the recharge rate of nameplate request slots

As this is meant to be a passive way of requesting profiles reducing
the recharge rate gives larger profile transfers a bit more breathing
room in high-population scenarios. With this change you'll now need to
wait 90 seconds for requests to be sent to 40 on-screen nameplates
instead of 60.

The wait period for on-screen nameplates before requests are sent has
also been increased to 3 seconds, in the spirit of Classic WoW mount
cast times.

* Dequeue pending requests on register data updates

If we receive updated data in our register for a unit, we'll dequeue
any pending request under the assumption that it was probably requested
by some other means.

* Add basic prioritization of nameplates by range

Keyword is "basic" - we just follow DBM's style of range check by
testing if a player is in range of a given item ID; in this case the
Mistletoe item is used because it's available on all game versions
and is relatively small at 23 yards.

---
## [Buildstarted/linksfordevs](https://github.com/Buildstarted/linksfordevs)@[6057f6ff84...](https://github.com/Buildstarted/linksfordevs/commit/6057f6ff84e07a9c7921267f72e8df2590638028)
#### Monday 2021-05-10 22:09:28 by Ben Dornis

Updating: 5/10/2021 10:00:00 PM

 1. Added: Teenage Years Frivolously Spent | vinliao
    (https://vinliao.com/teenage/)
 2. Added: Stopping Time: An Appreciation of Objective-C
    (https://kocienda.micro.blog/2021/05/09/stopping-time-an.html)
 3. Added: left alone, together
    (https://phirephoenix.com/blog/2021-05-03/privacy)
 4. Added: Here's How 'Everything Bubbles' Pop
    (https://charleshughsmith.blogspot.com/2021/05/heres-how-everything-bubbles-pop.html?m=1)
 5. Added: microsoft/ebpf-for-windows
    (https://github.com/microsoft/ebpf-for-windows)
 6. Added: Issues of .NET
    (https://issuesof.net/)
 7. Added: How to Build an Egalitarian, Decentralized Search Engine Part 1: The Principles
    (https://chapra.blog/category/technology/how-to-build-a-search-engine/)
 8. Added: Understanding iOS application entry point
    (https://olszanowski.blog/posts/understanding-ios-app-entrypoint/)
 9. Added: Making sense of Elixir (improper) lists
    (https://dorgan.netlify.app/posts/2021/03/making-sense-of-elixir-(improper)-lists/)
10. Added: Dark dimmed mode available on GitHub Docs | GitHub Changelog
    (https://github.blog/changelog/2021-05-10-dark-dimmed-mode-available-on-github-docs/)
11. Added: My thoughts about the Principal role
    (https://www.galiglobal.com/blog/2021/20210313-The-principal-role.html)
12. Added: Building a Simple Air Quality Monitor
    (https://blog.jean-francois.im/2021/05/08/building-a-simple-air-quality-monitor/)
13. Added: Why are modern 50mm lenses so damned complicated?
    (https://www.dpreview.com/opinion/9236543269/why-are-modern-50mm-lenses-so-damned-complicated)
14. Added: Make a perfect responsive table
    (https://cuthanh.com/make-a-perfect-responsive-table?guid=none)
15. Added: The Mistake of A New Laptop
    (https://atthis.link/blog/2021/reassesstech.html)
16. Added: Minecraft Modding: Laser Gun - Alan Zucconi
    (https://www.alanzucconi.com/2021/04/01/minecraft-laser-gun/)
17. Added: Boeing 787s must be turned off and on every 51 days to prevent 'misleading data' being shown to pilots
    (https://www.theregister.com/2020/04/02/boeing_787_power_cycle_51_days_stale_data)
18. Added: Hell site
    (https://ar.al/2021/05/10/hell-site/)
19. Added: Nijute: how to solve impossible problems
    (https://gojko.net/2021/05/10/nijute-solving-impossible-problems/)

Generation took: 00:09:18.9633684
 Maintenance update - cleaning up homepage and feed

---

# [<](2021-05-09.md) 2021-05-10 [>](2021-05-11.md)

