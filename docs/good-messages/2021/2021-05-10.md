# [<](2021-05-09.md) 2021-05-10 [>](2021-05-11.md)

5,830,935 events, 1,216,040 push events, 1,940,693 commit messages, 152,130,341 characters


## [alexhb1/mapUON@94ab043f5a...](https://github.com/alexhb1/mapUON/commit/94ab043f5a654d7506bafc8a3465db785590a302)
##### 2021-05-10 02:48:15 by PennSylvannia

Well shit

totally fucking forgot to do derf's shit, but eh whatever
everything else should be good
maybe. RDA, Ultima, and WS are gonna get into a skirimish in the future, should be funny

---
## [VedantPol/CODEFORCES_ROUND_SOLUTIONS@5fd4e300a3...](https://github.com/VedantPol/CODEFORCES_ROUND_SOLUTIONS/commit/5fd4e300a3b22ca0c652f449d7f9cb6574e60a0d)
##### 2021-05-10 06:18:27 by vedant pol

Hulk   Codeforces Round #366 (Div. 2)

Dr. Bruce Banner hates his enemies (like others don't). As we all know, he can barely talk when he turns into the incredible Hulk. That's why he asked you to help him to express his feelings.

Hulk likes the Inception so much, and like that his feelings are complicated. They have n layers. The first layer is hate, second one is love, third one is hate and so on...

For example if n = 1, then his feeling is "I hate it" or if n = 2 it's "I hate that I love it", and if n = 3 it's "I hate that I love that I hate it" and so on.

Please help Dr. Banner.

Input
The only line of the input contains a single integer n (1 ≤ n ≤ 100) — the number of layers of love and hate.

Output
Print Dr.Banner's feeling in one line.

Examples
inputCopy
1
outputCopy
I hate it
inputCopy
2
outputCopy
I hate that I love it
inputCopy
3
outputCopy
I hate that I love that I hate it

---
## [kaatish/cudf@8a666a04e0...](https://github.com/kaatish/cudf/commit/8a666a04e0123744eb259d88ac4c04b0b6de4303)
##### 2021-05-10 06:21:31 by Vyas Ramasubramani

Refactor Python and Cython internals for groupby aggregation (#7818)

This PR makes some improvements to the groupby/aggregation code that I identified while working on #7731. The main purpose is to make the code logic easier to follow and reduce some unnecessary complexity; I see minor but measurable performance improvements (2-5% for small datasets) as well, but those are mostly just side effects here. Specifically, it makes the following changes:

1. Inlines the logic for dropping unsupported aggregations. The old function was repetitive and necessitated looping over the aggregations twice, whereas the new approach drops unwanted aggregations on the fly so it only loops once. The new code also makes it so that you only construct a C aggregation object once.
2. Merges the logic from `_AggregationFactory` into `Aggregation`, and removes the constructor for `Aggregation`. The one downside here is that the Cython `Aggregation` object's constructor no longer places it in a valid state; however, in practice the object is always constructed via either the `make_aggregation` function or its various factories, and the object's constructor was only every used in `_drop_unsupported_aggs` anyway. The benefit is we remove the fragmentation between these two classes, making the code much more readable, and the `Aggregation` class actually serves a purpose now beyond just providing a single property `kind` that is only used once: it is now the primary way that other Cython files interact with aggregations. This also means that in most places other Cython modules don't need to work with `unique_ptr[aggregation]` as much anymore (although they do still have to move `Aggregation.c_obj` for performance reasons). `make_aggregation` now returns the Cython class instead of the underlying C++ one.
3. Modified all the "allowed aggregations" sets to use the uppercase names of the aggregations. In addition to simplifying the code a tiny bit, this helps reduce confusion between the aggregation names used in Python for pandas compatibility and the libcudf names (for instance, `idxmin` vs `argmin`, now `ARGMIN`).
4. Explicitly defines all the aggregations on a groupby. I discussed this briefly with @shwina, the change has pros and cons. The benefit is that all of these methods are properly documented now, there's less magic (the binding of methods to a class after its definition can be confusing for less experienced Python developers and has a lot of potential gotchas), and we can use the simpler string-based agg definition wherever possible. The downside is that we now have to define all of these methods. I think the change is definitely an improvement, but I'm happy to change it back if anyone can suggest a better alternative. In the long run we probably need to find a way to share both code and docstrings more effectively between all aggregations (DataFrame, Series, and GroupBy).

Authors:
  - Vyas Ramasubramani (https://github.com/vyasr)
  - GALI PREM SAGAR (https://github.com/galipremsagar)

Approvers:
  - Karthikeyan (https://github.com/karthikeyann)
  - Ashwin Srinath (https://github.com/shwina)
  - GALI PREM SAGAR (https://github.com/galipremsagar)

URL: https://github.com/rapidsai/cudf/pull/7818

---
## [Aditi-Karve/ML_Python@b82d91c022...](https://github.com/Aditi-Karve/ML_Python/commit/b82d91c022fd8785500816203333ecc0db6fa25a)
##### 2021-05-10 08:27:48 by Aditi Karve

Heart Disease Yes/No - Decision Tree

Heart Disease Prediction
Abstract: Heart disease is easier to treat when it is detected in the early stages. Machine learning techniques may aid a more efficient analysis in the prediction of the disease. Moreover, this prediction is one of the most central problems in medicine, as it is one of the leading diseases related to an unhealthy lifestyle. So, an early prediction of this disease will be useful for a cure or aversion. Problem Statement: Analyze the heart disease dataset to explore the machine learning algorithms and build decision tree model to predict the disease. Dataset Information: Each attribute in the heart disease dataset is a medical risk factor.
Variable Description:
Column
Description age Age of the patient gender Gender of the patient - (0,1) - (Male, Female) chest_pain It refers to the chest pain experienced by the patient -(0,1,2,3) rest_bps Blood pressure of the patient while resting(in mm/Hg) cholesterol Patient's cholesterol level (in mg/dl) fasting_blood_sugar The blood sugar of the patient while fasting rest_ecg Potassium level (0,1,2) thalach The patient’s maximum heart rate exer_angina It refers to exercise-induced angina - (1=Yes, 0=No)
PG Program in Analytics
Problem Statement – Decision Tree
old_peak It is the ST depression induced by exercise relative to rest(ST relates to the position on ECG plots) slope It refers to the slope of the peak of the exercise ST-Segment- (0,1,2) ca Number of major vessels - (0,1,2,3,4) thalassemia It refers to thalassemia which is a blood disorder - (0,1,2,3) target The patient has heart disease or not - (1=Yes, 0=No)
Scope: ● Understand data by performing exploratory data analysis ● Training and building classification algorithms to predict heart disease ● Understand various model performance metrics and measure the performance of each model
Learning Outcome:
The students should be able to predict heart disease from medical records with the help of classification models. They should also be able to perform EDA and re-build the model and check if there is any significant change in the predictive scores.

---
## [Aditi-Karve/ML_Python@93ceaa1522...](https://github.com/Aditi-Karve/ML_Python/commit/93ceaa152202fd2f97e08e65f377f4b93dacdba9)
##### 2021-05-10 08:44:09 by Aditi Karve

Student Grades - Linear Regression/Random forest

This data approach student achievement in secondary education of two Portuguese schools. The data attributes include student grades, demographic, social and school-related features) and it was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). In [Cortez and Silva, 2008], the two datasets were modeled under binary/five-level classification and regression tasks. Important note: the target attribute G3 has a strong correlation with attributes G2 and G1. This occurs because G3 is the final year grade (issued at the 3rd period), while G1 and G2 correspond to the 1st and 2nd period grades. It is more difficult to predict G3 without G2 and G1, but such prediction is much more useful (see paper source for more details).
Attribute Information:
1.	school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)
2.	sex - student's sex (binary: 'F' - female or 'M' - male)
3.	age - student's age (numeric: from 15 to 22)
4.	address - student's home address type (binary: 'U' - urban or 'R' - rural)
5.	famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)
6.	Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)
7.	Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)
8.	Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education)
9.	Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
10.	Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
11.	reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')
12.	guardian - student's guardian (nominal: 'mother', 'father' or 'other')
13.	traveltime - home to school travel time (numeric: 1 - 1 hour)
14.	studytime - weekly study time (numeric: 1 - 10 hours)
15.	failures - number of past class failures (numeric: n if 1<=n<3, else 4)
16.	schoolsup - extra educational support (binary: yes or no)
17.	famsup - family educational support (binary: yes or no)
18.	paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
19.	activities - extra-curricular activities (binary: yes or no)
20.	nursery - attended nursery school (binary: yes or no)
21.	higher - wants to take higher education (binary: yes or no)
22.	internet - Internet access at home (binary: yes or no)
23.	romantic - with a romantic relationship (binary: yes or no)
24.	famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
25.	freetime - free time after school (numeric: from 1 - very low to 5 - very high)
26.	goout - going out with friends (numeric: from 1 - very low to 5 - very high)
27.	Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
28.	Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
29.	health - current health status (numeric: from 1 - very bad to 5 - very good)
30.	absences - number of school absences (numeric: from 0 to 93)
these grades are related with the course subject, Math or Portuguese:
1.	G1 - first period grade (numeric: from 0 to 20)
2.	G2 - second period grade (numeric: from 0 to 20)
3.	G3 - final grade (numeric: from 0 to 20, output target)

---
## [mrakgr/The-Spiral-Language@7844a6a511...](https://github.com/mrakgr/The-Spiral-Language/commit/7844a6a5112b322a71e5243ea33854aa212c2f0c)
##### 2021-05-10 10:59:18 by Marko Grdinić

"9:35am. Yesterday I got off Fang Yuan's wild ride. Now that I've fully caught up to Reverend Insanity, I am finally free to enjoy other things.

9:45am. Right now I am chilling. I'll talk later about my ideas.

10:15am. Oh Tog's hiatus will end in a week.

10:20am. Forget that, let me focus for a bit. It is time to start ramping up the pace.

I've been continuing on the theme from yesterday and thinking about hierarchical RL and long term credit assignment.

In 2019 I tried to replace backprop, but now I am changing my mind. Even so, there is no doubt that backprop is not the right tool for very long term time horizons. The inputs and their traces simply cannot be kept in memory for too long. The answer is without a doubt to predict the gradients rather than react to them. There is nothing in the calculus derivation that says how this could be done. The right rules for doing that are a complete mystery.

In general, the issue of credit assignment over long term horizons is why hierarchical RL is necessary. You need systems that attend to different timescales. That way if you have a game that requires millions or billions of steps to get a reward, you can compress the time over multiple layers of hierarchy and push the internal simulation of the world towards the desired ends.

For the outside world which cannot be manipulated directly, the internal model must be warped so the good states become more likely.

10:40am. Look at what we have now - GANs for unsupervised learning, ??? for long term credit assignment. How could I bring that all together to make a system capable of achieving human level intelligence?

I have no idea how to turn vague notions of what is right into proper algorithms.

My programming skill might be peak human level, but it quickly becomes useless when rational quantities get involved. I cannot reason out optimization processes. I put in so much effort, but the level I want to be at is negligibly closer compared to when I started.

10:45am. Still, one thing I am going to do is stop putting RNNs on a pedestal.

RNNs have the allure of being Turing complete, but that does not matter. What I need is stability and ease of training. And fixed time horizon backprop is not going to be able to train them to the level that I need. In fact, the fixed time horizon is not the problem here, on toy problems we know that backprop cannot deal with RNNs properly over large number of timesteps. This is even with LSTMs. The only thing that works are skip connections through time.

So RNNs are probably not the reason why the brain has its powers. I should not expect that backprop can do real program search using them.

The secrets of intelligence are not really related to the particular architecture being used. Feedforward ones should do.

Rather my own inventions in the recent PL monthly review, and then the GMN paper point as to where the trend is. Ironically, my 2019 push was in the right direction. It is all in how the nets are optimized.

Cracking long term credit assignment is all in figuring out novel training methods.

Also GANs might not be the end of the unsupervised learning story. There might be other games the network can play internally.

11am. We barely probed the surface of that. Our understanding of learning is broad, but shallow.

But now that I've said this, it allows me to make a determination.

I should not be hesitant to hack the PyTorch library to implement the training scheme that I want. If I need to hack the backward pass for matmults and biases, so be it. If I could see a ML textbook for the future with the all the intelligence secrets unearthed, it is a 100% certainty that I would need to do that anyway.

Also I do not have to worry about recurrence and weight sharing.

MLP-Mixer is proof enough that feedforward nets are all that I need for any conceivable task.

It is all in the way they are trained.

But there is a trap in this way of thinking.

11:05am. Autoencoders do not really work for unsupervised learning. They work on Mnist, but result in blurry images on larger datasets as the network is primed to learn the average pixel intensities.

If I took an autoencoder and tried to solve the problem directly I could not do it. It is not possible to go from autoencoders to GANs conceptually. Even if I hypothesized that an attention mechanism that instructs the generator to create less blurry images is needed, I'd have no idea how to do that.

It is likely that I could look at autoencoders and say - oh they need better optimizers. The images are blurry because they are not optimizing well. So I'd go down the rabbit hole of trying to force them to work using second order methods.

Even if I could think of the GAN architecture, being too pure in my principles might be a disadvantage. Once I notice the instability of it, I could conclude that I am on the wrong track and discarded it. In truth, the methods to stabilize 2p min max games exist and it is the kind of training method I cannot anticipate or reason out.

11:15am. This is how ML was from the start. It is easy to imagine, but hard to penetrate. Nobody can do it. All they can do is be patient scientists and test out their ideas rigorously.

Sigh. The original GAN is simple enough. The GMN duality gap method is simple enough as well. There were all sort of stabilization tricks and complicated cost function like the Wasserstein one, but those aren't the truth.

11:20am. The way to propagate credit in CFR sampling methods turned out to be simple as well. Regarding policy averaging, it seemed like a huge secret initially, but all I have to do is use dropout and do key sharing between policy and value nets. There seems to be multiple ways of doing policy averaging depending how current policy is weighted, but that does not matter.

Optimizing an ensemble of current policies seems to work. I feared non-stationarity, but the solution is trivial.

So in the end, the real contribution of CFR is that ommitting the self probability is good.

11:25am. Actually it is not really appreciated at this juncture, but a policy net can serve as a metal model of the self. This is not really obvious if you are just doing value function learning, but once you start dividing the self probability, the story gains a deeper profoundity.

11:30am. I should not worry too much and just go forward. We are all stuck on autoencoders and trying to get to GANs. We want to go beyond, but are glue towards thinking in terms of current methods. I should just accept this and not be overly harsh on what exists today.

I was wrong to suggest that backprop needs to be replaced outright in 2019. Its rules will have its place in future systems.

The future rules will be simple much like it. They are just hidden by a shroud of uncertainty. I cannot infer them.

I cannot wait anymore. While the rest of the field figures things out, I must start cultivating RL agents. Feeding them data and using them is something I need to become proficient in. Right now my develpment in this is behind many people whose programming skills are much less than mine.

I pushed my talent to its limit and Spiral is my contribution to the world. If anybody asks, I'll point them to it.

11:40am. Now I am finally thinking about the work that I need to do today.

It seems that the right way to load data into these nets will take some work. Feedforward nets are one thing, but seems like transformers will be hard to deal with as well. Just how would I batch their inputs?

This is really hard. Do I have no choice, but to commit fully to fixed length feedforward nets. It seems so. Unlike with supervised learning, in transformers I cannot just batch the traces so they all have equal size.

11:50am. No...feedforward nets aren't the answer. For long sequences, I do not want every element to have its own embedding. It would slow down learning too much. If feedfoward nets are the answer, then they would be dominant on NLP tasks instead of transforms.

It is transformers that I need here. Leduc is one thing, I am going to have to use transformers for full NL Holdem. Nothing else will suffice.

I complained about batching inputs, but it is not like it can't be done. I'll have to endure a slowdown, and will have to pad the shorter traces in the sequence with zeroes, but it will go through.

12:05pm. Yeah, I have to go with transformer architectures for sequence learning. Feedforward nets will not cut it except on toy tasks like Leduc. Even if I had to pad them to do proper batching, I should just do that. It is not more work that it would take to shove all the data into a feedforward net.

This is really making me wonder about GNNs in general.

You know what, let me read that Geometric DL paper. The talk by Veličković really flared up my imagination. It is 160 pages so it will take me a while to cover.

12:10pm. I really should watch some videos on Neother's Theorem.

https://www.youtube.com/watch?v=B-dtMvEauiM
Noether's Theorem Explained (Part 1/6) - Introduction

Let me read the paper and then I'll watch these videos.

12:30pm. I am reading the paper, but I am not getting much out of it. The talk was interesting, but I feel like just skimming the paper itself.

https://petar-v.com/talks/Algo-WWW.pdf
Graph Representation Learning for Algorithmic Reasoning

Maybe I can find a talk of this. Forget the GDL paper. It is category theory stuff. I am not going to be able to digest it.

https://www.youtube.com/watch?v=IPQ6CPoluok
Graph Representation Learning for Algorithmic Reasoning

Here is the talk by Veličković. Let me go get breakfast."

---
## [Skyrat-SS13/Skyrat-tg@42bd885efd...](https://github.com/Skyrat-SS13/Skyrat-tg/commit/42bd885efd93231c9876ec6b0fee0beb8974124b)
##### 2021-05-10 12:25:47 by SkyratBot

[MIRROR] Fixes a fuck ton more harddels (#5476)

* Fixes a fuck ton more harddels (#58779)

Redoes how geese handle eating shit, it was fucking stupid and caused harddels, and while this method is technically slower in the best case, it's a fucking goose
Fixes action related harddels, I hate how they work but at least this way they won't hold refs.
Fixes the hierophont causing its beacon to harddel
Removes the M variable from megafauna actions, it was used like a typed owner and caused harddels, so I burned it
Fixes target and targets_from harddels, replaces all setters of target with LoseTarget and GiveTarget, which should help maintain behavior. I'm not sure if this breaks anything, but if it does we should fix the assumptions that code makes instead of reverting this change
Fixes more area_senstive_contents related harddels, we need to allow the mob to move before clearing out its list.
Fixes marked object harddels (I'm coming for you admin team)
Fixes a language based human harddel
Fixes managed overlay related harddels (This was just emissive blockers, but I think this is a good safety net to have. If we clear the overlay list we should clear this one as well)
Fixes bot core harddels, I hate the fact that this exists but it has no reason to know who its owner is
Adds a walk(src, 0) to simple_animal destroy, it's the best bang for the buck in terms of stopping spurious harddels. Walk related harddels aren't that expensive in the first place, since byond does the same thing I'm doing here, but this makes finding mob harddels easier, so let's go with it
I fixed another source of part harddels, I hate fullupgrade so much
Fixes all the sound loop harddels

* Fixes a fuck ton more harddels

Co-authored-by: LemonInTheDark <58055496+LemonInTheDark@users.noreply.github.com>

---
## [mrakgr/The-Spiral-Language@0eea385e9c...](https://github.com/mrakgr/The-Spiral-Language/commit/0eea385e9c4d3c31d3619b07bbadf98e3536a2cc)
##### 2021-05-10 16:54:45 by Marko Grdinić

"1:55pm. The talk was quite interesting. I'll check out the NEE paper.

https://arxiv.org/abs/2006.08084
Neural Execution Engines - Learning to Execute Subroutines

The general theme really gives a strong story on what the secret of the brain's generalization performance really is. Algorithms generalize, but input output mappings do not.

> Second, to generalize to unseen data, we show that encoding numbers with a binary representation leads to embeddings with rich structure once trained on downstream tasks like addition or multiplication.

This really caught my attention. I am debating whether to encode poker pot and stack sizes as one hot or as binary embeddings and am not sure.

Let me read this paper and then I'll watch the Neother's Theorem ones.

> We do not use positional encodings (which we found to hurt performance) and use single-headed attention

Hmmm...

> Prior work has found that scalar numbers have difficulty representing large ranges [25] and that
binary is a useful representation that generalizes well [12, 21].

I am going to look into this.

2:25pm. Let me do the chores here. Then the Ayakashi chapter. Then Noether's theory lectures. Then I will try to do some programming.

I should forget about the difference between regular and transformer nets. In one I just shove everything into one huge matrix while in the other, I have a separate matrix for every timestep. It might be worth optimizing their initialization, but I should not concern myself with the specifics too much at the moment. I'll write the Cuda backend and optimize the crap out of everything when the time comes.

For now I should just focus on getting things off the ground. Getting that elite player will take development over time. I can make a player in a few weeks, but getting to the top of the world will take longer.

2:40pm. What this algorithmic reasoning stuff would useful for, more than just poker, is trading. Trading is all about extrapolation, and to be good the net should be weighting an ensemble of trading algos internally. Input-output mappings will flame out here.

3:15pm. Done with chores and Ayakashi. Let me watch the videos.

https://youtu.be/B-dtMvEauiM?t=131

Lagrangian mechanics...did I study that in high school? I don't remember, I got barely passing grades for all subjects during that time and I slept through class. My high school phyisics experience was pure trash. The prof was the most boring person imaginable, and every test required us to write an essay on the subject being covered.

This is in contrast to the elementary school where the prof was great and the tests were multiple choice questions and math problems. I liked that.

I guess I'll check out the videos. I said I would study physics at some point. Now seems to be as good time as any.

https://www.youtube.com/watch?v=JdFD55SnpKo&list=PLx5rbdJH2sWaTh4qL_XUpeX46Qpdr7VUt

They are two hours. I guess I'll spend the rest of the work day on this. I do not feel like programming.

If tragedy strikes (even more than it has) I'll just get a regular job.

After the experience of applying at AI companies my current plan would be to take the first offer whatever it is and search for a higher paying job after that.

3:55pm. https://youtu.be/JdFD55SnpKo?list=PLx5rbdJH2sWaTh4qL_XUpeX46Qpdr7VUt&t=1560

This is extremely boring.

Forget this, let me do some programming.

4:15pm. Just a moment. Can transformer's attention heads look into the future or just the past? I think they can look forward too otherwise I'd have to pass state like in a recurrent net.

https://youtu.be/S27pHKBEp30?t=861

Ah, it goes both ways right. I do remember it being mentioned that is is n^2. Yes, so it is a feedforward net and I can't keep around the previous inputs.

4:25pm. https://www.youtube.com/watch?v=ygInBb1fS9Y
Prof. Geoffrey Hinton | Part-whole hierarchies in neural networks | LIVE with Univ.AI

I do not feel like programming after all. Let me watch this. Hadn't heard from Hinton for a while.

https://youtu.be/ygInBb1fS9Y?t=243

Wow, I did end up imagining 4 corners.

5pm. Had to pause as I heard buzzing and saw that wasps are flying around my room. Swatted them. Maybe they were bees, I can't tell them apart.

5:15pm. Agh, again. Let me get back to the video. I'll just close the damn door.

https://youtu.be/ygInBb1fS9Y?t=3613
Geof: (on the importance of belief) Oh, it is huge. You can't do your best work on something you don't believe in.

https://youtu.be/ygInBb1fS9Y?t=4003
Geof: ...Now we have, we are getting to the machines that can do like an exaflop. And once you can do an exaflop, you have a comparable compute power to a human brain. And there may well be much better ways of using that much computer power than what we've evolved. So it is perfectly reasonable that we may be able to make intelligent systems that work in utterly different way. But that wasn't my research agenda. Because my research agenda started in the 70s when brains were much better than computers. My brain was to understand how brains did it. I think this issue comes up with backpropagation and the issue whether brains can really do backpropagation. If you look at language models, say GPT-3, that has about a fifth of a trillion connections. And if you ask how much of a brain has a fifth of a trillion connections, and the answer is about half a cubic millimeter. So when you look at a brain scan one of the voxels in a brain scan, has more weights in it that the whole of GPT-3. So that's interesting, because it is surprising that there is enough synapses in one voxel, that is one cubic millimeter of your brain, to be able to have this incredible model of huge amounts of structure and language. And it may well be because backpropagation is better at packing information into the parameters than whatever the brain uses.

Geof: The brain has a different problem which is we don't live for very long. That is, you only have about 10 billion fixations in your lifetime. But you have a 100 trillion connections. So the problem in the brain isn't how you pack a lot of knowledge into not many connections. It is the opposite problem of how you use not much experience to learn a lot, but have lots of connections. So it is quite conceivable that the brain doesn't use backpropagation and the brain isn't good as packing information into connections as backpropagation is.

6:25pm. He said a bit that the symbolic and the neural divide does not exist which I did not feel like transcribing. It is an ancient clash from decades ago and it is not really important. The QA is quite interesting and Hinton shows his usual dry British humor.

I think I've gotten over a lot of my inertia and even felt like doing programming for a moment. Tomorrow I am going to implement the feedforward parallel training scheme and the uniform player. I've rolled it around quite a bit in my mind.

I am still trying to overcome my depression of not being able to reason it all out with my power. I want pride to work, but the situation is forcing me to be humble. Programming problems I typically overcome by being prideful, but that won't happen here.

It is frustrating because intelligence is all around us. The methods are waiting to be found out.

It is like the GAN problem. I could have gone and invented GANs myself. But I could not have done that while being fixated on autoencoders.

So I am going to have to learn to live with the pain of being able to predict, but not being able to take the crucial step that would allow me meteoric success. Being optimistic in my predictions is not something I should give up, but I also cannot mistake the limit of my grasp.

Once I entered the field, I became a bear myself when I've should have stayed a bull. I've let the circumstances affect me.

6:35pm. I have to make up my mind to accept that mastering poker via ML will be an ongoing development. It will take me more than a month. Maybe it will take me years. I should resolve myself to do this and see it through to completion. Tomorrow, I'll do the parallel training and the unform player. The day after that I'll go beyond that and so on.

I like the Bill Gates quote that people overestimate what they can do in a month, but underestimate what the can do in a year.

6:40pm. Even once I train the agent, that does not mean I'll start printing money like crazy. It will be an ongoing battle against the online dens' defenses. I'll experience setbacks and asset forfeitures. I'll have to look around and play in many different places.

Maybe I won't make anything at all.

But poker is my best first bet to make something. Trading requires too much of a starting stake for me. Mainstream games are too big for the agents to handle. And any other uses of ML would require me to get a job at some company. I'd rather work for myself instead.

6:45pm. Let me stop here for the day. I won't what I should pick as my hobby. I have a huge backlog of manga, novels and anime, Rance Quest, Pathfinder Kingmaker. Since RI recommendations were so on point, maybe Lord Of Mysteries is something I should give a try? But it has 1.6k chapters and I am wary of picking up something like that when it took me so long to get through RI. I must have been reading it for months or even longer.

Either way, there is no wrong choice when it comes to how leisure time is spent.

6:50pm. I can't achieve greatness through the sheer weight of my programming skill. Even if it is exceptional, it does not matter when I can infer AI breakthroughs directly. And if others really want to, they can pick up Spiral and benefit from the functional programming features it provides.

But even if greatness is out of reach, if I pick a game as my target and cultivate it every day, I should be able to attain something. At the very least, it will be different than the last six years. The small steps made every day, will carry me a long distance in the end.

Since it is too hard, I should not think of myself as pursuing AI. Rather mastering the game itself should be my primary goal, and AI is just a means to an end."

---
## [mathemage/CompetitiveProgramming@6f95096808...](https://github.com/mathemage/CompetitiveProgramming/commit/6f95096808951298d1841eb37b7af053789599dd)
##### 2021-05-10 19:20:02 by Karel Ha

Upsolve AtCoder ABC 200E - Patisserie ABC 2

AC !!!!!!!!!!!!!!!!!! YEAH !

Analysis:
- very neat/smart trick with telescopic sum of TP()s
  - window of last TP() values -> no need to re-sum ~ DP

- took hell lot of thinking
  <- switch from combinatorics counting approach
  - to DP (can't simply multiply counts independently <- counts
    dependent on specific B and S-B values)
  - seen this once in a TopCoder problem (k-th lexicographic, counting
    combinations)
    -> reused that approach
    - but wrong!
=> BEWARE OF PATTERN MATCHING
=> ALWAYS VERIFY PROPERLY CORRECTNESS!
   - either a full proof
   - or at least 1-3 custom testcases (+ include those)

- pre-submit bugs:
  - forgotten +1 in TP return value
  - swapped min max in TP
    - max()-min() instead of min()-max()
  - wrong value set for T:
    - T=K;
      <- left over from previous implementation (didn't think about it too much)
    - T=max(1LL,remaining-N) + K - 1LL;
      <- i.e. correct lower bound from TP()

Signed-off-by: Karel Ha <mathemage@gmail.com>

---
## [Total-RP/Total-RP-3@bf634280e8...](https://github.com/Total-RP/Total-RP-3/commit/bf634280e8cdc60c2c9531aaa7ebd26a9dfbc9a0)
##### 2021-05-10 21:04:04 by Daniel Yates

Implement smart nameplate queue strategy (#566)

* Implement smart nameplate queue strategy

This reworks the nameplate request system to use a much smarter queue
mechanism than we currently have.

Our existing approach only applies a cooldown on a per-character level,
such that we only throttle requests sent to nameplates of units we've
already seen at most once per 90 seconds. Unfortunately it doesn't
account for a few real-world issues.

Firstly, nameplates often quickly appear on-screen and then go off.
These can be thought of as one-hit wonders for people you're probably
not going to interact with and whose data isn't quite so important.

Secondly, you can walk into a field of a hundred players and turn your
nameplates on and suddenly request every visible nameplates' profile
immediately all at the same time. This completely throttles comms
on both sender and receiver ends.

To resolve this a smarter queue system is implemented for nameplate
requests with the following characteristics.

To solve the first issue, an enqueued request for a nameplate must first
exceed a minimum eligibility time before the request is sent. This
is kept small - at around 2.5 seconds - and should solve the issue of
nameplates briefly popping onto the screen an disappearing issuing a
full request for their data.

For the second issue, the queue system has an internal semaphore
representing request "slots". Each attempt to send a request will
first attempt to obtain a slot and decrement an internal counter; if
no slots are available the request won't be dispatched immediately and
must wait until a "recharge period" has elapsed. Each tick of the period
regains a single request slot.

The idea behind this is to significantly hamstring the ability to burst
request nameplate information; if you walk into a field of a hundred
players who all stay on screen to exceed their eligibility period, you
will only send at-most five requests to people on-screen immediately and
then one additional request each time a slot is regained every ~1.25s
thereafter.

Finally, the existing system of applying a per-character cooldown is
kept around - so if you repeatedly see a nameplate for someone you've
already requested then they will gain a significantly increased
eligibility time. The timer for this has been increased from 90 seconds
to 5 minutes; the idea is if you're interacting with someone for that
long you'll probably have seen their tooltip accidentally and requested
updated data anyway, or that they won't have made any change to their
profile worth querying.

The changes described significantly reduce the amount of requests that
can be sent in any short period of time, however for the user experience
this will come at a significantly increased latency as far as seeing an
undecorated nameplate and actually receiving their profile data.

To minimize this, there also exists a basic system of prioritized
requests. Prioritized requests will always be placed at the front of
the queue and apply to the nameplates of units that are either in your
friends list - both character and Battle.net, units that you are
presently grouped with, and members of your own guild.

* Minor changes to tweakable variable names

Primarily this just renames a few things around the cooldowns for
requests for people who haven't yet been submitted and for those
that have had requests submitted.

* Nerf the recharge rate of nameplate request slots

As this is meant to be a passive way of requesting profiles reducing
the recharge rate gives larger profile transfers a bit more breathing
room in high-population scenarios. With this change you'll now need to
wait 90 seconds for requests to be sent to 40 on-screen nameplates
instead of 60.

The wait period for on-screen nameplates before requests are sent has
also been increased to 3 seconds, in the spirit of Classic WoW mount
cast times.

* Dequeue pending requests on register data updates

If we receive updated data in our register for a unit, we'll dequeue
any pending request under the assumption that it was probably requested
by some other means.

* Add basic prioritization of nameplates by range

Keyword is "basic" - we just follow DBM's style of range check by
testing if a player is in range of a given item ID; in this case the
Mistletoe item is used because it's available on all game versions
and is relatively small at 23 yards.

---
## [Buildstarted/linksfordevs@6057f6ff84...](https://github.com/Buildstarted/linksfordevs/commit/6057f6ff84e07a9c7921267f72e8df2590638028)
##### 2021-05-10 22:09:28 by Ben Dornis

Updating: 5/10/2021 10:00:00 PM

 1. Added: Teenage Years Frivolously Spent | vinliao
    (https://vinliao.com/teenage/)
 2. Added: Stopping Time: An Appreciation of Objective-C
    (https://kocienda.micro.blog/2021/05/09/stopping-time-an.html)
 3. Added: left alone, together
    (https://phirephoenix.com/blog/2021-05-03/privacy)
 4. Added: Here's How 'Everything Bubbles' Pop
    (https://charleshughsmith.blogspot.com/2021/05/heres-how-everything-bubbles-pop.html?m=1)
 5. Added: microsoft/ebpf-for-windows
    (https://github.com/microsoft/ebpf-for-windows)
 6. Added: Issues of .NET
    (https://issuesof.net/)
 7. Added: How to Build an Egalitarian, Decentralized Search Engine Part 1: The Principles
    (https://chapra.blog/category/technology/how-to-build-a-search-engine/)
 8. Added: Understanding iOS application entry point
    (https://olszanowski.blog/posts/understanding-ios-app-entrypoint/)
 9. Added: Making sense of Elixir (improper) lists
    (https://dorgan.netlify.app/posts/2021/03/making-sense-of-elixir-(improper)-lists/)
10. Added: Dark dimmed mode available on GitHub Docs | GitHub Changelog
    (https://github.blog/changelog/2021-05-10-dark-dimmed-mode-available-on-github-docs/)
11. Added: My thoughts about the Principal role
    (https://www.galiglobal.com/blog/2021/20210313-The-principal-role.html)
12. Added: Building a Simple Air Quality Monitor
    (https://blog.jean-francois.im/2021/05/08/building-a-simple-air-quality-monitor/)
13. Added: Why are modern 50mm lenses so damned complicated?
    (https://www.dpreview.com/opinion/9236543269/why-are-modern-50mm-lenses-so-damned-complicated)
14. Added: Make a perfect responsive table
    (https://cuthanh.com/make-a-perfect-responsive-table?guid=none)
15. Added: The Mistake of A New Laptop
    (https://atthis.link/blog/2021/reassesstech.html)
16. Added: Minecraft Modding: Laser Gun - Alan Zucconi
    (https://www.alanzucconi.com/2021/04/01/minecraft-laser-gun/)
17. Added: Boeing 787s must be turned off and on every 51 days to prevent 'misleading data' being shown to pilots
    (https://www.theregister.com/2020/04/02/boeing_787_power_cycle_51_days_stale_data)
18. Added: Hell site
    (https://ar.al/2021/05/10/hell-site/)
19. Added: Nijute: how to solve impossible problems
    (https://gojko.net/2021/05/10/nijute-solving-impossible-problems/)

Generation took: 00:09:18.9633684
 Maintenance update - cleaning up homepage and feed

---

# [<](2021-05-09.md) 2021-05-10 [>](2021-05-11.md)

