# [<](2021-05-16.md) 2021-05-17 [>](2021-05-18.md)

3,183,368 events, 1,523,810 push events, 2,425,197 commit messages, 187,593,401 characters


## [freebsd/freebsd-src@9a2fac6ba6...](https://github.com/freebsd/freebsd-src/commit/9a2fac6ba65fbd14d37ccedbc2aec27a190128ea)
##### 2021-05-17 00:05:18 by Kirk McKusick

Fix handling of embedded symbolic links (and history lesson).

The original filesystem release (4.2BSD) had no embedded sysmlinks.
Historically symbolic links were just a different type of file, so
the content of the symbolic link was contained in a single disk block
fragment. We observed that most symbolic links were short enough that
they could fit in the area of the inode that normally holds the block
pointers. So we created embedded symlinks where the content of the
link was held in the inode's pointer area thus avoiding the need to
seek and read a data fragment and reducing the pressure on the block
cache. At the time we had only UFS1 with 32-bit block pointers,
so the test for a fastlink was:

	di_size < (NDADDR + NIADDR) * sizeof(daddr_t)

(where daddr_t would be ufs1_daddr_t today).

When embedded symlinks were added, a spare field in the superblock
with a known zero value became fs_maxsymlinklen. New filesystems
set this field to (NDADDR + NIADDR) * sizeof(daddr_t). Embedded
symlinks were assumed when di_size < fs->fs_maxsymlinklen. Thus
filesystems that preceeded this change always read from blocks
(since fs->fs_maxsymlinklen == 0) and newer ones used embedded
symlinks if they fit. Similarly symlinks created on pre-embedded
symlink filesystems always spill into blocks while newer ones will
embed if they fit.

At the same time that the embedded symbolic links were added, the
on-disk directory structure was changed splitting the former
u_int16_t d_namlen into u_int8_t d_type and u_int8_t d_namlen.
Thus fs_maxsymlinklen <= 0 (as used by the OFSFMT() macro) can
be used to distinguish old directory formats. In retrospect that
should have just been an added flag, but we did not realize we
needed to know about that change until it was already in production.

Code was split into ufs/ffs so that the log structured filesystem could
use ufs functionality while doing its own disk layout. This meant
that no ffs superblock fields could be used in the ufs code. Thus
ffs superblock fields that were needed in ufs code had to be copied
to fields in the mount structure. Since ufs_readlink needed to know
if a link was embedded, fs_maxlinklen gets copied to mnt_maxsymlinklen.

The kernel panic that arose to making this fix was triggered when a
disk error created an inode of type symlink with no allocated data
blocks but a large size. When readlink was called the uiomove was
attempted which segment faulted.

static int
ufs_readlink(ap)
	struct vop_readlink_args /* {
		struct vnode *a_vp;
		struct uio *a_uio;
		struct ucred *a_cred;
	} */ *ap;
{
	struct vnode *vp = ap->a_vp;
	struct inode *ip = VTOI(vp);
	doff_t isize;

	isize = ip->i_size;
	if ((isize < vp->v_mount->mnt_maxsymlinklen) ||
	    DIP(ip, i_blocks) == 0) { /* XXX - for old fastlink support */
		return (uiomove(SHORTLINK(ip), isize, ap->a_uio));
	}
	return (VOP_READ(vp, ap->a_uio, 0, ap->a_cred));
}

The second part of the "if" statement that adds

	DIP(ip, i_blocks) == 0) { /* XXX - for old fastlink support */

is problematic. It never appeared in BSD released by Berkeley because
as noted above mnt_maxsymlinklen is 0 for old format filesystems, so
will always fall through to the VOP_READ as it should. I had to dig
back through `git blame' to find that Rodney Grimes added it as
part of ``The big 4.4BSD Lite to FreeBSD 2.0.0 (Development) patch.''
He must have brought it across from an earlier FreeBSD. Unfortunately
the source-control logs for FreeBSD up to the merger with the
AT&T-blessed 4.4BSD-Lite conversion were destroyed as part of the
agreement to let FreeBSD remain unencumbered, so I cannot pin-point
where that line got added on the FreeBSD side.

The one change needed here is that mnt_maxsymlinklen is declared as
an `int' and should be changed to be `u_int64_t'.

This discovery led us to check out the code that deletes symbolic
links. Specifically

	if (vp->v_type == VLNK &&
	    (ip->i_size < vp->v_mount->mnt_maxsymlinklen ||
	     datablocks == 0)) {
		if (length != 0)
			panic("ffs_truncate: partial truncate of symlink");
		bzero(SHORTLINK(ip), (u_int)ip->i_size);
		ip->i_size = 0;
		DIP_SET(ip, i_size, 0);
		UFS_INODE_SET_FLAG(ip, IN_SIZEMOD | IN_CHANGE | IN_UPDATE);
		if (needextclean)
			goto extclean;
		return (ffs_update(vp, waitforupdate));
	}

Here too our broken symlink inode with no data blocks allocated
and a large size will segment fault as we are incorrectly using the
test that we have no data blocks to decide that it is an embdedded
symbolic link and attempting to bzero past the end of the inode.
The test for datablocks == 0 is unnecessary as the test for
ip->i_size < vp->v_mount->mnt_maxsymlinklen will do the right
thing in all cases.

The test for datablocks == 0 was added by David Greenman in this commit:

Author: David Greenman <dg@FreeBSD.org>
Date:   Tue Aug 2 13:51:05 1994 +0000

    Completed (hopefully) the kernel support for old style "fastlinks".

    Notes:
	svn path=/head/; revision=1821

I am guessing that he likely earlier added the incorrect test in the
ufs_readlink code.

I asked David if he had any recollection of why he made this change.
Amazingly, he still had a recollection of why he had made a one-line
change more than twenty years ago. And unsurpisingly it was because
he had been stuck between a rock and a hard place.

FreeBSD was up to 1.1.5 before the switch to the 4.4BSD-Lite code
base. Prior to that, there were three years of development in all
areas of the kernel, including the filesystem code, from the combined
set of people including Bill Jolitz, Patchkit contributors, and
FreeBSD Project members. The compatibility issue at hand was caused
by the FASTLINKS patches from Curt Mayer. In merging in the 4.4BSD-Lite
changes David had to find a way to provide compatibility with both
the changes that had been made in FreeBSD 1.1.5 and with 4.4BSD-Lite.
He felt that these changes would provide compatibility with both systems.

In his words:
``My recollection is that the 'FASTLINKS' symlinks support in
FreeBSD-1.x, as implemented by Curt Mayer, worked differently than
4.4BSD. He used a spare field in the inode to duplicately store the
length. When the 4.4BSD-Lite merge was done, the optimized symlinks
support for existing filesystems (those that were initialized in
FreeBSD-1.x) were broken due to the FFS on-disk structure of
4.4BSD-Lite differing from FreeBSD-1.x. My commit was needed to
restore the backward compatibility with FreeBSD-1.x filesystems.
I think it was the best that could be done in the somewhat urgent
circumstances of the post Berkeley-USL settlement. Also, regarding
Rod's massive commit with little explanation, some context: John
Dyson and I did the initial re-port of the 4.4BSD-Lite kernel to
the 386 platform in just 10 days. It was by far the most intense
hacking effort of my life. In addition to the porting of tons of
FreeBSD-1 code, I think we wrote more than 30,000 lines of new code
in that time to deal with the missing pieces and architectural
changes of 4.4BSD-Lite. We didn't make many notes along the way.
There was a lot of pressure to get something out to the rest of the
developer community as fast as possible, so detailed discrete commits
didn't happen - it all came as a giant wad, which is why Rod's
commit message was worded the way it was.''

Reported by:  Chuck Silvers
Tested by:    Chuck Silvers
History by:   David Greenman Lawrence
MFC after:    1 week
Sponsored by: Netflix

---
## [jdbener/Project-Delta-Playtesting-Files@9523c41523...](https://github.com/jdbener/Project-Delta-Playtesting-Files/commit/9523c41523f506daacabc7f245333a1e9d3a17bf)
##### 2021-05-17 00:30:24 by jdbener

Balance Tweaks 5/16/21

‚Ä¢ Added Bedroom Star, Unwarranted Citizen's Arrest, Weaponized Disease, and Imparter of Wisdom.
‚Ä¢ Most Vitality cards have had their attack power reduced by 1.
‚Ä¢ Most Hacker cards have had their attack power increased by 1.
‚Ä¢ Take Fate in Your Own Hands' cost has been reduced from 3 Emotion Emotion to 2 Emotion Emotion.
‚Ä¢ Sadness's cost has been reduced from 4 emotion emotion emotion to 1 emotion emotion emotion.
‚Ä¢ Samuel's Banked Favors has been replaced with Samuel, Falsify their Orders.
‚Ä¢ Daring Recruit now has Sluggish.
‚Ä¢ Hack's cost has been reduced by 1 generic.
‚Ä¢ Rampager's Fury has had its cost increased by 2 generic.
‚Ä¢ Standard Issue Big Gun once again has tradeable, but now doesn't automatically attach itself to a combatant when it enters the battlefield (so its tradeable ability must be used to attach it.)
‚Ä¢ Lust has been renamed to Infatuation (continuing the de-maturifying process), it also now forces you to reveal the chosen combatants.
‚Ä¢ The Musician cards which only bring combatant related keywords/effects now ignore non-combatant cards in your deck.
‚Ä¢ Many of the Multi-energy-type cards have been buffed.
 1) Reaction Augmenter now gives +2/+2 instead of +1/+1.
 2) Torment the Opponent now gives +2/+3 and -3/-2 instead of +1/+1 and -1/-1.
 3) High Kick now gives +4/-2 instead of +2/+0.
 4) Angel's Katana gives +3/+0 instead of +1/+0. (Not multi-typed but related)
 5) "Realizing I'm an Imitation of Humans"  has had +0/+1 and +0/+2 added to its lest of choices.
 6) Vocal Training now has Append to Card instead of Append to Combatant.
 7) "Gonna Jump Now and be Free" has response.
 8) Musician's Laptop has tradeable.
 9) Cocaine Induced Inspiration has been changed to work like other drug cards, it also allows you to pay more energy to apply its effect to more cards in your deck.
 10) Street Performer's effect is now a may instead of a must.
 11) Amp up the Volume now has Advantageous.

---
## [Koi-3088/ForkBot.NET@7fed6c95dc...](https://github.com/Koi-3088/ForkBot.NET/commit/7fed6c95dc68f9220c80e551aff8669d69dc2468)
##### 2021-05-17 05:31:34 by Koi-3088

Minor clean.
Revise TradeCord "traded" check, remove potential user path straggler entries because paranoia, some minor fixes.
TradeCord fixes (shocker, I know).
Extract Json serializer.
Minor clean and fixes.
Minor fixes.
Fix Milcery when an Alcremie variant is a parent.
Update to latest Core and ALM dependencies.
Handle non-shiny events in a better way.
Work around a race condition?
Simplify and de-bugify trade completion check.
Fix indexing, improve chance for Melmetal-Gmax because it's nigh impossible to get.
Rework TradeCord internals, add new functionality:
-Migrate user data from ".txt" files to a serialized Json (migration for a large amount of users will take a few minutes, be patient).
-Make TradeCord configurable, add its own settings category.
-Add some template events with an optional end timer (YYYY/MM/DD 8PM as an example, though any local time format should work).
-Add barebones Pokedex (counter, flavor text).
-Can check dex completion by typing `$dex`, check missing entries by typing `$dex missing`.
-Completing the Pokedex will slightly improve shiny rate.
-Can now mass release cherish event Pokemon and shinies ($massrelease shiny/cherish).
-Various tweaks, improvements, and bugfixes.

Slightly change FixOT's behavior:
-If a shown Pokemon is illegal and an event, attempt to find a match within the MGDB first.
-Try to force users to trade away the shown Pokemon, log attempt to change shown Pokemon.
Add consideration for easter eggs being enabled in settings, fix Suicune
Change species rng for TradeCord, some bugfixes (I really need to rewrite this mess)
Add check if we're using ListUtil for Giveaway instead of TradeCord.
Amend commit since I'm squashing and force-pushing while bringing the fork in line with the main branch
Add Giveaway module to Discord bot (#22)

Thanks, rigrassm.
Co-authored-by: Koi-3088 <61223145+Koi-3088@users.noreply.github.com>
Specify USB port instead of adding the first result (can be found via Device Manager).
Re-add boolean check because we don't want to fix everything
FixOT will attempt to regenerate illegal Pok√©mon.
Apply trash bytes for reasons.
Minor TradeCord fixes and adjustments.
Minor clean for C#9
Use "GetValidPreEvolutions()" instead of "GetPreEvolutions()".
Index forms correctly.
Fix the fixed and re-introduced empty daycare index error.
*an* Ultra Ball.
Add EvoTree breeding for TradeCord.
Remove unnecessary value declarations for pinging on encounter match.
Mildly beautify EncounterBot mark output.
Integrate Anubis' system update prevention into Soft Reset and Regigigas Encounter Modes.
Rename "Regi" Encounter Mode to "Soft Reset".
Speed up "A" clicks for Regigigas and Soft Reset modes.
Add Mark logging output for EncounterBot.
Fix oops (re-order logic, remove unnecessary lines).
Add optional species and form specification for $massrelease
Use an obscure string splitter because people like symbols in their names.
Fix things that broke after rebasing to the latest main repo commit.
Use a less unfortunate field name and value splitter...again.
Fix Marowak-Alola always generating as an NPC trade.
Add filters for "$list <species>" to narrow down results.
Fix Cherish Pichu and Octillery
Stop making dumb mistakes, me (implying the rest of it isn't a dumb mistake).
Can't breed antiques.
Use a less unfortunate embed name and value splitter
Add Melmetal-Gmax to TradeCord.
Add ability to search by caught ball.
Have MassRelease ignore events.
Add specific regional form breeding.
Revise egg rate and egg shiny chance.
Have trade evolutions hold an Everstone.
Add an extra right click when navigating to settings for AutoRoll.
Add reworked encounter/egg/fossil logs.
Minor clean.
Minor clean.
Get rid of EncounterBot, FossilBot, EggFetch text logs until I properly rework them.
Break on an empty page due to aggressive rounding
Add multi-page lists for Tradecord.
More random bugfixes.
Fix some bugs before major clean
Add Language parameter for TradeCord.
Change trainer info input format for TradeCord.
Move focus on Showdown set instead of randomizing a pkm file.
Allow user to enter whatever they want for $list, handle edge cases like Kommo-o
Add "$list all" to show non-duplicate caught species.
Automatically remove from favorites if trading or gifting (small QOL thing).
Change how favorites are removed from user file.
Revert base egg shiny chance nerf.
Fix daycare
Add favorites command to TradeCord.
Slightly nerf eggs.
Fix TradeCord list for shinies
Add TradeCord (my dumbest and messiest project so far, Archit pls don't hate the mess).
Add Showdown output for Star/Square shinies and OTGender.
Add optional link code input for FixOT.
Change how OTName, TID, SID is displayed.
Add Regigigas SR bot.
Add SoJ Camp SR bot.
Ribbons now work with EggTrade (remove ribbons if egg).
Remove EggRoll.
Add another filter for FixOT
Fix.. FixOT
Update offsets for EncounterBot catching.
Slightly change StrongSpawn to work with Regi SR and make it its own mode.
Make SpinTrade only available for USB-Botbase
Update valid eggs for CT
winforms: resize icon.ico to fix crash at startup on unix using mono
Rework Spin, read initial in-game coordinates in order to correct drift
Add TID, SID, Language output for Showdown
Remove obsolete OT and Language parsing
Very minor clean until I have time for a proper one.
Detach controller when stopping USB bot.
Actually set LastUsedBall for EncounterBot (missed when bringing in line with main repo)
Move extra RaidBot timings following the official commit
Remove PKHeX Discord invite from Readme.md

Maybe fewer people will pester devs now about my unofficial fork?
Update for latest main repo EncounterBot commits.
Update README.md
Add back best commit: Red's SpinTrade.
Add egg trades, foreign Dittos and OT for Twitch.
If ItemMule is enabled, also display the item a user is receiving.
Add periodic time sync toggle for all methods of hosting (except for non-soft locked AutoRoll) to (hopefully) prevent den rollover during extended hosts.

Add routine to exit a lobby for SoftLock if no players are ready in time (to preserve soft lock).

Add a routine to recover from disbanded lobbies (when someone disconnects unexpectedly) for SoftLock.

Add a routine to restart game if all else fails and we're stuck in a raid.

Add a routine for adding and deleting friends if we're soft locked and raids go empty.

Slightly reorganize settings, extract methods, minor clean.
Don't use such a generic file name for stream assets.
Check USB port index for running bots. Should fix adding additional USB bots when no config is saved.
Add fixed met date for FixOT.
How do I boolean
Change airplane mode logic, tweak timings and routine for soft lock lobby exit
Rework EggRoll cooldown (static list in favor of a txt file).
Start clean up and refactor
Add setting to increase delay after pressing "Home" after a date skip.
Use USB port index for blocking and sprite pngs if connection type is USB
Add option for airplane host (usb-botbase required)
Add option to softlock on selected species for AutoRoll
Add automatic compatibility for all console languages when date skipping (have to set ConsoleLanguage under ScreenDetection)
Attempt to fix multiple USB device add and connect...again
Minor clean
Fix oops?
Handle add/remove of bots
Distinguish between multiple USB devices, tweak BotRemoteControl for USB, other various fixes
Add SpA modifier for foreign Dittos
Add alpha USB-Botbase support
Fix DateTime parsing for European format for EggRoll
Set fixed EggMetDate and MetDate for EggRoll
More FixOT filters
Remove Beheeyem. Oops.
Split EggRoll into its own routine and trade type, only output "Receiving: Mysterious Egg" if routine is EggRoll, other minor tweaks and fixes
Make FixOT its own queue with roles and counts
Add a couple more OTs to $fix
Parsing for EggRaffle auto-clear and $clearcooldown
Adjust timings and split Watt collecting clicks for AutoRoll
Fix oops with file attachments for Ditto
Further improvements for OT, memes for invalid pokemon (disable EasterEggs)
Add spaces, digits for OT
Randomize memes, cut down bloat
Fix miscellaneous bots after Anubis' recent QOL additions
-Ignore events for OT because headache.
-Add overlooked "$convert <generation>" input for OT.
-Move $clearcooldown to SudoModule
-Clear timer automatically if NoTrainerFound
-More reliable Dittos
-Foreign Dittos for $convert
-Command to clear cooldown for EggRaffle in case trade gets disconnected
-Fix "Trade finished" line to keep result secret
-EggRaffle as a toggle, option to specify channels
-Seed Check output to both DMs and Channel (apparently some want it)
-Randomly generated egg raffle via a "$roll" command with a configurable cooldown
-FixAdOT reworked, has its own command "$fix" and no longer overrides $clone
-Ball: <value> output for Showdown sets
-Fix oversight
-Option to output Seed Check results to Discord channel with a User mention
-Showdown set output for OT name and eggs
-Basic "OT: <name>" option without Showdown set output
-Initial $convert support for EggTrade
-Egg moves for EggTrade test attempt
-Minor update
-EggTrade (by nicknaming a Pok√©mon "Egg" using $trade)
-Failsafe for memes if enabled but field left blank or incomplete
-Niche breedable Ditto trade mode.
Add minimize button
EggFetch text logs
StrongSpawn mode for EncounterBot
Re-add EncounterBot Master Ball catching
More parsing for FixAdOTs
Park Ball as held item instead of string
Actually remove the offset instead of saying I did
Initial DLC commit
Faster code entry
Removed catching for EncounterBot (need a new offset)
CloneBot mode to fix Nickname and OT if adverts detected

---
## [mrakgr/The-Spiral-Language@54ac849adb...](https://github.com/mrakgr/The-Spiral-Language/commit/54ac849adb928f831c542053eab660b3f70796e5)
##### 2021-05-17 09:55:00 by Marko Grdiniƒá

"10:10am. I've had difficulty falling asleep last night as my will is starting to concentrate. Stacking GANs independent as modules is my biggest hope for the future. I should be feeding them data and nurturing them.

The duality gap algo from the GMN paper for stabilizing their training is trustworthy. FID scores are sensitive to mode collapse so I should be able to trust them.

If it was before, stacking independent GAN modules would be suicide as their instability would compound, but if they are stable, they can form the foundation for an actual learning system.

If I passed them sequences, they would learn to predict the future.

The things I am missing for full AGI are some of the things the brain does. Maybe there will be different models better at pattern detection. Maybe those fast/slow weight ideas need revisiting.

But for the first time the foundations are now here. Long term memory formation can be induced with temporal bottlenecks and the saliency trick I've developed.

https://people.idsia.ch/~juergen/fast-weight-programmer-1991-transformer.html

Oh, he says that 2021 has new stuff.

https://arxiv.org/abs/2102.11174
Linear Transformers Are Secretly Fast Weight Memory Systems

Maybe attention is really all I need.

> Recent work proposed ‚Äúlinear Transformers‚Äù with constant size memory and time complexity linear in sequence length (Katharopoulos et al., 2020; Choromanski et al., 2021; Shen et al., 2018).

Huh they are not n^2 in sequence length? This is interesting!

> Therefore, we introduce an improved update rule inspired by recent work on fast weight memories (Schlag et al., 2021).

https://arxiv.org/abs/2011.07831
Learning associative inference using fast weight memory

Oh, Schlag is the first author of both papers.

10:50am. Had to take a break. For the morning my focus will be on these two papers. Without a doubt, the answer to the brain's short term memory questions is an architecture like this.

If I can provide an answer to the issues Simon Thorpe posed, I will have everything I need to scale my ideas to something much higher, even if the hardware is not there.

The computational insufficiency of the present does not matter. What matters is that my ideas will give me a clear path of advancement.

11:20am. The first paper was interesting, although propagating through the weights will blow up the memory requirements. Let me go for the second paper which uses LSTMs + fast weights. Something like that should be the answer.

> Lastly, we also evaluate Ba‚Äôs Fast Weights which attend to the recent past (JBFW; Ba et al. (2016a)) but were unable to find hyperparameters that converged.

Oh, lol.

> In our experiments, the LSTM-based agent requires more episodes, a bigger network, and eventually overfit to the training graphs. The FWM-based agent however trains faster and generalises to randomly sampled graphs.

> . The FWM agent (blue) has a slow LSTM with 32 hidden units and a fast weight memory of size 16 √ó 162 . We compare to LSTM agents with different sized hidden states. The largest LSTM has 4096 hidden units (red) which roughly matches the number of temporal variables of the FWM. The FWM has 14k trainable weights which is by far the lowest. The largest LSTM has 67.4M weights which is roughly 4814 times more than the FWM. The relative factor of each LSTM is added to the legend. All LSTMs take longer to train and eventually overfit on the training data. Due to the overfitting, the LSTM does not have to explore, which results in a higher total reward on training environments but a lower total reward on test environments.

If I am going to use an RNN, I should definitely use one that employs fast weights.

They use a fairly small weight matrix for the memories. That makes sense. They make a point that the human brain has a small working capacity as well.

11:45am. I don't know. It does not really matter at this stage.

Some kind of memory augmented RNN will come out in force in the future and become dominant, similar to how transformers are strong now. On neurochips, RNNs will be more efficient than feedforward nets whereas the story is the opposite on the GPUs.

11:50am. I am going to stop here. Enough of these papers. It is time for programming.

The way to advance in life is to practice agent cultivation. I might not have all the answers yet, but GAN training for prediction and my module ideas will stick around.

I am going to make a small RL agent that does even do unsupervised learning, but past that I am going to do my best to make agents that do. I will scale up. The modules I invented and the GAN duality gap method are what is going to make the whole thing scalable. I will believe in it.

The power of the agents will be my own power.

All I have to do to reach the answer at the end of the path is to build. It is time that I stop hesitating.

11:55am. I'll implement the uniform player after the breakfast."

---
## [hamadmarri/cacule-cpu-scheduler@a200a648c8...](https://github.com/hamadmarri/cacule-cpu-scheduler/commit/a200a648c841de8f524bcb97bf1647436798cf4f)
##### 2021-05-17 14:41:55 by Hamad Marri

This is a testing version for RDB.

Background
----------
RDB wasn't stable and it was experimental. This time we
need to push RDB forward and make it stable. You can think
of RDB as a stupid but super lightweight load balancer. RDB
will never be able to distribute tasks wisely among CPUs as
same as CFS load balancer does. However, CFS needs many stats
and task accounting in order to choose wisely ‚Äì RDB doesn‚Äôt.
So RDB shines when there is simple load balancing to do. It
tries to take a decision based on interactivity score (IS)
which is already been used in CacULE, so no overhead stats
counting.

Previous RDB version and its issues
-----------------------------------
Previous RDB works good in certain situations, and many
users don‚Äôt like it because it didn‚Äôt work well with them.
However, others had good experience with it. Personally,
RDB was working great on my OpenSuse Tumbleweed/Leap, but
when I tried it on Ubuntu it was a disaster. So these new
changes are made to make RDB stable on most OS and on
different configs.

Changes
-------
The first change I made that makes RDB works on Ubuntu as
good as it was on OpenSuse is changing `try_pull_any` in
`active_balance` to `try_push_any`. This change could effect
negatively the RDB performance with users where previous RDB
was good on their machines. Therefore, this change must be
tested. I have added 4 syctls for rdb testing. And `try_push_any`
is included in testing where testers can enable/disable. Below
is an explanation of each sysctl.

kernel.rdb_try_push_any_enable
------------------------------
Purpose:
	To choose between `try_pull_any` and `try_push_any`

Possible values (default 0):
	0: means RDB uses `try_pull_any` as same as old version
	1: to run `try_push_any` instead of `try_pull_any`

I recommend testing this first.

kernel.rdb_balance_guard_ms
---------------------------
Purpose:
	The trigger_load_balance runs in every tick. For
	High HZ values, the load balance could be overwhelming.
RDB load balance includes locking which can reduce the
performance. The balance guard can help to avoid running
load balance on every tick. For example, rdb_balance_guard_ms=3
will only run load balance every 3ms. Setting rdb_balance_guard_ms
depends on HZ. If you want load balancer run every 2ms while HZ=500
then it is not needed and better to disable rdb_balance_guard_ms
since 500HZ already (1000ms/500HZ = 2ms). However, if you have
1000HZ and want to avoid load balancer from running every 1ms,
you could set rdb_balance_guard_ms=4ms for example to make load
balancer run every 4ms. Less rdb_balance_guard_ms values (or 0 to
disable) could make sure tasks are balanced ASAP, but with the
cost of locking/blocking time. High rdb_balance_guard_ms values
can relax balancing locking but with the cost of imbalanced
workload for that period of time (i.e. if rdb_balance_guard_ms=100ms)
there will be no balancing for 100ms (except for newidle_balance
which is not effected by rdb_balance_guard_ms).

Possible values (default 3):
	0: to disable
	non 0: to enable with the value set

I recommend first set this to 0, then test rdb_try_push_any_enable.
After you chose which is better (push or pull), then try tune
rdb_balance_guard_ms.

kernel.scale_down_hz_value
--------------------------
Purpose:
	To reduce the calling of the following functions to the
	specified HZ if the runqueue (rq) has only 1 runnable task.
	- rq_lock
	- arch_scale_thermal_pressure
	- update_thermal_load_avg
	- task_tick
	- calc_global_load_tick
	- psi_task_tick
	- perf_event_task_tick
	- trigger_load_balance

Notice that trigger_load_balance is also controlled by
rdb_balance_guard_ms. scale_down_hz_value doesn‚Äôt scale down the
whole HZ, nor affects interrupts. It is just an if statement
to return from calling the above functions (see
core.c:scheduler_tick()). Since RDB load balancer is simple,
we can actually mess with scheduler_tick wishing to increase
performance. There is another reason to consider this feature
in case of `try_push_any_enable` is enabled.

For example, if we have cpu0 and cpu1. cpu0 has 3 tasks and
cpu1 has only 1 task. Assuming scale_down_hz_value=2, cpu0
will not skip any tick since it has more than 1 task (cpu0
wont be effected by scale_down_hz_value), however, cpu1 is
effected because it has only 1 task in its rq. Cpu0 will
call the above functions only 1 time per second since the
scale_down_hz_value=1 means 1HZ = 1 call/s. Now cpu1 is not
going to call trigger_load_balance during 1s to pull tasks
from cpu0. That‚Äôs why I believe `try_push_any_enable=1` works
together with `scale_down_hz_value` because cpu0 can push tasks
to cpu1 while cpu1‚Äôs HZ is scaled down. The benefits are less
locking time since cpu1 doesn‚Äôt bother other cpus asking to
pull tasks, and cpu1 can save more time not calling the above
functions since it has only 1 task in rq. The feature runs as
following: assume HZ=500, if scale_down_hz_value=100, then
500 / 100 = 5. The above functions will be skipped 5 ‚Äì 1 = 4
times. They will be called once every 5 ticks. Setting
scale_down_hz_value=500 is same as scale_down_hz_value=0
since 500 / 500 = 1, and then 1 ‚Äì 1 = 0, so no skip.

Notice that this is an integer division! So the following
values when HZ=500 are the same:
scale_down_hz_value=300 ‚Üí 500/300=1.6  =1
scale_down_hz_value=400 ‚Üí 500/400=1.25 =1
scale_down_hz_value=250 ‚Üí 500/500=1    =1

However
scale_down_hz_value=250 ‚Üí 500/250      =2

So you need to consider the integer division first,
then divide HZ/(skip) to get the actual HZ

For example, you might assume scale_down_hz_value=84
would mean 1000ms/84hz = 11.9ms! But that is
incorrect since 500HZ/84 = 5.952380952 by taking the floor
it is going to be 5 skips, then 500/5 = 100, therefore
you are actually scaling it down to 100HZ i.e. 10ms.

Possible values (default 0):
	0: disabled the feature
	1: tick per second
	2-2000: scaled down hz based on original HZ
              and integer division.

kernel.average_vruntime_enable
------------------------------
Purpose:
rdb disables autogroup, which can be a problem because
sometimes autogroup provides better interactivity since
it gathers tasks in groups and count their vruntime as
one entity. I have made something similar to autogroups
where update_curr updates all parents with delta_fair
(func. update_parents()). Also normalize_lifetime updates
the diff of vruntime to all parents. The use is in
calc_interactivity where instead of taking cn.vruntime
we take the average vruntime from the task to its highest
root parent (func. average_vruntime). Since every parent/task
has the total of vruntime of its children (due to update_curr
updating parents) the average is taking of the task would
take on the account that if this task belongs to a very
busy group (based on fork hierarchy) or not. The task
will have higher vruntime average if it is a child of
busy group (going thru the path to root parent).

Example:
Let‚Äôs assume the following task tree:

systemd (10)
‚îú‚îÄ‚îÄ calculator (1)
‚îú‚îÄ‚îÄ fakeroot (6)
‚îÇ   ‚îú‚îÄ‚îÄ make1 (3)
‚îÇ   ‚îú‚îÄ‚îÄ make2 (1)
‚îÇ   ‚îú‚îÄ‚îÄ make3 (0)
‚îÇ   ‚îî‚îÄ‚îÄ make4 (2)
‚îî‚îÄ‚îÄ sh (3)
    ‚îî‚îÄ‚îÄ fish (3)
        ‚îî‚îÄ‚îÄ cat (3)

These tasks could be distributed on multiple cpus. Let‚Äôs assume
the two tasks cat, and make3 on the same rq. The cat has
IS=4 while make3‚Äôs IS=0. With average_vruntime_enable=0,
make3 will run since its IS is less than cat‚Äôs IS. However
make3 is a task in one group doing when task which is compiling.
4 makes are doing compiling while cat is only when interactive
task that is competing with other 4 tasks. With average_vruntime_enable=1,
The average IS is for cat and make3 is calculated as following:

make3 = (0+6+10)/3 = 16/3 = 5.3
fakeroot=6 since it is the sum of its children, the same thing
for other parents.

cat = (3+3+3+10)/4 = 19/4 = 4.75

So cat will be picked to run instead of make3

Potential of improper decision:
Assuming that cat and make3 are the only 2 tasks in cpu0.
make3 could be starved because its group is doing great job
on cpu1! So, it is better to consider averaging with respect
of cpus too. I will try to enhance this feature on future, but
for now let‚Äôs test and see if this feature provide any better.

Note while testing average_vruntime_enable:
Moving from average_vruntime_enable=0 to average_vruntime_enable=1
is ok since parents have 0 updates, however, after you tested
with average_vruntime_enable=1 and while testing or after
few seconds you turned average_vruntime_enable to 0, there will
be an issue since most grouped tasks were been restricted to
their group but suddenly you opened the freedom to all tasks.
Most likely when compiling with average_vruntime_enable=1 then
switch to average_vruntime_enable=0 while compiling will cause
system slowdown. So it is not recommended when testing to switch
from average_vruntime_enable 1‚Üí0. I recommend a reboot to switch
back from 1 to 0. I don‚Äôt want you to have incorrect conclusion
due to this switch from 1 to 0. The safest way is test with
average_vruntime_enable=0, then do:
sysctl -w kernel.average_vruntime_enable=1 | tee -a /etc/sysctl.conf

and reboot to start a fresh test with average_vruntime_enable=1

Lastly

kernel.sched_interactivity_threshold
------------------------------------
sched_interactivity_threshold value can also affect the RDB
behavior. You can tune this after you have tunned the rdbs values.

Please test which values give better response and performance.
And please compare your best RDB tunned with previous RDB, and
also compare it with CacULE+autogroup.

Please take your time. This will be a very important test
to make RDB stable and make it usable.

Thank you

---
## [pytorch/pytorch@9354a68e7d...](https://github.com/pytorch/pytorch/commit/9354a68e7d8c4680a115b70b9b14565cd42cb03f)
##### 2021-05-17 19:26:06 by Brian Hirsh

[codegen] split out backend-specific information from NativeFunction in the model (#57361)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57361

Data model change in the codegen, which splits backend-specific information out of `NativeFunction`

### Overview
Currently in the codegen, native_functions.yaml has backend-specific information about each operator that is encoded directly into the data model, in the `NativeFunction` object. That's reasonable, since the native_functions.yaml is the source of truth for information about an operator, and the data model encodes that information into types.

Now that external backends can use the codegen though, that information is technically incomplete/inaccurate. In another PR, I tried patching the information on the `NativeFunction` object with the additional external information, by updating the `dispatch` entry to contain the external backend kernel name and dispatch key.

Instead, this PR tries to split out that information. The `NativeFunction` class contains all information about an operator from native_functions.yaml that's backend-independent and is known never to change regardless of what extra information backends provide. We also build up a backend "index", which is basically a mapping from [backend] -> [backend-specific-metadata]. Reading in an external backend yaml just involves updating that index with the new backend.

There were a few places where `NativeFunction` used the dispatch table directly, that I encoded as properties directly on the NativeFunction object (e.g. `is_abstract`). They were mostly around whether or not the operator has a composite kernel, which isn't something that's going to change for any external backends.

This has a few advantages:
- We can more easily re-use the existing logic in `native_function.py` and `register_dispatch_key.py` for both native and external backends, since they both involve a NativeFunction + a particular backend index
- The data in the data model will be the same regardless of how the codegen is run. Running the codegen with a new external backend doesn't change the data inside of NativeFunction or an existing backend index. It just adds a new index for that backend.
- There are several of codegen areas that don't care about backend-specific information: mostly the tracing and autograd codegen. We can reason about the codegen there more easily, knowing that backend-specific info is entirely uninvolved.

An alternative to this split would be to augment the NativeFunction objects with external backend information at the time that we create them. So the external codegen could read both native_functions.yaml and the external backend's yaml at the same time, and construct a NativeObject with a full dispatch table (including the XLA entry), and the correct setting of structured (taking into account both yamls). One disadvantage to this approach is that NativeFunction objects now contain different stuff depending on how you ran the codegen, and you have to make sure that any changes to the codegen can properly handle all the different variants.

### Data Model Changes
Removed 3 classes, which are used by the external codegen:
- ExternalBackendFunction
- ExternalBackendFunctionsGroup
- ExternalBackendMetadata

And added two new ones:
- BackendIndex
- BackendMetadata

`BackendIndex` contains any info that's specific to that backend, plus a mapping from operator names to backend specific metadata about the operator. One example of backend-specific info that's not operator-dependent is the fact that XLA prefers to implement functional kernels instead of out kernels (and so when they eventually mark an op as structured, they're going to mark the functional op and not the out op).

`BackendMetadata` contains info specific to an (operator, backend) pair. Right now, that's just (a) the name of the kernel, and (b) whether or not that operator is structured.

### Questions
I wanted to get this PR up earlier so I could get feedback, but there are a few things I want to call out:

**Dealing with `structured`.**
This PR separates out the notion of `structured` into two bits of information:
- Does [operator] have a meta() function. This is backend-agnostic, and is represented by the `structured` property on `NativeFunction`, same as before. This is used, e.g., to decide what signatures to add to `MetaFunctions.h`.
- Does [operator, backend] have an impl() function. This is backend dependent; even though technically all in-tree backends are forced to write impl() functions for an operator when we port the op to structured in native_functions.yaml, out-of-tree backends can decide to opt in independently. This is represented as a property on `BackendMetadata`. This is used in most other cases, e.g. in `RegisterDispatchKey` when we're deciding whether or not to gen a structured or unstructured wrapper.

I also baked `is_structured_dispatch_key` directly into each BackendIndex. So for operators marked "structured" in native_functions.yaml, their corresponding CPU/CUDA BackendIndex entries will be marked structured, and all others (except for potentially external backends) will not.

I ended up trying to deal with `structured` in this change since it's technically backend dependent (XLA can opt kernels into structured separately from in-tree ops), but that may have been too ambitious: it's technically not relevant until we actually add support for structured external kernels. If it's not clear that this is the right path for dealing with structured and we want to push that off, I'm fine with backing out the bits of this PR that make `structured` backend-dependent. I don't see anything *too* controversial related to structured in the change, but I tried to call out any areas in the comments

**Localizing the fact that external backends follow Dispatcher convention.**
Another thing that's sort of backend specific that I didn't totally address in this PR is the fact the fact that in-tree backends follow the Native API while external backends follow the Dispatcher API. I painted over that in `native_functions.py` by adding a helper, `kernel_signature`, that takes in a native function and gives you the "correct" signature for the specified backend- NativeSignature for in-tree backends, and DispatcherSignature for out-of-tree backends. In order to make that fully useable though, we'll need `NativeSignature` and `DispatcherSignature` to have matching interfaces. I didn't bother with that in this PR, which is why `gen_external_aten_fallbacks.py` still has a bunch of direct references to the dispatcher API. Thinking of adding it in a later PR but wanted to see if anyone has other opinions.

Maybe `is_external()` shouldn't even be a property on the BackendMetadata, and anything the codegen does that requires asking for that information should just be better abstracted away.

**Thoughts on the `BackendIndex` / `BackendMetadata` breakdown.**
One thing that's annoying right now is that to query for various pieces of metadata, you call helper functions like `backend_index.structured(f)`, which queries that particular backend and tells you if that specific NativeFunctionGroup is structured for that backend. It has to return an `Optional[bool]` though, since you have to handle the case where that operator doesn't have a kernel for that backend at all. So users of those helpers end up with a bunch of optionals that they need to unpack, even if they know at some point that the result isn't None. I think it would be easier instead to just store the NativeFunction object as a field directly on the BackendMetadata. Curious if there are any other opinions on a better way to model it though.

Test Plan: Imported from OSS

Reviewed By: navahgar

Differential Revision: D28474362

Pulled By: bdhirsh

fbshipit-source-id: 41a00821acf172467d764cb41e771e096542f661

---
## [Perkedel/Kaded-fnf-mods@a52f4748a4...](https://github.com/Perkedel/Kaded-fnf-mods/commit/a52f4748a41195c31824abeb554c283ced81b03e)
##### 2021-05-17 20:17:37 by Joel Robert Justiawan

oh vey

new stuff the Yellow screen stage here!

propose Guruku Tersayang but it's Funkin. what if Boyfriend, Girlfriend, and Pico got a brand new university and is actually comfortable to go through?
damnit! well, uh, so Pico voice effect I've learnt from KawaiSprite and bbpanzu video https://youtu.be/ODFOpoXjzaA here has trouble. NO, it wasn't anything. but here the effect Pico has is only good for rapping. and unfortunately is not compatible with vocal lexical song like most, if not rapping. yeah idk what to do... I'm sorry Tom Fulp, ninja, GenoX, bbpanzu, and uh... etc. my result is terrible.

THE KICKSTARTER GOING TO CLOSE IN HOURS LEFT
https://www.kickstarter.com/projects/funkin/friday-night-funkin-the-full-ass-game
it didn't reach the max goal üò≠üò≠, but hey! we got them! üòÅüòÅüòÅ

---
## [Total-RP/Total-RP-3@bf634280e8...](https://github.com/Total-RP/Total-RP-3/commit/bf634280e8cdc60c2c9531aaa7ebd26a9dfbc9a0)
##### 2021-05-17 20:51:52 by Daniel Yates

Implement smart nameplate queue strategy (#566)

* Implement smart nameplate queue strategy

This reworks the nameplate request system to use a much smarter queue
mechanism than we currently have.

Our existing approach only applies a cooldown on a per-character level,
such that we only throttle requests sent to nameplates of units we've
already seen at most once per 90 seconds. Unfortunately it doesn't
account for a few real-world issues.

Firstly, nameplates often quickly appear on-screen and then go off.
These can be thought of as one-hit wonders for people you're probably
not going to interact with and whose data isn't quite so important.

Secondly, you can walk into a field of a hundred players and turn your
nameplates on and suddenly request every visible nameplates' profile
immediately all at the same time. This completely throttles comms
on both sender and receiver ends.

To resolve this a smarter queue system is implemented for nameplate
requests with the following characteristics.

To solve the first issue, an enqueued request for a nameplate must first
exceed a minimum eligibility time before the request is sent. This
is kept small - at around 2.5 seconds - and should solve the issue of
nameplates briefly popping onto the screen an disappearing issuing a
full request for their data.

For the second issue, the queue system has an internal semaphore
representing request "slots". Each attempt to send a request will
first attempt to obtain a slot and decrement an internal counter; if
no slots are available the request won't be dispatched immediately and
must wait until a "recharge period" has elapsed. Each tick of the period
regains a single request slot.

The idea behind this is to significantly hamstring the ability to burst
request nameplate information; if you walk into a field of a hundred
players who all stay on screen to exceed their eligibility period, you
will only send at-most five requests to people on-screen immediately and
then one additional request each time a slot is regained every ~1.25s
thereafter.

Finally, the existing system of applying a per-character cooldown is
kept around - so if you repeatedly see a nameplate for someone you've
already requested then they will gain a significantly increased
eligibility time. The timer for this has been increased from 90 seconds
to 5 minutes; the idea is if you're interacting with someone for that
long you'll probably have seen their tooltip accidentally and requested
updated data anyway, or that they won't have made any change to their
profile worth querying.

The changes described significantly reduce the amount of requests that
can be sent in any short period of time, however for the user experience
this will come at a significantly increased latency as far as seeing an
undecorated nameplate and actually receiving their profile data.

To minimize this, there also exists a basic system of prioritized
requests. Prioritized requests will always be placed at the front of
the queue and apply to the nameplates of units that are either in your
friends list - both character and Battle.net, units that you are
presently grouped with, and members of your own guild.

* Minor changes to tweakable variable names

Primarily this just renames a few things around the cooldowns for
requests for people who haven't yet been submitted and for those
that have had requests submitted.

* Nerf the recharge rate of nameplate request slots

As this is meant to be a passive way of requesting profiles reducing
the recharge rate gives larger profile transfers a bit more breathing
room in high-population scenarios. With this change you'll now need to
wait 90 seconds for requests to be sent to 40 on-screen nameplates
instead of 60.

The wait period for on-screen nameplates before requests are sent has
also been increased to 3 seconds, in the spirit of Classic WoW mount
cast times.

* Dequeue pending requests on register data updates

If we receive updated data in our register for a unit, we'll dequeue
any pending request under the assumption that it was probably requested
by some other means.

* Add basic prioritization of nameplates by range

Keyword is "basic" - we just follow DBM's style of range check by
testing if a player is in range of a given item ID; in this case the
Mistletoe item is used because it's available on all game versions
and is relatively small at 23 yards.

---
## [turanct/consumed-in@49fbf32253...](https://github.com/turanct/consumed-in/commit/49fbf32253900fdadfcbf35da9cd951f93b29fe1)
##### 2021-05-17 22:29:00 by turanct

Listened to a small batch of podcasts

- https://www.vrt.be/vrtnws/nl/2021/04/11/planeet-frank-onze-weerman-over-luchtvochtigheid-weercomputers/
- https://maintainable.fm/episodes/bertold-kolics-are-you-carrying-the-weight-of-dead-code-UdbWSBGs
- http://walkingthefloor.com/episode-190-del-mccoury/
- https://www.thisamericanlife.org/735/bloody-feelings
- https://tonemob.com/podcast/nft-guitar-pedals-w-tom-from-tomkat-pedals/
- https://podtail.com/en/podcast/who-is/who-is-deb-haaland/
- https://softwarecraftspodcast.libsyn.com/interview-with-peter-maddison
- https://99percentinvisible.org/episode/welcome-to-jurassic-art-redux/
- https://tonemob.com/podcast/brian-fallon-chris-benson/
- https://timharford.com/2021/04/cautionary-tales-the-fragile-barrier-between-reality-and-fantasy/
- https://play.acast.com/s/deconstructed/thenewmexicospring
- https://www.newyorker.com/magazine/2021/04/26/the-incredible-rise-of-north-koreas-hacking-army

---

# [<](2021-05-16.md) 2021-05-17 [>](2021-05-18.md)

