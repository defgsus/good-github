# [<](2021-05-10.md) 2021-05-11 [>](2021-05-12.md)

6,393,446 events, 964,798 push events, 1,533,534 commit messages, 130,054,457 characters


## [vlggms/tegustation@7c4efd1d77...](https://github.com/vlggms/tegustation/commit/7c4efd1d7762abc8402f1f9a030c25f3d6715045)
##### 2021-05-11 00:16:55 by SmArtKar

Fixes god damn simplemob chems (#124)

* Fixes god damn simplemob chems

Retards, I already gave you the fucking fix a year ago

* Update holder.dm

---
## [wahello/cockroach@fb9f72220e...](https://github.com/wahello/cockroach/commit/fb9f72220e5af7cda5e6683df29ea8e4e2098ddf)
##### 2021-05-11 01:18:31 by craig[bot]

Merge #62728 #63073 #63214

62728: roachtest: don't call t.Fatal in FailOnReplicaDivergence r=erikgrinaker a=tbg

In #61990 we had this method catch a stats divergence on teardown in an
otherwise successful test. The call to `t.Fatal` in that method
unfortunately prevented the logs from being collected, which is not
helpful.

Release note: None


63073: roachtest: retire `bank` tests r=nvanbenschoten,andreimatei a=tbg

The `bank` roachtests are a basic form of Jepsen test. We were hoping
that they could provide additional benefit due to being less complex
than a full Jepsen test. In my opinion, this calculus has not worked
out. We've spent many hours staring at this test deadlocked when it
wasn't CockroachDB's fault at all. Besides, it's not adding anything
meaningful on top of our Jepsen tests plus KVNemesis plus the TPCC
invariant checks, so we are removing the `bank` family of tests here.

Should we want to reintroduce these tests, we should do it at the level
of TestCluster, or at least augment the `bank` workload to incorporate
these invariant checks instead.

Closes #62754.
Closes #62749
Closes #53871.

Release note: None


63214: kvserver: reduce ReplicaGCQueueInactivityThreshold to 12 hours r=ajwerner a=erikgrinaker

`ReplicaGCQueueInactivityThreshold` specifies the interval at which the
GC queue checks whether a replica has been removed from the canonical
range descriptor, and was set to 10 days. This is a fallback for when we
fail to detect the removal and GC the replica immediately. However, this
could occasionally cause stale replicas to linger for 10 days, which
surprises users (e.g. by causing alerts if the stale replica thinks the
range has become underreplicated).

This patch reduces the threshold to 12 hours, which is a more reasonable
timeframe for users to expect things to "sort themselves out". The
operation to read the range descriptor is fairly cheap, so this is not
likely to cause any problems, and the interval is therefore not jittered
either.

Resolves #63212, touches #60259.

Release note (ops change): Replica garbage collection now checks
replicas against the range descriptor every 12 hours (down from 10 days)
to see if they should be removed. Replicas that fail to notice they have
been removed from a range will therefore linger for at most 12 hours
rather than 10 days.

Co-authored-by: Tobias Grieger <tobias.b.grieger@gmail.com>
Co-authored-by: Tobias Schottdorf <tobias.schottdorf@gmail.com>
Co-authored-by: Erik Grinaker <grinaker@cockroachlabs.com>

---
## [penny64/goonstation@24f6cd7530...](https://github.com/penny64/goonstation/commit/24f6cd75306650a172e69ebe2a7a79c3a1824150)
##### 2021-05-11 02:54:57 by BatElite

Resprites building materials & standardises their code a bunch (#4086)

* Sprites & functions & oh god this needs unfucking
* Do rods, mostly. We have procs for this shit y'all
* Tile stacking/taking cleanup
* Sheet consumption stuff
* Addresses review, enlargens and changes sprites

Co-authored-by: ZeWaka <zewakagamer@gmail.com>

---
## [NicholasReis/Stock-Analysis@b435f50778...](https://github.com/NicholasReis/Stock-Analysis/commit/b435f50778f263348adb1fa81448211406221365)
##### 2021-05-11 05:19:38 by NicholasReis

Shits fucked and so am I.
I created a temporary file to test data and build the functionality so I could take a break from the nightmare that is reading the JSON formatting the yahoo.finance team produced.
I will continue working on reading the info from the file since I could later just output to a file once I format the yahoo.IHateUserInteraction string.

I am close, but python is a cruel bitch. We'll figure her out though. Eventually.

---
## [Hahoolah/Programvaruarkitektur@ab5de31f38...](https://github.com/Hahoolah/Programvaruarkitektur/commit/ab5de31f387c558d6c9fbbf2b10d7fe76899a81f)
##### 2021-05-11 08:06:48 by jesper-and

god fucking damnit fuck you github yo u massive swine

spent 8 hours after our meeting trying to merge this fucking asshole. Decided to just hard move the entire filepackage instead and it worked. If i have to merge something again id rather shave my head.

TRUCKFSM implemented, apply it to a truck prefab, remove the truckforward and then youre good to go.

---
## [mrakgr/The-Spiral-Language@ca89f42572...](https://github.com/mrakgr/The-Spiral-Language/commit/ca89f425729c78c010562be3caaa5490bd951c31)
##### 2021-05-11 10:32:40 by Marko GrdiniÄ‡

"9:45am. I am up. Let me chill a bit and then I will start.

10:10am. Done chilling.

https://www.reddit.com/r/MachineLearning/comments/n9fti7/d_a_few_helpful_pytorch_tips_examples_included/

///

I compiled some tips for PyTorch, these are things I used to make mistakes on or often forget about. I also have a Colab with examples linked below and a video version of these if you prefer that. I would also love to see if anyone has any other useful pointers!

* Create tensors directly on the target device using the device parameter.

* Use Sequential layers when possible for cleaner code.

* Don't make lists of layers, they don't get registered by the nn.Module class correctly. Instead you should pass the list into a Sequential layer as an unpacked parameter.

* PyTorch has some awesome objects and functions for distributions that I think are underused at torch.distributions.

* When storing tensor metrics in between epochs, make sure to call .detach() on them to avoid a memory leak.

* You can clear GPU cache with torch.cuda.empty_cache(), which is helpful if you want to delete and recreate a large model while using a notebook.

* Don't forget to call model.eval() before you start testing! It's simple but I forget it all the time. This will make necessary changes to layer behavior that changes in between training and eval stages (e.g. stop dropout, batch norm averaging)

Edit: I see a lot of people talking about things that are clarified in the Colab and the video I linked. Definitely recommend checking out one or the other if you want some clarification on any of the points!

This video goes a bit more in depth: https://youtu.be/BoC8SGaT3GE

Link to code: https://colab.research.google.com/drive/15vGzXs_ueoKL0jYpC4gr9BCTfWt935DC?usp=sharing

///

This post is good for me. In not too long, I am going to have to get deeply familiar with PyTorch. And I was wondering how to stop the backward pass from accumulating during testing.

10:10am. Let me watch the interview from yesterday...

https://youtu.be/CY_LEa9xQtg?t=2479
Risto Miikkulainen: Neuroevolution and Evolutionary Computation | Lex Fridman Podcast

This is how far I got yesterday. Let me put in just 20m and then I will do some programming. I think that my vacation has gone for long enough and it is time to pick up the pace soon.

10:35am. Enough of the interviews. Let me start.

I've explored all corners of my mind. The windy emotions swept my sailboat of consciousness to various different places. I've traveled enough.

While I cannot make breakthrough with the sheer power of my mind like in the PL field, and while I cannot grasp the light of the Singularity in the present, I can make the net of predictions.

Not just me, but all of us know how things are going to go. We know what pieces are necessary. This mysterious, but inexplicable intuition is pushing us forward. So I should not have doubts about how events will unfold. The base architectures will get perfected, the inner games needed for unsupervised learning will get mastered, and multi-temporal architectures will get invented. Then all the pieces will be set.

As weak as my programming skills are in the grand scheme of things, they are still my strongest powers.

If I had the future understanding, they are what would allow me to start the Singularity even in the present.

Spiral might have set me back in terms of my ML cultivation, but it will pay for itself over the long term. I could end up living a surprisingly long time.

10:45am. I can only be great through programming, so that is what I should be doing.

10:50am. Make the agent.

Almost immediatelly, even if do not automate everything and merely use it as an advisor, that will mean that I have superhuman ability in the betting game of my choice. That is what the essence of ML is.

10:55am. I might have made Spiral because I did not have faith in the present methods. It might have been to escape the responsibility of fighting in the present.

But I also did it because I had faith in the future of ML. A lot has happened in past six years, and the field has definitely progressed. It will progress as the years go by in the future.

10:55am. I should be making agents to improve my abilities. Not just for betting games, but for everything. For living.

11am. Once I have my first agent, I'll have my first ability. Once I have my first ability, I can consider myself not a loser.

So let me do it. Just like in the past six years, I should go forward without stopping.

And maybe if I am lucky, my ML abilities will improve along the way. There is a reason to be hopeful here. But absolutely nothing will happen if I do not put in the work.

If I stop moving forward, the Singularity will happen without me. I'll lose everything and will attain nothing.

```
// Filters the elements of an array using the function.
inl filter forall (ar : * -> * -> *) {create; set; index; length} dim {number} el. f (ar : ar dim el) =
    inl nearTo = length ar
    inl ar' : ar _ _ = create nearTo
    for (from:0 nearTo:) (fun i count => if f i then set ar' i (index ar i) . count + 1 else count) 0
    |> fun nearTo => init nearTo (index ar')
```

Wait, I am confused. Why am I passing in the index here? Damn.

```
// Filters the elements of an array using the function.
inl filter forall (ar : * -> * -> *) {create; set; index; length} dim {number} el. f (ar : ar dim el) =
    inl nearTo = length ar
    inl ar' : ar _ _ = create nearTo
    for (from:0 nearTo:) (fun i count =>
        inl x = index ar i
        if f x then set ar' i x . count + 1 else count
        ) 0
    |> fun nearTo => init nearTo (index ar')
```

This is how filter should have been done.

```
// Picks out the elements from the array.
inl choose forall (ar : * -> * -> *) {create; set; index; length} dim {number} el. f (ar : ar dim el) =
    inl nearTo = length ar
    inl ar' : ar _ _ = create nearTo
    for (from:0 nearTo:) (fun i count =>
        inl x = index ar i
        match f x with
        | Some: x => set ar' i x . count + 1
        | None => count
        ) 0
    |> fun nearTo => init nearTo (index ar')
```

Here is choose. Let me make a non-generic version.

```
// Picks out the elements from the array.
inl choose forall (ar : * -> * -> *) {create; set; index; length} dim {number} el el'. : _ -> ar dim el -> ar dim el' = choose
```

Good.

```
inl run batch_size p game =
    let rec loop (l : a u64 _) =
        inl cs,update =
            l |> am.choose function
                | Some: (Action: player_state,game_state,pid,actions,next) =>
                    Some: player_state,game_state,pid,actions
                | _ => None
            |> p
        am.mapFold (fun i => function
            | Some: (Action: _,_,_,_,next) => (Some: next (index cs i)), i+1
            | None => None, i
            ) 0 l
        |> fst |> loop |> update
    loop (am.init batch_size fun _ => Some: game pl2_init)
```

Hmmmm...no wait. The update is wrong somehow. Yeah, I need to combine what I get from loop with the terminals.

11:45am.

```
inl run batch_size p game =
    let rec loop (l : a u64 _) =
        inl cs,update =
            l |> am.choose function
                | Some: (Action: player_state,game_state,pid,actions,next) =>
                    Some: player_state,game_state,pid,actions
                | _ => None
            |> p
        let r =
            am.mapFold (fun i => function
                | Some: (Action: _,_,_,_,next) => (Some: next (index cs i)), i+1
                | None => None, i
                ) 0 l
            |> fst |> loop
            |> am.mapFold (fun i => function
                | Some: (Action: _,_,_,_,next) => (Some: next (index cs i)), i+1
                | None => None, i
                ) 0
            |> fst |> update
        am.mapFold (fun i a =>
            match a with
            | Some: (Terminal: _,_,r) => (Some: r), i
            | Some: (Action: _) => (Some: index r i), i+1
            | _ => None, i
            ) 0 l
        |> fst
    loop (am.init batch_size fun _ => Some: game pl2_init)
```

Gah, making sure that I am only touching the selected actions is driving me insane. Let me just get their indexes first. I should not have bothered with making choose. Let me just get their indexes instead and I will do my map operations over that.

12:10pm.

```

inl run batch_size p game =
    let rec loop (l : a u64 _) =
        inl action_indices : ra u64 _ = am.empty
        l |> am.iteri (fun i => function
            | Some: (Action: _) => rm.add is i
            | _ => ()
            )
        inl map_actions f : a u64 _ =
            is |> am.generic.map fun i =>
                match index l i with
                | Some: (Action: x) => f i x
                | _ => failwith "impossible"
        inl cs,update =
            map_actions fun i (player_state,game_state,pid,actions,next) =>
                player_state,game_state,pid,actions
            |> p
        inl r =
            is |> am.generic.map fun i =>
        ()
```

Gah, it is not going well. My earlier plans are all in shambles. Let me take a break here. I think I might split the rewards from the actions at every juncture.

12:30pm. Should I push the patch now that I modified the core library? Forget it, the connection has been iffy lately."

---
## [Florane/sdl_game@505b047fe1...](https://github.com/Florane/sdl_game/commit/505b047fe1cba977828d7be0b838b70d578c1dcf)
##### 2021-05-11 12:30:23 by Florane

god fucking dammit fucking delete this shit i fucking hate it

---

# [<](2021-05-10.md) 2021-05-11 [>](2021-05-12.md)

