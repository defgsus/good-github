# [<](2021-05-18.md) 2021-05-19 [>](2021-05-20.md)

3,644,898 events, 1,515,607 push events, 2,456,091 commit messages, 211,656,665 characters


## [TyLanger/Tasks@ba5fc614dd...](https://github.com/TyLanger/Tasks/commit/ba5fc614dd8ba81f39c85be3a8b0438e80a6e3e4)
##### 2021-05-19 06:45:56 by TyLanger

FewestElites, GetLucky, Temp Item Changes

New tasks: FewestElites, GetLucky
Temp item changes
instead of losing all 5 temp items at end of stage, you lose 3, then next
stage you lose the last 2.

FewestElites
Kill the fewest elites to win

GetLucky
It just picks a random player to win at the end of the stage.
Might be kinda cool if it scaled with your luck stat from clovers and
purity, but oh well.
Could also be cool if the winner gained something and everyone else lost
something. Like winner gets a clover and losers get purity. Forced purity
would be annoying.

---
## [mrakgr/The-Spiral-Language@b05f58facb...](https://github.com/mrakgr/The-Spiral-Language/commit/b05f58facbabe5a1538b58eef7ac2dd73adc08f5)
##### 2021-05-19 10:40:49 by Marko GrdiniÄ‡

"9:25am. I've been chilling for 25m and I will start soon. Watching DraQu and Zero Master inspired me to start playing Doom Eternal again. I can't find a torrent for Ancient Gods, so I am going for the original. I was surprised that it has been over a year since I last touched it. I thought it was earlier.

I really feel groggy right now.

...Ok, let me go for the videos. The way to do memories properly is my biggest priority.

Yesterday going through the thesis really made me smitten with EqProp. I do think it is the right thing to do.

Today, let me study up more on memories. It won't be as hard as yesterday. All I have to do is see what Sepp and Dmitry have to say on the subject.

https://www.youtube.com/watch?v=bsdPZJKOlQs
Modern Hopfield Networks - Dr Sepp Hochreiter

https://www.youtube.com/watch?v=k3YmWrK6wxo
Hopfield Networks in 2021 - Fireside chat between Sepp Hochreiter and Dmitry Krotov | NeurIPS 2020

https://www.youtube.com/watch?v=DKyzcbNr8WE
John Hopfield: Physics View of the Mind and Neurobiology | Lex Fridman Podcast #76

https://www.youtube.com/watch?v=_QVUyXhu59I
Large Associative Memory Problem in Neurobiology and Machine Learning - Dmitry Krotov, PhD

I'll skip the first one, and start with the second. Let me start. I said that I would watch a bit yesterday, but I did not feel like it.

9:35am. If I can stop reading the HN threads.

https://youtu.be/k3YmWrK6wxo?t=116
> Integrate associative memories into deep learning architectures

Yeah, this is exactly what I want. Simon Thorpe gave me a kick in this direction. I need to form a view on how things will go. I might not be able to do this myself, but I can speculate.

https://youtu.be/k3YmWrK6wxo?t=210

I had no idea that Modern Hopfield Networks was a paper. I'll look it up later.

https://youtu.be/k3YmWrK6wxo?t=496

I really do not see how these things are equal to each other. Well, the right most sides sure. But the energy function not so much.

9:55am. At any rate, if transformers are modern Hopfield nets, then transformers are the ideal form of memory that I've been seeking form the start.

https://youtu.be/k3YmWrK6wxo?t=1219

These references might be of interest.

https://youtu.be/k3YmWrK6wxo?t=1539

Here he is suggesting that some of the matrices might be trained using backprop and other using Hebbian learning.

https://www.youtube.com/watch?v=W-O7AZNzbzQ
DDPM - Diffusion Models Beat GANs on Image Synthesis

https://arxiv.org/abs/2102.09672
Improved Denoising Diffusion Probabilistic Models

https://arxiv.org/abs/2105.05233
Diffusion Models Beat GANs on Image Synthesis

I'll leave this for later.

11am. Actually I had to read the last one. I am surprised that this is a supervised rather than an unsupervised model. It mentions that it requires more computation than GANs too.

11:15am. https://youtu.be/k3YmWrK6wxo?t=4340

Almost done with the first video. Here he is talking about how associate memory helps with molecule synthesis by a large amount when that are few examples.

https://youtu.be/k3YmWrK6wxo?t=4500

Here he is saying that transformers can do only one kind of computation, but Hopfield nets can do more, so the mind should be open.

https://www.youtube.com/watch?v=_QVUyXhu59I
Large Associative Memory Problem in Neurobiology and Machine Learning - Dmitry Krotov, PhD

Let me move to this.

https://youtu.be/_QVUyXhu59I?t=947

If it is not biological, then how could the brain possibly make do with the classical formulation?

https://youtu.be/_QVUyXhu59I?t=1035

Apparently by assuming some higher order structure.

https://youtu.be/_QVUyXhu59I?t=1467

This theory stuff is not my thing.

https://youtu.be/_QVUyXhu59I?t=2368
> ...Indeed the system looks like a generalization of the attention mechanism in NLP system. And indeed if you provide a more flexible attention mechanism that can evolve multiple steps and do some form of pattern completion this looks like a very promising setup for improving transformer architectures.

Though the quote is catchy, he does not actually provide such a system. He also talks about training these memories using something other than backprop. These two things would be of great interest to me.

12:15pm. Let me move on.

https://www.youtube.com/watch?v=bsdPZJKOlQs
Modern Hopfield Networks - Dr Sepp Hochreiter

https://youtu.be/bsdPZJKOlQs?t=186

Looking it so far, it seems like a longer version of the fireside chat talk he gave. Let me go for it.

12:40pm. Had to take a break, but let me segue into breakfast."

---
## [Dudemanguy/mpv@4b7bf787ce...](https://github.com/Dudemanguy/mpv/commit/4b7bf787ce3016978c4b92bb9316d388448bc7ea)
##### 2021-05-19 13:46:50 by Dudemanguy

wayland: use VOCTRL_WAIT_CALLBACK for rendering

This is actually a very nice simplification that should have been
thought of years ago (sue me). In a nutshell, the story with the
wayland code is that the frame callback and swap buffer behavior doesn't
fit very well with mpv's rendering loop. It's been refactored/changed
quite a few times over the years and works well enough but things could
be better. The current iteration works with an external swapchain to
check if we have frame callback before deciding whether or not to
render. This logic was implemented in both egl and vulkan.

This does have its warts however. There's some hidden state detection
logic which works but is kind of ugly. Since wayland doesn't allow
clients to know if they are actually visible (questionable but
whatever), you can just reasonably assume that if a bunch of callbacks
are missed in a row, you're probably not visible. That's fine, but it is
indeed less than ideal since the threshold is basically entirely
arbitrary and mpv does do a few wasteful renders before it decides that
the window is actually hidden.

The biggest urk in the vo_wayland_wait_frame is the use of
wl_display_roundtrip. Wayland developers would probably be offended by
the way mpv abuses that function, but essentially it was a way to have
semi-blocking behavior needed for display-resample to work. Since the
swap interval must be 0 on wayland (otherwise it will block the entire
player's rendering loop), we need some other way to wait on vsync. The
idea here was to dispatch and poll a bunch of wayland events, wait (with
a timeout) until we get frame callback, and then wait for the compositor
to process it. That pretty much perfectly waits on vsync and lets us
keep all the good timings and all that jazz that we want for mpv. The
problem is that wl_display_roundtrip is conceptually a bad function. It
can internally call wl_display_dispatch which in certain instances,
empty event queue, will block forever. Now strictly speaking, this
probably will never, ever happen (once I was able to to trigger it by
hardcoding an error into a compositor), but ideally
vo_wayland_wait_frame should never infinitely block and stall the
player. Unfortunately, removing that function always lead to problems
with timings and unsteady vsync intervals so it survived many refactors.

Until now, of course. In wayland, the ideal is to never do wasteful
rendering (i.e. don't render if the window isn't visible). Instead of
wrestling around with egl/vulkan contexts, we can just move this logic
upwards to a higher level, vo.c itself. A new VOCTRL
(VOCTRL_WAIT_CALLBACK) is introduced and called right before the
vo->driver->draw_frame call in render_frame. If the backend in question
implements that event (wayland in this case), then it can employ it sown
custom block/wait before telling vo.c whether or not to draw.  If this
VOCTRL doesn't exist, then the call just always returns true so nothing
changes.

This slight rearrangement of the wait allows for several simplifications
to be made. Namely, wl_display_roundtrip stops being needed. Instead, we
can rely entirely on totally nonblocking calls (dispatch_pending, flush,
and so on). We still need to poll the fd here to actually get the frame
callback event from the compositor, but there's no longer any reason to
do extra waiting. As soon as we get the callback, we immediately draw.
This works quite well and has stable vsync (display-resample and audio).
Additionally, all of the logic about hidden states is no longer needed.
If vo_wayland_wait_frame times out, it's okay to assume immediately that
the window is not visible and skip rendering.

Since all of the special waiting is handled in the wayland_common code,
the external swapchain stuff implemented in egl and vulkan wayland
become completely unneeded. Revert back to the "simple" way and remove
all the cruft. We still need to do presentation statistics of course,
but the rest is not needed. In vulkan, specifically for wayland, an
extra start_frame param was implemented. Since this is not needed
anymore and likely nobody else will need such a mechanism, the entire
thing can be removed.

Unfortunately, there's one limitation on this new approach. It will only
work correctly if the compositor implements presentation time. That
means a reduced version of the old way still has to be carried around in
vo_wayland_wait_frame. So if the compositor has no presentation time,
then we are forced to use wl_display_roundtrip and juggle some funny
assumptions about whether or not the window is hidden or not. Plasma is
the only real notable compositor without presentation time at this stage
so perhaps this "legacy" mechanism could be removed in the future.

---
## [canalplus/rx-player@179eb40e05...](https://github.com/canalplus/rx-player/commit/179eb40e05df46ffe20f564b77dcf972c45b6e90)
##### 2021-05-19 14:34:23 by Paul Berberian

remove RxJS code from the transports code

After doing a proof-of-concept looking at how some parts of the code
looks like without RxJS (#916), this is a first functional proposal
which looks good enough to me to be merged.

It removes all RxJS code from the `transports` code in `src/transports`.

As a reminder, the reasons for doing this are:

  1. Observables are complicated and full of implicit behaviors
     (lazily running, sync or async, unsubscribing automatically after
     the last unsubscription etc.) which is difficult to reason about,
     especially for a newcomer.

     Things like exploiting schedulers through the `deferSubscriptions`
     util to work-around some subtle potential race-conditions, or
     merging Observables in a specific order for similar reasons, are
     ugly hacks that are difficult to explain to someone not familiar
     with that code.

     Even for us with multiple years of experience with it, we sometimes
     struggle with it.

  2. Promises, event listeners - and direct callbacks in general - are
     generally much more explicit and most developpers (at least JS/TS
     devs) are familiar with them.

  3. Call stacks are close to inexploitable when using RxJS.

  4. Promises provide async/await syntax which can improve drastically
     the readability of our async-heavy code, which for the moment
     suffer from callback hells almost everywhere.

However, I'm still not sure if this wish (getting rid of RxJS) is shared
by other maintainers and/or contributors, so it is still only a proposal.

Thoughts?

---
## [mrakgr/The-Spiral-Language@1dd5cea8cc...](https://github.com/mrakgr/The-Spiral-Language/commit/1dd5cea8cc154574173a964b00186a2b5ec76016)
##### 2021-05-19 16:32:38 by Marko GrdiniÄ‡

"1:40pm. Done with breakfast. Let me watch the video and I will do the chores.

1:55pm. https://youtu.be/bsdPZJKOlQs?t=1062

This pattern reconstruction ability is genuinely fascinating. Right now, I am thinking how I could fit such systems into a backprop like scheme without necessarily backproping through it all.

Prediction ultimately is merely reconstruction. The way a memory system would predict is to input the present as the first part, clamping the inputs to that, and extract the future based on that.

I really like my GAN idea, but I am wondering whether it would be too slow for such a thing?

2:15pm. https://youtu.be/bsdPZJKOlQs?t=1980

This is so boring. He keeps yammering on and on about proof details.

2:30pm. https://youtu.be/bsdPZJKOlQs?t=3045

I don't entirely get this, but it is important.

Energy based models really are the way to go.

And I have good understanding of how to make them modular. I've been thinking while watching these videos whether it would make sense to have the lower layers program the upper ones, and it does not.

I thought of using a Hopfield net and using the EqProp update as auxiliary storage, but I can't think of a single good way of doing this that would allow me to propagate gradient to the main network.

In contrast, I can very easily understand how to do simple hierarchical reconstruction.

My GAN idea needs some work. I think the main ingredients of an AGI system are there, but remember those memory games by Thorpe?

The kind of architectures I have in mind would not be fast enough...

Or maybe they would? I'll have to think about them more.

https://youtu.be/bsdPZJKOlQs?t=3157

Hmmm, this is supposed to replace the attention mechanism? I thought was supposed to be it.

2:50pm. https://youtu.be/bsdPZJKOlQs?t=3618

Let me take a break here.

3pm. Let me resume. 26m more of this to go.

Things are definitely forming in my mind. I understand why feedback connections are necessary. If you imagine a temporal hierarchy where the lower level layer attends to only a short timeframe, then it needs help from the upper layers in order to improve its predictions so they take in consideration the entirety of its experience.

In order to predict the next step, the memory of one's entire life is necessary.

With energy based models, those connections are implicit. You'd just run the model and let it settle to the equilibruim and this would fill in the prediction. But energy based models can be made explicit and converted into a differentible one. Rather than optimize, it would be better to have a reverse model and do it in one step.

3:10pm. I'll admit, it would be quite cool if magic of analogue hardware obliviates the need for this and end up giving the reverse model for free. I do not understand exactly how the physics of the system itself would make settling to an equilibrium extremely fast.

I can almost touch the complete system. The pieces really are here in the present.

I've been worried about memory since Thorpe's talk, but Hopfield nets do all of that reconstructive magic, and they are literally transformers. Well, transformers are a bit short - they don't have the right training procedure and the feedback connections.

3:20pm. https://youtu.be/bsdPZJKOlQs?t=3912

Here he is going to go into depth on deep learning + small datasets.

https://youtu.be/bsdPZJKOlQs?t=4957

RNNs can deal with formal languages, but transformers struggle on them.

3:50pm. Done. Let me take a break and do the chores here.

4:05pm. https://www.youtube.com/watch?v=DKyzcbNr8WE
John Hopfield: Physics View of the Mind and Neurobiology | Lex Fridman Podcast #76

Let me do the chores here. After that I'll go for this interview. After that I'll look at the Hopfield net papers.

4:25pm. The ideas are coming to me. There is nothing wrong with having a transformer like mechanism for short term memory. It certainly makes more sense than lugging a large weight matrix that changes from timestep to timestep.

But there would be nothing wrong with having a longer term hopield layer that does not change for the duration of the training episode. The reason for that is so I can backprop through its weight matrix and not have to copy it. Updating that particular weight matrix could be done in a separate phase.

4:30pm. Another idea that I am wondering - what if I fused the PRONG and Hopfield net in the linear part? Then I could keep around the same weight matrix, but just multiply the gradient through the associative memory during optimization.

What sort of effect would this have?

RL methods in particular, in the linear segment can keep track of eligibility traces. This would be something in the same vein as that.

...I have absolutely no idea whether this can or even should work. But it might be more stable, than having a large unsupervised part affecting the computation of a particular layer. It is really a natural idea when one thinks about it.

4:40pm. Forget this, I should focus on the interview. It does not really matter right now. It is not like I am going to bother implementing any of this on the GPU.

I am thinking of RNNs that do large number of internal computation steps. Having a long term working memory storage of past results could speed up that processing significantly.

https://arxiv.org/abs/2105.08050
Pay Attention to MLPs
> Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple attention-free network architecture, gMLP, based solely on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.

Twitter comment:
2017: Attention is all you need
2021: You don't need the transformers

4:40pm. https://arxiv.org/abs/2008.02217
Hopfield Networks is All You Need

I thought this paper came out years ago, but I see that its date is near the end of 2020.

I'll read it later.

https://pni.princeton.edu/john-hopfield/john-j.-hopfield-now-what

Lex mentioned this article so I paused the interview to look it up. That is how I found the `Pay Attention to MLPs` paper.

5:10pm. https://youtu.be/DKyzcbNr8WE?t=1229

He thinks it is a couple of generations till AGI.

5:30pm. If Hopfield nets are so powerful even on the linear segment that they can take in an exponential number of patterns even in the linear segment, instead of having a large replay buffer, what would happen if I used one of them as a replay buffer and sampled from them?

5:35pm. Wow, the `Hopfield Networks is All You Need` is 95 pages long.

> We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-ofthe-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers

The paper says that these new layers can be integrated in DL architectures. I should they would just be a separate linear thing.

6:15pm. Done with the interview. Let me take a break.

6:20pm. I am back. I am not sure that I got that much out of those interviews today. They were quite boring.

My next target is definitely the Hopfield nets paper. I'll leave it for tomorrow.

I never had an interest in these nets until now, mostly because Hinton said in his course that Hopfield nets can only store a limited number of examples. I heard about the paper above, but I thought it was just a theory paper proving that HF nets were transformers. So that did not catch my interest. It was not until I wasted these vids today that I realized what remarkable ability these nets have. I have to go for it.

In the future, these might be the key to making nets that have long term memory. I should read up on the subject and get my anxiety out of the way.

Eventually I will have to figure out how to integrate real life data with the self play aspects of training agents, and this will help. Most of what goes into the brain is memory.

John Hopfield made a good remark than only having feedforward computation like deep NNs have now is equivalent to having a computer that does only one clock cycle.

I need to go a bit further in my understanding.

6:30pm. Let me stop here for the day. Let me shot demons and watch anime for a while. Tomorrow, I should finish this trip down the memory lane. After that I should hopefully have enough to start programming. If I am lucky, this paper will give me some now toys to play with. Transformers are an old trick right now."

---
## [pytorch/pytorch@9354a68e7d...](https://github.com/pytorch/pytorch/commit/9354a68e7d8c4680a115b70b9b14565cd42cb03f)
##### 2021-05-19 18:27:43 by Brian Hirsh

[codegen] split out backend-specific information from NativeFunction in the model (#57361)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57361

Data model change in the codegen, which splits backend-specific information out of `NativeFunction`

### Overview
Currently in the codegen, native_functions.yaml has backend-specific information about each operator that is encoded directly into the data model, in the `NativeFunction` object. That's reasonable, since the native_functions.yaml is the source of truth for information about an operator, and the data model encodes that information into types.

Now that external backends can use the codegen though, that information is technically incomplete/inaccurate. In another PR, I tried patching the information on the `NativeFunction` object with the additional external information, by updating the `dispatch` entry to contain the external backend kernel name and dispatch key.

Instead, this PR tries to split out that information. The `NativeFunction` class contains all information about an operator from native_functions.yaml that's backend-independent and is known never to change regardless of what extra information backends provide. We also build up a backend "index", which is basically a mapping from [backend] -> [backend-specific-metadata]. Reading in an external backend yaml just involves updating that index with the new backend.

There were a few places where `NativeFunction` used the dispatch table directly, that I encoded as properties directly on the NativeFunction object (e.g. `is_abstract`). They were mostly around whether or not the operator has a composite kernel, which isn't something that's going to change for any external backends.

This has a few advantages:
- We can more easily re-use the existing logic in `native_function.py` and `register_dispatch_key.py` for both native and external backends, since they both involve a NativeFunction + a particular backend index
- The data in the data model will be the same regardless of how the codegen is run. Running the codegen with a new external backend doesn't change the data inside of NativeFunction or an existing backend index. It just adds a new index for that backend.
- There are several of codegen areas that don't care about backend-specific information: mostly the tracing and autograd codegen. We can reason about the codegen there more easily, knowing that backend-specific info is entirely uninvolved.

An alternative to this split would be to augment the NativeFunction objects with external backend information at the time that we create them. So the external codegen could read both native_functions.yaml and the external backend's yaml at the same time, and construct a NativeObject with a full dispatch table (including the XLA entry), and the correct setting of structured (taking into account both yamls). One disadvantage to this approach is that NativeFunction objects now contain different stuff depending on how you ran the codegen, and you have to make sure that any changes to the codegen can properly handle all the different variants.

### Data Model Changes
Removed 3 classes, which are used by the external codegen:
- ExternalBackendFunction
- ExternalBackendFunctionsGroup
- ExternalBackendMetadata

And added two new ones:
- BackendIndex
- BackendMetadata

`BackendIndex` contains any info that's specific to that backend, plus a mapping from operator names to backend specific metadata about the operator. One example of backend-specific info that's not operator-dependent is the fact that XLA prefers to implement functional kernels instead of out kernels (and so when they eventually mark an op as structured, they're going to mark the functional op and not the out op).

`BackendMetadata` contains info specific to an (operator, backend) pair. Right now, that's just (a) the name of the kernel, and (b) whether or not that operator is structured.

### Questions
I wanted to get this PR up earlier so I could get feedback, but there are a few things I want to call out:

**Dealing with `structured`.**
This PR separates out the notion of `structured` into two bits of information:
- Does [operator] have a meta() function. This is backend-agnostic, and is represented by the `structured` property on `NativeFunction`, same as before. This is used, e.g., to decide what signatures to add to `MetaFunctions.h`.
- Does [operator, backend] have an impl() function. This is backend dependent; even though technically all in-tree backends are forced to write impl() functions for an operator when we port the op to structured in native_functions.yaml, out-of-tree backends can decide to opt in independently. This is represented as a property on `BackendMetadata`. This is used in most other cases, e.g. in `RegisterDispatchKey` when we're deciding whether or not to gen a structured or unstructured wrapper.

I also baked `is_structured_dispatch_key` directly into each BackendIndex. So for operators marked "structured" in native_functions.yaml, their corresponding CPU/CUDA BackendIndex entries will be marked structured, and all others (except for potentially external backends) will not.

I ended up trying to deal with `structured` in this change since it's technically backend dependent (XLA can opt kernels into structured separately from in-tree ops), but that may have been too ambitious: it's technically not relevant until we actually add support for structured external kernels. If it's not clear that this is the right path for dealing with structured and we want to push that off, I'm fine with backing out the bits of this PR that make `structured` backend-dependent. I don't see anything *too* controversial related to structured in the change, but I tried to call out any areas in the comments

**Localizing the fact that external backends follow Dispatcher convention.**
Another thing that's sort of backend specific that I didn't totally address in this PR is the fact the fact that in-tree backends follow the Native API while external backends follow the Dispatcher API. I painted over that in `native_functions.py` by adding a helper, `kernel_signature`, that takes in a native function and gives you the "correct" signature for the specified backend- NativeSignature for in-tree backends, and DispatcherSignature for out-of-tree backends. In order to make that fully useable though, we'll need `NativeSignature` and `DispatcherSignature` to have matching interfaces. I didn't bother with that in this PR, which is why `gen_external_aten_fallbacks.py` still has a bunch of direct references to the dispatcher API. Thinking of adding it in a later PR but wanted to see if anyone has other opinions.

Maybe `is_external()` shouldn't even be a property on the BackendMetadata, and anything the codegen does that requires asking for that information should just be better abstracted away.

**Thoughts on the `BackendIndex` / `BackendMetadata` breakdown.**
One thing that's annoying right now is that to query for various pieces of metadata, you call helper functions like `backend_index.structured(f)`, which queries that particular backend and tells you if that specific NativeFunctionGroup is structured for that backend. It has to return an `Optional[bool]` though, since you have to handle the case where that operator doesn't have a kernel for that backend at all. So users of those helpers end up with a bunch of optionals that they need to unpack, even if they know at some point that the result isn't None. I think it would be easier instead to just store the NativeFunction object as a field directly on the BackendMetadata. Curious if there are any other opinions on a better way to model it though.

Test Plan: Imported from OSS

Reviewed By: navahgar

Differential Revision: D28474362

Pulled By: bdhirsh

fbshipit-source-id: 41a00821acf172467d764cb41e771e096542f661

---
## [newstools/1970-national-mirror-nigeria@0d4d337e14...](https://github.com/newstools/1970-national-mirror-nigeria/commit/0d4d337e14712983253f1d03b8892539760d24aa)
##### 2021-05-19 19:10:04 by Billy Einkamerer

Created Text For URL [www.nationalmirroronline.net/jason-derulo-welcomes-a-baby-boy-with-his-girlfriend-jena-frumes-7665.html]

---
## [GDACollab/SeaStarCrossedLovers@d8b20905f2...](https://github.com/GDACollab/SeaStarCrossedLovers/commit/d8b20905f21838663f2ce63f63d4cd35934a617b)
##### 2021-05-19 19:11:45 by ambiguousname

Fixed the bug for blocks not being deleted. I think.

I spent way too much time on this.

The most interesting thing about King Charles the First,
Is that he was 5'6" at the start of his reign,
But only 4'8" tall at the end of it.

Because of

Oliver Cromwell, Lord Protector of England (Puritan)
Born in 1599, died in 1658 (September).
Was at first (only) MP for Huntingdon (but then)
He led the Iron Side Cavalry at Marston Moor in 1644 and won.
Then he founded the New Model Army, and praise be, beat the Cavaliers at Naseby.
And the King fled up North, like an axe, to the Scots.

But under the terms of John Pimms' 'Solomn League and Covenant',
The Scots handed King Charles I over to

Oliver Cromwell, Lord Protector of England (and his warts)
Born in 1599, died in 1658 (September)
But, alas (Oi Vay!), disagreement then broke out (between)
The Presbyterian Parliament and the military
Who meant to have an independent bent and so,
The second civil war broke out and the round head ranks
Faced the cavaliers at Preston, Lancs.,
And the King lost again, silly thing (stupid King).

And Cromwell sent Colonel Pride
To purge the House of Commons of the Presbyterian Royalists,
Leaving behind only the Rump Parliament

Which appointed a High Court at Westminster Hall
To indict Charles I for Tyranny (gasp).
Charles was sentenced to death
Even though he refused to accept that the court had
Jurisdiction (say goodbye to his head).
Poor King Charles laid his head on the block (January 1649)
Down came the axe and

In the silence that followed,
The only sound that could be heard was a solitary giggle from.

Oliver Cromwell, Lord Protector of England (Ole)
Born in 1599, died in 1658 (September)
Then he smashed (Ireland), set up the Commonwealth (and more!).
He crushed the Scots at Worcester, and beat the Dutch at sea in 1653, and then

He dissolved the Rump Parliament,
And with Lambert's consent wrote the instrument of Government
Under which Oliver was Protector at last, The End!

---
## [Dudemanguy/mpv@18ff9c5a22...](https://github.com/Dudemanguy/mpv/commit/18ff9c5a225c18fe406660bbeed03683c69a0637)
##### 2021-05-19 19:14:38 by Dudemanguy

wayland: simplify render loop

This is actually a very nice simplification that should have been
thought of years ago (sue me). In a nutshell, the story with the
wayland code is that the frame callback and swap buffer behavior doesn't
fit very well with mpv's rendering loop. It's been refactored/changed
quite a few times over the years and works well enough but things could
be better. The current iteration works with an external swapchain to
check if we have frame callback before deciding whether or not to
render. This logic was implemented in both egl and vulkan.

This does have its warts however. There's some hidden state detection
logic which works but is kind of ugly. Since wayland doesn't allow
clients to know if they are actually visible (questionable but
whatever), you can just reasonably assume that if a bunch of callbacks
are missed in a row, you're probably not visible. That's fine, but it is
indeed less than ideal since the threshold is basically entirely
arbitrary and mpv does do a few wasteful renders before it decides that
the window is actually hidden.

The biggest urk in the vo_wayland_wait_frame is the use of
wl_display_roundtrip. Wayland developers would probably be offended by
the way mpv abuses that function, but essentially it was a way to have
semi-blocking behavior needed for display-resample to work. Since the
swap interval must be 0 on wayland (otherwise it will block the entire
player's rendering loop), we need some other way to wait on vsync. The
idea here was to dispatch and poll a bunch of wayland events, wait (with
a timeout) until we get frame callback, and then wait for the compositor
to process it. That pretty much perfectly waits on vsync and lets us
keep all the good timings and all that jazz that we want for mpv. The
problem is that wl_display_roundtrip is conceptually a bad function. It
can internally call wl_display_dispatch which in certain instances,
empty event queue, will block forever. Now strictly speaking, this
probably will never, ever happen (once I was able to to trigger it by
hardcoding an error into a compositor), but ideally
vo_wayland_wait_frame should never infinitely block and stall the
player. Unfortunately, removing that function always lead to problems
with timings and unsteady vsync intervals so it survived many refactors.

Until now, of course. In wayland, the ideal is to never do wasteful
rendering (i.e. don't render if the window isn't visible). Instead of
wrestling around with hidden states and possible missed vblanks, let's
rearrange the wayland rendering logic so we only ever draw a frame when
the frame callback is returned to use (within a reasonable timeout to
avoid blocking forever).

This slight rearrangement of the wait allows for several simplifications
to be made. Namely, wl_display_roundtrip stops being needed. Instead, we
can rely entirely on totally nonblocking calls (dispatch_pending, flush,
and so on). We still need to poll the fd here to actually get the frame
callback event from the compositor, but there's no longer any reason to
do extra waiting. As soon as we get the callback, we immediately draw.
This works quite well and has stable vsync (display-resample and audio).
Additionally, all of the logic about hidden states is no longer needed.
If vo_wayland_wait_frame times out, it's okay to assume immediately that
the window is not visible and skip rendering.

Unfortunately, there's one limitation on this new approach. It will only
work correctly if the compositor implements presentation time. That
means a reduced version of the old way still has to be carried around in
vo_wayland_wait_frame. So if the compositor has no presentation time,
then we are forced to use wl_display_roundtrip and juggle some funny
assumptions about whether or not the window is hidden or not. Plasma is
the only real notable compositor without presentation time at this stage
so perhaps this "legacy" mechanism could be removed in the future.

---
## [pytorch/pytorch@09fdb7cb65...](https://github.com/pytorch/pytorch/commit/09fdb7cb654dc52ad62e042c12a89cd6ed3462b1)
##### 2021-05-19 20:31:03 by Brian Hirsh

Update base for Update on "add a boxed CPU fallback kernel"

This PR replaces the existing code-generated CPU fallback kernels that XLA uses with a single boxed CPU fallback.

Current state: there are a couple different design ideas that I want to point out, but the logic for the actually kernel is mostly done and passing tests.

### Design

To preface, I'm not 100% tied to the current design and I'm putting the PR up now for opinions and totally open to alternatives, some of which I listed below. Actually after writing this description, I'm leaning toward the following changes:
* Confirm whether or not we can remove all C++ logging info directly in the yaml.


**Current Design**

All of the CPU fallback codegen is deleted. In its place, XLA (and other external backends, later) can choose to opt into a CPU fallback by adding the following code in a C++ file. I have an corresponding [xla-side PR with the xla changes](https://github.com/pytorch/xla/pull/2945/files#diff-1a005c10039f0cb11130a3b740f5de716d2f10acaea121017016025861886798R1).

There's no actual requirement to split up the code into a .h and .cpp file, but that's necessary in the XLA case because they sometimes need to call the fallback directly from their handcrafted kernels.

```
// xla_cpu_fallback.h
#include <ATen/native/CPUFallback.h>
...
void xla_cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack);
...
```
```
// xla_cpu_fallback.cpp
#include "torch_xla/csrc/aten_cpu_fallback.h"
...
void xla_cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
  // Do custom logging here
  ...
  // Call the actual boxed CPU fallback.
  at::native::cpu_fallback(op, stack);
}

TORCH_LIBRARY_IMPL(_, XLA, m) {
  m.fallback(torch::CppFunction::makeFromBoxedFunction<&xla_cpu_fallback>());
}
```

Now that the fallback is exposed in the backend, they can call it directly. Doing so requires converting from an unboxed to a boxed context, which we provide a utility function before. E.g.:
```
#include <ATen/native/CPUFallback.h>

at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::native::call_fallback_fn<&xla_cpu_fallback, decltype(at::addmm)>::call("aten::addmm", self, mat1, mat2, beta, alpha);
  }
  ...
}
```

That `decltype(at::addmm)` logic isn't actually used everywhere in the xla-side PR yet, since you hit issues with overloads. I could use it everywhere once #58092 lands.

**Alternatives: The API for calling the CPU fallback directly is ugly, can we make it nicer?**
We could change the api to use `at::redispatch`, which would make it look something like this:
```
at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::redispatch::addmm(c10::DispatchKeySet(c10::DispatchKey::CPUFallback), self, mat1, mat2, beta, alpha);
  }
  ...
}
```
Which definitely feels cleaner, but also requires adding a new DispatchKey just for this use case. Conditionally calling the CPU fallback doesn't sound like a hugely important use case, so I don't know if giving up one of our 64 dispatch key slots is worth the API improvement. Totally open to other opinions though!


Another more mild improvement that would avoid having to pass operator string names (including overloads) around would be to codegen (yet another) namespaced API. Something like this:
```
at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::fallback::addmm<&xla_cpu_fallback>(self, mat1, mat2, beta, alpha);
  }
  ...
}
```

Writing that out actually I actually like it more (I think it'll let us get rid of `decltype(...)`). Maybe that is nice enough to warrant a new codegen API - I haven't tried adding that yet, but if people like it I'm happy to try it out.

**More alternatives**
The current design also involves the backend manually writing and registering the boxed fallback themselves, but an alternative would be for us to do it in codegen too: they would just need to pass in all of the C++ logging that they want done in the fallback, directly through the yaml. The main downsides:
* Backend code that wants to call the fallback needs to abide by whatever convention our codegen uses to name the generated boxed fallback.
* Passing custom C++ logging through yaml is just more fragile: right now xla uses an `iostream` to log each tensor arg in the operator, so we'd have to either force other backends into the same convention or figure something else out later.

To be fair, we actually already do that: XLA has custom per-tensor-arg logging for all of the generated `out` wrappers in the codegen, which we do by passing their C++ logging info through the yaml. This seems unnecessary though, since `out` wrappers just call into a functional kernel, which is hand written with its own custom logging. So my take is: try to remove custom C++ logging from the yaml, and if it turns out to be really necessary, then we may as well take advantage of that to codegen the fallback.

### Performance impact

While ops that fall back to CPU aren't exactly hot path, we probably don't want to use a boxed fallback if it turns out to be an absolute perf killer.

I ran my benchmarks using callgrind, benchmarking both `at::add` and `at::add_out` run on XLA. My callgrind benchmark for `at::add` can be found here (the add_out benchmark looks basically the same): https://www.internalfb.com/phabricator/paste/view/P415418587. I created the benchmark by hacking the existing xla C++ test build scripts and throwing in a reference to callgrind.

I also attached the full callgrind output for each benchmark; the full output is actually pretty noise and hard to parse, but I focused on everything underneath the `at::add()` call in the output, which was much more stable. My guess is that it's due to some heavyweight async startup processing that xla does.

`at::add`:
before: 88,505,130 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415421001
after: 102,185,654 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415421273
delta: ~15.5% increase

`at::add_out`:
before: 63,897,395 instructions. Full output: https://www.internalfb.com/intern/everpaste/?handle=GBrrKwtAPlix9wUEAOZtrFXpdO5UbsIXAAAz
after: 73,170,346 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415423227
delta: ~14.5% increase

High level takeaway: A framework overhead increase of 10-20% doesn't seem too horrible for the CPU fallback use case.

For structured, functional ops that requires a CPU fallback, we're actually in an unfortunate situation: we're doing even more work than necessary. Our codegen automatically creates a `CompositeExplicitAutograd` kernel which calls into the `out` operator. So the extra work that we end up doing is:
* An extra dispatcher hop: (at::add -> CompositeExplicitAutograd -> CPUFallback -> at::native::add) instead of (at::add -> CPUFallback -> at::native::add)
* An unnecessary tensor allocation (the CompositeExplicitAutograd kernel uses at::empty() to create an output tensor, which is immediately overwritten by the CPU fallback)
* An unnecessary meta() call (the CompositeExplicitAutograd kernel calls it to create the output tensor, but we call it again in the CPU kernel).
* unboxing->boxing->unboxing logic (this is the only strictly required piece)

There are definitely ways to avoid the unnecessary work explained above: one would be to give the boxed fallback higher priority than composite keys (there's [an issue for it here](https://github.com/pytorch/pytorch/issues/55104)), and codegen fallthroughs for all composite ops. It'll require more infra to set up, so I see it as more of a perf knob that we can apply if we need it later.

Unfortunately I couldn't dig much deeper into the differences aside from the aggregate change in instructions, since it looks like callgrind fudged some of the instruction attribution (`at::to_cpu` takes up a ton of instructions, but I don't see any attribution for the `at::native::add` kernel anywhere).




[ghstack-poisoned]

---
## [bsdjhb/freebsd@9a2fac6ba6...](https://github.com/bsdjhb/freebsd/commit/9a2fac6ba65fbd14d37ccedbc2aec27a190128ea)
##### 2021-05-19 22:03:41 by Kirk McKusick

Fix handling of embedded symbolic links (and history lesson).

The original filesystem release (4.2BSD) had no embedded sysmlinks.
Historically symbolic links were just a different type of file, so
the content of the symbolic link was contained in a single disk block
fragment. We observed that most symbolic links were short enough that
they could fit in the area of the inode that normally holds the block
pointers. So we created embedded symlinks where the content of the
link was held in the inode's pointer area thus avoiding the need to
seek and read a data fragment and reducing the pressure on the block
cache. At the time we had only UFS1 with 32-bit block pointers,
so the test for a fastlink was:

	di_size < (NDADDR + NIADDR) * sizeof(daddr_t)

(where daddr_t would be ufs1_daddr_t today).

When embedded symlinks were added, a spare field in the superblock
with a known zero value became fs_maxsymlinklen. New filesystems
set this field to (NDADDR + NIADDR) * sizeof(daddr_t). Embedded
symlinks were assumed when di_size < fs->fs_maxsymlinklen. Thus
filesystems that preceeded this change always read from blocks
(since fs->fs_maxsymlinklen == 0) and newer ones used embedded
symlinks if they fit. Similarly symlinks created on pre-embedded
symlink filesystems always spill into blocks while newer ones will
embed if they fit.

At the same time that the embedded symbolic links were added, the
on-disk directory structure was changed splitting the former
u_int16_t d_namlen into u_int8_t d_type and u_int8_t d_namlen.
Thus fs_maxsymlinklen <= 0 (as used by the OFSFMT() macro) can
be used to distinguish old directory formats. In retrospect that
should have just been an added flag, but we did not realize we
needed to know about that change until it was already in production.

Code was split into ufs/ffs so that the log structured filesystem could
use ufs functionality while doing its own disk layout. This meant
that no ffs superblock fields could be used in the ufs code. Thus
ffs superblock fields that were needed in ufs code had to be copied
to fields in the mount structure. Since ufs_readlink needed to know
if a link was embedded, fs_maxlinklen gets copied to mnt_maxsymlinklen.

The kernel panic that arose to making this fix was triggered when a
disk error created an inode of type symlink with no allocated data
blocks but a large size. When readlink was called the uiomove was
attempted which segment faulted.

static int
ufs_readlink(ap)
	struct vop_readlink_args /* {
		struct vnode *a_vp;
		struct uio *a_uio;
		struct ucred *a_cred;
	} */ *ap;
{
	struct vnode *vp = ap->a_vp;
	struct inode *ip = VTOI(vp);
	doff_t isize;

	isize = ip->i_size;
	if ((isize < vp->v_mount->mnt_maxsymlinklen) ||
	    DIP(ip, i_blocks) == 0) { /* XXX - for old fastlink support */
		return (uiomove(SHORTLINK(ip), isize, ap->a_uio));
	}
	return (VOP_READ(vp, ap->a_uio, 0, ap->a_cred));
}

The second part of the "if" statement that adds

	DIP(ip, i_blocks) == 0) { /* XXX - for old fastlink support */

is problematic. It never appeared in BSD released by Berkeley because
as noted above mnt_maxsymlinklen is 0 for old format filesystems, so
will always fall through to the VOP_READ as it should. I had to dig
back through `git blame' to find that Rodney Grimes added it as
part of ``The big 4.4BSD Lite to FreeBSD 2.0.0 (Development) patch.''
He must have brought it across from an earlier FreeBSD. Unfortunately
the source-control logs for FreeBSD up to the merger with the
AT&T-blessed 4.4BSD-Lite conversion were destroyed as part of the
agreement to let FreeBSD remain unencumbered, so I cannot pin-point
where that line got added on the FreeBSD side.

The one change needed here is that mnt_maxsymlinklen is declared as
an `int' and should be changed to be `u_int64_t'.

This discovery led us to check out the code that deletes symbolic
links. Specifically

	if (vp->v_type == VLNK &&
	    (ip->i_size < vp->v_mount->mnt_maxsymlinklen ||
	     datablocks == 0)) {
		if (length != 0)
			panic("ffs_truncate: partial truncate of symlink");
		bzero(SHORTLINK(ip), (u_int)ip->i_size);
		ip->i_size = 0;
		DIP_SET(ip, i_size, 0);
		UFS_INODE_SET_FLAG(ip, IN_SIZEMOD | IN_CHANGE | IN_UPDATE);
		if (needextclean)
			goto extclean;
		return (ffs_update(vp, waitforupdate));
	}

Here too our broken symlink inode with no data blocks allocated
and a large size will segment fault as we are incorrectly using the
test that we have no data blocks to decide that it is an embdedded
symbolic link and attempting to bzero past the end of the inode.
The test for datablocks == 0 is unnecessary as the test for
ip->i_size < vp->v_mount->mnt_maxsymlinklen will do the right
thing in all cases.

The test for datablocks == 0 was added by David Greenman in this commit:

Author: David Greenman <dg@FreeBSD.org>
Date:   Tue Aug 2 13:51:05 1994 +0000

    Completed (hopefully) the kernel support for old style "fastlinks".

    Notes:
	svn path=/head/; revision=1821

I am guessing that he likely earlier added the incorrect test in the
ufs_readlink code.

I asked David if he had any recollection of why he made this change.
Amazingly, he still had a recollection of why he had made a one-line
change more than twenty years ago. And unsurpisingly it was because
he had been stuck between a rock and a hard place.

FreeBSD was up to 1.1.5 before the switch to the 4.4BSD-Lite code
base. Prior to that, there were three years of development in all
areas of the kernel, including the filesystem code, from the combined
set of people including Bill Jolitz, Patchkit contributors, and
FreeBSD Project members. The compatibility issue at hand was caused
by the FASTLINKS patches from Curt Mayer. In merging in the 4.4BSD-Lite
changes David had to find a way to provide compatibility with both
the changes that had been made in FreeBSD 1.1.5 and with 4.4BSD-Lite.
He felt that these changes would provide compatibility with both systems.

In his words:
``My recollection is that the 'FASTLINKS' symlinks support in
FreeBSD-1.x, as implemented by Curt Mayer, worked differently than
4.4BSD. He used a spare field in the inode to duplicately store the
length. When the 4.4BSD-Lite merge was done, the optimized symlinks
support for existing filesystems (those that were initialized in
FreeBSD-1.x) were broken due to the FFS on-disk structure of
4.4BSD-Lite differing from FreeBSD-1.x. My commit was needed to
restore the backward compatibility with FreeBSD-1.x filesystems.
I think it was the best that could be done in the somewhat urgent
circumstances of the post Berkeley-USL settlement. Also, regarding
Rod's massive commit with little explanation, some context: John
Dyson and I did the initial re-port of the 4.4BSD-Lite kernel to
the 386 platform in just 10 days. It was by far the most intense
hacking effort of my life. In addition to the porting of tons of
FreeBSD-1 code, I think we wrote more than 30,000 lines of new code
in that time to deal with the missing pieces and architectural
changes of 4.4BSD-Lite. We didn't make many notes along the way.
There was a lot of pressure to get something out to the rest of the
developer community as fast as possible, so detailed discrete commits
didn't happen - it all came as a giant wad, which is why Rod's
commit message was worded the way it was.''

Reported by:  Chuck Silvers
Tested by:    Chuck Silvers
History by:   David Greenman Lawrence
MFC after:    1 week
Sponsored by: Netflix

---

# [<](2021-05-18.md) 2021-05-19 [>](2021-05-20.md)

