# [<](2021-05-18.md) 2021-05-19 [>](2021-05-20.md)

3,644,898 events, 1,515,607 push events, 2,456,091 commit messages, 211,656,665 characters


## [MIllhouse36/portfolio_react](https://github.com/MIllhouse36/portfolio_react)@[c11a549eea...](https://github.com/MIllhouse36/portfolio_react/commit/c11a549eeae373f4ecc9bb83d9de168db94ef78a)
#### Wednesday 2021-05-19 00:00:45 by Kevin

Uninstalled that bullshit ass react router bootstrap that shit was booty!

---
## [Skyrat-SS13/Skyrat-tg](https://github.com/Skyrat-SS13/Skyrat-tg)@[de46177cf7...](https://github.com/Skyrat-SS13/Skyrat-tg/commit/de46177cf76497cc12810fbe83b5c88b7f96ddc9)
#### Wednesday 2021-05-19 00:01:46 by Gandalf

MOVES ALL FUCKING CLOTHING ICONS TO MASTER FILES AND BANS THE USE OF CLOTHING MODULE ICON FILES - [DEVCON 1] (#5781)

* FUCK YOU

* Update readme.md

---
## [GarrettGunnell/World-Generator](https://github.com/GarrettGunnell/World-Generator)@[0e4fd97c1e...](https://github.com/GarrettGunnell/World-Generator/commit/0e4fd97c1e8dd64bbf3b9ff39a6517cad6d5f207)
#### Wednesday 2021-05-19 04:57:09 by Garrett Gunnell

finally fix these god awful shadows in unity holy shit

---
## [symfony/symfony](https://github.com/symfony/symfony)@[69a0b29fab...](https://github.com/symfony/symfony/commit/69a0b29fab688f4a34ed59fa51fe77fc2d0f1ef9)
#### Wednesday 2021-05-19 07:46:48 by Fabien Potencier

feature #41175 [Security] [RememberMe] Add support for parallel requests doing remember-me re-authentication (Seldaek)

This PR was squashed before being merged into the 5.3-dev branch.

Discussion
----------

[Security] [RememberMe] Add support for parallel requests doing remember-me re-authentication

| Q             | A
| ------------- | ---
| Branch?       | 5.x
| Bug fix?      | yes
| New feature?  | yes ish <!-- please update src/**/CHANGELOG.md files -->
| Deprecations? | no <!-- please update UPGRADE-*.md and src/**/CHANGELOG.md files -->
| Tickets       | Fix #40971, Fix #28314, Fix #18384
| License       | MIT
| Doc PR        | symfony/symfony-docs#... <!-- required for new features -->

This is a possible implementation to gather feedback mostly..

`TokenVerifierInterface` naming is kinda bad perhaps.. But my goal would be to merge it in TokenProviderInterface for 6.0 so it's not so important. Not sure if/how to best indicate this in terms of deprecation notices.

Anyway wondering if this would be an acceptable implementation (ideally in an application I would probably override the new methods from DoctrineTokenProvider to something like this which is less of a hack and does expiration properly:

```php
    public function verifyToken(PersistentTokenInterface $token, string $tokenValue)
    {
        if (hash_equals($token->getTokenValue(), $tokenValue)) {
            return true;
        }

        if (!$this->cache->hasItem('rememberme-' . $token->getSeries())) {
            return false;
        }

        /** `@var` CacheItem $item */
        $item = $this->cache->getItem('rememberme-' . $token->getSeries());
        $oldToken = $item->get();

        return hash_equals($oldToken, $tokenValue);
    }

    public function updateExistingToken(PersistentTokenInterface $token, string $tokenValue, \DateTimeInterface $lastUsed): void
    {
        $this->updateToken($token->getSeries(), $tokenValue, $lastUsed);

        /** `@var` CacheItem $item */
        $item = $this->cache->getItem('rememberme-'.$token->getSeries());
        $item->set($token->getTokenValue());
        $item->expiresAfter(60);
        $this->cache->save($item);
    }
```

If you think it'd be fine to require optionally the cache inside DoctrineTokenProvider to enable this feature instead of the hackish way I did it, that'd be ok for me too.

The current `DoctrineTokenProvider` implementation of `TokenVerifierInterface` relies on the lucky fact that series are generated using `base64_encode(random_bytes(64))` which always ends in the `==` padding of base64, so that allowed me to store an alternative token value temporarily by replacing `==` with `_`.

Alternative implementation options:

1. Inject cache in `DoctrineTokenProvider` and do a proper implementation (as shown above) that way
2. Do not implement at all in `DoctrineTokenProvider` and let users who care implement this themselves.
3. Implement as a new `token_verifier` option that could be configured on the `firewall->remember_me` key so you can pass an implementation if needed, and possibly ship a default one using cache that could be autoconfigured
4. Add events that allow modifying the token to be verified, and allow receiving the newly updated token incl series, instead of TokenVerifierInterface, but then we need to inject a dispatcher in RememberMeAuthenticator.

`@chalasr` `@wouterj` sorry for the long description but in the hope of getting this included in 5.3.0, if you can provide guidance I will happily work on this further tomorrow to try and wrap it up ASAP.

Commits
-------

1992337d87 [Security] [RememberMe] Add support for parallel requests doing remember-me re-authentication

---
## [NikoTheNeko/Seventino-Midnight-Cafe](https://github.com/NikoTheNeko/Seventino-Midnight-Cafe)@[f0933a3845...](https://github.com/NikoTheNeko/Seventino-Midnight-Cafe/commit/f0933a38452a0bfb0dd27a735193f9c916258971)
#### Wednesday 2021-05-19 10:07:26 by Nikolas Sanchez

Made the chocolate syrup consistent & added automaically adjusted goal bars

Kinda pog and kinda snack.

Chocolate syrup is now consistent across the syrups, this hopefully would lead to less confusion because of consistency. An arrow was also added so that way we can keep track of shit so the player knows which thing they're interacting with.

Goal bars are also now implemented so that way you can actually see what your goal is. Instead of going "wow they all share the same kinda coffee uhh ðŸ¥´ðŸ¥´ðŸ¥´ basic ass" players will now have to complete different thigns.

THIS ALSO ALLOWS PLAYERS TO PROGRESS PASS QUEST 1 SO YA KNO WE CAN FINALLY TEST THAT YEET!!!

---
## [cineafx/arma-artillery-rangetable-creator](https://github.com/cineafx/arma-artillery-rangetable-creator)@[0e0868e7f8...](https://github.com/cineafx/arma-artillery-rangetable-creator/commit/0e0868e7f8c3cbd1703e1795f7bdad4ab49de34d)
#### Wednesday 2021-05-19 10:13:13 by Karl

Revert "Revert "I fucking hate nginx config bullshit""

This reverts commit 9a16b95d49d47c1563ba32e91c07491b03c72436.

---
## [samuelunderhill12/Report-](https://github.com/samuelunderhill12/Report-)@[96dbd4fe59...](https://github.com/samuelunderhill12/Report-/commit/96dbd4fe59e5945c295910847c3776ff7d18aae8)
#### Wednesday 2021-05-19 10:38:19 by samuelunderhill12

Adding Reflection report here too, as it may be easier to read

This semester is my first year of my degree, and as this has been one of my first classes in the faculty of Engineering and Information Technology, it has been interesting. For my groups report, my contribution was the background research and context. I found this interesting as I enjoyed learning about how to roll out an effective vaccine. It also helped my broader knowledge in the area as I did not know about the complexities involved in the strategies for vaccine implementation. I also enjoyed this as doing the more technical aspects of the assignment would have not been my strongpoint.  

 

The rest of the assignment was split between my fellow group members. Denise was in charge of the introduction of our report and explaining the 3 models we had selected to look at our problem: Agent Based Models, Alternate Path Models, and Cheapest Network Model. Vinh identified the current models being used and discussed how they did not fully address or solve the problem. And finally, Arjun described and illustrated the data sources for our model and explained how each of the 3 models will be applied to our problem.  

The majority of the coding element of the assignment was completed by Arjun and Vinh, as they decided they both wanted to do the code only option. They did endure the most of it, while Denise and I offered support where we could.  

 

The positives I learnt from this project were: teamwork seriously does make the dream work, the knowledge I gained about the concept of modelling and â€¦. |    Our team was at a serious disadvantage at the start, because it took us ages to get organized and catch up as a group. This was due to many varied reasons, such as Vinh conducting his learning from distance, and the rest of us not having any contact with each other apart from our time from the Zooms. However, through teamwork and communication we passed this adversity, fairly divvying up the work. This landed us an exceedingly high mark, in our pre-submission. This showed me how effective teamwork is, when used. Despite the negative outcomes we faced. The knowledge I gained on modelling from this assignment was hugely positive. While most of the knowledge I have, has been learnt from the lectures. This consolidated it further, allowing me to put it into practice.  

 

 

If I was faced with a real-world problem, and had to use modelling to solve it, I would start by analyzing the problem, by researching its background and context, just like I have done in this assignment.  This would allow me to ascertain the complexities of the problem beforehand. Which is vital when deciding what models I would use to solve them because the implementation of the most effective model depends on the problem, and if I didnâ€™t understand the problem, it will be impossible to implement the most effective solution. For example, if the problem is complex with many different contributing non-related factors, I would use Agent Based Modelling, as it allows the thinker to compare the different traits of the thing being modelled. However, if the problem is not as complex and the cause of the problem is simpler, a model such as the Alternate Paths Model, where only one trait is considered in the model. I then would make a prototypes and collect data.

---
## [nuke-ops/Nostra-13](https://github.com/nuke-ops/Nostra-13)@[90624aab0b...](https://github.com/nuke-ops/Nostra-13/commit/90624aab0b8c4cc59a84490714fd06984d67a7aa)
#### Wednesday 2021-05-19 10:39:46 by AnonChangechong

Merge pull request #144 from nuke-ops/IHateYouByond

Fuck you too byond

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[b05f58facb...](https://github.com/mrakgr/The-Spiral-Language/commit/b05f58facbabe5a1538b58eef7ac2dd73adc08f5)
#### Wednesday 2021-05-19 10:40:49 by Marko GrdiniÄ‡

"9:25am. I've been chilling for 25m and I will start soon. Watching DraQu and Zero Master inspired me to start playing Doom Eternal again. I can't find a torrent for Ancient Gods, so I am going for the original. I was surprised that it has been over a year since I last touched it. I thought it was earlier.

I really feel groggy right now.

...Ok, let me go for the videos. The way to do memories properly is my biggest priority.

Yesterday going through the thesis really made me smitten with EqProp. I do think it is the right thing to do.

Today, let me study up more on memories. It won't be as hard as yesterday. All I have to do is see what Sepp and Dmitry have to say on the subject.

https://www.youtube.com/watch?v=bsdPZJKOlQs
Modern Hopfield Networks - Dr Sepp Hochreiter

https://www.youtube.com/watch?v=k3YmWrK6wxo
Hopfield Networks in 2021 - Fireside chat between Sepp Hochreiter and Dmitry Krotov | NeurIPS 2020

https://www.youtube.com/watch?v=DKyzcbNr8WE
John Hopfield: Physics View of the Mind and Neurobiology | Lex Fridman Podcast #76

https://www.youtube.com/watch?v=_QVUyXhu59I
Large Associative Memory Problem in Neurobiology and Machine Learning - Dmitry Krotov, PhD

I'll skip the first one, and start with the second. Let me start. I said that I would watch a bit yesterday, but I did not feel like it.

9:35am. If I can stop reading the HN threads.

https://youtu.be/k3YmWrK6wxo?t=116
> Integrate associative memories into deep learning architectures

Yeah, this is exactly what I want. Simon Thorpe gave me a kick in this direction. I need to form a view on how things will go. I might not be able to do this myself, but I can speculate.

https://youtu.be/k3YmWrK6wxo?t=210

I had no idea that Modern Hopfield Networks was a paper. I'll look it up later.

https://youtu.be/k3YmWrK6wxo?t=496

I really do not see how these things are equal to each other. Well, the right most sides sure. But the energy function not so much.

9:55am. At any rate, if transformers are modern Hopfield nets, then transformers are the ideal form of memory that I've been seeking form the start.

https://youtu.be/k3YmWrK6wxo?t=1219

These references might be of interest.

https://youtu.be/k3YmWrK6wxo?t=1539

Here he is suggesting that some of the matrices might be trained using backprop and other using Hebbian learning.

https://www.youtube.com/watch?v=W-O7AZNzbzQ
DDPM - Diffusion Models Beat GANs on Image Synthesis

https://arxiv.org/abs/2102.09672
Improved Denoising Diffusion Probabilistic Models

https://arxiv.org/abs/2105.05233
Diffusion Models Beat GANs on Image Synthesis

I'll leave this for later.

11am. Actually I had to read the last one. I am surprised that this is a supervised rather than an unsupervised model. It mentions that it requires more computation than GANs too.

11:15am. https://youtu.be/k3YmWrK6wxo?t=4340

Almost done with the first video. Here he is talking about how associate memory helps with molecule synthesis by a large amount when that are few examples.

https://youtu.be/k3YmWrK6wxo?t=4500

Here he is saying that transformers can do only one kind of computation, but Hopfield nets can do more, so the mind should be open.

https://www.youtube.com/watch?v=_QVUyXhu59I
Large Associative Memory Problem in Neurobiology and Machine Learning - Dmitry Krotov, PhD

Let me move to this.

https://youtu.be/_QVUyXhu59I?t=947

If it is not biological, then how could the brain possibly make do with the classical formulation?

https://youtu.be/_QVUyXhu59I?t=1035

Apparently by assuming some higher order structure.

https://youtu.be/_QVUyXhu59I?t=1467

This theory stuff is not my thing.

https://youtu.be/_QVUyXhu59I?t=2368
> ...Indeed the system looks like a generalization of the attention mechanism in NLP system. And indeed if you provide a more flexible attention mechanism that can evolve multiple steps and do some form of pattern completion this looks like a very promising setup for improving transformer architectures.

Though the quote is catchy, he does not actually provide such a system. He also talks about training these memories using something other than backprop. These two things would be of great interest to me.

12:15pm. Let me move on.

https://www.youtube.com/watch?v=bsdPZJKOlQs
Modern Hopfield Networks - Dr Sepp Hochreiter

https://youtu.be/bsdPZJKOlQs?t=186

Looking it so far, it seems like a longer version of the fireside chat talk he gave. Let me go for it.

12:40pm. Had to take a break, but let me segue into breakfast."

---
## [Wieku/danser-go](https://github.com/Wieku/danser-go)@[e4091b4d2c...](https://github.com/Wieku/danser-go/commit/e4091b4d2c67add038b85c093ca7d2ac923b0164)
#### Wednesday 2021-05-19 14:03:19 by Sebastian Krajewski

Allow reading unicode settings.json files (fuck you WinBlows)

---
## [canalplus/rx-player](https://github.com/canalplus/rx-player)@[179eb40e05...](https://github.com/canalplus/rx-player/commit/179eb40e05df46ffe20f564b77dcf972c45b6e90)
#### Wednesday 2021-05-19 14:34:23 by Paul Berberian

remove RxJS code from the transports code

After doing a proof-of-concept looking at how some parts of the code
looks like without RxJS (#916), this is a first functional proposal
which looks good enough to me to be merged.

It removes all RxJS code from the `transports` code in `src/transports`.

As a reminder, the reasons for doing this are:

  1. Observables are complicated and full of implicit behaviors
     (lazily running, sync or async, unsubscribing automatically after
     the last unsubscription etc.) which is difficult to reason about,
     especially for a newcomer.

     Things like exploiting schedulers through the `deferSubscriptions`
     util to work-around some subtle potential race-conditions, or
     merging Observables in a specific order for similar reasons, are
     ugly hacks that are difficult to explain to someone not familiar
     with that code.

     Even for us with multiple years of experience with it, we sometimes
     struggle with it.

  2. Promises, event listeners - and direct callbacks in general - are
     generally much more explicit and most developpers (at least JS/TS
     devs) are familiar with them.

  3. Call stacks are close to inexploitable when using RxJS.

  4. Promises provide async/await syntax which can improve drastically
     the readability of our async-heavy code, which for the moment
     suffer from callback hells almost everywhere.

However, I'm still not sure if this wish (getting rid of RxJS) is shared
by other maintainers and/or contributors, so it is still only a proposal.

Thoughts?

---
## [infolcop/kataClashOfClans](https://github.com/infolcop/kataClashOfClans)@[e39c2d37fb...](https://github.com/infolcop/kataClashOfClans/commit/e39c2d37fbef034247cf7cf33e6d2dbfd6f64b94)
#### Wednesday 2021-05-19 14:52:14 by infolcop

Create README.md

We are going to build a game. No game knowldege needed to resolve the exercises :)

As with most of the games the essential need are resources.
We have several type of resources : gems, gold, blue elixir and black elixir.
The purpose of our first exercise is to validate our black elixir production formula. 
Our black elixir is extracted from
deep inside earth by greedy dwarfs and they are not happy if we do not pay the correct price.

The rules are simple:
	- For producing black elixir two other resources are needed gems and blue elixir
	- The formula states that for producing 100 units of black elixir we need 5 gems and 200 units  of blue elixir
	  or 2 gems and 500 units of blue elixir
	- If quantity of gems supplied is bellow 2 gems or 200 units of blue elixir the dwarfs are not happy 
	- If the quantities suplied are not the exact ones (or exact multiplication of them) the dwars are also not happy they 
	do not like complex computations and will ask you to pay the exact price
	- BlackElixir is produced by BlackElixirFactory building

The purpose of the exercise:
	- write all tests defined in the "Ex1.java" based on the information provided above 
	- the code which should be covered by tests is found in "BlackElixirFactory.java"
	- are all the tests defined enough to cover all cases ?  if not please complete with the missing tests

* Estimated time: 5-10 minutes *

*********************************************************** EX2 *******************************************************
So we are advancing well in our development, nice job :0 . It is time to add some troops into our game play.
Troops are tricky little thing our game can have troops which can be produced with blue or black elixir.
In order to produce troops you will have to have some training camps (training camps are buiddings responsible for producing your troops).
Training camps can be of 2 types 
	- "BlueTrainingCamp" which can train the following troops consuming blue elixir resources : Bomber, Giant and Miner
	- "BlackTrainingCamp" which can train the following consuming black elixir resources : Dragon
	
Each unit has offensive and defensive hitpoints as described bellow:
 - "Dragon" | OffensiveHitpoints: 7500 | DefensiveHitpoints: 5000 |
 - "Giant"  | OffensiveHitpoints: 3000 | DefensiveHitpoints: 4500 |
 - "Miner"  | OffensiveHitpoints: 1200 | DefensiveHitpoints:  800 |
 - "Bomber" | OffensiveHitpoints:  800 | DefensiveHitpoints:  350 |


 Question 1:
  - Based on the information provided above please complete the implementation of the units : Dragon,Bomber,Giant and Miner
  in order to pass all tests included in  "Ex2.java"
  - Feel free to describe your solution and what are you planning to do.
  - Very important: You are not allowed to change the tests, the purpose of the exercise is to complete the implementation and 
  make the tests valid
 
 * Estimated time: 5-10 minutes *

 Question 2:
	When a unit takes a hit his life (DefensiveHitpoints) are decresing based on the offensiveHitpoint of the attack.
	When the unit Defensive Hitpoints are equal to 0 or fall bellow 0 the unit emits a last message shout before dyning .
	Each unit has a special shout message which it emits when is dying:
	  - "Dragon" - "I WILL RISE AGAIN FROM THE ASHES"
	  - "Giant"  - "OUR IRON FISTS WILL BE REMEMBERED FOREVER"
	  - "Bomber" - "WHEN YOU play with Explosives is dangerous bussiness"
	  - "Miner"  - "we return in the ground"
  
  Your role is to finish the implementation of the "ReceiveHit" method for all units in order to pass all tests 
  included in unde the namespace: "Ex2.java"
    
 * Estimated time: 5-10 minutes *

 Question 3:
	You arrived at a step where you have implemented quite a few things. Is there something you would have done differrent or you would want to
	refactor ? Explain your ideas and suggestions.

	Implement them.

 * Estimated time: 5-15 minutes *

***************************************************** EX3 ********************************************
Now that we have our troop units ready is time to see some action.

Each unit has the ability to move around the map in different ways.
The map is splited in equaly divided space units.
There are 3 types of movement behaviours:
	- Walk - when a unit walks it can cover 3 map spaces in 1 second
	- TunnelUnder - when a unit tunnels under it can cover 5 map spaces in 1 second
	- Fly - when a unit flies it can cover 8 map spaces in 1 second

Our troop units movement type is the following:
	- Bomber => Walk
	- Giant  => Walk
	- Dragon => Fly
	- Miner  => TunnelUnder

Question 1:
    - Provide the code implementation of the Move action for each of the troop units above
	- All tests included into the namespace "Ex3.java" should pass

* Estimated time: 5-15 minutes *

Question2: 

=> Uncomment the Q2 tests and comment With_Bomber_Unit_For_1_second_Then_It_Should_Walk_And_Cover_3_Map_Spaces

	- We have decided that the bomber moves too slow so we want him to run :)
	- Implement the Run behaviour : when a unit runs it can cover 6 map spaces into one second
	- All tests included into the namespace "Ex3.java" should pass

* Estimated time: 5-10 minutes *

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[1dd5cea8cc...](https://github.com/mrakgr/The-Spiral-Language/commit/1dd5cea8cc154574173a964b00186a2b5ec76016)
#### Wednesday 2021-05-19 16:32:38 by Marko GrdiniÄ‡

"1:40pm. Done with breakfast. Let me watch the video and I will do the chores.

1:55pm. https://youtu.be/bsdPZJKOlQs?t=1062

This pattern reconstruction ability is genuinely fascinating. Right now, I am thinking how I could fit such systems into a backprop like scheme without necessarily backproping through it all.

Prediction ultimately is merely reconstruction. The way a memory system would predict is to input the present as the first part, clamping the inputs to that, and extract the future based on that.

I really like my GAN idea, but I am wondering whether it would be too slow for such a thing?

2:15pm. https://youtu.be/bsdPZJKOlQs?t=1980

This is so boring. He keeps yammering on and on about proof details.

2:30pm. https://youtu.be/bsdPZJKOlQs?t=3045

I don't entirely get this, but it is important.

Energy based models really are the way to go.

And I have good understanding of how to make them modular. I've been thinking while watching these videos whether it would make sense to have the lower layers program the upper ones, and it does not.

I thought of using a Hopfield net and using the EqProp update as auxiliary storage, but I can't think of a single good way of doing this that would allow me to propagate gradient to the main network.

In contrast, I can very easily understand how to do simple hierarchical reconstruction.

My GAN idea needs some work. I think the main ingredients of an AGI system are there, but remember those memory games by Thorpe?

The kind of architectures I have in mind would not be fast enough...

Or maybe they would? I'll have to think about them more.

https://youtu.be/bsdPZJKOlQs?t=3157

Hmmm, this is supposed to replace the attention mechanism? I thought was supposed to be it.

2:50pm. https://youtu.be/bsdPZJKOlQs?t=3618

Let me take a break here.

3pm. Let me resume. 26m more of this to go.

Things are definitely forming in my mind. I understand why feedback connections are necessary. If you imagine a temporal hierarchy where the lower level layer attends to only a short timeframe, then it needs help from the upper layers in order to improve its predictions so they take in consideration the entirety of its experience.

In order to predict the next step, the memory of one's entire life is necessary.

With energy based models, those connections are implicit. You'd just run the model and let it settle to the equilibruim and this would fill in the prediction. But energy based models can be made explicit and converted into a differentible one. Rather than optimize, it would be better to have a reverse model and do it in one step.

3:10pm. I'll admit, it would be quite cool if magic of analogue hardware obliviates the need for this and end up giving the reverse model for free. I do not understand exactly how the physics of the system itself would make settling to an equilibrium extremely fast.

I can almost touch the complete system. The pieces really are here in the present.

I've been worried about memory since Thorpe's talk, but Hopfield nets do all of that reconstructive magic, and they are literally transformers. Well, transformers are a bit short - they don't have the right training procedure and the feedback connections.

3:20pm. https://youtu.be/bsdPZJKOlQs?t=3912

Here he is going to go into depth on deep learning + small datasets.

https://youtu.be/bsdPZJKOlQs?t=4957

RNNs can deal with formal languages, but transformers struggle on them.

3:50pm. Done. Let me take a break and do the chores here.

4:05pm. https://www.youtube.com/watch?v=DKyzcbNr8WE
John Hopfield: Physics View of the Mind and Neurobiology | Lex Fridman Podcast #76

Let me do the chores here. After that I'll go for this interview. After that I'll look at the Hopfield net papers.

4:25pm. The ideas are coming to me. There is nothing wrong with having a transformer like mechanism for short term memory. It certainly makes more sense than lugging a large weight matrix that changes from timestep to timestep.

But there would be nothing wrong with having a longer term hopield layer that does not change for the duration of the training episode. The reason for that is so I can backprop through its weight matrix and not have to copy it. Updating that particular weight matrix could be done in a separate phase.

4:30pm. Another idea that I am wondering - what if I fused the PRONG and Hopfield net in the linear part? Then I could keep around the same weight matrix, but just multiply the gradient through the associative memory during optimization.

What sort of effect would this have?

RL methods in particular, in the linear segment can keep track of eligibility traces. This would be something in the same vein as that.

...I have absolutely no idea whether this can or even should work. But it might be more stable, than having a large unsupervised part affecting the computation of a particular layer. It is really a natural idea when one thinks about it.

4:40pm. Forget this, I should focus on the interview. It does not really matter right now. It is not like I am going to bother implementing any of this on the GPU.

I am thinking of RNNs that do large number of internal computation steps. Having a long term working memory storage of past results could speed up that processing significantly.

https://arxiv.org/abs/2105.08050
Pay Attention to MLPs
> Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple attention-free network architecture, gMLP, based solely on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.

Twitter comment:
2017: Attention is all you need
2021: You don't need the transformers

4:40pm. https://arxiv.org/abs/2008.02217
Hopfield Networks is All You Need

I thought this paper came out years ago, but I see that its date is near the end of 2020.

I'll read it later.

https://pni.princeton.edu/john-hopfield/john-j.-hopfield-now-what

Lex mentioned this article so I paused the interview to look it up. That is how I found the `Pay Attention to MLPs` paper.

5:10pm. https://youtu.be/DKyzcbNr8WE?t=1229

He thinks it is a couple of generations till AGI.

5:30pm. If Hopfield nets are so powerful even on the linear segment that they can take in an exponential number of patterns even in the linear segment, instead of having a large replay buffer, what would happen if I used one of them as a replay buffer and sampled from them?

5:35pm. Wow, the `Hopfield Networks is All You Need` is 95 pages long.

> We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-ofthe-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers

The paper says that these new layers can be integrated in DL architectures. I should they would just be a separate linear thing.

6:15pm. Done with the interview. Let me take a break.

6:20pm. I am back. I am not sure that I got that much out of those interviews today. They were quite boring.

My next target is definitely the Hopfield nets paper. I'll leave it for tomorrow.

I never had an interest in these nets until now, mostly because Hinton said in his course that Hopfield nets can only store a limited number of examples. I heard about the paper above, but I thought it was just a theory paper proving that HF nets were transformers. So that did not catch my interest. It was not until I wasted these vids today that I realized what remarkable ability these nets have. I have to go for it.

In the future, these might be the key to making nets that have long term memory. I should read up on the subject and get my anxiety out of the way.

Eventually I will have to figure out how to integrate real life data with the self play aspects of training agents, and this will help. Most of what goes into the brain is memory.

John Hopfield made a good remark than only having feedforward computation like deep NNs have now is equivalent to having a computer that does only one clock cycle.

I need to go a bit further in my understanding.

6:30pm. Let me stop here for the day. Let me shot demons and watch anime for a while. Tomorrow, I should finish this trip down the memory lane. After that I should hopefully have enough to start programming. If I am lucky, this paper will give me some now toys to play with. Transformers are an old trick right now."

---
## [xScottishKrissx/the-knews-live](https://github.com/xScottishKrissx/the-knews-live)@[f201eb4b10...](https://github.com/xScottishKrissx/the-knews-live/commit/f201eb4b10529a10500bae0d17006d0204e3f3ff)
#### Wednesday 2021-05-19 17:53:32 by Chris Dunne

V12.6.3.1 - Small Steps

I'm at a point with the like button where I can properly update the array and the value of like is what I want. This is insanely annoying and I don't know why i am finding this so troublesome. It's a glorified counter and Im struggling with it. After everything else I've done this is the thing that's getting to me. Enough for today, back tommorow to see if I can do better.

---
## [newstools/1970-national-mirror-nigeria](https://github.com/newstools/1970-national-mirror-nigeria)@[0d4d337e14...](https://github.com/newstools/1970-national-mirror-nigeria/commit/0d4d337e14712983253f1d03b8892539760d24aa)
#### Wednesday 2021-05-19 19:10:04 by Billy Einkamerer

Created Text For URL [www.nationalmirroronline.net/jason-derulo-welcomes-a-baby-boy-with-his-girlfriend-jena-frumes-7665.html]

---
## [Dudemanguy/mpv](https://github.com/Dudemanguy/mpv)@[18ff9c5a22...](https://github.com/Dudemanguy/mpv/commit/18ff9c5a225c18fe406660bbeed03683c69a0637)
#### Wednesday 2021-05-19 19:14:38 by Dudemanguy

wayland: simplify render loop

This is actually a very nice simplification that should have been
thought of years ago (sue me). In a nutshell, the story with the
wayland code is that the frame callback and swap buffer behavior doesn't
fit very well with mpv's rendering loop. It's been refactored/changed
quite a few times over the years and works well enough but things could
be better. The current iteration works with an external swapchain to
check if we have frame callback before deciding whether or not to
render. This logic was implemented in both egl and vulkan.

This does have its warts however. There's some hidden state detection
logic which works but is kind of ugly. Since wayland doesn't allow
clients to know if they are actually visible (questionable but
whatever), you can just reasonably assume that if a bunch of callbacks
are missed in a row, you're probably not visible. That's fine, but it is
indeed less than ideal since the threshold is basically entirely
arbitrary and mpv does do a few wasteful renders before it decides that
the window is actually hidden.

The biggest urk in the vo_wayland_wait_frame is the use of
wl_display_roundtrip. Wayland developers would probably be offended by
the way mpv abuses that function, but essentially it was a way to have
semi-blocking behavior needed for display-resample to work. Since the
swap interval must be 0 on wayland (otherwise it will block the entire
player's rendering loop), we need some other way to wait on vsync. The
idea here was to dispatch and poll a bunch of wayland events, wait (with
a timeout) until we get frame callback, and then wait for the compositor
to process it. That pretty much perfectly waits on vsync and lets us
keep all the good timings and all that jazz that we want for mpv. The
problem is that wl_display_roundtrip is conceptually a bad function. It
can internally call wl_display_dispatch which in certain instances,
empty event queue, will block forever. Now strictly speaking, this
probably will never, ever happen (once I was able to to trigger it by
hardcoding an error into a compositor), but ideally
vo_wayland_wait_frame should never infinitely block and stall the
player. Unfortunately, removing that function always lead to problems
with timings and unsteady vsync intervals so it survived many refactors.

Until now, of course. In wayland, the ideal is to never do wasteful
rendering (i.e. don't render if the window isn't visible). Instead of
wrestling around with hidden states and possible missed vblanks, let's
rearrange the wayland rendering logic so we only ever draw a frame when
the frame callback is returned to use (within a reasonable timeout to
avoid blocking forever).

This slight rearrangement of the wait allows for several simplifications
to be made. Namely, wl_display_roundtrip stops being needed. Instead, we
can rely entirely on totally nonblocking calls (dispatch_pending, flush,
and so on). We still need to poll the fd here to actually get the frame
callback event from the compositor, but there's no longer any reason to
do extra waiting. As soon as we get the callback, we immediately draw.
This works quite well and has stable vsync (display-resample and audio).
Additionally, all of the logic about hidden states is no longer needed.
If vo_wayland_wait_frame times out, it's okay to assume immediately that
the window is not visible and skip rendering.

Unfortunately, there's one limitation on this new approach. It will only
work correctly if the compositor implements presentation time. That
means a reduced version of the old way still has to be carried around in
vo_wayland_wait_frame. So if the compositor has no presentation time,
then we are forced to use wl_display_roundtrip and juggle some funny
assumptions about whether or not the window is hidden or not. Plasma is
the only real notable compositor without presentation time at this stage
so perhaps this "legacy" mechanism could be removed in the future.

---
## [Horsey-/Simply-Love-SM5-1](https://github.com/Horsey-/Simply-Love-SM5-1)@[b8b6a71a77...](https://github.com/Horsey-/Simply-Love-SM5-1/commit/b8b6a71a77ae62d38e5893a330a72cefe3c103e0)
#### Wednesday 2021-05-19 19:18:56 by Horsey

Make highscore character limit configurable (4-9 characters) in Simply Love Options

This commit allows machine operators to select the limit to the number of highscore characters that are selectable for players. The default is 9, but the option to select any number between 4 and 9 is available. I strongly believe that if players are going to use offensive words for their highscore name, they already would be doing so. It only takes 4 letters to type "FUCK", and even if you set the limit to 3, "FCK" is still "offensive". 9 characters should give players a lot more leeway to use more fitting nicknames, and even fit most of first names into a highscore name.

9 is the most characters that would fit comfortably, but with a bit of reclaiming dead space in the theme I can't imagine 20-25 characters being much of a problem (yay full names?)

---
## [pytorch/pytorch](https://github.com/pytorch/pytorch)@[09fdb7cb65...](https://github.com/pytorch/pytorch/commit/09fdb7cb654dc52ad62e042c12a89cd6ed3462b1)
#### Wednesday 2021-05-19 20:31:03 by Brian Hirsh

Update base for Update on "add a boxed CPU fallback kernel"

This PR replaces the existing code-generated CPU fallback kernels that XLA uses with a single boxed CPU fallback.

Current state: there are a couple different design ideas that I want to point out, but the logic for the actually kernel is mostly done and passing tests.

### Design

To preface, I'm not 100% tied to the current design and I'm putting the PR up now for opinions and totally open to alternatives, some of which I listed below. Actually after writing this description, I'm leaning toward the following changes:
* Confirm whether or not we can remove all C++ logging info directly in the yaml.


**Current Design**

All of the CPU fallback codegen is deleted. In its place, XLA (and other external backends, later) can choose to opt into a CPU fallback by adding the following code in a C++ file. I have an corresponding [xla-side PR with the xla changes](https://github.com/pytorch/xla/pull/2945/files#diff-1a005c10039f0cb11130a3b740f5de716d2f10acaea121017016025861886798R1).

There's no actual requirement to split up the code into a .h and .cpp file, but that's necessary in the XLA case because they sometimes need to call the fallback directly from their handcrafted kernels.

```
// xla_cpu_fallback.h
#include <ATen/native/CPUFallback.h>
...
void xla_cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack);
...
```
```
// xla_cpu_fallback.cpp
#include "torch_xla/csrc/aten_cpu_fallback.h"
...
void xla_cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
  // Do custom logging here
  ...
  // Call the actual boxed CPU fallback.
  at::native::cpu_fallback(op, stack);
}

TORCH_LIBRARY_IMPL(_, XLA, m) {
  m.fallback(torch::CppFunction::makeFromBoxedFunction<&xla_cpu_fallback>());
}
```

Now that the fallback is exposed in the backend, they can call it directly. Doing so requires converting from an unboxed to a boxed context, which we provide a utility function before. E.g.:
```
#include <ATen/native/CPUFallback.h>

at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::native::call_fallback_fn<&xla_cpu_fallback, decltype(at::addmm)>::call("aten::addmm", self, mat1, mat2, beta, alpha);
  }
  ...
}
```

That `decltype(at::addmm)` logic isn't actually used everywhere in the xla-side PR yet, since you hit issues with overloads. I could use it everywhere once #58092 lands.

**Alternatives: The API for calling the CPU fallback directly is ugly, can we make it nicer?**
We could change the api to use `at::redispatch`, which would make it look something like this:
```
at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::redispatch::addmm(c10::DispatchKeySet(c10::DispatchKey::CPUFallback), self, mat1, mat2, beta, alpha);
  }
  ...
}
```
Which definitely feels cleaner, but also requires adding a new DispatchKey just for this use case. Conditionally calling the CPU fallback doesn't sound like a hugely important use case, so I don't know if giving up one of our 64 dispatch key slots is worth the API improvement. Totally open to other opinions though!


Another more mild improvement that would avoid having to pass operator string names (including overloads) around would be to codegen (yet another) namespaced API. Something like this:
```
at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::fallback::addmm<&xla_cpu_fallback>(self, mat1, mat2, beta, alpha);
  }
  ...
}
```

Writing that out actually I actually like it more (I think it'll let us get rid of `decltype(...)`). Maybe that is nice enough to warrant a new codegen API - I haven't tried adding that yet, but if people like it I'm happy to try it out.

**More alternatives**
The current design also involves the backend manually writing and registering the boxed fallback themselves, but an alternative would be for us to do it in codegen too: they would just need to pass in all of the C++ logging that they want done in the fallback, directly through the yaml. The main downsides:
* Backend code that wants to call the fallback needs to abide by whatever convention our codegen uses to name the generated boxed fallback.
* Passing custom C++ logging through yaml is just more fragile: right now xla uses an `iostream` to log each tensor arg in the operator, so we'd have to either force other backends into the same convention or figure something else out later.

To be fair, we actually already do that: XLA has custom per-tensor-arg logging for all of the generated `out` wrappers in the codegen, which we do by passing their C++ logging info through the yaml. This seems unnecessary though, since `out` wrappers just call into a functional kernel, which is hand written with its own custom logging. So my take is: try to remove custom C++ logging from the yaml, and if it turns out to be really necessary, then we may as well take advantage of that to codegen the fallback.

### Performance impact

While ops that fall back to CPU aren't exactly hot path, we probably don't want to use a boxed fallback if it turns out to be an absolute perf killer.

I ran my benchmarks using callgrind, benchmarking both `at::add` and `at::add_out` run on XLA. My callgrind benchmark for `at::add` can be found here (the add_out benchmark looks basically the same): https://www.internalfb.com/phabricator/paste/view/P415418587. I created the benchmark by hacking the existing xla C++ test build scripts and throwing in a reference to callgrind.

I also attached the full callgrind output for each benchmark; the full output is actually pretty noise and hard to parse, but I focused on everything underneath the `at::add()` call in the output, which was much more stable. My guess is that it's due to some heavyweight async startup processing that xla does.

`at::add`:
before: 88,505,130 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415421001
after: 102,185,654 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415421273
delta: ~15.5% increase

`at::add_out`:
before: 63,897,395 instructions. Full output: https://www.internalfb.com/intern/everpaste/?handle=GBrrKwtAPlix9wUEAOZtrFXpdO5UbsIXAAAz
after: 73,170,346 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415423227
delta: ~14.5% increase

High level takeaway: A framework overhead increase of 10-20% doesn't seem too horrible for the CPU fallback use case.

For structured, functional ops that requires a CPU fallback, we're actually in an unfortunate situation: we're doing even more work than necessary. Our codegen automatically creates a `CompositeExplicitAutograd` kernel which calls into the `out` operator. So the extra work that we end up doing is:
* An extra dispatcher hop: (at::add -> CompositeExplicitAutograd -> CPUFallback -> at::native::add) instead of (at::add -> CPUFallback -> at::native::add)
* An unnecessary tensor allocation (the CompositeExplicitAutograd kernel uses at::empty() to create an output tensor, which is immediately overwritten by the CPU fallback)
* An unnecessary meta() call (the CompositeExplicitAutograd kernel calls it to create the output tensor, but we call it again in the CPU kernel).
* unboxing->boxing->unboxing logic (this is the only strictly required piece)

There are definitely ways to avoid the unnecessary work explained above: one would be to give the boxed fallback higher priority than composite keys (there's [an issue for it here](https://github.com/pytorch/pytorch/issues/55104)), and codegen fallthroughs for all composite ops. It'll require more infra to set up, so I see it as more of a perf knob that we can apply if we need it later.

Unfortunately I couldn't dig much deeper into the differences aside from the aggregate change in instructions, since it looks like callgrind fudged some of the instruction attribution (`at::to_cpu` takes up a ton of instructions, but I don't see any attribution for the `at::native::add` kernel anywhere).




[ghstack-poisoned]

---
## [marconovino/hodler](https://github.com/marconovino/hodler)@[396df6bcdd...](https://github.com/marconovino/hodler/commit/396df6bcdd93e69033255b8fb6499363fcf2556f)
#### Wednesday 2021-05-19 20:57:29 by Marco

trying write_image instead of savefig (fuck you matplotlib)

---
## [brainstormerjr/HelloWorldOS](https://github.com/brainstormerjr/HelloWorldOS)@[346c47d36d...](https://github.com/brainstormerjr/HelloWorldOS/commit/346c47d36d032ed7556419c0633422b18e90a66e)
#### Wednesday 2021-05-19 22:47:45 by brainstormerjr

Change colors

My team and I spent hours to find the perfect saturation of colors to ensure a premium shopping experience (god we are in desperate need of a designer if you know any color theory at all contact me pls)

---

# [<](2021-05-18.md) 2021-05-19 [>](2021-05-20.md)

