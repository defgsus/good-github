# [<](2021-05-19.md) 2021-05-20 [>](2021-05-21.md)

3,951,823 events, 1,448,622 push events, 2,365,869 commit messages, 185,728,016 characters


## [dgl-intel/pytorch@77981b2725...](https://github.com/dgl-intel/pytorch/commit/77981b272581485f15f9e5d2f5fb27ee9435e77e)
##### 2021-05-20 00:04:13 by Brian Hirsh

Update base for Update on "add a boxed CPU fallback kernel"

This PR replaces the existing code-generated CPU fallback kernels that XLA uses with a single boxed CPU fallback.

Current state: there are a couple different design ideas that I want to point out, but the logic for the actually kernel is mostly done and passing tests.

### Design

To preface, I'm not 100% tied to the current design and I'm putting the PR up now for opinions and totally open to alternatives, some of which I listed below. Actually after writing this description, I'm leaning toward the following changes:
* Confirm whether or not we can remove all C++ logging info directly in the yaml.


**Current Design**

All of the CPU fallback codegen is deleted. In its place, XLA (and other external backends, later) can choose to opt into a CPU fallback by adding the following code in a C++ file. I have an corresponding [xla-side PR with the xla changes](https://github.com/pytorch/xla/pull/2945/files#diff-1a005c10039f0cb11130a3b740f5de716d2f10acaea121017016025861886798R1).

There's no actual requirement to split up the code into a .h and .cpp file, but that's necessary in the XLA case because they sometimes need to call the fallback directly from their handcrafted kernels.

```
// xla_cpu_fallback.h
#include <ATen/native/CPUFallback.h>
...
void xla_cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack);
...
```
```
// xla_cpu_fallback.cpp
#include "torch_xla/csrc/aten_cpu_fallback.h"
...
void xla_cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
  // Do custom logging here
  ...
  // Call the actual boxed CPU fallback.
  at::native::cpu_fallback(op, stack);
}

TORCH_LIBRARY_IMPL(_, XLA, m) {
  m.fallback(torch::CppFunction::makeFromBoxedFunction<&xla_cpu_fallback>());
}
```

Now that the fallback is exposed in the backend, they can call it directly. Doing so requires converting from an unboxed to a boxed context, which we provide a utility function before. E.g.:
```
#include <ATen/native/CPUFallback.h>

at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::native::call_fallback_fn<&xla_cpu_fallback, decltype(at::addmm)>::call("aten::addmm", self, mat1, mat2, beta, alpha);
  }
  ...
}
```

That `decltype(at::addmm)` logic isn't actually used everywhere in the xla-side PR yet, since you hit issues with overloads. I could use it everywhere once #58092 lands.

**Alternatives: The API for calling the CPU fallback directly is ugly, can we make it nicer?**
We could change the api to use `at::redispatch`, which would make it look something like this:
```
at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::redispatch::addmm(c10::DispatchKeySet(c10::DispatchKey::CPUFallback), self, mat1, mat2, beta, alpha);
  }
  ...
}
```
Which definitely feels cleaner, but also requires adding a new DispatchKey just for this use case. Conditionally calling the CPU fallback doesn't sound like a hugely important use case, so I don't know if giving up one of our 64 dispatch key slots is worth the API improvement. Totally open to other opinions though!


Another more mild improvement that would avoid having to pass operator string names (including overloads) around would be to codegen (yet another) namespaced API. Something like this:
```
at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::fallback::addmm<&xla_cpu_fallback>(self, mat1, mat2, beta, alpha);
  }
  ...
}
```

Writing that out actually I actually like it more (I think it'll let us get rid of `decltype(...)`). Maybe that is nice enough to warrant a new codegen API - I haven't tried adding that yet, but if people like it I'm happy to try it out.

**More alternatives**
The current design also involves the backend manually writing and registering the boxed fallback themselves, but an alternative would be for us to do it in codegen too: they would just need to pass in all of the C++ logging that they want done in the fallback, directly through the yaml. The main downsides:
* Backend code that wants to call the fallback needs to abide by whatever convention our codegen uses to name the generated boxed fallback.
* Passing custom C++ logging through yaml is just more fragile: right now xla uses an `iostream` to log each tensor arg in the operator, so we'd have to either force other backends into the same convention or figure something else out later.

To be fair, we actually already do that: XLA has custom per-tensor-arg logging for all of the generated `out` wrappers in the codegen, which we do by passing their C++ logging info through the yaml. This seems unnecessary though, since `out` wrappers just call into a functional kernel, which is hand written with its own custom logging. So my take is: try to remove custom C++ logging from the yaml, and if it turns out to be really necessary, then we may as well take advantage of that to codegen the fallback.

### Performance impact

While ops that fall back to CPU aren't exactly hot path, we probably don't want to use a boxed fallback if it turns out to be an absolute perf killer.

I ran my benchmarks using callgrind, benchmarking both `at::add` and `at::add_out` run on XLA. My callgrind benchmark for `at::add` can be found here (the add_out benchmark looks basically the same): https://www.internalfb.com/phabricator/paste/view/P415418587. I created the benchmark by hacking the existing xla C++ test build scripts and throwing in a reference to callgrind.

I also attached the full callgrind output for each benchmark; the full output is actually pretty noise and hard to parse, but I focused on everything underneath the `at::add()` call in the output, which was much more stable. My guess is that it's due to some heavyweight async startup processing that xla does.

`at::add`:
before: 88,505,130 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415421001
after: 102,185,654 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415421273
delta: ~15.5% increase

`at::add_out`:
before: 63,897,395 instructions. Full output: https://www.internalfb.com/intern/everpaste/?handle=GBrrKwtAPlix9wUEAOZtrFXpdO5UbsIXAAAz
after: 73,170,346 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415423227
delta: ~14.5% increase

High level takeaway: A framework overhead increase of 10-20% doesn't seem too horrible for the CPU fallback use case.

For structured, functional ops that requires a CPU fallback, we're actually in an unfortunate situation: we're doing even more work than necessary. Our codegen automatically creates a `CompositeExplicitAutograd` kernel which calls into the `out` operator. So the extra work that we end up doing is:
* An extra dispatcher hop: (at::add -> CompositeExplicitAutograd -> CPUFallback -> at::native::add) instead of (at::add -> CPUFallback -> at::native::add)
* An unnecessary tensor allocation (the CompositeExplicitAutograd kernel uses at::empty() to create an output tensor, which is immediately overwritten by the CPU fallback)
* An unnecessary meta() call (the CompositeExplicitAutograd kernel calls it to create the output tensor, but we call it again in the CPU kernel).
* unboxing->boxing->unboxing logic (this is the only strictly required piece)

There are definitely ways to avoid the unnecessary work explained above: one would be to give the boxed fallback higher priority than composite keys (there's [an issue for it here](https://github.com/pytorch/pytorch/issues/55104)), and codegen fallthroughs for all composite ops. It'll require more infra to set up, so I see it as more of a perf knob that we can apply if we need it later.

Unfortunately I couldn't dig much deeper into the differences aside from the aggregate change in instructions, since it looks like callgrind fudged some of the instruction attribution (`at::to_cpu` takes up a ton of instructions, but I don't see any attribution for the `at::native::add` kernel anywhere).




[ghstack-poisoned]

---
## [i3roly/glibc_ddwrt@56abe0ddcd...](https://github.com/i3roly/glibc_ddwrt/commit/56abe0ddcdac83f7590dab762b3b0aefc06c2877)
##### 2021-05-20 00:08:21 by gagan sidhu

update kernels, tighten libraries, use DD-WRT iproute

back in when i started this gangsta shit (lol), i had to make my own
iproute2, since BS was on a ghetto version. at that time, i used his
makefile from the old version to make my own. this was a recurring theme.

since i had to rework much of my build tree to accommodate multiplatform
builds, i decided to tighten up the libraries in the existing mipsel
builds.

you'll notice there's only one libdnet library (hey, gotta pay respect)
instead of two.

i'm now using his iproute2 instead of my own since it's probably better.

he does some silly things that are noticeable on the much-faster TP-LINK
AX50. for example, in rc/init.c he calls devinit via command line (like
'system'), which essentially can destroy init since the code doesn't
wait for a return. for the much-faster interaptiv, this became
noticeable and destroyed the entire init process.
    -as such, i've moved the mounting and mknod creation for the nvram
    to the rc/init.c, in addition to leaving the code in
    services/sysinit/devinit.c.
    -i don't think he can change things like this when he's supporting
    like 1000 routers. it's easier to do when you're working with a
    handful. i can't see my number going that much higher. the AX50 is a
    challenge and it's seriously fast, so i decided to give it a go.

i didn't test this because i'm at 20 days uptime and the existing build
is good, and i wanna keep my uptime going. but there should be no
problems.

if there are, file a ticket and enjoy the existing build as much as i
am.

---
## [huaxz1986/pytorch@9354a68e7d...](https://github.com/huaxz1986/pytorch/commit/9354a68e7d8c4680a115b70b9b14565cd42cb03f)
##### 2021-05-20 02:06:11 by Brian Hirsh

[codegen] split out backend-specific information from NativeFunction in the model (#57361)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57361

Data model change in the codegen, which splits backend-specific information out of `NativeFunction`

### Overview
Currently in the codegen, native_functions.yaml has backend-specific information about each operator that is encoded directly into the data model, in the `NativeFunction` object. That's reasonable, since the native_functions.yaml is the source of truth for information about an operator, and the data model encodes that information into types.

Now that external backends can use the codegen though, that information is technically incomplete/inaccurate. In another PR, I tried patching the information on the `NativeFunction` object with the additional external information, by updating the `dispatch` entry to contain the external backend kernel name and dispatch key.

Instead, this PR tries to split out that information. The `NativeFunction` class contains all information about an operator from native_functions.yaml that's backend-independent and is known never to change regardless of what extra information backends provide. We also build up a backend "index", which is basically a mapping from [backend] -> [backend-specific-metadata]. Reading in an external backend yaml just involves updating that index with the new backend.

There were a few places where `NativeFunction` used the dispatch table directly, that I encoded as properties directly on the NativeFunction object (e.g. `is_abstract`). They were mostly around whether or not the operator has a composite kernel, which isn't something that's going to change for any external backends.

This has a few advantages:
- We can more easily re-use the existing logic in `native_function.py` and `register_dispatch_key.py` for both native and external backends, since they both involve a NativeFunction + a particular backend index
- The data in the data model will be the same regardless of how the codegen is run. Running the codegen with a new external backend doesn't change the data inside of NativeFunction or an existing backend index. It just adds a new index for that backend.
- There are several of codegen areas that don't care about backend-specific information: mostly the tracing and autograd codegen. We can reason about the codegen there more easily, knowing that backend-specific info is entirely uninvolved.

An alternative to this split would be to augment the NativeFunction objects with external backend information at the time that we create them. So the external codegen could read both native_functions.yaml and the external backend's yaml at the same time, and construct a NativeObject with a full dispatch table (including the XLA entry), and the correct setting of structured (taking into account both yamls). One disadvantage to this approach is that NativeFunction objects now contain different stuff depending on how you ran the codegen, and you have to make sure that any changes to the codegen can properly handle all the different variants.

### Data Model Changes
Removed 3 classes, which are used by the external codegen:
- ExternalBackendFunction
- ExternalBackendFunctionsGroup
- ExternalBackendMetadata

And added two new ones:
- BackendIndex
- BackendMetadata

`BackendIndex` contains any info that's specific to that backend, plus a mapping from operator names to backend specific metadata about the operator. One example of backend-specific info that's not operator-dependent is the fact that XLA prefers to implement functional kernels instead of out kernels (and so when they eventually mark an op as structured, they're going to mark the functional op and not the out op).

`BackendMetadata` contains info specific to an (operator, backend) pair. Right now, that's just (a) the name of the kernel, and (b) whether or not that operator is structured.

### Questions
I wanted to get this PR up earlier so I could get feedback, but there are a few things I want to call out:

**Dealing with `structured`.**
This PR separates out the notion of `structured` into two bits of information:
- Does [operator] have a meta() function. This is backend-agnostic, and is represented by the `structured` property on `NativeFunction`, same as before. This is used, e.g., to decide what signatures to add to `MetaFunctions.h`.
- Does [operator, backend] have an impl() function. This is backend dependent; even though technically all in-tree backends are forced to write impl() functions for an operator when we port the op to structured in native_functions.yaml, out-of-tree backends can decide to opt in independently. This is represented as a property on `BackendMetadata`. This is used in most other cases, e.g. in `RegisterDispatchKey` when we're deciding whether or not to gen a structured or unstructured wrapper.

I also baked `is_structured_dispatch_key` directly into each BackendIndex. So for operators marked "structured" in native_functions.yaml, their corresponding CPU/CUDA BackendIndex entries will be marked structured, and all others (except for potentially external backends) will not.

I ended up trying to deal with `structured` in this change since it's technically backend dependent (XLA can opt kernels into structured separately from in-tree ops), but that may have been too ambitious: it's technically not relevant until we actually add support for structured external kernels. If it's not clear that this is the right path for dealing with structured and we want to push that off, I'm fine with backing out the bits of this PR that make `structured` backend-dependent. I don't see anything *too* controversial related to structured in the change, but I tried to call out any areas in the comments

**Localizing the fact that external backends follow Dispatcher convention.**
Another thing that's sort of backend specific that I didn't totally address in this PR is the fact the fact that in-tree backends follow the Native API while external backends follow the Dispatcher API. I painted over that in `native_functions.py` by adding a helper, `kernel_signature`, that takes in a native function and gives you the "correct" signature for the specified backend- NativeSignature for in-tree backends, and DispatcherSignature for out-of-tree backends. In order to make that fully useable though, we'll need `NativeSignature` and `DispatcherSignature` to have matching interfaces. I didn't bother with that in this PR, which is why `gen_external_aten_fallbacks.py` still has a bunch of direct references to the dispatcher API. Thinking of adding it in a later PR but wanted to see if anyone has other opinions.

Maybe `is_external()` shouldn't even be a property on the BackendMetadata, and anything the codegen does that requires asking for that information should just be better abstracted away.

**Thoughts on the `BackendIndex` / `BackendMetadata` breakdown.**
One thing that's annoying right now is that to query for various pieces of metadata, you call helper functions like `backend_index.structured(f)`, which queries that particular backend and tells you if that specific NativeFunctionGroup is structured for that backend. It has to return an `Optional[bool]` though, since you have to handle the case where that operator doesn't have a kernel for that backend at all. So users of those helpers end up with a bunch of optionals that they need to unpack, even if they know at some point that the result isn't None. I think it would be easier instead to just store the NativeFunction object as a field directly on the BackendMetadata. Curious if there are any other opinions on a better way to model it though.

Test Plan: Imported from OSS

Reviewed By: navahgar

Differential Revision: D28474362

Pulled By: bdhirsh

fbshipit-source-id: 41a00821acf172467d764cb41e771e096542f661

---
## [bdraco/home-assistant.io@5ddb6529c1...](https://github.com/bdraco/home-assistant.io/commit/5ddb6529c13dd4ba8f5aa364b2560740a77bf178)
##### 2021-05-20 03:35:18 by elyobelyob

I found that a 4pm or other non midnight required buffering. (#16182)

* Update history_stats.markdown

---
title: History Stats
description: Instructions about how to integrate historical statistics into Home Assistant.
ha_category:
  - Utility
  - Sensor
ha_iot_class: Local Polling
ha_release: 0.39
ha_quality_scale: internal
ha_domain: history_stats
---

The `history_stats` sensor platform provides quick statistics about another integration or platforms, using data from the [`history`](/integrations/history/) integration.

It can track how long the integration has been in a specific state, in a custom time period.

Examples of what you can track:

- How long you were at home this week
- How long the lights were ON yesterday
- How long you watched TV today

## Configuration

To enable the history statistics sensor, add the following lines to your `configuration.yaml`:

{% raw %}

```yaml
# Example configuration.yaml entry
sensor:
  - platform: history_stats
    name: Lamp ON today
    entity_id: light.my_lamp
    state: 'on'
    type: time
    start: '{{ now().replace(hour=0, minute=0, second=0) }}'
    end: '{{ now() }}'
```

{% endraw %}

{% configuration %}
entity_id:
  description: The entity you want to track.
  required: true
  type: string
state:
  description: The states you want to track.
  required: true
  type: [list, string]
name:
  description: Name displayed on the frontend. Note that it is used by Home Assistant to generate sensor's `object_id` so it is advisable to choose a unique one and change name for frontend using [customization](/docs/configuration/customizing-devices/#friendly_name) or via [Lovelace](/lovelace/entities/#name).
  required: false
  default: unnamed statistics
  type: string
type:
  description: "The type of sensor: `time`, `ratio`, or `count`."
  required: false
  default: time
  type: string
start:
  description: When to start the measure (timestamp or datetime).
  required: false
  type: template
end:
  description: When to stop the measure (timestamp or datetime).
  required: false
  type: template
duration:
  description: Duration of the measure.
  required: false
  type: time
{% endconfiguration %}

<div class='note'>

  You have to provide **exactly 2** of `start`, `end` and `duration`.
<br/>
  You can use [template extensions](/topics/templating/#home-assistant-template-extensions) such as `now()` or `as_timestamp()` to handle dynamic dates, as shown in the examples below.

</div>

## Sensor type

Depending on the sensor type you choose, the `history_stats` integration can show different values:

- **time**: The default value, which is the tracked time, in hours
- **ratio**: The tracked time divided by the length of your period, as a percentage
- **count**: How many times the integration you track was changed to the state you track

## Time periods

The `history_stats` integration will execute a measure within a precise time period. You should always provide 2 of the following :
- When the period starts (`start` variable)
- When the period ends (`end` variable)
- How long is the period (`duration` variable)

As `start` and `end` variables can be either datetimes or timestamps, you can configure almost any period you want.

### Duration

The duration variable is used when the time period is fixed. Different syntaxes for the duration are supported, as shown below.

```yaml
# 6 hours
duration: 06:00
```

```yaml
# 1 minute, 30 seconds
duration: 00:01:30
```

```yaml
# 2 hours and 30 minutes
duration:
  # supports seconds, minutes, hours, days
  hours: 2
  minutes: 30
```

<div class='note'>

  If the duration exceeds the number of days of history stored by the `recorder` component (`purge_keep_days`), the history statistics sensor will not have all the information it needs to look at the entire duration. For example, if `purge_keep_days` is set to 7, a history statistics sensor with a duration of 30 days will only report a value based on the last 7 days of history.

</div>

### Examples

Here are some examples of periods you could work with, and what to write in your `configuration.yaml`:

**Today**: starts at 00:00 of the current day and ends right now.

{% raw %}

```yaml
    start: '{{ now().replace(hour=0, minute=0, second=0) }}'
    end: '{{ now() }}'
```

{% endraw %}

**Yesterday**: ends today at 00:00, lasts 24 hours.

{% raw %}

```yaml
    end: '{{ now().replace(hour=0, minute=0, second=0) }}'
    duration:
      hours: 24
```

{% endraw %}

**This morning (6AM - 11AM)**: starts today at 6, lasts 5 hours.

{% raw %}

```yaml
    start: '{{ now().replace(hour=6, minute=0, second=0) }}'
    duration:
      hours: 5
```

{% endraw %}

**Current week**: starts last Monday at 00:00, ends right now.

Here, last Monday is _today_ as a timestamp, minus 86400 times the current weekday (86400 is the number of seconds in one day, the weekday is 0 on Monday, 6 on Sunday).

{% raw %}

```yaml
    start: '{{ as_timestamp( now().replace(hour=0, minute=0, second=0) ) - now().weekday() * 86400 }}'
    end: '{{ now() }}'
```

{% endraw %}

**Next 4pm **: ends today at 00:00, lasts 30 days. Easy one.

{% raw %}

```yaml
    end: '{{ now().replace(hour=0, minute=0, second=0) }}'
    duration:
      days: 30
```

{% endraw %}

**Last 30 days**: ends today at 00:00, lasts 30 days. Easy one.

{% raw %}

```yaml
    end: '{{ now().replace(hour=0, minute=0, second=0) }}'
    duration:
      days: 30
```

{% endraw %}


** 4PM always in the future**: ends in the future at 16:00, starts 24 hours before.

{% raw %}

```yaml
    end: '{{ (now().replace(minute=0,second=0) + timedelta(hours=8)).replace(hour=16) }}'
    duration:
      hours: 24
```

{% endraw %}

**All your history** starts at timestamp = 0, and ends right now.

{% raw %}

```yaml
    start: '{{ 0 }}'
    end: '{{ now() }}'
```

{% endraw %}

<div class='note'>

  The `/developer-tools/template` page of your Home Assistant UI can help you check if the values for `start`, `end` or `duration` are correct. If you want to check if your period is right, just click on your component, the `from` and `to` attributes will show the start and end of the period, nicely formatted.

</div>

* $pm - 4pm example implemented

* Tweak

* Update source/_integrations/history_stats.markdown

Very happy with this change ...

Co-authored-by: Franck Nijhof <frenck@frenck.nl>

* Update source/_integrations/history_stats.markdown

Co-authored-by: Franck Nijhof <frenck@frenck.nl>

---
## [woolosh/joshuamoralesdev@e3fe5d0945...](https://github.com/woolosh/joshuamoralesdev/commit/e3fe5d09456b96ab739bd03c04855e8ab7e925df)
##### 2021-05-20 04:27:37 by Joshua Morales

Update README.md

<h1 align="center">Hi 👋, I'm Joshua</h1>
<h3 align="center">A Happy Learner</h3>

- 🔭 I’m currently working on **advanced Rails, JavaScript, and React projects**

- 🌱 I’m currently learning **so much! (Rails, React, ...)**

- 🤝 I’m looking for help with **Angular, C#, working in "the cloud", data analytics**

- 👨‍💻 All of my projects are available at [https://github.com/woolosh](https://github.com/woolosh)

- 📫 How to reach me **joshuapmorales@gmail.com**

- 📄 Know about my experiences [https://www.linkedin.com/in/joshua-morales-software-engineer/](https://www.linkedin.com/in/joshua-morales-software-engineer/)

- ⚡ Fun fact **I was in a band once, signed to Universal Records; and it lasted one year.**

<h3 align="left">Connect with me:</h3>
<p align="left">
<a href="https://linkedin.com/in/https://www.linkedin.com/in/joshua-morales-software-engineer/" target="blank"><img align="center" src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/neutral-icons/src/images/icons/Social/linked-in-alt.svg" alt="https://www.linkedin.com/in/joshua-morales-software-engineer/" height="30" width="40" /></a>
</p>

<h3 align="left">Languages and Tools:</h3>
<p align="left"> <a href="https://getbootstrap.com" target="_blank"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/bootstrap/bootstrap-plain-wordmark.svg" alt="bootstrap" width="40" height="40"/> </a> <a href="https://www.w3schools.com/css/" target="_blank"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/css3/css3-original-wordmark.svg" alt="css3" width="40" height="40"/> </a> <a href="https://git-scm.com/" target="_blank"> <img src="https://www.vectorlogo.zone/logos/git-scm/git-scm-icon.svg" alt="git" width="40" height="40"/> </a> <a href="https://www.w3.org/html/" target="_blank"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/html5/html5-original-wordmark.svg" alt="html5" width="40" height="40"/> </a> <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript" target="_blank"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/javascript/javascript-original.svg" alt="javascript" width="40" height="40"/> </a> <a href="https://www.postgresql.org" target="_blank"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/postgresql/postgresql-original-wordmark.svg" alt="postgresql" width="40" height="40"/> </a> <a href="https://rubyonrails.org" target="_blank"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/rails/rails-original-wordmark.svg" alt="rails" width="40" height="40"/> </a> <a href="https://reactjs.org/" target="_blank"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/react/react-original-wordmark.svg" alt="react" width="40" height="40"/> </a> <a href="https://www.ruby-lang.org/en/" target="_blank"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/ruby/ruby-original.svg" alt="ruby" width="40" height="40"/> </a> <a href="https://sass-lang.com" target="_blank"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/sass/sass-original.svg" alt="sass" width="40" height="40"/> </a> <a href="https://www.sqlite.org/" target="_blank"> <img src="https://www.vectorlogo.zone/logos/sqlite/sqlite-icon.svg" alt="sqlite" width="40" height="40"/> </a> </p>

---
## [canalplus/rx-player@3368b3dad8...](https://github.com/canalplus/rx-player/commit/3368b3dad87448af61656121023d6c109c904c58)
##### 2021-05-20 08:43:25 by Paul Berberian

remove RxJS code from the transports code

After doing a proof-of-concept looking at how some parts of the code
looks like without RxJS (#916), this is a first functional proposal
which looks good enough to me to be merged.

It removes all RxJS code from the `transports` code in `src/transports`.

As a reminder, the reasons for doing this are:

  1. Observables are complicated and full of implicit behaviors
     (lazily running, sync or async, unsubscribing automatically after
     the last unsubscription etc.) which is difficult to reason about,
     especially for a newcomer.

     Things like exploiting schedulers through the `deferSubscriptions`
     util to work-around some subtle potential race-conditions, or
     merging Observables in a specific order for similar reasons, are
     ugly hacks that are difficult to explain to someone not familiar
     with that code.

     Even for us with multiple years of experience with it, we sometimes
     struggle with it.

  2. Promises, event listeners - and direct callbacks in general - are
     generally much more explicit and most developpers (at least JS/TS
     devs) are familiar with them.

  3. Call stacks are close to inexploitable when using RxJS.

  4. Promises provide async/await syntax which can improve drastically
     the readability of our async-heavy code, which for the moment
     suffer from callback hells almost everywhere.

However, I'm still not sure if this wish (getting rid of RxJS) is shared
by other maintainers and/or contributors, so it is still only a proposal.

Thoughts?

---
## [VedantPol/CODEFORCES_ROUND_SOLUTIONS@5ab8873657...](https://github.com/VedantPol/CODEFORCES_ROUND_SOLUTIONS/commit/5ab887365733f60402c9a5c7ee59f3f7d4b852e6)
##### 2021-05-20 10:15:16 by vedant pol

Amusing Joke round #101 div2

So, the New Year holidays are over. Santa Claus and his colleagues can take a rest and have guests at last. When two "New Year and Christmas Men" meet, thear assistants cut out of cardboard the letters from the guest's name and the host's name in honor of this event. Then the hung the letters above the main entrance. One night, when everyone went to bed, someone took all the letters of our characters' names. Then he may have shuffled the letters and put them in one pile in front of the door.

The next morning it was impossible to find the culprit who had made the disorder. But everybody wondered whether it is possible to restore the names of the host and his guests from the letters lying at the door? That is, we need to verify that there are no extra letters, and that nobody will need to cut more letters.

Help the "New Year and Christmas Men" and their friends to cope with this problem. You are given both inscriptions that hung over the front door the previous night, and a pile of letters that were found at the front door next morning.

Input
The input file consists of three lines: the first line contains the guest's name, the second line contains the name of the residence host and the third line contains letters in a pile that were found at the door in the morning. All lines are not empty and contain only uppercase Latin letters. The length of each line does not exceed 100.

Output
Print "YES" without the quotes, if the letters in the pile could be permuted to make the names of the "New Year and Christmas Men". Otherwise, print "NO" without the quotes.

Examples
inputCopy
SANTACLAUS
DEDMOROZ
SANTAMOROZDEDCLAUS
outputCopy
YES
inputCopy
PAPAINOEL
JOULUPUKKI
JOULNAPAOILELUPUKKI
outputCopy
NO
inputCopy
BABBONATALE
FATHERCHRISTMAS
BABCHRISTMASBONATALLEFATHER
outputCopy
NO
Note
In the first sample the letters written in the last line can be used to write the names and there won't be any extra letters left.

In the second sample letter "P" is missing from the pile and there's an extra letter "L".

In the third sample there's an extra letter "L".

---
## [mrakgr/The-Spiral-Language@27779701a1...](https://github.com/mrakgr/The-Spiral-Language/commit/27779701a18e091d9a2f66d9d8f0df3d0faa129e)
##### 2021-05-20 10:54:37 by Marko Grdinić

"9:45am. I slept well tonight. I just need to go a bit further. I half about half of the true architecture in mind. Though I have the salience idea in mind, by itself it won't be enough to enforce temporal sparsity. But I do not want to just jam a bottleneck. I need one more step here, there should be some middle ground.

Also I need to bridge the gap in understanding on how to use unsupervised memory in NN layers. Transformers are supervised and they are great, but there should be more.

I won't be able to figure it out just like that, but I'll keep it in mind over the coming years. Right now, it does not even matter, since anything I could implement would just fry the GPU. I won't be able to move from simple games like poker for a while.

9:50am. Holy shit, Miura died. I thought it would be a joke when I saw it in the catalogue.

10:15am. Let me chill just a bit more.

10:30am. Let me start. I've slacked enough.

Time for the paper.

https://arxiv.org/abs/2008.02217
Hopfield Networks is All You Need

https://www.youtube.com/watch?v=IP3W7cI01VY
Lecture 11/16 : Hopfield nets and Boltzmann machines

Let me watch this for a bit. I want to remind myself of what Boltzmann machines are.

https://youtu.be/IP3W7cI01VY?t=2372

For the temporal bottleneck, it should be possible to smoothly interpolate between time horizons. The kind of bottleneck where each layer above is 2x slower might in theory be able to attent to exponentially long time horizons. But after a certain point, they'd never learn anything because the time horizons are simply too long. There has to be a better way of going about this. And just using saliency like I'd suggested would not be enough. It has to be a different kind of saliency.

10:50am. The video by Hinton is interesting, but back then they did not know what we know now.

https://youtu.be/IP3W7cI01VY?t=3002

Ah, I see. So Boltzman machines are quite similar to Hopfield nets. Visible are just units that have been clamped to some value.

https://youtu.be/IP3W7cI01VY?t=3078

Huh, this is the softmax. What are `u`, `g` all possible configurations? Ah, I see. In that case Boltzmann machines are not just an energy based model, but a probability distribution. Hopfield nets are not the later.

The first time I saw this video which was when it came out, I did not understand this at all, but now my understanding is much more vivid.

https://youtu.be/SY7ilsii2YM?t=16
Lecture 12/16 : Restricted Boltzmann machines (RBMs)

Let me watch this a little as well.

https://youtu.be/SY7ilsii2YM?t=390

Yeah, I understand now why I had so much difficulty understanding this back then. I did not understand the concept of sampling from a probability distribution. In fact, even now I am not sure how I'd sample from a Boltzmann machine. But it is not like these concepts are new to me. And now that I've read the EqProp thesis, I can relate that knowledge to this. The update rules for the weights of a Hopfield net and this are the same.

11:15am. How is sampling a RBM done? I forgot that.

11:30am. https://youtu.be/SY7ilsii2YM?t=1397

This is really vivid to me now. Back then I did not really understand how the weight updates for RBMs were derived. Probably because they were made up. But now there is a connection to EqProp.

https://youtu.be/SY7ilsii2YM?t=1787

I think I can anticipate how the sampling is done. I remember it iterating between forward and backward steps.

https://youtu.be/SY7ilsii2YM?t=2852

Yeah, I understand why you'd want to use a model that is an associative memory for this.

I am done with the video. I really get it completely now. Well, almost completely, I don't get how I'd implement MCMC for raw Boltzmann machines, but that does not matter.

11:45am. Let me go back to the paper.

3/95. So eq 3 here is for the reconstruction. Yeah, it is quite similar to the transformer rule. In the transformer X is calculated on the fly based on the keys. And big epsilon here is the query.

12pm. Ah, I thought that this might allow me to compress the data, but all the patterns need to be stored in a matrix as they are. Though, I guess that explains why they are exponential in capacity. It might be possible to reduce the store, by randomly sampling the matrix...

Ah, I see. If I have a replay buffer, I can easily turn that into a Boltzmann layer and sample from it.

12:15pm. Hmmm, is this really it. I would not have figured that the update rule given in the paper would work. Suppose you have a weight matrix of patterns like...

```
1,1,1
1,1,0
```

Then if you match a pattern like `1,1,0.1`, you'd expect it to be closer to the last one, but if a naive multiplication is done...

```
1*1+1*1+1*0.1=2.1
1*1+1*1+0*0.1=2
```

Then taking the softmax and summing the elements up, the reconstruction would be closer to the first element. I am not sure what to think about that. Simply because of that mental experiment, I'd assume that the algorithm is wrong.

Instead of multiplication it might be possible to fix it by taking the square distance. Or the absolute distance.

Hmmm, actually, suppose that 0.1 was negative. Then it would get further away from the first element.

So maybe it is fine. The noise would even out under this rule.

But the distance is the one which would get get glued to the first element even if it should not. The rule in the paper is the one which is in fact roboust.

12:30pm. Hmmm, one extra this paper gives me is a way of taking the weight matrix of a layer, and reconstructing its ideal input.

12:50pm. 11/95. Let me take a break here. I've read the body of the paper, the part starting here is the appendix.

So it turns out I did not need anything special to make the ideal associative single layer memories. The transformers were already it. When I saw the examples by Simon Thorpe, I was sure DL could not do it, but now I think it could given more capacity."

---
## [RajanKumarYadav/Machine-Learning-on-Microsoft-Azure@967416028c...](https://github.com/RajanKumarYadav/Machine-Learning-on-Microsoft-Azure/commit/967416028cdbc2bf8946261baad4faec7b74be8d)
##### 2021-05-20 15:22:23 by Rajan Kumar Yadav

Pima Indians Diabetes Database

This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.

Content
The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.

Acknowledgements
Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.

Inspiration
Can you build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?

---
## [mrakgr/The-Spiral-Language@6513fb5402...](https://github.com/mrakgr/The-Spiral-Language/commit/6513fb5402a65012238e176590f88918a6b4b2d3)
##### 2021-05-20 17:54:27 by Marko Grdinić

"2:25pm. Chores...

2:45pm. Let me resume reading. Time to dive into the appendix.

3:55pm. I admit, I hadn't been focusing too much, instead I am thinking what I could do using these Hopfield nets.

I could do more efficient credit assignment in RL. Instead of learning a value function, I could put the samples in the replay buffer, iterate forward and propagate them backward for each element in the vector. As long as the look ahead is not too long which is the case for each hand in poker, this would work well.

4pm. But it would be inefficient.

I can see how this could be used for hierarchical reconstruction.

Damn it, I am so close. I have all the main ingredients, but I need just a bit more to understand true temporal hierarchies. I know I said I already had that with the GAN module idea. The modules themselves are fine, but I was wrong about using an esemble to draw in the samples.

4:10pm. No, I do not know...I am missing insight here.

4:15pm. No, my ideas are too simplistic. I could have figured out the Hopfield update on my own, in fact such a rule could have easily occured to me, maybe it even did, but I cannot reason out its correctness or not through sheer imagination. It is a pity I do not have a mental module that implements all my ideas on its own.

I guess the subject of temporal data compression will have to be given time.

It is like a sponge, except I should imagine it in more than just 3 + time dimensions. Which I can't.

4:20pm. Ok, I am done with Hopfield nets. How will I finish the day? That paper by LeCunn on energy based models?

https://www.youtube.com/results?search_query=long+term+credit+assignment

This?

https://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-40-introduction-to-neural-computation-spring-2018/lecture-videos/index.htm

This is the only video I did not bother watching on Hopfield nets.

https://www.youtube.com/watch?v=WgynzzThQuA
Deep Learning — Jürgen Schmidhuber / Serious Science

Ah, let me watch random videos.

No, this is too basic.

https://www.youtube.com/watch?v=JWFFrugIfu4
Juergen Schmidhuber - Artificial Intelligence Lecture NIPS2017

Schmidhuber really does not have any good long videos out there.

4:45pm. Let me watch it.

4:55pm. https://youtu.be/JWFFrugIfu4?t=586

Here he mentions NN distilation just as I was thinking about it. I am starting to come around to the idea that randomly dropping information in the higher modules is always wrong. Instead I should be thinking about it in terms of model compression. Instead of ever dropping information, would it be viable for me to take the pattern matrix and compress it.

Now that I have these Hopfield nets that can take an exponential number of patterns, the next step would be to compress them. That seems a lot more viable as a method of doing temporal credit assignment.

5pm. Let me actually see what he has to say.

5:10pm. He mentions a few things like those recursive self improver players nobody can implement near the end.

Ilya is right. Weights should be long term memories. The way to go is to take these short term Hopfield layers and distil them into their own NN. This would allow the retrivals to be ammortized.

Using GANs for long term memory distilation?

Rather than having a separate nets to distil the short term memories, why not distil them into the main network?

Then I just arrive at where I started with GAN as temporal modules.

5:20pm. It is an interesting perspective.

Ok, let me go back to the earlier idea. Suppose GAN modules can do it.

The trouble with backprop is that it is dense. The weights get updated frequently depending on how sparse the gradients and inputs are.

Sparsity might be the answer. Maybe even current methods could work if sparsity is maximized. Current ANNs are very dense, in contrast to the brain's.

5:25pm. Ok, enough of this. I am done studying. The paper by LeCunn is from 2006, so I do not feel like reading it.

Let me watch the lecture I linked to.

https://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-40-introduction-to-neural-computation-spring-2018/lecture-videos/index.htm

The last one is on Hopfield nets.

I want to see what neurobiology has to say about them.

5:40pm. https://youtu.be/gt52wUN3VrQ?t=703

This lecture is not so bad. I know very little about the actual brain. Here he is saying that most neurons have the same behavior as using a nonlinearity such as tanh. The sodium channels begin to saturate and they can't fire any faster.

...Ok, my prediction is that I need to see follow up work on Hopfield nets and the transformer. Transformers are great, but once NN distilation gets underway for amortizing the queries, and feedback connections get added between modules to help the lower modules refine their predictions, and RNNs start getting used due to their efficiency - then that is when things will start getting serious.

I am not sure what the right way to do NN distilation is, but no doubt others will get to it.

I just need to hold steady until the findings get here.

Tomorrow, I am going to get back into programming. I think I have enough knowledge at this point in time.

6pm. https://youtu.be/gt52wUN3VrQ?t=1504

Let me take a break here.

6:15pm. Let me resume.

He mentions that the Hopfield net is the best model of the way memory works in the hippocampus.

7:05pm. Done with lunch. Let me watch the lecture to completion and I will call it a day.

https://youtu.be/gt52wUN3VrQ?t=3863

This lecture is disappointing. The cover the old style, inefficient Hopfield nets and not the cool new ones. Neuroscience is really just sitting on its hands until the hardware required for them to measure biological brains at scale gets here.

7:35pm. I left a little comment saying that these are old school nets.

Let me close for the day here. Time to game a little. The trip down the memory lane is over. Tomorrow, I will program. I doubt that there will be anything to distract me from that anymore. I definitely won't start researching network distilation as I know that won't get me anywhere."

---
## [Terrain2/Jelly-Drift@99e00f5f94...](https://github.com/Terrain2/Jelly-Drift/commit/99e00f5f94ab0d1973f9163ce4c18bf4f9ae5da9)
##### 2021-05-20 21:12:47 by Terrain2

(CURSED) Migrate to New Input System

This makes some input effects a lot easier. This version of the game is also INCREDIBLY CURSED, with the bug that lets you have many chaos timers and also whatever the fuck the controls are!

---

# [<](2021-05-19.md) 2021-05-20 [>](2021-05-21.md)

