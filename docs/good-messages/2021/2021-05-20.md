# [<](2021-05-19.md) 2021-05-20 [>](2021-05-21.md)

3,951,823 events, 1,448,622 push events, 2,365,869 commit messages, 185,728,016 characters


## [i3roly/glibc_ddwrt](https://github.com/i3roly/glibc_ddwrt)@[56abe0ddcd...](https://github.com/i3roly/glibc_ddwrt/commit/56abe0ddcdac83f7590dab762b3b0aefc06c2877)
#### Thursday 2021-05-20 00:08:21 by gagan sidhu

update kernels, tighten libraries, use DD-WRT iproute

back in when i started this gangsta shit (lol), i had to make my own
iproute2, since BS was on a ghetto version. at that time, i used his
makefile from the old version to make my own. this was a recurring theme.

since i had to rework much of my build tree to accommodate multiplatform
builds, i decided to tighten up the libraries in the existing mipsel
builds.

you'll notice there's only one libdnet library (hey, gotta pay respect)
instead of two.

i'm now using his iproute2 instead of my own since it's probably better.

he does some silly things that are noticeable on the much-faster TP-LINK
AX50. for example, in rc/init.c he calls devinit via command line (like
'system'), which essentially can destroy init since the code doesn't
wait for a return. for the much-faster interaptiv, this became
noticeable and destroyed the entire init process.
    -as such, i've moved the mounting and mknod creation for the nvram
    to the rc/init.c, in addition to leaving the code in
    services/sysinit/devinit.c.
    -i don't think he can change things like this when he's supporting
    like 1000 routers. it's easier to do when you're working with a
    handful. i can't see my number going that much higher. the AX50 is a
    challenge and it's seriously fast, so i decided to give it a go.

i didn't test this because i'm at 20 days uptime and the existing build
is good, and i wanna keep my uptime going. but there should be no
problems.

if there are, file a ticket and enjoy the existing build as much as i
am.

---
## [gitster/git](https://github.com/gitster/git)@[ae1a7eefff...](https://github.com/gitster/git/commit/ae1a7eefffe60425e6bf6a2065e042ae051cfb6c)
#### Thursday 2021-05-20 06:58:11 by Jeff King

fetch-pack: signal v2 server that we are done making requests

When fetching with the v0 protocol over ssh (or a local upload-pack with
pipes), the server closes the connection as soon as it is finished
sending the pack. So even though the client may still be operating on
the data via index-pack (e.g., resolving deltas, checking connectivity,
etc), the server has released all resources.

With the v2 protocol, however, the server considers the ssh session only
as a transport, with individual requests coming over it. After sending
the pack, it goes back to its main loop, waiting for another request to
come from the client. As a result, the ssh session hangs around until
the client process ends, which may be much later (because resolving
deltas, etc, may consume a lot of CPU).

This is bad for two reasons:

  - it's consuming resources on the server to leave open a connection
    that won't see any more use

  - if something bad happens to the ssh connection in the meantime (say,
    it gets killed by the network because it's idle, as happened in a
    real-world report), then ssh will exit non-zero, and we'll propagate
    the error up the stack.

The server is correct here not to hang up after serving the pack. The v2
protocol's design is meant to allow multiple requests like this, and
hanging up would be the wrong thing for a hypothetical client which was
planning to make more requests (though in practice, the git.git client
never would, and I doubt any other implementations would either).

The right thing is instead for the client to signal to the server that
it's not interested in making more requests. We can do that by closing
the pipe descriptor we use to write to ssh. This will propagate to the
server upload-pack as an EOF when it tries to read the next request (and
then it will close its half, and the whole connection will go away).

It's important to do this "half duplex" shutdown, because we have to do
it _before_ we actually receive the pack. This is an artifact of the way
fetch-pack and index-pack (or unpack-objects) interact. We hand the
connection off to index-pack (really, a sideband demuxer which feeds
it), and then wait until it returns. And it doesn't do that until it has
resolved all of the deltas in the pack, even though it was done reading
from the server long before.

So just closing the connection fully after index-pack returns would be
too late; we'd have held it open much longer than was necessary. And
teaching index-pack to close the connection is awkward. It's not even
seeing the whole conversation (the sideband demuxer is, but it doesn't
actually know what's in the packets, or when the end comes).

Note that this close() is happening deep within the transport code. It's
possible that a caller would want to perform other operations over the
same ssh transport after receiving the pack. But as of the current code,
none of the callers do, and there haven't been discussions of any plans
to change this. If we need to support that later, we can probably do so
by passing down a flag for "you're the last request on the transport;
it's OK to close" instead of the code just assuming that's true.

The description above all discusses v2 ssh, so it's worth thinking about
how this interacts with other protocols:

  - in v0 protocols, we could do the same half-duplex shutdown (it just
    goes into the v0 do_fetch_pack() instead). This does work, but since
    it doesn't have the same persistence problem in the first place,
    there's little reason to change it at this point.

  - local fetches against git-upload-pack on the same machine will
    behave the same as ssh (they are talking over two pipes, and see EOF
    on their input pipe)

  - fetches against git-daemon will run this same code, and close one of
    the descriptors. In practice, this won't do anything, since there
    our two descriptors are dups of each other, and not part of a
    half-duplex pair. The right thing would probably be to call
    shutdown(SHUT_WR) on it. I didn't bother with that here. It doesn't
    face the same error-code problem (since it's just a TCP connection),
    so it's really only an optimization problem. And git:// is not that
    widely used these days, and has less impact on server resources than
    an ssh termination.

  - v2 http doesn't suffer from this problem in the first place, as our
    pipes terminate at a local git-remote-https, which is passing data
    along as individual requests via curl. Probably curl is keeping the
    TCP/TLS connection open for more requests, and we might be able to
    tell it manually "hey, we are done making requests now". But I think
    that's much less important. It again doesn't suffer from the
    error-code problem, and HTTP keepalive is pretty well understood
    (importantly, the timeouts can be set low, because clients like curl
    know how to reconnect for subsequent requests if necessary). So it's
    probably not worth figuring out how to tell curl that we're done
    (though if we do, this patch is probably the first step anyway;
    fetch-pack closes the pipe back to remote-https, which would be the
    signal that it should tell curl we're done).

The code is pretty straightforward. We close the pipe at the right
moment, and set it to -1 to mark it as invalid. I modified the later
cleanup code to avoid calling close(-1). That's not strictly necessary,
since close(-1) is a noop, but hopefully makes things a bit more obvious
to a reader.

I suspect that trying to call more transport functions after the close()
(e.g., calling transport_fetch_refs() again) would fail, as it's not
smart enough to realize we need to re-open the ssh connection. But
that's already true when v0 is in use. And no current callers want to do
that (and again, the solution is probably a flag in the transport code
to keep things open, which can be added later).

There's no test here, as the situation it covers is inherently racy (the
question is when upload-pack exits, compared to when index-pack finishes
resolving deltas and exits). The rather gross shell snippet below does
recreate the problematic situation; when run on a sufficiently-large
repository (git.git works fine), it kills an "idle" upload-pack while
the client is resolving deltas, leading to a failed clone.

    (
	    git clone --no-local --progress . foo.git 2>&1
	    echo >&2 "clone exit code=$?"
    ) |
    tr '\r' '\n' |
    while read line
    do
	    case "$done,$line" in
	    ,Resolving*)
		    echo "hit resolving deltas; killing upload-pack"
		    killall -9 git-upload-pack
		    done=t
		    ;;
	    esac
    done

Reported-by: Greg Pflaum <greg.pflaum@pnp-hcl.com>
Signed-off-by: Jeff King <peff@peff.net>
Signed-off-by: Junio C Hamano <gitster@pobox.com>

---
## [newstools/1970-national-mirror-nigeria](https://github.com/newstools/1970-national-mirror-nigeria)@[8c1f245e25...](https://github.com/newstools/1970-national-mirror-nigeria/commit/8c1f245e25cbfba0a59e102a7e06b009504d1454)
#### Thursday 2021-05-20 07:42:40 by Billy Einkamerer

Created Text For URL [www.nationalmirroronline.net/rapper-a-ap-confirms-he-is-dating-rihanna-says-she-is-the-love-of-his-life-7681.html]

---
## [VedantPol/CODEFORCES_ROUND_SOLUTIONS](https://github.com/VedantPol/CODEFORCES_ROUND_SOLUTIONS)@[5ab8873657...](https://github.com/VedantPol/CODEFORCES_ROUND_SOLUTIONS/commit/5ab887365733f60402c9a5c7ee59f3f7d4b852e6)
#### Thursday 2021-05-20 10:15:16 by vedant pol

Amusing Joke round #101 div2

So, the New Year holidays are over. Santa Claus and his colleagues can take a rest and have guests at last. When two "New Year and Christmas Men" meet, thear assistants cut out of cardboard the letters from the guest's name and the host's name in honor of this event. Then the hung the letters above the main entrance. One night, when everyone went to bed, someone took all the letters of our characters' names. Then he may have shuffled the letters and put them in one pile in front of the door.

The next morning it was impossible to find the culprit who had made the disorder. But everybody wondered whether it is possible to restore the names of the host and his guests from the letters lying at the door? That is, we need to verify that there are no extra letters, and that nobody will need to cut more letters.

Help the "New Year and Christmas Men" and their friends to cope with this problem. You are given both inscriptions that hung over the front door the previous night, and a pile of letters that were found at the front door next morning.

Input
The input file consists of three lines: the first line contains the guest's name, the second line contains the name of the residence host and the third line contains letters in a pile that were found at the door in the morning. All lines are not empty and contain only uppercase Latin letters. The length of each line does not exceed 100.

Output
Print "YES" without the quotes, if the letters in the pile could be permuted to make the names of the "New Year and Christmas Men". Otherwise, print "NO" without the quotes.

Examples
inputCopy
SANTACLAUS
DEDMOROZ
SANTAMOROZDEDCLAUS
outputCopy
YES
inputCopy
PAPAINOEL
JOULUPUKKI
JOULNAPAOILELUPUKKI
outputCopy
NO
inputCopy
BABBONATALE
FATHERCHRISTMAS
BABCHRISTMASBONATALLEFATHER
outputCopy
NO
Note
In the first sample the letters written in the last line can be used to write the names and there won't be any extra letters left.

In the second sample letter "P" is missing from the pile and there's an extra letter "L".

In the third sample there's an extra letter "L".

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[27779701a1...](https://github.com/mrakgr/The-Spiral-Language/commit/27779701a18e091d9a2f66d9d8f0df3d0faa129e)
#### Thursday 2021-05-20 10:54:37 by Marko Grdinić

"9:45am. I slept well tonight. I just need to go a bit further. I half about half of the true architecture in mind. Though I have the salience idea in mind, by itself it won't be enough to enforce temporal sparsity. But I do not want to just jam a bottleneck. I need one more step here, there should be some middle ground.

Also I need to bridge the gap in understanding on how to use unsupervised memory in NN layers. Transformers are supervised and they are great, but there should be more.

I won't be able to figure it out just like that, but I'll keep it in mind over the coming years. Right now, it does not even matter, since anything I could implement would just fry the GPU. I won't be able to move from simple games like poker for a while.

9:50am. Holy shit, Miura died. I thought it would be a joke when I saw it in the catalogue.

10:15am. Let me chill just a bit more.

10:30am. Let me start. I've slacked enough.

Time for the paper.

https://arxiv.org/abs/2008.02217
Hopfield Networks is All You Need

https://www.youtube.com/watch?v=IP3W7cI01VY
Lecture 11/16 : Hopfield nets and Boltzmann machines

Let me watch this for a bit. I want to remind myself of what Boltzmann machines are.

https://youtu.be/IP3W7cI01VY?t=2372

For the temporal bottleneck, it should be possible to smoothly interpolate between time horizons. The kind of bottleneck where each layer above is 2x slower might in theory be able to attent to exponentially long time horizons. But after a certain point, they'd never learn anything because the time horizons are simply too long. There has to be a better way of going about this. And just using saliency like I'd suggested would not be enough. It has to be a different kind of saliency.

10:50am. The video by Hinton is interesting, but back then they did not know what we know now.

https://youtu.be/IP3W7cI01VY?t=3002

Ah, I see. So Boltzman machines are quite similar to Hopfield nets. Visible are just units that have been clamped to some value.

https://youtu.be/IP3W7cI01VY?t=3078

Huh, this is the softmax. What are `u`, `g` all possible configurations? Ah, I see. In that case Boltzmann machines are not just an energy based model, but a probability distribution. Hopfield nets are not the later.

The first time I saw this video which was when it came out, I did not understand this at all, but now my understanding is much more vivid.

https://youtu.be/SY7ilsii2YM?t=16
Lecture 12/16 : Restricted Boltzmann machines (RBMs)

Let me watch this a little as well.

https://youtu.be/SY7ilsii2YM?t=390

Yeah, I understand now why I had so much difficulty understanding this back then. I did not understand the concept of sampling from a probability distribution. In fact, even now I am not sure how I'd sample from a Boltzmann machine. But it is not like these concepts are new to me. And now that I've read the EqProp thesis, I can relate that knowledge to this. The update rules for the weights of a Hopfield net and this are the same.

11:15am. How is sampling a RBM done? I forgot that.

11:30am. https://youtu.be/SY7ilsii2YM?t=1397

This is really vivid to me now. Back then I did not really understand how the weight updates for RBMs were derived. Probably because they were made up. But now there is a connection to EqProp.

https://youtu.be/SY7ilsii2YM?t=1787

I think I can anticipate how the sampling is done. I remember it iterating between forward and backward steps.

https://youtu.be/SY7ilsii2YM?t=2852

Yeah, I understand why you'd want to use a model that is an associative memory for this.

I am done with the video. I really get it completely now. Well, almost completely, I don't get how I'd implement MCMC for raw Boltzmann machines, but that does not matter.

11:45am. Let me go back to the paper.

3/95. So eq 3 here is for the reconstruction. Yeah, it is quite similar to the transformer rule. In the transformer X is calculated on the fly based on the keys. And big epsilon here is the query.

12pm. Ah, I thought that this might allow me to compress the data, but all the patterns need to be stored in a matrix as they are. Though, I guess that explains why they are exponential in capacity. It might be possible to reduce the store, by randomly sampling the matrix...

Ah, I see. If I have a replay buffer, I can easily turn that into a Boltzmann layer and sample from it.

12:15pm. Hmmm, is this really it. I would not have figured that the update rule given in the paper would work. Suppose you have a weight matrix of patterns like...

```
1,1,1
1,1,0
```

Then if you match a pattern like `1,1,0.1`, you'd expect it to be closer to the last one, but if a naive multiplication is done...

```
1*1+1*1+1*0.1=2.1
1*1+1*1+0*0.1=2
```

Then taking the softmax and summing the elements up, the reconstruction would be closer to the first element. I am not sure what to think about that. Simply because of that mental experiment, I'd assume that the algorithm is wrong.

Instead of multiplication it might be possible to fix it by taking the square distance. Or the absolute distance.

Hmmm, actually, suppose that 0.1 was negative. Then it would get further away from the first element.

So maybe it is fine. The noise would even out under this rule.

But the distance is the one which would get get glued to the first element even if it should not. The rule in the paper is the one which is in fact roboust.

12:30pm. Hmmm, one extra this paper gives me is a way of taking the weight matrix of a layer, and reconstructing its ideal input.

12:50pm. 11/95. Let me take a break here. I've read the body of the paper, the part starting here is the appendix.

So it turns out I did not need anything special to make the ideal associative single layer memories. The transformers were already it. When I saw the examples by Simon Thorpe, I was sure DL could not do it, but now I think it could given more capacity."

---
## [Notaverysmartdev/UnityProject](https://github.com/Notaverysmartdev/UnityProject)@[7e18d1406d...](https://github.com/Notaverysmartdev/UnityProject/commit/7e18d1406d2116bb1abc8e4406f07b20c60cfed6)
#### Thursday 2021-05-20 12:24:38 by Vladis Kosta

(Made the tank able to cilmb up slopes its kinda clunky rn so its not that great but took me hours to make)

---
## [elan-ev/tobira](https://github.com/elan-ev/tobira)@[18ac20e1db...](https://github.com/elan-ev/tobira/commit/18ac20e1db62b3b4a7327340efabd220b05fa8b6)
#### Thursday 2021-05-20 12:46:57 by Lukas Kalbertodt

Fix typo in fixture text

Oh gosh, I remember even looking up how to escape ' in this SQL string.
Whhyyyyyy. I am shame.

---
## [neekless/proton_kernel_wahoo](https://github.com/neekless/proton_kernel_wahoo)@[fa51e47fb9...](https://github.com/neekless/proton_kernel_wahoo/commit/fa51e47fb9fd009aa35b43312018f8ef82821fcb)
#### Thursday 2021-05-20 12:51:26 by Peter Zijlstra

sched/core: Fix ttwu() race

Paul reported rcutorture occasionally hitting a NULL deref:

  sched_ttwu_pending()
    ttwu_do_wakeup()
      check_preempt_curr() := check_preempt_wakeup()
        find_matching_se()
          is_same_group()
            if (se->cfs_rq == pse->cfs_rq) <-- *BOOM*

Debugging showed that this only appears to happen when we take the new
code-path from commit:

  2ebb17717550 ("sched/core: Offload wakee task activation if it the wakee is descheduling")

and only when @cpu == smp_processor_id(). Something which should not
be possible, because p->on_cpu can only be true for remote tasks.
Similarly, without the new code-path from commit:

  c6e7bd7afaeb ("sched/core: Optimize ttwu() spinning on p->on_cpu")

this would've unconditionally hit:

  smp_cond_load_acquire(&p->on_cpu, !VAL);

and if: 'cpu == smp_processor_id() && p->on_cpu' is possible, this
would result in an instant live-lock (with IRQs disabled), something
that hasn't been reported.

The NULL deref can be explained however if the task_cpu(p) load at the
beginning of try_to_wake_up() returns an old value, and this old value
happens to be smp_processor_id(). Further assume that the p->on_cpu
load accurately returns 1, it really is still running, just not here.

Then, when we enqueue the task locally, we can crash in exactly the
observed manner because p->se.cfs_rq != rq->cfs_rq, because p's cfs_rq
is from the wrong CPU, therefore we'll iterate into the non-existant
parents and NULL deref.

The closest semi-plausible scenario I've managed to contrive is
somewhat elaborate (then again, actual reproduction takes many CPU
hours of rcutorture, so it can't be anything obvious):

					X->cpu = 1
					rq(1)->curr = X

	CPU0				CPU1				CPU2

					// switch away from X
					LOCK rq(1)->lock
					smp_mb__after_spinlock
					dequeue_task(X)
					  X->on_rq = 9
					switch_to(Z)
					  X->on_cpu = 0
					UNLOCK rq(1)->lock

									// migrate X to cpu 0
									LOCK rq(1)->lock
									dequeue_task(X)
									set_task_cpu(X, 0)
									  X->cpu = 0
									UNLOCK rq(1)->lock

									LOCK rq(0)->lock
									enqueue_task(X)
									  X->on_rq = 1
									UNLOCK rq(0)->lock

	// switch to X
	LOCK rq(0)->lock
	smp_mb__after_spinlock
	switch_to(X)
	  X->on_cpu = 1
	UNLOCK rq(0)->lock

	// X goes sleep
	X->state = TASK_UNINTERRUPTIBLE
	smp_mb();			// wake X
					ttwu()
					  LOCK X->pi_lock
					  smp_mb__after_spinlock

					  if (p->state)

					  cpu = X->cpu; // =? 1

					  smp_rmb()

	// X calls schedule()
	LOCK rq(0)->lock
	smp_mb__after_spinlock
	dequeue_task(X)
	  X->on_rq = 0

					  if (p->on_rq)

					  smp_rmb();

					  if (p->on_cpu && ttwu_queue_wakelist(..)) [*]

					  smp_cond_load_acquire(&p->on_cpu, !VAL)

					  cpu = select_task_rq(X, X->wake_cpu, ...)
					  if (X->cpu != cpu)
	switch_to(Y)
	  X->on_cpu = 0
	UNLOCK rq(0)->lock

However I'm having trouble convincing myself that's actually possible
on x86_64 -- after all, every LOCK implies an smp_mb() there, so if ttwu
observes ->state != RUNNING, it must also observe ->cpu != 1.

(Most of the previous ttwu() races were found on very large PowerPC)

Nevertheless, this fully explains the observed failure case.

Fix it by ordering the task_cpu(p) load after the p->on_cpu load,
which is easy since nothing actually uses @cpu before this.

Fixes: c6e7bd7afaeb ("sched/core: Optimize ttwu() spinning on p->on_cpu")
Reported-by: Paul E. McKenney <paulmck@kernel.org>
Tested-by: Paul E. McKenney <paulmck@kernel.org>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lkml.kernel.org/r/20200622125649.GC576871@hirez.programming.kicks-ass.net
Signed-off-by: atndko <z1281552865@gmail.com>
(cherry picked from commit d07c4446c3afe13344e106896b76f12af946919a)

 Conflicts:
	kernel/sched/core.c

---
## [platoengine/platoanalyze](https://github.com/platoengine/platoanalyze)@[49f22d2b76...](https://github.com/platoengine/platoanalyze/commit/49f22d2b76cbb14dde8a0948857ed791fce63e08)
#### Thursday 2021-05-20 13:32:54 by Jonathan Brent Russ

no error... no warning... not even a message like 'hey arent nullptrs cool? check out the sweet word vomit that will follow' ... just a nullptr baby! Well now Analyze only has 5 nullptrs being returned without warnings... the nullptr party is sad now but the potential future users just got slightly happier with their english error message. :) ... oh and now you can actually use the volume again in a weighted sum in Analyze. which means you can use the new stress constraint functions... lovely!

---
## [TYPO3/TYPO3.CMS](https://github.com/TYPO3/TYPO3.CMS)@[e0a47f7437...](https://github.com/TYPO3/TYPO3.CMS/commit/e0a47f743700cae25dcf4c26737c1d75a95a9dec)
#### Thursday 2021-05-20 13:58:05 by Christian Kuhn

[TASK] Render TS and pageTS snippets with CodeMirror

The TypoScript parser has a built in syntax hightlighter.
This is used by ext:tstemplate template analyzer and a sub
section of ext:info PageTsConfig.

We however have a much better syntax highlighter: t3editor.

The patch commits some long overdue love to the template
analyzer to use CodeMirror rendering for TypoScript
snippets. It kicks in if t3editor is loaded, otherwise a
textarea is rendered.

This allows to drop a metric ton of horky borky code from
TypoScriptParser and friends - enabling further refactorings
that may eventually end up with performance improvements
in some happy future?

Resolves: #94173
Releases: master
Change-Id: I895d76c6b8b801d2499b7afe2cf21bfb811ea02b
Reviewed-on: https://review.typo3.org/c/Packages/TYPO3.CMS/+/69139
Tested-by: Oliver Bartsch <bo@cedev.de>
Tested-by: Christian Kuhn <lolli@schwarzbu.ch>
Tested-by: core-ci <typo3@b13.com>
Tested-by: Benjamin Franzke <bfr@qbus.de>
Reviewed-by: Oliver Bartsch <bo@cedev.de>
Reviewed-by: Christian Kuhn <lolli@schwarzbu.ch>
Reviewed-by: Benjamin Franzke <bfr@qbus.de>

---
## [RajanKumarYadav/Machine-Learning-on-Microsoft-Azure](https://github.com/RajanKumarYadav/Machine-Learning-on-Microsoft-Azure)@[967416028c...](https://github.com/RajanKumarYadav/Machine-Learning-on-Microsoft-Azure/commit/967416028cdbc2bf8946261baad4faec7b74be8d)
#### Thursday 2021-05-20 15:22:23 by Rajan Kumar Yadav

Pima Indians Diabetes Database

This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.

Content
The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.

Acknowledgements
Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.

Inspiration
Can you build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?

---
## [SD11B-Group-3/VaccInfo](https://github.com/SD11B-Group-3/VaccInfo)@[e0027f6f17...](https://github.com/SD11B-Group-3/VaccInfo/commit/e0027f6f17b8511b7bc3c4e03f2d1b9fdca112f7)
#### Thursday 2021-05-20 16:56:12 by Dana Louise

Merge pull request #3 from SD11B-Group-3/Kean---Don't-open-if-you're-not-Kean-for-the-love-of-God

Kean   don't open if you're not kean for the love of god

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[6513fb5402...](https://github.com/mrakgr/The-Spiral-Language/commit/6513fb5402a65012238e176590f88918a6b4b2d3)
#### Thursday 2021-05-20 17:54:27 by Marko Grdinić

"2:25pm. Chores...

2:45pm. Let me resume reading. Time to dive into the appendix.

3:55pm. I admit, I hadn't been focusing too much, instead I am thinking what I could do using these Hopfield nets.

I could do more efficient credit assignment in RL. Instead of learning a value function, I could put the samples in the replay buffer, iterate forward and propagate them backward for each element in the vector. As long as the look ahead is not too long which is the case for each hand in poker, this would work well.

4pm. But it would be inefficient.

I can see how this could be used for hierarchical reconstruction.

Damn it, I am so close. I have all the main ingredients, but I need just a bit more to understand true temporal hierarchies. I know I said I already had that with the GAN module idea. The modules themselves are fine, but I was wrong about using an esemble to draw in the samples.

4:10pm. No, I do not know...I am missing insight here.

4:15pm. No, my ideas are too simplistic. I could have figured out the Hopfield update on my own, in fact such a rule could have easily occured to me, maybe it even did, but I cannot reason out its correctness or not through sheer imagination. It is a pity I do not have a mental module that implements all my ideas on its own.

I guess the subject of temporal data compression will have to be given time.

It is like a sponge, except I should imagine it in more than just 3 + time dimensions. Which I can't.

4:20pm. Ok, I am done with Hopfield nets. How will I finish the day? That paper by LeCunn on energy based models?

https://www.youtube.com/results?search_query=long+term+credit+assignment

This?

https://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-40-introduction-to-neural-computation-spring-2018/lecture-videos/index.htm

This is the only video I did not bother watching on Hopfield nets.

https://www.youtube.com/watch?v=WgynzzThQuA
Deep Learning — Jürgen Schmidhuber / Serious Science

Ah, let me watch random videos.

No, this is too basic.

https://www.youtube.com/watch?v=JWFFrugIfu4
Juergen Schmidhuber - Artificial Intelligence Lecture NIPS2017

Schmidhuber really does not have any good long videos out there.

4:45pm. Let me watch it.

4:55pm. https://youtu.be/JWFFrugIfu4?t=586

Here he mentions NN distilation just as I was thinking about it. I am starting to come around to the idea that randomly dropping information in the higher modules is always wrong. Instead I should be thinking about it in terms of model compression. Instead of ever dropping information, would it be viable for me to take the pattern matrix and compress it.

Now that I have these Hopfield nets that can take an exponential number of patterns, the next step would be to compress them. That seems a lot more viable as a method of doing temporal credit assignment.

5pm. Let me actually see what he has to say.

5:10pm. He mentions a few things like those recursive self improver players nobody can implement near the end.

Ilya is right. Weights should be long term memories. The way to go is to take these short term Hopfield layers and distil them into their own NN. This would allow the retrivals to be ammortized.

Using GANs for long term memory distilation?

Rather than having a separate nets to distil the short term memories, why not distil them into the main network?

Then I just arrive at where I started with GAN as temporal modules.

5:20pm. It is an interesting perspective.

Ok, let me go back to the earlier idea. Suppose GAN modules can do it.

The trouble with backprop is that it is dense. The weights get updated frequently depending on how sparse the gradients and inputs are.

Sparsity might be the answer. Maybe even current methods could work if sparsity is maximized. Current ANNs are very dense, in contrast to the brain's.

5:25pm. Ok, enough of this. I am done studying. The paper by LeCunn is from 2006, so I do not feel like reading it.

Let me watch the lecture I linked to.

https://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-40-introduction-to-neural-computation-spring-2018/lecture-videos/index.htm

The last one is on Hopfield nets.

I want to see what neurobiology has to say about them.

5:40pm. https://youtu.be/gt52wUN3VrQ?t=703

This lecture is not so bad. I know very little about the actual brain. Here he is saying that most neurons have the same behavior as using a nonlinearity such as tanh. The sodium channels begin to saturate and they can't fire any faster.

...Ok, my prediction is that I need to see follow up work on Hopfield nets and the transformer. Transformers are great, but once NN distilation gets underway for amortizing the queries, and feedback connections get added between modules to help the lower modules refine their predictions, and RNNs start getting used due to their efficiency - then that is when things will start getting serious.

I am not sure what the right way to do NN distilation is, but no doubt others will get to it.

I just need to hold steady until the findings get here.

Tomorrow, I am going to get back into programming. I think I have enough knowledge at this point in time.

6pm. https://youtu.be/gt52wUN3VrQ?t=1504

Let me take a break here.

6:15pm. Let me resume.

He mentions that the Hopfield net is the best model of the way memory works in the hippocampus.

7:05pm. Done with lunch. Let me watch the lecture to completion and I will call it a day.

https://youtu.be/gt52wUN3VrQ?t=3863

This lecture is disappointing. The cover the old style, inefficient Hopfield nets and not the cool new ones. Neuroscience is really just sitting on its hands until the hardware required for them to measure biological brains at scale gets here.

7:35pm. I left a little comment saying that these are old school nets.

Let me close for the day here. Time to game a little. The trip down the memory lane is over. Tomorrow, I will program. I doubt that there will be anything to distract me from that anymore. I definitely won't start researching network distilation as I know that won't get me anywhere."

---
## [Perkedel/Kaded-fnf-mods](https://github.com/Perkedel/Kaded-fnf-mods)@[1be2137758...](https://github.com/Perkedel/Kaded-fnf-mods/commit/1be21377589dc7940ee5bfc08cf9cdab204d6348)
#### Thursday 2021-05-20 20:35:16 by Joel Robert Justiawan

habrumishket

attempted to optimize compiler conditional #if desktop directive thingy for other platforms like Android, neko, and hl.

kinda works. okay it doesn't. it just crashed. idk what happened. it just builds but not working at all.

thancc @luckydog7 with the https://github.com/luckydog7/Funkin-android the fnf android port for the reference.

referensi hah huh hi ha hu

apa

pls help. kade must be also on android idk how lol lmao!

also I've continued the gf-ht xml. OMG, so many frames I got to cover, oh God. pls help!

---
## [rorydale/pointbreakradio](https://github.com/rorydale/pointbreakradio)@[9b0cd8fbbc...](https://github.com/rorydale/pointbreakradio/commit/9b0cd8fbbc72e0f11268d7728b401cfc715e5fef)
#### Thursday 2021-05-20 22:50:21 by Rory Dale

2021-05-20

Thursday, May 20th, 2021 - the 1990s progressive rock show! Slipping into something a little more comfortable, today's show was a meander through 1990s progressive rock, year by year. I love the epics, I love the weirdness, I love how challenging it all was and still is, and I like being able to reflect on it all now a little more objectively and see where it all fits into the larger musical picture. I hope you enjoy it too!

---

# [<](2021-05-19.md) 2021-05-20 [>](2021-05-21.md)

