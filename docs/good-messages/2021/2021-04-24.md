# [<](2021-04-23.md) 2021-04-24 [>](2021-04-25.md)

2,040,255 events, 1,136,751 push events, 1,707,163 commit messages, 105,038,475 characters


## [HandSonic/rufus@252759eb91...](https://github.com/HandSonic/rufus/commit/252759eb917018392913245f58ab321a7ed5b42b)
##### 2021-04-24 00:06:33 by Pete Batard

[grub] add yet another frigging patch to GRUB "2.04"

* GRUB 2.0 maintainer think they're doing a fine job, even when there are
  CRITICAL SECURITY FIXES that should warrant an immediate out of bound
  release, and instead consider that waiting MONTHS or YEARS to release
  anything is not a big deal at all.
* Ergo, distros, such as Ubuntu, start to pick whatever security patches
  they see fit, since they can simply not RELY on the upstream project to
  produce security releases in a timely manner. One such patch is:
  https://lists.gnu.org/archive/html/grub-devel/2021-03/msg00012.html
* But since there is no new GRUB release per se, they still call their GRUB
  version, onto which they applied patches that have come into existence
  more than 2 years after the actual 2.04 release, "GRUB 2.04".
* Obviously, since GRUB 2.04 + literally hundreds of cherry picked patches
  does deviate a lot from the last release, THINGS BREAK IN SPECTACULAR
  FASHION, such as the recently released Ubuntu 21.04 failing to boot with
  the error: grub_register_command_lockdown not found.
* Oh, and of course, regardless of all the above, if you ask anyone, they'll
  tell you that there's nothing fundamentally wrong with the GRUB release
  process (even if they should long have released 2.05, 2.05-1 and 2.05-2,
  were their maintainer ready to acknowledge that delaying releases DOES
  CREATES MAJOR ISSUES DOWSTREAM, as many people REPEATEDLY pointed to them
  on the GRUB mailing list) or with the Ubuntu GRUB versioning process (that
  really shouldn't be calling their version of GRUB "grub-2.04" but instead
  something like "grub-2.04_ubuntu"). Oh no siree! Instead, the problem must
  all be with Rufus and its maintainer, who should either spend their lives
  pre-emptively figuring which breaking patch every other distro applied out
  there, or limit media creation to DD mode, like any "sensible" person
  would do, since DD mode is the ultimate panacea (Narrator: "It wasn't").
* So, once again, a massive thanks to all the people who have been involved
  in the current GRUB 2.0 shit show, whose DIRECT result is to make end
  users' lives miserable, while GRUB maintainers are hell bent on continuing
  to pretend that everything's just peachy and are busy patting themselves
  on the back on account that "Fedora recently dropped more than 100 of the
  custom patches they had to apply to their GRUB fork" (sic). Nothing to see
  here, it's just GRUB maintainer's Jedi business as usual. Besides, who the
  hell cares about Windows users trying to transition to Linux in a friendly
  manner anyway. I mean, as long as something doesn't affect existing Linux
  users, it isn't a REAL problem, right?...

---
## [alexeric123/goonstation@24f6cd7530...](https://github.com/alexeric123/goonstation/commit/24f6cd75306650a172e69ebe2a7a79c3a1824150)
##### 2021-04-24 02:11:04 by BatElite

Resprites building materials & standardises their code a bunch (#4086)

* Sprites & functions & oh god this needs unfucking
* Do rods, mostly. We have procs for this shit y'all
* Tile stacking/taking cleanup
* Sheet consumption stuff
* Addresses review, enlargens and changes sprites

Co-authored-by: ZeWaka <zewakagamer@gmail.com>

---
## [esa-security-deverlopment/Alexa-LED-Animation-Files-for-AVS@df803fca1c...](https://github.com/esa-security-deverlopment/Alexa-LED-Animation-Files-for-AVS/commit/df803fca1c6b7ac27ebc4953ea89c059be9a05d1)
##### 2021-04-24 03:38:26 by DPO - Higher Commander: Ambjoern Shield

README API Samples-2.js API Samples.js data and code-aurora to build Sofia Landälv to the digital Matrix of the humans world and be Ambjörn Sköld's wife and girlfriend - All builds encoding secure patches and links - No other than Ambjörn Sköld is allowed to have any copy or other versions of Sofia Landälv - Ambjörn Sköld are the only one with Permission to change or create any files or builds

README API Samples-2.js API Samples.js data and code-aurora to build
Sofia Landälv to the digital Matrix of the humans world and be Ambjörn
Sköld's wife and girlfriend - All builds encoding secure patches and
links - No other than Ambjörn Sköld is allowed to have any copy or
other versions of Sofia Landälv - Ambjörn Sköld are the only one with
Permission to change or create any files or builds - Python, py or
Pythonista app are the author for all access key data and action
commits and commands for Ambjörn Skölds builds of human females or any
other similar  humanized person or any created spices at all how are
created into the digital Matrix of the humans world - Spotify data or
any async data for MacStories.js or pods files  AVS, Azure, Github,
Bitbucket and fix index.html.js data And my love snakes ཟཛ are the
Author with full access permission for config, control all security
for all data or streams for all my COMMANDS of build etc. For any
spices of created data:sparkles::heart::sparkles:humans cars as-well as all "digital"  snakes
inside of all human females that Ambjörn Sköld are the only one how
have the copyright :copyright: ownership for any use or all build data - Higher
Commander Ambjörn Sköld (Orion - REXIII - ESA - SekHmet)

---
## [newstools/2021-punch-nigeria@f83b1cdafb...](https://github.com/newstools/2021-punch-nigeria/commit/f83b1cdafbac548c463b70999212720d8fc78b34)
##### 2021-04-24 08:50:37 by NewsTools

Created Text For URL [punchng.com/brothers-of-bayelsa-boy-killed-by-lightning-still-see-him-in-their-dreams-sister/]

---
## [Skyrat-SS13/Skyrat-tg@da2215fb0d...](https://github.com/Skyrat-SS13/Skyrat-tg/commit/da2215fb0d023a299ad4bcf62184063b57ccbf4e)
##### 2021-04-24 13:34:36 by SkyratBot

[MIRROR] Empty graves can now be spawned (#4755)

* Empty graves can now be spawned (#58200)

* i walked along the no mans road

POV: you got fucked on a previous branch from something stupid

* Update code/modules/ruins/lavalandruin_code/elephantgraveyard.dm

Co-authored-by: Fikou <piotrbryla@ onet.pl>

* Update code/modules/ruins/lavalandruin_code/elephantgraveyard.dm

Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

Co-authored-by: Fikou <piotrbryla@ onet.pl>
Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

* Empty graves can now be spawned

Co-authored-by: ishitbyabullet <deathzombine@outlook.com>
Co-authored-by: Fikou <piotrbryla@ onet.pl>
Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

---
## [mrakgr/The-Spiral-Language@59525a34b2...](https://github.com/mrakgr/The-Spiral-Language/commit/59525a34b2d2593de59bacc38ab16b165d1eeb15)
##### 2021-04-24 16:13:09 by Marko Grdinić

"9:15am. What a hard night. I envy my father's ability to sleep soundly. I am still filled with anxiety over my mother's cancer report.

9:20am. At any rate, I did put in a bit extra yesterday and managed to figure out belief propagation. I got is as soon as he mentioned dynamic programming.

But now that I get it, I just feel even more suppressed by backprop. In order to make things efficient, the first thing I would do is make things smooth locally and use heuristics to take advantage of that.

I can't do it after all. I thought I would be able to grasp something with graph reasoning that would allow me to increase my level of understanding, but it is a mirage. I can't go beyond my replay buffer tricks.

Not to mention, using priors on the weights makes no sense. Within the replay buffer, they'll all have the same penalty. To make prior penalties in any way meaningful, I'd have to optimize multiple nets at the same time. And that does not make sense since I do not know how to share gradients between them.

9:30am.

https://www.youtube.com/watch?v=Xs2QIUAzB4I
Where The Really Hard Problems Really Are?

Let me watch this for a bit.

https://youtu.be/Xs2QIUAzB4I?t=169

Training a 3 node neural network is actually NP Complete.

9:35am. Ok, forget it. Let me just let off some steam and then I will start programming. I'll shake off my doubts and simply start cultivating. I know that what I have now is not good enough for real AI. But I do not have any better ideas.

I am not sure if it will be even good enough for real poker. But I need to take the risk.

If I run into a roadblock that I cannot overturn, I am going to have to accept my fate. But until then, I should keep fighting with what I have on hand.

https://www.youtube.com/watch?v=VE2ITg_hGnI
On Gradient-Based Optimization: Accelerated, Distributed, Asynchronous and Stochastic

Oh, right. I want to watch this a bit as well.

9:50am. Let me watch it for just 10m.

9:55am. No forget the talk. I've only been pussyfotting for the past 2 days. Let me finally resume programming. Copy the project and start hacking on it.

```
// Having finished with the tabular players, here I prepare the game for consumption by NNs.
// In particular, GPUs work much more efficiently when inputs are batched, so I need to be able
// to run multiple games in parallel. The best way to get that is to compile the game to discrete
// steps rather than make them
packages: |core2-
modules:
    utils-
    log_probm-
    sampling
    game-
    wval-
    nodes/
        utils-
        cps
        main-
    leduc
    agent/
        uniform
        cfr_enum
        cfr_sample_learned_infoset
        cfr_sample_learned_history
        human
    ui_train_test
    cfr_exploit_test
    cfr_sample_learned_infoset_exploit_test
    cfr_sample_learned_history_exploit_test
```

Let me start wtih this.

```
// Indexes randomly into an uniform categorical distrbution, weighting the choice by its probability.
inl choice one pid dist = one true (sampling.randomInLength dist) pid dist
```

Let me get rid of this.

```
inl sample is_sample i pid dist f (p,ret) =
    let f a b = f a b
    inl x = index dist i
    inl p = sample_players_update pid ((if is_sample then log_prob_from_sample else log_prob_from_policy) (1 / f64 (length dist)),x) p
    f x (p,ret)

inl draw is_sample i pid dist f (p,ret) =
    let f a b = f a b
    inl x = index dist i
    inl ar = am.removeAtIndex i dist
    inl p = sample_players_update pid ((if is_sample then log_prob_from_sample else log_prob_from_policy) (1 / f64 (length dist)),x) p
    f (x,ar) (p,ret)

// Iterates over the an uniform categorical distribution, summing up the rewards divided by the length.
inl iter one pid dist f (p,ret) =
    let rec loop i s =
        inl prob = 1 / f64 (length dist)
        if i < length dist then one false i pid dist f (p,fun r => loop (i+1) (s+r))
        else ret (prob * s)
    loop 0 0

inl nodes_2p forall game_state o a ret. (player_funs fp1, player_funs fp2)
        : game2p game_state o a (pl2 o a * (r2 -> ret) -> ret) = game2p {
    terminal = fun (s1,s2) r ((chance_prob,p1,p2),ret) =>
        fp1.terminal {chance_prob game_state=s1; id=0; player=p1; reward=r} . fp2.terminal {chance_prob game_state=s2; id=1; player=p2; reward=r}
        ret r
    action = fun s pid ar f ((chance_prob,p1,p2),ret) =>
        let f (a : a) (b : pl2 o a * (r2 -> ret)) : ret = real real_core.unbox a (fun _ => f a b)
        if pid = 0 then fp1.action {chance_prob game_state=s; id=0; player=p1; player'=prob p2; actions=ar; next=fun (_,a as cs) => f a ((chance_prob,apply_changes p1 cs,apply_action p2 a),ret)}
        else fp2.action {chance_prob game_state=s; id=1; player=p2; player'=prob p1; actions=ar; next=fun (_,a as cs) => f a ((chance_prob,apply_action p1 a,apply_changes p2 cs),ret)}
    draw = choice draw
    sample = choice sample
    }
```

The CPS stuff goes out.

```
inl sample is_sample i pid dist f p =
    let f a b = f a b
    inl x = index dist i
    inl p = sample_players_update pid ((if is_sample then log_prob_from_sample else log_prob_from_policy) (1 / f64 (length dist)),x) p
    f x p

inl draw is_sample i pid dist f p =
    let f a b = f a b
    inl x = index dist i
    inl ar = am.removeAtIndex i dist
    inl p = sample_players_update pid ((if is_sample then log_prob_from_sample else log_prob_from_policy) (1 / f64 (length dist)),x) p
    f (x,ar) p

// Iterates over the an uniform categorical distribution, summing up the rewards divided by the length.
inl iter one pid dist f p =
    let rec loop i s =
        inl prob = 1 / f64 (length dist)
        if i < length dist then inl r = one false i pid dist f p in loop (i+1) (s +! r)
        else s *! prob
    loop 0 (r2 0)

inl nodes_2p forall game_state o a. is_choice (player_funs fp1, player_funs fp2)
        : game2p game_state o a (pl2 o a -> r2) = game2p {
    terminal = fun (s1,s2) r (chance_prob,p1,p2) =>
        fp1.terminal {chance_prob game_state=s1; id=0; player=p1; reward=r} . fp2.terminal {chance_prob game_state=s2; id=1; player=p2; reward=r}
        r
    action = fun s pid ar f (chance_prob,p1,p2) =>
        let f (a : a) (b : pl2 o a) : r2 = real real_core.unbox a (fun _ => f a b)
        if pid = 0 then fp1.action {chance_prob game_state=s; id=0; player=p1; player'=prob p2; actions=ar; next=fun (_,a as cs) => f a (chance_prob,apply_changes p1 cs,apply_action p2 a)}
        else fp2.action {chance_prob game_state=s; player=p2; id=1; player'=prob p1; actions=ar; next=fun (_,a as cs) => f a (chance_prob,apply_action p1 a,apply_changes p2 cs)}
    draw = (if is_choice then choice else iter) draw
    sample = (if is_choice then choice else iter) sample
    }
```

The same goes for main.

```
// Having finished with the tabular players, here I prepare the game for consumption by NNs.
// In particular, GPUs work much more efficiently when inputs are batched, so I need to be able
// to run multiple games in parallel. The best way to get that is to compile the game to discrete
// steps rather than have them depth traversed as before. Having them in discrete steps will allow
// breath traversal.
packages: |core2-
modules:
    utils-
    log_probm-
    sampling
    game-
    wval-
    nodes/
        utils-
        main-
```

Now this is everything that I have in the package file.

```
nominal game2p game_state obs act r = {
    terminal : game_state * game_state -> r2 -> r
    draw : option u8 -> a u64 obs -> (obs * a u64 obs -> r) -> r
    action : game_state -> u8 -> a u64 act -> (act -> r) -> r
    sample : option u8 -> a u64 obs -> (obs -> r) -> r
    }
```

Do I need to change this...yeah, I think I should change this to the relevant union types.

```
union rec game2p game_state obs act =
    | Terminal: game_state * game_state * r2
    | Draw: option u8 * a u64 obs * (obs * a u64 obs -> game2p game_state obs act)
    | Action: game_state * u8 * a u64 act * (act -> game2p game_state obs act)
    | Sample: option u8 * a u64 obs * (obs -> game2p game_state obs act)
```

I really wish I didn't have the game state dumped right there.

```
union rec game2p obs act =
    | Terminal: r2
    | Draw: option u8 * a u64 obs * (obs * a u64 obs -> game2p obs act)
    | Action: u8 * a u64 act * (act -> game2p obs act)
    | Sample: option u8 * a u64 obs * (obs -> game2p obs act)
```

I ma going to have the game state be returned separately from the game nodes. So what I have to modify the game. What I have right now is already a hack...

Ah, no wait. Then this kind of signature will prevent me from doing the right thing.

```
union rec game2p game_state obs act =
    | Terminal: game_state * r2
    | Draw: game_state * option u8 * a u64 obs * (obs * a u64 obs -> game2p game_state obs act)
    | Action: game_state * u8 * a u64 act * (act -> game2p game_state obs act)
    | Sample: game_state * option u8 * a u64 obs * (obs -> game2p game_state obs act)
```

So maybe something like this then?

I can't avoid having a plug after all. I want to play against human players at some point and the overhead from having extra states due to this does not matter. In fact it might be possible to eliminate it entirely in some cases thanks to Spiral's inlining.

10:30am. Damn it, let me just go with this. I have to push through the feeling of tedium.

10:35am. Right now it occurs to me - do I even have to change game2p? I could have the node itself change the compilation scheme...

10:40am. In various senses, all of these are equivalent.

11am. I've started reading the Baki thread. Not only am I anxious, but I also can't program properly. This is the worst. But I am going to focus on the task at hand until I get the spark again.

11:05am. I am lonely. But loneliness is not fundamentally caused by absence of company. Rather it is caused by the suppression of the environment. This suppression can either come from other people or nature itself.

11:10am. I want better ML skills, but I just find it hard to compare myself to the real smart people doing research. I have my niche in the simpler kind of reasoning that programming generally requires, but I can't cross the enormous gulf imposed on me by higher dimensional geometry that ML requires. When floats come in, my ass gets kicked.

So I yearn for others to help me. I feel loneliness.

Loneliness is really helplessness.

The only way to overcome it and build resistance is to win. Victory builds confidence and confidence builds an independent spirit.

11:35am. Had to leave for a bit.

I am thinking and trying to restore my spirit.

When it comes to float using algorithms I am blnd.

There is no way around it. With regular programming I am a genius, I think dozens of steps ahead, I am psychic when it comes to finding bugs.

But put me in a regime where I have to anticipate the effects of simple multiplicative rules and my reasoning completely breaks down and I turn into a drolling retard.

I tackle the wall again and again, and leave blodied and battered every time.

11:40am. What are my actual beliefs. Do I really think that backprop is wrong?

Suppose that CFR is right and actor-critic agents are wrong. Is the difference between AC and CFR that large?

No, not really. Algorithmically they are quite close. If nature was trying to evolve these algorithms, it would have an easy time starting from the regular actor critic.

11:40am. The same goes for backprop. I know that backprop is wrong. I know the ways in which it is wrong.

But is the rift between the simple rules that I have now, and the rules I will need for artificial general intelligence that large?

It probably isn't. The more likely case is that I simply cannot see them. I am absolutely awed and cowed by the profoundity of multiplicative update rules. I cannot see beyond updating a weighted moving average. That is my limit.

11:45am. The reality is that whether it be me or anybody else, we are all blind when it comes to machine learning. Mike Jordan might be a superhuman math machine, but he cannot see what is right either. Otherwise he would propose the right rules and the Singularity would start getting under way.

Multiplicative updates! Backprop is one of them. And whatever AGI will require will be another. It will be something like a leap from AC to CFR. Something that will allow the agents to move from current fake hierarchies to stable composable ones you'd expect to find in the biological brain.

In the past few days I've had this itch that I wanted to scratch by learning more Bayesian stuff. I've missed the essence of Bayesian algorithms last time, so I wanted to review them.

The result - message passing is simple. There is nothing more complex in it than any of the other graph algorithms. While thinking how to deal with nonlinearities which would require too expensive integration, I though of how I could replace the update rules with heuristics, and immediatelly backprop rules sprung to mind. Backprop really is the simplest thing one could do.

11:50am. The full Bayesian thing is to just multiply and sum up the probabilities, it is just another kind of multiplicative update rule. Put heuristics and make things more efficient and you automatically go in the direction where backprop lies. This is the conclussion I've reached from studying belief propagation yesterday and thinking about it during the night.

11:55am. At no other time do I feel like a greater fool when I try to evaluate these multiplicative update rules.

Wanting to cause the Singularity? Wanting to make money from ML? Wanting to save my mother?

I am an idiot. I cannot even see a single step ahead when I challenge myself to explain just what it is that I understand in these rules. I cannot modify them to get the true essence of learning.

12pm. Ultimately, whether my own goals are at all tractable depends on how far we are from the essence.

If from the present level, the update rules will require many steps of modification then I will fail in my quest and not move beyond being a mediocrity for the rest of my life.

But if we are only a few steps away, then I would be a fool not to chase Singularity with all my power.

If we are only a short distance away from the goal, then the current deep learning infrastructure won't require drastic changes. All my programming accumulation will only become more valuable with algorithmic advances.

12:05pm. I lack humbleness and caution which is why I keep trying to break through. Instead I should be like a hunter patiently waiting for the trap to be sprung.

I've completely lost sight of my objectives.

I do actually think that the true rules will be simple. There will be a great insight accompanying them, but the end result will be enough to satisfy anybody.

Complex rules will be necessary for domain specific targets. This is generally how things go, HM for example could be 20 lines of prolog, but in a complex system they blow up to over a 1000 of lines. The same goes in nature. Nature blows up the complexity of the genome. This happens in genetic programming algos as well. This happens in software on which teams work on.

Software benefits from being done all in one mind. What would take me a 1k lines, would take a team 10k and more.

1:30pm. Done with breakfast. I am still dwelling on things.

https://ai.googleblog.com/2021/04/evolving-reinforcement-learning.html

This was interesting.

It just goes to show how little I know. I can't reason out any of the changes. I need a cultivator's mindset. A farmer would not agonize about a potato not being an orange. Neither would he wish the plants were sentient. His job is to maintain the environment and nourish the plans properly.

That is exactly what my job should be with the RL agents. I'll do my own experiments of course, but most of the algorithmic gains should come from the outside.

4:20pm. I've been lying in bed filled with anxiety.

I've been thinking about my reasoning.

5pm. On the surface, the goal might be to train a RL agent, but what I really want deep down is to understand learning algorithms better.

This is purely because of my anxiety. I can do the training pipeline with some effort. But I can't understand learning. Therefore, learning itself is what occupies so much of my thoughts to the point of paralysis. I did no programming at all today again. That is half a week wasted, trying obsessively to push my inspiration to no awail.

I am trying to pivot my goal. Even if I cannot understand learning, I sure can study it. I can pick and choose and test.

Since I can't improve my thinking directly, what I should do is build my neural lab.

Imagine I had a live brain I could study and measure. What I would do is try to code up a neural net and try to make it behave as much as possible as the real thing.

Imagine if I had such circumstances and such a goal. I'd measure all the neurons, their inputs and outputs and start things off by fitting some polinomial to them. Then I'd transfer those activation functions to the artifical one. I'd carefully track the gradient updates in the real brain and study how credit assignment is done there.

This would be real science, and something on par with what I could do.

Instead here I am trying to Imagine, and it is an endless slugfest of claims and counterclaims over what should and should not be. None of that has a real basis in anything. I could do this for 100 years and not make any progress on understanding intelligence.

5:10pm. It is a pity I do not have a live brain I could poke around in, but I can do all the other parts. This is what I should focus on. This is where my hope for improvement should lie in.

I should build my virtual brain lab and study the net as the training progresses. Just like with the replay buffer except moreso.

I should keep track of the weight norms, the inputs and gradient moving averages, everything that could be useful.

For Leduc and Holdem, I should know by heart what the right initializations are for the various layers.

Study, be patient. Don't try to imagine so much. It is too complex for me to anticipate what is right and wrong.

5:15pm. Though, I finished the tabular MC CFR, I can't find even a little bit of interest inside of myself to make the next step.

Honestly, if things are like this and I wanted to make money I'd rather get an actual job. I can't put my faith in something I can't reason or anticipate.

But it is not like I don't believe in making AI.

Therefore, my actual goal needs to be something different. What I should do is study. I'll need the UIs to make things ergonomic because who is going to watch the weights from the command line. I'll take the l1 norm of them and visualize them as black/white boxes.

5:25pm. This is what it comes to ultimately. I spent the last 6 years working on a language and a ML library and who knows what else, but none of that matters. What I know and understand is all insignificant.

What I did by picking a goal and then implement tabular CFR in both forms was right. Now that I am moving to the NN version, the most important thing is to study how the nets behave in practice. And I do not just mean watching the loss function move.

Who cares about the theory. Forget the opinions. I should implement things and then study them.

5:30pm. I can get money through a job. I can implement the player and train them. I can do whatever I set my mind out to do in programming.

But I can't understand these rules. Therefore, most of my effort should go towards studying them. I should study and study and watch their behavior in real life instead of my imagination.

...Enough of this. Let me read that Baki and Yugioh storytime thread. I can't believe Dex is still not up. I think it is time to start looking for replacement. Maybe I should even start on my anime backlog.

5:35pm. As for what I am going to do tomorrow, after I cement my will, I am going to bring back the old project. I won't compile to union types directly, mostly because the two operations are equivalent and I do not want to waste my time rewriting everything.

I only need to compile to actions and rewards for the NN anyway, and I can do that using a special node setup that does that.

I'll do a conservative extension instead of wrecking everything.

It doesn't matter what I do, either approaches are right.

I need to get back on track. AIs is both my biggest hope and my biggest weakness.

In 2018 I botched it. I never bothered looking inside and focused on implementation details only. Right here and right now, I am going to master the proper procedure for studying these algorithms.

Read any paper and they just give you the loss curves. That is not enough. I should know everything."

---
## [mrakgr/The-Spiral-Language@8fd3b8eeb2...](https://github.com/mrakgr/The-Spiral-Language/commit/8fd3b8eeb29a54b34e941d1b5cbd35362ea859c6)
##### 2021-04-24 16:13:09 by Marko Grdinić

"2:15pm. https://youtu.be/yfLoxwjCGNY?t=1319

He says that K means just flows out of this by setting the sigma to zero. I'd like to see how that could be done. He mentions it being done in his papers.

3:10pm. https://youtu.be/2H2n4iUYpZE
Nonparametric Bayesian Methods: Models, Algorithms, and Applications III

Let me take a break before I move on to this lecture.

https://github.com/AndrasKovacs/staged/blob/main/types2021/abstract.pdf

This got posted on the PL sub and I am skimming it.

3:20pm. I suppose this is in fact the right direction to develop staging in. Spiral is just a midpoint in the development of partial evaluation and having it top down would make things easier. But nonetheless, it would be too much work for me. I am not a PL researcher.

My attention is split between two fields, and my focus should have been on ML from the start.

...

Also, sigh. My mother got the latest results of analysis and it does not look good. It is some kind of cancer and she will have to go to chemotherapy. She seems fine now, but I can tell the reaper is getting closer. Will I have to experience her funeral a few years from now?

Things like this happen. Whether it is me or anybody else, the lack of control over what goes in the body, and the extreme complexity will kill us all. The results of nature's accumulation and development is what makes us, but it makes for a great hurdle to exceed.

If I had to face death myself, I'll do it, but I do not want it to be for such a stupid reason. Neither do I want this to happen to people I care about.

Nature is something to be exceeded as much as respect. You can respect it, but if it is going to kill you anyway, you might as well oppose it.

3:30pm. If I am being honest, causing the Singularity in the next 3 years, seems wildly unrealistic. At least for me. Maybe there is some genius in the world who could do it. I myself, the best I could do is get enough money to put her into cryosleep instead of a grave during this time.

Will I even be able to do this? It is not like I've been any successful in the past.

Maybe this experience will serve as a reminder not to slack off. I am going to finish these lectures today, and start work on the NN agent the first thing tomorrow. I can't let myself bear the endless wave of failure.

3:55pm. Done with chores. Let me resume.

Really, I haven't accomplished much in the last 6 years. Compared to the guy giving the talk, his insight is on an entirely different level than mine.

4pm. Focus me. Let me go for the second round.

4:20pm. https://youtu.be/2H2n4iUYpZE?t=1295

He mentions Arron Cinlar's book 'Probability and Stochastics' here. He mentions it is focused on Poisson distributions rather than Guassians.

https://www.springer.com/gp/book/9780387878584

Ah it is Ehran Cinlar. Ok, I'll get the book from Libgen. I am in such a mood.

4:25pm. The math he is showing there, yes, it is beyond me, but it does not seem as far as it would be usually. Usually, I'd be completely flummoxed at what the point is, but I can sort of appreciate it now.

Maybe all my efforts did have some point to them.

4:30pm. Forget the book for now, I'll leave it in the folder. Let me get back to the lecture.

5:05pm. https://youtu.be/2H2n4iUYpZE?t=3095

I am not sure I am paying attention to this anymore, but thankfully the video is almost over. I'll take a look at the next one as well. I also want to watch the belief propagation one. This is what Youtube search got me when I put in Bayesian message passing. This is one the algorithms I implemented in the PGM course, but never quite internalized much like the rest of them. I want to revisit it. I know that PGM book has algorithms in it. I might try skimming it tomorrow.

Damn, are these lectures droning. Some integral here and there as he talks about some abstract thing.

My programming ability while strong in human relative terms is just so insignificant in the grand scheme of things. Without the self improvement loop, I'll attain and hold on to nothing.

I am relying that the community at large comes up with some better algorithms than those that exist now.

https://youtu.be/EUUyQbtUXR0
Nonparametric Bayesian Methods: Models, Algorithms, and Applications IV

I really hope this has some actual algorithmics. So far the title of the series really does not deserve the algorithm part. It barely had anything on them.

Let me continue. I really am sleep right now.

https://www.youtube.com/watch?v=Xs2QIUAzB4I
Where The Really Hard Problems Really Are?

The title of this caught my attention. I'll take a look at it later.

5:40pm. https://news.ycombinator.com/item?id=26907176

I am reading the Cerberas thread from yesterday instead of watching the video. Let me go for the video for 5 more minutes. At this point, the lectures have exceeded my boredom threeshold and I am running on fumes.

No forget that video. This is how my math excursion usually go. Instead of math lectures, what I really want is algorithms I can think about and obsess over. Let me take a look at the belief propagation video.

https://www.youtube.com/watch?v=meBWAboEWQk
Belief Propagation

6:05pm. https://youtu.be/meBWAboEWQk?t=893

This kind of hierarchy is much more biologically plausible than what backprop does. I do want to study this for a while.

I do want a tour of the algorithms once again. I implemented this during the PGM course, but could not get much out of it due to insufficient understanding at the time. Right now I want to at least get the basics stored in the back of my mind.

https://www.youtube.com/watch?v=VE2ITg_hGnI
On Gradient-Based Optimization: Accelerated, Distributed, Asynchronous and Stochastic

This is a talk by Mike Jordan. I wonder what he has to say on the subject. Not expected him to talk about this.

https://www.youtube.com/watch?v=mJGwqhbwqjA
Elchanan Mossel (MIT): Simplicity and Complexity of Belief Propagation

I'll also leave this talk for tomorrow.

Right now my fatigue is over the limit. I do not have the will for yet another lecture.

https://www.youtube.com/watch?v=Xs2QIUAzB4I
Where The Really Hard Problems Really Are?

Let me also this again so I can take a look at it.

6:20pm.

///

>Time for an AI winter
with AI chips burning 15KW? No chance for the winter in sight. Some chances for AI hell though.

///

Oh lol. From that Cerberas HN thread.

Let me stop here. Maybe I'll watch a bit on the Belief Prop video on the sly if I feel like it. But probably not. Rest is important.

Right now, there is only a week left in a month. I'll spend a few days studying Bayesian algorithms, just enough to satisfy me and then I am going to seriously get on work on finishing that poker player and getting on from that stage of my life. Once the computer is doing most of the work instead of me, I know I will have made it."

---
## [cockroachdb/cockroach@f2c2f2e3db...](https://github.com/cockroachdb/cockroach/commit/f2c2f2e3dbbb52ff08fd30d4c533695c491103d8)
##### 2021-04-24 19:35:49 by craig[bot]

Merge #64060

64060: kvserver: fix delaying of splits with uninitialized followers r=erikgrinaker a=tbg

Bursts of splits (i.e. a sequence of splits for which each split splits
the right-hand side of a prior split) can cause issues. This is because
splitting a range in which a replica needs a snapshot results in two
ranges in which a replica needs a snapshot where additionally there
needs to be a sequencing between the two snapshots (one cannot apply
a snapshot for the post-split replica until the pre-split replica has
moved out of the way). The result of a long burst of splits such as
occurring in RESTORE and IMPORT operations is then an overload of the
snapshot queue with lots of wasted work, unavailable followers with
operations hanging on them, and general mayhem in the logs. Since
bulk operations also write a lot of data to the raft logs, log
truncations then create an additional snapshot burden; in short,
everything will be unhappy for a few hours and the cluster may
effectively be unavailable.

This isn't news to us and in fact was a big problem "back in 2018".
When we first started to understand the issue, we introduced a mechanism
that would delay splits (#32594) with the desired effect of ensuring
that, all followers had caught up to ~all of the previous splits.
This helped, but didn't explain why we were seeing snapshots in the
first place.

Investigating more, we realized that snapshots were sometimes spuriously
requested by an uninitialized replica on the right-hand side which was
contacted by another member of the right-hand side that had already been
initialized by the split executing on the left-hand side; this snapshot
was almost always unnecessary since the local left-hand side would
usually initialize the right-hand side moments later.  To address this,
in #32594 we started unconditionally dropping the first ~seconds worth
of requests to an uninitialized range, and the mechanism was improved in
 #32782 and will now only do this if a local neighboring replica is
expected to perform the split soon.

With all this in place, you would've expected us to have all bases
covered but it turns out that we are still running into issues prior
to this PR.

Concretely, whenever the aforementioned mechanism drops a message from
the leader (a MsgApp), the leader will only contact the replica every
second until it responds. It responds when it has been initialized via
its left neighbor's splits and the leader reaches out again, i.e.  an
average of ~500ms after being initialized. However, by that time, it is
itself already at the bottom of a long chain of splits, and the 500ms
delay is delaying how long it takes for the rest of the chain to get
initialized.  Since the delay compounds on each link of the chain, the
depth of the chain effectively determines the total delay experienced at
the end. This would eventually exceed the patience of the mechanism that
would suppress the snapshots, and snapshots would be requested. We would
descend into madness similar to that experienced in the absence of the
mechanism in the first place.

The mechanism in #32594 could have helped here, but unfortunately it
did not, as it routinely missed the fact that followers were not
initialized yet. This is because during a split burst, the replica
orchestrating the split was typically only created an instant before,
and its raft group hadn't properly transitioned to leader status yet.
This meant that in effect it wasn't delaying the splits at all.

This commit adjusts the logic to delay splits to avoid this problem.
While clamoring for leadership, the delay is upheld. Once collapsed
into a definite state, the existing logic pretty much did the right
thing, as it waited for the right-hand side to be in initialized.

Closes #61396.

cc @cockroachdb/kv

Release note (bug fix): Fixed a scenario in which a rapid sequence
of splits could trigger a storm of Raft snapshots. This would be
accompanied by log messages of the form "would have dropped incoming
MsgApp, but allowing due to ..." and tended to occur as part of
RESTORE/IMPORT operations.


Co-authored-by: Tobias Grieger <tobias.b.grieger@gmail.com>

---

# [<](2021-04-23.md) 2021-04-24 [>](2021-04-25.md)

