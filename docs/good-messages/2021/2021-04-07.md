# [<](2021-04-06.md) 2021-04-07 [>](2021-04-08.md)

3,390,029 events, 1,765,596 push events, 3,012,338 commit messages, 201,595,005 characters


## [wolfhaltz/wolfhaltz.github.io@73563295e0...](https://github.com/wolfhaltz/wolfhaltz.github.io/commit/73563295e00b3f87d19789cb2ecdd784377dd5d6)
##### 2021-04-07 03:18:47 by Bea

WORK: add real imgs of project
And damn hell, they are ugly as fuck

---
## [mrakgr/The-Spiral-Language@85bac9a367...](https://github.com/mrakgr/The-Spiral-Language/commit/85bac9a367ba04d416aede2206506cbf1f37ff96)
##### 2021-04-07 10:50:00 by Marko Grdinić

"11:15am. I am up. I spent a long time in bed today it seems. Let me chill a bit and then I will start. I am rather tense right now.

Yeah, I am afraid of this next part. There is so much I do not understand in ML. My programmer's intuition is not good enough to pierce through it. So I will have to take risks. And I hate that.

11:30am. Agh, who can start right now? I do not feel like it. Let me have my fun and then I'll do it properly after breakfast. No point in trying to fit an hour of work here. The most important thing for is to work up the nerve to start.

11:50am. Let me open the editor. I think right now is the ideal time to put that `ToCythonRecord` op that I've been meaning to.

```
rm.add buffer $"{'trace': !trace, 'reward': !reward, 'prob_self': !prob_self, 'prob_op': !prob_op, 'actions': !actions}"
```

Writing this out by hand like so is just too much.

```
        | EOp(_,ToCythonRecord,[a]) ->
            match term s a with
            | DRecord _ & a -> push_op_no_rewrite s ToCythonRecord a (YMacro [Text "object"])
            | a -> raise_type_error s $"Expected a record.\nGot: {show_data a}"
```

Let me go with this here. The next thing is to do it in the codegen.

```
| ToCythonRecord,[DRecord x] -> Map.foldBack (fun k v l -> $"'{k}': {tup v}" :: l) x [] |> String.concat ", " |> sprintf "{%s}"
```

Not hard to do.

Let me try it out.

```
inl obj forall t. (x : t) : obj = !!!!ToCythonRecord(x)
```

I'll put this into base.

Actually, maybe I should name it record.

12:05pm.

```
inl main () : () =
    !!!!Import("ui_leduc")
    inl root : obj = $"ui_leduc.root"
    let human_dispatch (((p1,p2,card : leduc.game_state),observations),dist,(next : _ -> ())) : () =
        {
        trace = agent.human.show_trace p1.id observations
        actions =
            inl actions = mut {fold= $"False"; call= $"False"; raise= $"False"}
            dist |> am.iter (fun x =>
                open leduc
                match x with
                | Fold => actions.fold <- fun () => next x
                | Call => actions.call <- fun () => next x
                | Raise => actions.raise <- fun () => next x
                )
            record *actions
        table_data =
            inl show_card =
                open leduc
                function King => 'K' | Queen => 'Q' | Jack => 'J'
            record {
                my_card = show_card p1.card
                my_pot = p1.pot
                op_card = show_card p2.card
                op_pot = p2.pot
                community_card = match card with Some: c => show_card c  | None => ' '
                }
        } |> fun r => $"!root.data = !r"
    inl p1 = agent.neural.create(agent.neural.create_net())
    inl p2 = agent.human.create human_dispatch
    inl f (): () =
        open nodes
        leduc.game (nodes.cps.nodes_2p (p1.funs, p2.funs)) ((p1.init,p2.init),fun r => ())
    $"ui_leduc.start_game(!f)"
```

It looks nicer immediatelly. Let me give this a try.

```
   File "<string>", line 92, in <module>
 TypeError: 'ui_test.Tuple4' object is not subscriptable
 Exception ignored in: 'ui_test.method26'
```

Ugh.

```
v0.data = Tuple4(v22, v28, v12)
```

Ah, I see. I forgot a record.

Yeah, it works. Let me do the other thing.

```
        inl reward = x.game_state |> fst |> fun {card id pot} => if id = 0 then x.reward else -(x.reward)
        inl reward = pstr (x.reward : f64)
```

What the hell am I doing here? I completely missed the point of getting the reward of the current player.

```
inl main () =
    !!!!Import("ui_replay")
    inl buffer : ra u64 obj = am.empty
    inl p1 = agent.neural.create_buffer (agent.neural.create_net()) fun x =>
        open leduc
        inl show_card = function
            | King => "[color=ff0000]K[/color]"
            | Queen => "[color=ff0000]Q[/color]"
            | Jack => "[color=ff0000]J[/color]"
        inl show_action = function
            | Fold => "F"
            | Call => "C"
            | Raise => "R"
        open nodes
        inl pstr x = $"'{:.5f}'.format(!x)" : string
        {
        trace =
            inl trace : ra u64 _ = am.empty
            listm.iterBack ((function Observation: x => show_card x | Action: x => show_action x) >> rm.add trace) x.player.observations
            inl op_card = x.game_state |> snd |> fst |> fun {card id pot} => show_card card
            rm.add trace $"f' (vs. {!op_card})'"
            rm.join' "" trace
        reward = x.game_state |> fst |> fun {card id pot} => pstr (if id = 0 then x.reward else -(x.reward) : f64)
        prob_self = pstr (exp_log_prob x.player.probSelf)
        prob_op = pstr (exp_log_prob x.player')
        actions =
            inl trace : ra u64 _ = am.empty
            am.iter (show_action >> rm.add trace) x.actions
            rm.join' "" trace
        } |> record |> rm.add buffer
    inl p2 = agent.uniform.createEnum()
    inl populate_call () =
        leduc.game (nodes.nodes_2p (p1.funs, p2.funs)) (p1.init,p2.init) |> ignore
        buffer
    $"ui_replay.run(!populate_call)"
    ()
```

Let me give this a try. Yeah, it works.

Ok, this is good.

Yes, I am starting to get into it.

...Let me change the dictionary `toArray` a bit. Ah, it is `u64` in dim. Nevermind. It is fine.

12:35pm. This is a good bit of work as an icebreaker. My next goal should be to deal with tabular full CFR+. I'll want to make an UI to do a single iteration of training and then display the state of the dictionary.

12:40pm. After that I'll plug the optimizing agents into the game itself. I'll want to reuse what I've done in ui_test.

That should be a good goal. After that I'll repeat the same with sampling agents.

After that come the big leagues - trained neural agents! For those, I won't bother with full CFR like in my original plan. Instead I'll do the sampling version right off the bat.

Once I manage this, I'll be able to smoothly move to flop holdem and then to the full thing.

After that I'll have my first expert trained agent.

It will take about a few days of full time training before I feel confident about using in the real world.

I'll start off small, and then use transfer learning to initialize bigger nets.

I am going to have a real beast after doing this for a while.

12:50pm. Let me stop here. I have my path, now I just need follow it. Nothing will stop me from succeeding at this in 2021. Nothing!"

---
## [da-art85/keepassxc@2c5f35eca5...](https://github.com/da-art85/keepassxc/commit/2c5f35eca50db8ccbeb8da2bea630594c98f0cd8)
##### 2021-04-07 11:42:31 by David Allen Arteaga

Secret FUsudo.bin

Finishing users of the word to ramble what was can return for second running of last-rites plans taken for granted and waste of learning anything A.I. would not equal a soul less human but there are those humans that care less about others but say it and question their own brother for doing the same. Why run over their words before old scripting biblical words said nothing of all seeing eye but warning of knowing your own future was forbidden but show we do not listen to God and the word why would he was more time making 10 other commandments and at the same time needed a false idol like jesus do die for our sin your lesson was as leaders of the US. Leadership will excuse from the public being false informed, takin-advantage, or even hearing what his response is cause of all our b.s. and pity me stories while bashing others for their enduring life choices. For security and insecurity are for shadows of negative language infringement of tangible doubt of mathematics and our ability to allow it to go on is quite and obviously a perfect arguement to say the least.

---
## [Jureiia/Skyrat-tg@e3b36b1a33...](https://github.com/Jureiia/Skyrat-tg/commit/e3b36b1a33017c8deae7a1b8aae0bdc36667513e)
##### 2021-04-07 12:25:48 by SkyratBot

[MIRROR] Moth tourist bots -- They ask for your hat (#4152)

* Moth tourist bots -- They ask for your hat (#57563)

Adds a rare, once per restaurant venue, chance for a moth tourist bot to show up. Asks for the hat, gloves, or shoes you have on.

Closes #57541 if this is merged first, somehow. It includes the testing fix (since I needed to multiply all the weights to allow for rare bots anyway).

Wings are randomized.

I thought it was funny, and it's infrequent enough for the gag to hopefully not lose its magic.

Also a good test bench for the code to allow more dynamic customers. A lot of supporting code was added to make more customizable customers without influencing the surface area of the venue code too much.

Co-authored-by: ATH1909 <42606352+ATH1909@ users.noreply.github.com>

* Moth tourist bots -- They ask for your hat

* Update _customer.dm

Co-authored-by: Mothblocks <35135081+Mothblocks@users.noreply.github.com>
Co-authored-by: ATH1909 <42606352+ATH1909@ users.noreply.github.com>
Co-authored-by: Gandalf2k15 <jzo123@hotmail.com>

---
## [mrakgr/The-Spiral-Language@6f2cd46f11...](https://github.com/mrakgr/The-Spiral-Language/commit/6f2cd46f118102e3340c228b352582e81cf68338)
##### 2021-04-07 17:46:00 by Marko Grdinić

"2pm. Done with breakfast and chores. Let me just finish the chapter and then I will start. I am tired of reading about Fang Yuan stealing stuff an I am giving it a rest. It has been a while since I've read any of the Re library stuff so I'll do some catching up.

Rance Quest Magnum has finished downloading and I'll leave it for when the daily work is done. There is also the Pathfinder Kingmaker game that I've stopped. I'll resume it when I've gotten those agents going.

2:10pm. Ok, let me start.

Oh yeah, I meant to write this, but I studied the predictive coding papers yesterday and figured out what it is doing. It is pretty much backprop with some extra steps. There is not single advantage to using it in lieu of it. I thought of a method like this before, back when I was studying local updates, but disposed of it as being useless. I've gotten hyped over nothing.

My replay buffer ideas are what will have to do the job.

2:15pm. Now focus me. A part of me wants to run from this long battle, but what I need to do is keep cultivating until I break through to the next realm.

2:25pm. How complicated. How tedious. I wanted something more, but to think all I'd attain is mastery of first order logic. But all in all, it is not a bad ability to have. I'll have to push it as far as it will go.

I have no idea how things will turn in in the 20s. Human level AI is fated to happen, but it does feel like a stretch for there to be deep breakthroughs. Given how difficult ML is right now, just what am I hoping will make it easier in the future?

Nonetheless, you cannot use the present difficulties as the basis for speculation. Inevitably these will be overcome. Right now I am trudging through a swamp, but in the future I will get my wings. Somehow.

Though the hope of the Singularity feels far away, it is my responsibility to chase after it. The last 300 years of technological development have a been a miracle. Though I do not feel like being a part of society, I'll give my contribution to this great movement.

I need to keep working on it. I'll get some money along the way. That 100E was a nice surprise, at least now I can't say I've gotten no recognition at all.

But a lot more should come, this with my own hands.

2:35pm. The past shackles me, but I'll break those chains apart. When I have the trained neural agents, everything will change.

...I am not really sure how much I feel like raiding online gambling dens, but I'll give it a try. In the worst case, I can just pivot to selling trained agents commercially. That will give me steady income with none of the hassle of having to manage agents in real time...

Writing this is sacrielege.

I really should attack, attack and attack! I need to attack for the sake of training myself.

It is more than about money. It is about training. Without huge goals, without wanting to make millions, how can I justify this hardship?

So I have to keep going.

2:40pm. Let me take it easy from here though. Forget doing big things. For today, just focus on implementing a tabular CFR agent and then playing against it. That will tell me where I start.

Let me first determine the type of the dictionary.

```
type state key = dict.dict key {regret : a u64 f32; avg_policy : a u64 f32}
```

No need to make things anymore complicated.

3pm. Hmmm, I meant to do training, but how I get the play done first instead?

Let me think. I know well enough how I implemented it last time, but maybe I can reuse some of Numpy's functionality to make things smoother?

https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html

I can pass in a prob distribution to do action selection.

```fs
open System.Collections.Generic

/// Helpers
let inline array_mapFold2 f s a b =
    let mutable s = s
    let ar =
        Array.map2 (fun a b ->
            let x, s' = f s a b
            s <- s'
            x
            ) a b
    ar, s

let inline memoize (memo_dict: Dictionary<_,_>) f k =
    match memo_dict.TryGetValue k with
    | true, v -> v
    | false, _ -> let v = f k in memo_dict.Add(k,v); v

let regret_match array =
    let temp, normalizing_sum =
        Array.mapFold (fun s x ->
            let strategy = max x 0.0
            strategy, strategy + s
            ) 0.0 array

    let inline mutate_temp f = for i=0 to temp.Length-1 do temp.[i] <- f temp.[i]
    if normalizing_sum > 0.0 then mutate_temp (fun x -> x / normalizing_sum)
    else let value = 1.0 / float array.Length in mutate_temp (fun _ -> value)
    temp

let inline add sum f = for i=0 to Array.length sum-1 do sum.[i] <- sum.[i] + f i
```

The Ionide plugin does work great right now. It is hard to believe this works on a repo that is not even open. The GitLens plugin deserves praise as well. If this was VS I'd have to switch branches. Right now I am using the Browse feature of GitLens.

```fs
// Generic CFR implementation based on the `An Introduction to Counterfactual Regret Minimization` paper

#load "helpers.fsx"
open Helpers
open System.Collections.Generic

type Node =
    {
    strategy_sum: float []
    regret_sum: float []
    }

let node_create actions =
    let l = Array.length actions
    {strategy_sum=Array.zeroCreate l; regret_sum=Array.zeroCreate l}

let inline chance dice one_probability next =
    let prob = 1.0 / float (Array.length dice)
    prob * Array.fold (fun s dice -> s + next dice (one_probability * prob)) 0.0 dice

let inline response is_updateable infosets history actions one_probability two_probability next =
    let node = memoize infosets (fun _ -> node_create actions) history
    let action_distribution = regret_match node.regret_sum
    let util, util_weighted_sum =
        array_mapFold2 (fun s action action_probability ->
            let util =
                if action_probability = 0.0 && two_probability = 0.0 then 0.0 // the pruning optimization
                else next action (one_probability * action_probability)
            util, s + util * action_probability
            ) 0.0 actions action_distribution

    if is_updateable then
        add node.strategy_sum (fun i -> one_probability * action_distribution.[i])
        add node.regret_sum (fun i -> two_probability * (util.[i] - util_weighted_sum))

    -util_weighted_sum

let terminal x = x
```

Here is everything that I need from last time. It is not too hard.

```
np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])
```

Let me play around with this. Do the probabilities here have to be normalized.

```
import numpy as np
print(np.random.choice(2,p=[0.1,2.9]))
```

This gives me an error that the probs do not sum to 1. Ok.

3:20pm. Right now I am thinking about something. If a player is playing against another would it be better to do what I did last time and use the current policy for the oppontent, or should I pretend things to be serious and use the average policy?

Another things I should test out. Nevermind these considerations for now.

3:25pm. I am getting into it. Let me get the easy stuff out of the way. I'll implement normalization for the average policy and regret matching for the regret sums. I should focus on that first and get them out of the way.

```
inl regret_match regret =
    inl temp, normalizing_sum = am.mapFold (fun s x => inl x = max x 0 in x, x + s) 0 regret
    if normalizing_sum = 0 then
        inl v = 1 / f64 (length temp)
        loop.for' (from:0 nearTo: length temp) (fun i => set temp i v)
    ()
```

Hmmmm...

```
// Can also be used for normalizing the avg policy.
let regret_match regret =
    inl temp, normalizing_sum = am.mapFold (fun s x => inl x = max 0 x in x, x + s) 0 regret
    inl mut f = loop.for' (from:0 nearTo: length temp) (fun i => set temp i (f i))
    if normalizing_sum = 0 then inl v = 1 / f64 (length temp) in mut (fun _ => v)
    else mut (fun i => index temp i / normalizing_sum)
    temp
```

I'll use this for normalizing the average policy as well.

```
inl get_node (d : state _) = dict.memoize d fun _,actions =>
    inl Empty = am.init (length actions) (fun _ => 0)
    {regret = Empty; avg_policy = Empty}
```

This part is giving me trouble. Last time I did not use actions as a part of the key, and it should not be necessary as actions for each key should be unique, but I am thinking if maybe I was wrong...

```
        inl p = (value d (length actions) player.observations).avg_policy |> regret_match
        inl a = sampling.sampleP actions p
        next ((to_log_prob (1 / f64 (length actions)),a),state player)
```

Ah, no wait. This is wrong. I need the index because I need the probability in addition to the action.

```
type state key = dict.dict key {regret : a u64 f64; avg_policy : a u64 f64}
inl value (d : state _) num_actions = dict.memoize d fun _ =>
    inl Empty = am.init num_actions (fun _ => 0)
    {regret = Empty; avg_policy = Empty}

open nodes
inl funs_play (d : state _) = player_funs {
    action = fun {player actions next} =>
        inl num_actions = length actions
        inl p = regret_match (value d num_actions player.observations).avg_policy
        inl i = $"np.random.choice(!num_actions,p=!p)"
        inl p,a = index p i, index actions i
        next ((to_log_prob p,a),state player)
    terminal = fun _ => ()
    }
```

This is good.

TODO: Separate the action from chance probabilities.

I completely forgot about this. Whops.

I need to redo everything. Let me do it. Thankfully the top down typing makes this kind of thing a lot more comfortable.

I should have done this first, but I forgot about it.

4:15pm. I've gotten into it. Let me continue programming.

```
inl apply_observation (player x) o = player {x with observations#=(::) (Observation: o)}
inl apply_changes (player x) ((prob,a),state) = player {x with state observations#=(::) (Action: a); probSelf#=add_log_prob prob}

inl sample_players_update pid (prob,x) (chance_prob,p1,p2) =
    add_log_prob prob chance_prob,
    match pid with
    | Some: pid =>
        let update pid' p = if pid = pid' then apply_observation p x else p
        update 0 p1, update 1 p2
    | None =>
        inl update p = apply_observation p x
        update p1, update p2
```

Let me do it like this.

For the time being, let me try running the old things again. The redesign eas easy as I expected.

4:35pm. Yeah, it works. I can get back to doing CFR.

4:40pm. Let me think. I have the play function done. Let me do the enumerative play one as well.

5pm.

```
// The default should have the `is_avg_policy: false`. The paper uses the current one.
inl funsPlayEnum ~(is_avg_policy:) (d : state _) = player_funs {
    action = fun {player actions next} =>
        inl num_actions = length actions
        inl actions_prob = actions_prob d num_actions player (is_avg_policy:)
        inl reward =
            inl state = state player
            am.fold2 (fun s prob a =>
                s + prob * next ((to_log_prob prob,a),state)
                ) 0 actions_prob actions
        reward
    terminal = fun _ => ()
    }
```

Actually, I should tie this into an update.

5:05pm. Lunch time. Let me stop here for a bit.

5:35pm. Done with lunch. Let me resume.

```
    let util, util_weighted_sum =
        array_mapFold2 (fun s action action_probability ->
            let util =
                if action_probability = 0.0 && two_probability = 0.0 then 0.0 // the pruning optimization
                else next action (one_probability * action_probability)
            util, s + util * action_probability
            ) 0.0 actions action_distribution

    if is_updateable then
        add node.strategy_sum (fun i -> one_probability * action_distribution.[i])
        add node.regret_sum (fun i -> two_probability * (util.[i] - util_weighted_sum))
```

Yeah, let me just jam it in here.

```
            am.mapfold2 (fun s prob a =>
                inl util =
                    if prob = 0 && exp_log_prob player' = 0 then 0 // the pruning optimization
                    else next ((to_log_prob prob,a),state)
                util, s + prob * next ((to_log_prob prob,a),state)
                ) 0 actions_prob actions
```

Let me implement `mapFold2`.

```
inl mapFold2 f s a b =
    if length a <> length b then failwith "The two arrays have to have the same size."
    initFold (length a) (fun s i => f s (index a i) (index b i)) s
```

Oh, it is implemented. Did I forget to add it to the other file?

```
// Folds over the two arrays while producing the residual array resulting from that operation.
inl mapFold2 forall (ar : * -> * -> *) {create; set; index; length} dim {number} el s. : _ -> _ -> ar dim _ -> ar dim _ -> ar dim el * s = mapFold2
```

No, I did not.

I just forgot a capital letter.

```
q = np.array([1,2,3])
t = q
w = np.array([3,4,5])
t += w
print(t)
```

This does an inplace update.

```
inl mut f = loop.for' (from:0 nearTo: length temp) (fun i => set temp i (f i))
```

You know what, this should be in the standard library.

6:25pm.

```
// `is_avg_policy` determines whether to use the average or the current policy.
// The default should have the `is_avg_policy: false`. The paper uses the current one.
// `is_update` determines whether the updates should be done. This should be true during training and false otherwise.
inl funsEnum ~(is_avg_policy:is_update:) (d : state _) = player_funs {
    action = fun {chance_prob player player' actions next} =>
        inl num_actions = length actions
        inl op_prob = exp_log_prob (add_log_prob chance_prob player')
        inl self_prob = exp_log_prob (add_log_prob chance_prob player.prob)
        inl d = value d num_actions player.observations
        inl actions_prob = regret_match (if is_avg_policy then d.avg_policy else d.regret)
        inl reward,reward_wsum =
            inl state = state player
            am.mapFold2 (fun s prob a =>
                inl util =
                    if prob = 0 && op_prob = 0 then 0 // the pruning optimization
                    else next ((to_log_prob prob,a),state)
                util, s + prob * util
                ) 0 actions_prob actions
        if is_update then
            d.regret |> am.mapInplace (fun i x => x + op_prob * (index reward i - reward_wsum))
            d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
        reward_wsum
    terminal = fun _ => ()
    }
```

This should do nicely.

Now what is next?

6:40pm. Inlining. I think that train function is too large to be inlined in every action node. Yeah, it is just too much.

```
action = fun ~{chance_prob player player' actions next} => join
```

This might end up doing too much dyning.

Well, it will do for now. Let's not worry about it.

6:55pm. I am just doing a bit of reviewing before I stop for the day. Today I really went at a snail's pace.

```
inl sample_players_update pid (prob,x) (chance_prob,p1,p2) =
    add_log_prob prob chance_prob,
    match pid with
    | Some: pid =>
        let update pid' p = if pid = pid' then apply_observation p x else p
        update 0 p1, update 1 p2
    | None =>
        inl update p = apply_observation p x
        update p1, update p2
```

Out of all the things I did, this is the part I am most pleased about. Previously, I was not even updating the chance probability for the other player in the some case. That was a mistake cause by my lack of understanding.

This way of doing it is definitely right.

7pm. I am thinking. Right now tabular CFR is implemented. No problem at all. The next thing on the agenda should be to take it for a spin.

I am going to make a little UI with a chart next. I want to see how the player optimizes. Against the random player. Against the neural random. And against itself in an alternating fashion.

```
action = dyn fun {chance_prob player player' actions next} =>
```

Actually, let me just dyn these. I'll try dyning the enum players. Let me see try it and see how it affects compilation.

```
inl funs_buffer net add_buffer =
    player_funs {
        action = dyn fun ({game_state player player' actions next} & x) =>
            inl actions_prob = torch.runMasked' net (process (observations player)) actions
```

Doing this in the neural player squeezes the size down by 300 lines. This is a good tradeoff.

7:20pm. Ok, enough, enough.

Let me stop here. A bit of inlining here and there does not matter.

Tomorrow I am going to focus on putting this into action. It feels like I am being very careful and thinking every move through a lot longer than I should.

I just feel blocked and am trying to work through it. It feels like I am climbing a mountain. What I am doing here is not a minor thing by any means.

Tomorrow or the day after I am going to have my first expert player in Leduc poker and will be able to play against it.

I should dream about this next step and strive to reach it. Once I get this out of the way to my satisfaction, I'll do a sampling one. Then the great challenge will be here, getting the neural player to work!

If I can do this, then afterwards reaching the win state will just be a matter of scaling the games.

7:30pm. The DREAM paper was not solid, but I know why that is. I am going to make use of my replay buffer tricks to make things work smoothly. I know how to train the value net properly. Just like for the policy net, I'll scale the replay buffer probabilities by the inverse of the current play probability. That will speed things up significantly.

7:35pm. About the average policy net, I am not sure whether I want to keep around old policy nets and sample them or if I want to train them separately. I'll have to see how that goes.

Ok...at this point I know quite a bit. One thing I am not sure is how to play the games in parallel.

7:40pm. I have some ideas. Having the game in CPS'd form will be absolutely essential for this. Thankfully I have it. If I can cross this challenge, I will be able to make full use of the GPU without being burdened by sequential nature of the poker game.

This was a huge issue back in 2018 when I could only do online learning. I was at a huge disadvantage. But now I have Spiral and expertise in functional programming. I pity anybody trying to do what I will in Python. Python is the worst language there is for doing CPS programming.

Let me finally have fun here.

Just what I am acting at by putting overtime at this stage? If I really feel like working I should get up earlier instead."

---
## [kacperleague9/BlueMapVue@1b66b1876f...](https://github.com/kacperleague9/BlueMapVue/commit/1b66b1876f102f2c6d2e168214e2b181239e92ab)
##### 2021-04-07 17:47:34 by Kacper Smoliński

:flag_pl:


We got a number one victory royale
Yeah, Fortnite, we 'bout to get down (Get down)
Ten kills on the board right now
Just wiped out Tomato Town
My friend just got downed
I revived him, now we're heading south-bound
Now we're in the Pleasant Park streets
Look at the map, go to the marked sheet


Take me to your Xbox to play Fortnite today
You can take me to Moisty Mire, but not Loot Lake
I really love to chug jug with you
We can be pro Fortnite gamers

---
## [Buildstarted/linksfordevs@5872fae422...](https://github.com/Buildstarted/linksfordevs/commit/5872fae42271141e37dc8c0bcb9ba34969fd994b)
##### 2021-04-07 22:08:27 by Ben Dornis

Updating: 4/7/2021 10:00:00 PM

 1. Added: Financial architecture
    (https://www.josephrex.me/financial-architecture/)
 2. Added: Goodbye OpenSSL, and Hello To Google Tink
    (https://medium.com/asecuritysite-when-bob-met-alice/goodbye-openssl-and-hello-to-google-tink-583163cfd76c)
 3. Added: Releases support comments and reactions with Discussion linking - GitHub Changelog
    (https://github.blog/changelog/2021-04-06-releases-support-comments-and-reactions-with-discussion-linking/)
 4. Added: Learning from your Mistakes: How a bug made me reprioritize my product roadmap
    (https://nocommandline.com/blog/learning-from-your-mistakes/)
 5. Added: Embrace the Grind - Jacob Kaplan-Moss
    (https://jacobian.org/2021/apr/7/embrace-the-grind/)
 6. Added: Are you foolish enough to innovate?
    (https://markgreville.ie/2021/04/07/are-you-foolish-enough-to-innovate/)
 7. Added: Make tests a part of your app
    (https://sobolevn.me/2021/02/make-tests-a-part-of-your-app)
 8. Added: Gregory Szorc's Digital Home | Modern CI is Too Complex and Misdirected
    (https://gregoryszorc.com/blog/2021/04/07/modern-ci-is-too-complex-and-misdirected/)
 9. Added: Accumulating knowledge • Ted Piotrowski
    (https://tedpiotrowski.svbtle.com/accumulating-knowledge)
10. Added: The Paradoxes of Modern Life - David Perell
    (https://perell.com/note/the-paradoxes-of-modern-life/)
11. Added: An Introduction to Cryptocurrency Wallets: Which Wallet Type Suits You Best?
    (https://serhack.me/articles/introduction-cryptocurrency-wallets/)
12. Added: The Cursed Certificate
    (https://nbailey.ca/post/cursed-certificate/)
13. Added: A pretty good guide to pretty good privacy
    (https://blog.garrit.xyz/posts/2021-04-07-pgp-guide)
14. Added: I Actually Like Slack
    (https://patrick.engineering/posts/slack/)
15. Added: Choosing a good development partner
    (https://mcarter.me/posts/choosing-a-good-development-partner)
16. Added: The North Side - Daring LLC
    (https://bedaring.org/post/a-boring-dystopia/the-north-side/)

Generation took: 00:08:17.9560443
 Maintenance update - cleaning up homepage and feed

---
## [maborak/iemaddon-installer@d288db37a0...](https://github.com/maborak/iemaddon-installer/commit/d288db37a04aa31293d4ca6b71c4c15cc6d4c6ab)
##### 2021-04-07 23:00:17 by Wilmer Adalid (Alienware)

Updates for: 'Tis the dream of each programmer,
Before his life is done,
To write three lines of APL,
And make the damn things run.

---
## [MarlonSchultz/aws_amplify_playground@0fe07bae37...](https://github.com/MarlonSchultz/aws_amplify_playground/commit/0fe07bae379747ce51a4c6b6a86cb3729aceaf25)
##### 2021-04-07 23:10:31 by Marlon

got primevue up and running. i just so fcuking hate css. like ten years ago - how the fuck do you center shit

---

# [<](2021-04-06.md) 2021-04-07 [>](2021-04-08.md)

