# [<](2021-05-31.md) 2021-06-01 [>](2021-06-02.md)

3,312,558 events, 1,522,501 push events, 2,539,726 commit messages, 187,704,484 characters


## [CapnMachaddish/Outpost-R505@e058b2410b...](https://github.com/CapnMachaddish/Outpost-R505/commit/e058b2410ba3812972695fce7170b855c8839688)
##### 2021-06-01 00:28:02 by death and coding

[modular][ready] formal wear for medical and engineering in loadout, gas masks unadded, but coded (#5156)

* for i just threw out the love of my dreams

* Update uniform.dm

* fuck

* biker

* Update glasses.dm

* I don't want any of my sprites in Skyrat. I appreciate you asking though.

* Revert "I don't want any of my sprites in Skyrat. I appreciate you asking though."

This reverts commit d980baf7ada95a04f74870103b2ee09ede67dcda.

* Revert "Revert "I don't want any of my sprites in Skyrat. I appreciate you asking though.""

This reverts commit ad42656117f90747c0aaf7ade3614a16d67fed4b.

* as requested

* casual cargo gear

Co-authored-by: louiseedwardstuart <bonniefluff>

---
## [philipl/mpv@f4e89dde36...](https://github.com/philipl/mpv/commit/f4e89dde36644edec7d09856ac83140317f0b687)
##### 2021-06-01 02:24:50 by Dudemanguy

wayland: simplify render loop

This is actually a very nice simplification that should have been
thought of years ago (sue me). In a nutshell, the story with the
wayland code is that the frame callback and swap buffer behavior doesn't
fit very well with mpv's rendering loop. It's been refactored/changed
quite a few times over the years and works well enough but things could
be better. The current iteration works with an external swapchain to
check if we have frame callback before deciding whether or not to
render. This logic was implemented in both egl and vulkan.

This does have its warts however. There's some hidden state detection
logic which works but is kind of ugly. Since wayland doesn't allow
clients to know if they are actually visible (questionable but
whatever), you can just reasonably assume that if a bunch of callbacks
are missed in a row, you're probably not visible. That's fine, but it is
indeed less than ideal since the threshold is basically entirely
arbitrary and mpv does do a few wasteful renders before it decides that
the window is actually hidden.

The biggest urk in the vo_wayland_wait_frame is the use of
wl_display_roundtrip. Wayland developers would probably be offended by
the way mpv abuses that function, but essentially it was a way to have
semi-blocking behavior needed for display-resample to work. Since the
swap interval must be 0 on wayland (otherwise it will block the entire
player's rendering loop), we need some other way to wait on vsync. The
idea here was to dispatch and poll a bunch of wayland events, wait (with
a timeout) until we get frame callback, and then wait for the compositor
to process it. That pretty much perfectly waits on vsync and lets us
keep all the good timings and all that jazz that we want for mpv. The
problem is that wl_display_roundtrip is conceptually a bad function. It
can internally call wl_display_dispatch which in certain instances,
empty event queue, will block forever. Now strictly speaking, this
probably will never, ever happen (once I was able to to trigger it by
hardcoding an error into a compositor), but ideally
vo_wayland_wait_frame should never infinitely block and stall the
player. Unfortunately, removing that function always lead to problems
with timings and unsteady vsync intervals so it survived many refactors.

Until now, of course. In wayland, the ideal is to never do wasteful
rendering (i.e. don't render if the window isn't visible). Instead of
wrestling around with hidden states and possible missed vblanks, let's
rearrange the wayland rendering logic so we only ever draw a frame when
the frame callback is returned to use (within a reasonable timeout to
avoid blocking forever).

This slight rearrangement of the wait allows for several simplifications
to be made. Namely, wl_display_roundtrip stops being needed. Instead, we
can rely entirely on totally nonblocking calls (dispatch_pending, flush,
and so on). We still need to poll the fd here to actually get the frame
callback event from the compositor, but there's no longer any reason to
do extra waiting. As soon as we get the callback, we immediately draw.
This works quite well and has stable vsync (display-resample and audio).
Additionally, all of the logic about hidden states is no longer needed.
If vo_wayland_wait_frame times out, it's okay to assume immediately that
the window is not visible and skip rendering.

Unfortunately, there's one limitation on this new approach. It will only
work correctly if the compositor implements presentation time. That
means a reduced version of the old way still has to be carried around in
vo_wayland_wait_frame. So if the compositor has no presentation time,
then we are forced to use wl_display_roundtrip and juggle some funny
assumptions about whether or not the window is hidden or not. Plasma is
the only real notable compositor without presentation time at this stage
so perhaps this "legacy" mechanism could be removed in the future.

---
## [fesh0r/mame@f2f52b28ff...](https://github.com/fesh0r/mame/commit/f2f52b28ff8f23c6778aa516d782a947e5679b66)
##### 2021-06-01 05:25:08 by r09

fmtowns_cd.xml: 13 new dumps, 12 replacements, 5 missing floppies added (#7874)

* Added the missing floppy image to OASYS/Win 2.0 (still not working due to lack of DD floppy support). [cyo.the.vile]
* Replaced the psydet5 and psydetf1 floppy images with cleaner unmodified copies. [cherokee]

New working software list additions (fmtowns_cd.xml)
-----------------------------------
Alice no Yakata 3 (1995-05-16) [redump.org]
Battle [redump.org]
Ehon Writer V1.1 L10 [redump.org]
Half Moon ni Kawaru made - Ramiya Ryou no Nijiiro Tamatebako [redump.org, wiggy2k]
Never Land [redump.org]
Oto to E no Deru Eigo Jisho No. 2 - English in Dream [redump.org]
Populous II - Trials of the Olympian Gods - Expert [redump.org]
Running Girls - Hashiri Onna II + Rance 4.1 / 4.2 Hint Disk [redump.org]
Soreike! Anpanman - Tanoshii Eigo Asobi [redump.org]
Toshiyuki Yoshino - Lullaby of BirdLand [redump.org]
True Heart (alt) [redump.org]
Viper GTS [redump.org]

New not working software list additions (fmtowns_cd.xml)
-----------------------------------
Scavenger 4 (HME-217B) [redump.org]

Replaced software list items (fmtowns_cd.xml)
----------------------------
Hanafuda de Pon! [redump.org]
Indiana Jones and the Fate of Atlantis [redump.org]
King's Quest V - Absence Makes the Heart Go Yonder [redump.org]
Kyan Kyan Collection - Daifugouhen [redump.org]
Kyuukyoku Tiger [redump.org]
Legends of Valour - Gouyuu no Densetsu [redump.org]
Life &amp; Death II - The Brain [redump.org]
Menzoberranzan - Yami no Monshou [redump.org]
Oshiete Noobow [redump.org]
Princess Danger [redump.org]
Scavenger 4 (HME-217A) [redump.org]
Wrestle Angels Special [redump.org]

Software list items promoted to working (fmtowns_cd.xml)
---------------------------------------
Nobunaga no Yabou - Sengoku Gun'yuuden [cherokee]
Windows 3.1 L11 [cyo.the.vile]

---
## [g8kig/tvision@45e408ac9d...](https://github.com/g8kig/tvision/commit/45e408ac9d41577177484df85ed4c0c766c3691d)
##### 2021-06-01 13:37:25 by magiblot

Fix (TCollection *) to (TSortedCollection *) cast in TSortedListBox::list()

God damn. Because TCollection and TSortedCollection are no more than forward declarations by the time TSortedListBox::list() is defined, the cast is implemented as a reinterpret_cast. As a consequence, invoking TSortedListBox::list() would provide the wrong result.

Amazingly, Borland C++ handles this fine, so I have been hugely confused while debugging this.

This is my first experience where C-style casts have silenced a compilation error.

---
## [KOFIWILLIAMS/Winners_Quote_App@57af0a3a9e...](https://github.com/KOFIWILLIAMS/Winners_Quote_App/commit/57af0a3a9e4076972dff7422e07fd7f3168b1624)
##### 2021-06-01 14:46:19 by Winner Williams

Winners Quote App simply contains a collection of Winner's quotes , Inspirational messages, Bible messages, life and Success quotes, Anniversaries, Millionaires messages, Love messages, Birthday wishes, Funny quotes, Fitness quotes and also powerful African proverbs.

Kindly check it out & share
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
PlayStore: https://play.google.com/store/apps/details?id=com.globetechconsultldt.winners_vibe
ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥
Overview of Winners Quotes App:  https://youtu.be/aKccgBQLeS0

---
## [EgorDinamit/tegustation@7c4efd1d77...](https://github.com/EgorDinamit/tegustation/commit/7c4efd1d7762abc8402f1f9a030c25f3d6715045)
##### 2021-06-01 14:52:40 by SmArtKar

Fixes god damn simplemob chems (#124)

* Fixes god damn simplemob chems

Retards, I already gave you the fucking fix a year ago

* Update holder.dm

---
## [AbenezerMamo/100@4ffe2e1e4c...](https://github.com/AbenezerMamo/100/commit/4ffe2e1e4c36da947e10807be730e5d4e933c39e)
##### 2021-06-01 16:06:59 by Abenezer Mamo

Another run-on

I get to say * b/c any of you shame a brother going into fatherhood, just know that we're watching. Different times. Different people. Different world. Be wise and do your own research but know that after wards you laugh about it and hope the universe took a note. Future young person listening to today's rap. HA! Don't even trip hommie you don't even phase me but make sure you can keep enjoying that * into your late years. Bumping the true crew love bump, "you ain't even know it!"

---
## [mrakgr/The-Spiral-Language@bf51d6e187...](https://github.com/mrakgr/The-Spiral-Language/commit/bf51d6e1874df8cb786f46393ef315c958d69273)
##### 2021-06-01 16:34:23 by Marko GrdiniÄ‡

"1:50pm.

```py
    def grad_state_probs(): # Prediction errors modulate the state probabilities.
        prediction_values = (head_weighted_values / head_value_weights)[action_indices,:] if head_weighted_values.shape[0] <= action_indices.shape[0] else head_weighted_values[action_indices,:] / head_value_weights[action_indices,:] # [batch_dim,state_dim]
        prediction_errors = torch.abs((action_values - prediction_values) * action_weights) # [batch_dim,state_dim]
```

Forgot to multiply by the action weights.

2:30pm. Let me finally resume. Where was I?

```
        def of_action_probs(action_probs, sample_probs):
            # action_probs[batch_dim,action_dim]
            # sample_probs[batch_dim,action_dim]
            qwe = state_probs.mm(values.t()) # [batch_dim,action_dim]
```

What should I name this?

3pm.

```py
import torch
import torch.distributions
from torch.functional import Tensor

def updates(state_probs : Tensor,head : Tensor,action_indices : Tensor,at_action_value : Tensor,at_action_weights : Tensor):
    # state_probs[batch_dim,state_dim]
    # head[action_dim*2,state_dim]
    # action_indices[batch_dim] : map (action_dim -> batch_dim)
    # at_action_value[batch_dim,1] : map (action_dim -> batch_dim)
    # at_action_weight[batch_dim,1] : map (action_dim -> batch_dim)
    num_actions = head.shape[0]//2
    head_weighted_values = head[:num_actions,:] # [action_dim,state_dim]
    head_value_weights = head[num_actions:,:] # [action_dim,state_dim]

    def update_head(): # Weighted moving average update. Works well with CFR's reweighting.
        state_weights = at_action_weights * state_probs # [batch_dim,state_dim]
        head_weighted_values[action_indices,:] += at_action_value * state_weights
        head_value_weights[action_indices,:] += state_weights

    def grads():
        values = head_weighted_values / head_value_weights # [action_dim,state_dim]
        def of_state_probs(): # Prediction errors modulate the state probabilities. The cool part is the centering.
            prediction_values_for_state = values[action_indices,:] # [batch_dim,state_dim]
            prediction_errors = torch.abs(at_action_value - prediction_values_for_state) # [batch_dim,state_dim]
            prediction_error_mean = (state_probs * prediction_errors).sum(-1,keepdim=True) # [batch_dim,1]
            at_action_weights * (prediction_errors - prediction_error_mean) # [batch_dim,state_dim]

        def of_action_probs(action_probs, sample_probs): # Implements the VR MC-CFR update.
            # action_probs[batch_dim,action_dim]
            # sample_probs[batch_dim,action_dim]
            prediction_values_for_action = state_probs.mm(values.t()) # [batch_dim,action_dim]
            at_action_sample_probs = torch.gather(sample_probs,-1,action_indices.unsqueeze(-1)) # [batch_dim,1]
            at_action_prediction_value = torch.gather(prediction_values_for_action,-1,action_indices.unsqueeze(-1)) # [batch_dim,1]
            at_action_prediction_adjustmnet = (at_action_value - at_action_prediction_value) / at_action_sample_probs # [batch_dim,1]
            torch.scatter_add(prediction_values_for_action,-1,action_indices.unsqueeze(-1),at_action_prediction_adjustmnet)
            -at_action_weights * action_probs * prediction_values_for_action # [batch_dim,action_dim]
        return of_state_probs, of_action_probs
    return update_head, grads
```

Here it is.

3:10pm. Doing some thinking whether adjusting the scale of the centered gradients to the orignal makes sense, and the conclussion is that it does not.

```
[9,8] * [1.0,0.0]
```

This would center it to `[0,-1]`. I do not want that to become `[0,-9]`. I mean, it could, but then what should I do with an input like `[9,3] * [1.0,0.0]` which would otherwise be `[0,-6]`. Do not want those to have the same grads. The rule I picked is the best one.

3:20pm. It might be good to match the scale of the log_softmax rule, but forget it. I won't bother with this. What I have now should be good enough.

What I've implemented here is the most straightforward thing imaginable, but it ticks all the boxes:

1) Tabular methods for learning values.
2) Bounded gradient updates for learning policies.

Since I am doing tabular methods on top of aggregated features, I won't have to worry about it having trouble learning the values. Since I am using weighted averaging, I do not even need a learning rate. I'll just decay the head by some factor after every policy update.

3:30pm. I am just chilling for a bit.

How about go back to watching the lecture by Bertsekas?

I do not feel like doing more programming right now. The next thing on the list should be the optimizer.

4:10pm. https://www.youtube.com/watch?v=MP-0PlYLWu0
Feature Based Aggregation and Deep Reinforcement Learning

Let me watch this for a bit.

4:30pm. https://youtu.be/MP-0PlYLWu0?t=3129

I am not getting much out of this, but he is talking about DeepChess which I haven't heard about. It extracts features using a NN.

Back when I read the episodic memory papers, my impression was that none of them use the tabular prediction error to train the layer below and use random projections. I wonder how DeepChess does it.

I am not going to look into this. Let me finish this, and then I will take a look at how the SGD optimizer is done. I'll write my own.

https://youtu.be/MP-0PlYLWu0?t=3270

Interesting that he mentions that recognizing success or failure is a problem. Yeah, this is something that is troubling me as well.

4:40pm. A memory is coming to my mind now that he is talking about tetris. Some OpenAI guy talked about the cross entropy method and how it crushed RL algos on tetris. Now that I've managed to get to the point where tabular RL is usable with deep nets, it might be worth looking into the CEM. I never got the algorithm for it. Maybe there is a video on it somewhere.

I know I studied this, but I never came close to understanding it.

https://www.youtube.com/watch?v=0h8Ql5-CpBE
Reinforcement Learning: Cross Entropy Method

Oh, lol. I had to check that I did not mute the sound by accident. This has no audio.

5pm. https://youtu.be/dmH1ZpcROMk?t=12
Reward Is Enough (Machine Learning Research Paper Explained)

Let me watch this for a bit.

5:05pm. Nevermind. I do no feel like listening to this, I am already checking out the SGD optimizer.

5:25pm.

```py
y = torch.nn.Sequential(torch.nn.Linear(4,6),torch.nn.Linear(6,2))
print(y[0].bias)
list(y.parameters())
```

Ok, decision time. I meant to grab the infinity norm of the bias together with the weights, but that won't be needed. I'll do a linear layer that makes the bias and the weight views on the same tensor if needed later. But it is not a huge deal.

To start things off, let me grab signSGD off the net somewhere. The SGD optimizer is actually rather complex.

```
y = torch.nn.Sequential(torch.nn.Linear(4,6),torch.nn.Linear(6,2))
for x in y:
    print(x)
```

Oh, I can access all the params like this.

https://discuss.pytorch.org/t/build-custom-param-groups-for-optimizer/43784/3

And this thread shows how to use param groups. Good. That means I do not have to do a whole separate layer with packed weights just to take the inf norm over the layer.

I can group the params in a Seq.

That is one thing done.

5:55pm. Let me take a break here.

6:10pm. Let me just test a few more things.

```
y = torch.nn.Sequential(torch.nn.Linear(4,6),torch.nn.Linear(6,2))
list(y)
```

This is nice. By doing this I get a list of individual modules. I can use this to group them.

The update I can do easily. I do not need a special optim object.

6:20pm. Hmmm, the default optimizer is actually pretty complex. I do not want to deal with it right now. Since my own requires nothing more than gradients, and does not keep around momentum, a function will suffice.

I should look into how optimizers work more, but it is not important right now. I'll just implement the functions that I need and move on with my life.

6:25pm. Tomorrow, I'll do the optimizer. It is not a big deal. I really got into it when doing the updates. It is a nice piece of work.

As I said, despite getting up at 7am, I did not do much today. But keeping the morale high is more important. If one tries to do too much, it is easy to get burnt out. Spending so much time daydreaming about the things I am doing might seem like a waste, but it is what keep me going.

Tomorrow, I will do the optimizer and after that I'll be checking out how to do the training loop in Python after that. I'll leave what is needed in Spiral to where it is, and do the flaky NN optimization parts that I'll have to often debug in Python. That is the way to the stress free programming life.

It won't be long before I have this scheme running and those tough-to-chew self play players optimizing."

---
## [CapnMachaddish/Outpost-R505@68cfa06465...](https://github.com/CapnMachaddish/Outpost-R505/commit/68cfa064650e58c9bd06c26da561ee945b8197a0)
##### 2021-06-01 18:33:23 by linnpap

[NON-MODULAR] More drugs. (Part 2) (#5170)

* whole bunch of shit

-gives quaaludes and opium more distinct effects
-adds pcp
-adds thc
-replaces space drugs in weed plants with thc
-adds hash
-adds dabs
-rebalances amount of drugs you can produce

* Update opium.dm

* newlines

* runtime fix

* runtime fix attempt 2

* amazing

* i dont recall the variable names being this fucked

* I FUCKING HATE GIT

* please so help me god

---
## [MyObliviousDeveloper/mom-release-monthly@fa26ff0b71...](https://github.com/MyObliviousDeveloper/mom-release-monthly/commit/fa26ff0b716fd59b2a2e4ceafb54c1d40924ae9a)
##### 2021-06-01 20:13:14 by MyObliviousDeveloper

v 0.33.0

- Dinner Kitchen > Stealth Check, higher level (feminization path)
- Night pussy scene edited to show up in "she is sleeping prone", since it is written as if she was
- Estrogen pills now show up in the inventory when you buy and get them
- The Market clearly states that users can find the stuff they buy at Pre Lunch, in the Living Room, checking the mail
- Trade Cumrag Panties for money or Good Boy Points in the Master Bedroom, First Afternoon
- Hand her Clean or Dirty panties after she has smeared cum all over her face during Netfux & Chill (using her hand)
- After giving the panties to $landlady (Master bedroom, First Afternoon), IF she is already ok with daily checks about vaginal checks, you can perform a "Taste test" (ranked poll, 2k words)
- Fixed lack of speech bubbles in some chapters

---
## [Mortalitas/GShade-Presets-Mirror@a94dd38f0f...](https://github.com/Mortalitas/GShade-Presets-Mirror/commit/a94dd38f0ff9c4ee8f6b1eda2dcaec6cf636bfb5)
##### 2021-06-01 21:48:31 by Mana

new preset!!! yooooo!

Hi I haven't done this in a while! I'm sorry about that I tried to upload one before but I realised I accidentally make a fork of the presets and- yeah
this one might be a little messy, if someone could wiggle into my DMs (Faeyth#0001) cuz I can't for the life of me remember how to clean them up myself, also are credits changes okay? I've been trying to be more open with myself and I'd like to link *proper* social media to my credits instead of my email cuz I was scared of human interaction.

Thanks much!!!

---

# [<](2021-05-31.md) 2021-06-01 [>](2021-06-02.md)

