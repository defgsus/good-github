# [<](2021-04-15.md) 2021-04-16 [>](2021-04-17.md)

2,960,273 events, 1,512,822 push events, 2,311,587 commit messages, 185,924,196 characters


## [vlaci/nix-doom-emacs@69c2a4e4b1...](https://github.com/vlaci/nix-doom-emacs/commit/69c2a4e4b15d12e2b47e1d8888c3db0445570e10)
##### 2021-04-16 00:27:23 by github-actions[bot]

test/doom.d/init.el: Updating from hlissner/doom-emacs - 193382e2

### Changes for test/doom.d/init.el

```diff
--- 
+++ 
@@ -111,7 +111,8 @@
 
        :lang
        ;;agda              ; types of types of types of types...
-       ;;cc                ; C/C++/Obj-C madness
+       ;;beancount         ; mind the GAAP
+       ;;cc                ; C > C++ == 1
        ;;clojure           ; java with a lisp
        ;;common-lisp       ; if you've seen one lisp, you've seen them all
        ;;coq               ; proofs-as-programs
@@ -124,6 +125,7 @@
        emacs-lisp        ; drown in parentheses
        ;;erlang            ; an elegant language for a more civilized age
        ;;ess               ; emacs speaks statistics
+       ;;factor
        ;;faust             ; dsp, but you get to keep your soul
        ;;fsharp            ; ML stands for Microsoft's Language
        ;;fstar             ; (dependent) types and (monadic) effects and Z3
@@ -138,9 +140,8 @@
        ;;julia             ; a better, faster MATLAB
        ;;kotlin            ; a better, slicker Java(Script)
        ;;latex             ; writing papers in Emacs has never been so fun
-       ;;lean
-       ;;factor
-       ;;ledger            ; an accounting system in Emacs
+       ;;lean              ; for folks with too much to prove
+       ;;ledger            ; be audit you can be
        ;;lua               ; one-based indices? one-based indices
        markdown          ; writing docs for people to ignore
        ;;nim               ; python + lisp at the speed of c
@@ -159,7 +160,7 @@
        ;;(ruby +rails)     ; 1.step {|i| p "Ruby is #{i.even? ? 'love' : 'life'}"}
        ;;rust              ; Fe2O3.unwrap().unwrap().unwrap().unwrap()
        ;;scala             ; java, but good
-       ;;scheme            ; a fully conniving family of lisps
+       ;;(scheme +guile)   ; a fully conniving family of lisps
        sh                ; she sells {ba,z,fi}sh shells on the C xor
        ;;sml
        ;;solidity          ; do you need a blockchain? No.
@@ -167,6 +168,7 @@
        ;;terra             ; Earth and Moon in alignment for performance.
        ;;web               ; the tubes
        ;;yaml              ; JSON, but readable
+       ;;zig               ; C, but simpler
 
        :email
        ;;(mu4e +gmail)

```

---
## [rapidsai/cudf@8a666a04e0...](https://github.com/rapidsai/cudf/commit/8a666a04e0123744eb259d88ac4c04b0b6de4303)
##### 2021-04-16 01:28:50 by Vyas Ramasubramani

Refactor Python and Cython internals for groupby aggregation (#7818)

This PR makes some improvements to the groupby/aggregation code that I identified while working on #7731. The main purpose is to make the code logic easier to follow and reduce some unnecessary complexity; I see minor but measurable performance improvements (2-5% for small datasets) as well, but those are mostly just side effects here. Specifically, it makes the following changes:

1. Inlines the logic for dropping unsupported aggregations. The old function was repetitive and necessitated looping over the aggregations twice, whereas the new approach drops unwanted aggregations on the fly so it only loops once. The new code also makes it so that you only construct a C aggregation object once.
2. Merges the logic from `_AggregationFactory` into `Aggregation`, and removes the constructor for `Aggregation`. The one downside here is that the Cython `Aggregation` object's constructor no longer places it in a valid state; however, in practice the object is always constructed via either the `make_aggregation` function or its various factories, and the object's constructor was only every used in `_drop_unsupported_aggs` anyway. The benefit is we remove the fragmentation between these two classes, making the code much more readable, and the `Aggregation` class actually serves a purpose now beyond just providing a single property `kind` that is only used once: it is now the primary way that other Cython files interact with aggregations. This also means that in most places other Cython modules don't need to work with `unique_ptr[aggregation]` as much anymore (although they do still have to move `Aggregation.c_obj` for performance reasons). `make_aggregation` now returns the Cython class instead of the underlying C++ one.
3. Modified all the "allowed aggregations" sets to use the uppercase names of the aggregations. In addition to simplifying the code a tiny bit, this helps reduce confusion between the aggregation names used in Python for pandas compatibility and the libcudf names (for instance, `idxmin` vs `argmin`, now `ARGMIN`).
4. Explicitly defines all the aggregations on a groupby. I discussed this briefly with @shwina, the change has pros and cons. The benefit is that all of these methods are properly documented now, there's less magic (the binding of methods to a class after its definition can be confusing for less experienced Python developers and has a lot of potential gotchas), and we can use the simpler string-based agg definition wherever possible. The downside is that we now have to define all of these methods. I think the change is definitely an improvement, but I'm happy to change it back if anyone can suggest a better alternative. In the long run we probably need to find a way to share both code and docstrings more effectively between all aggregations (DataFrame, Series, and GroupBy).

Authors:
  - Vyas Ramasubramani (https://github.com/vyasr)
  - GALI PREM SAGAR (https://github.com/galipremsagar)

Approvers:
  - Karthikeyan (https://github.com/karthikeyann)
  - Ashwin Srinath (https://github.com/shwina)
  - GALI PREM SAGAR (https://github.com/galipremsagar)

URL: https://github.com/rapidsai/cudf/pull/7818

---
## [pololi-stripe/my-checkout-demo@688873d11a...](https://github.com/pololi-stripe/my-checkout-demo/commit/688873d11a41b04d86c85a83721baec09006ac6c)
##### 2021-04-16 02:13:39 by Polo Li

pl - add gitignore

pl - change it to use my BR scenarios keys

pl - add config.ru

pl - add hello world page

pl - add payment_method_options and boleto beta version

pl - use heroku domain

pl - add webhook

pl - log webhook event

pl - miss the stupid return (I hate sinatra)

pl - move log

pl - debug webhook

pl - remove pubkey from code

---
## [mrakgr/The-Spiral-Language@139643ea93...](https://github.com/mrakgr/The-Spiral-Language/commit/139643ea93b2982284f3cd4db9da79d73e7a3db5)
##### 2021-04-16 11:21:32 by Marko Grdinić

"11:30am. Somehow that Groq talk is still ongoing with the recruiter trying to rope me in.

Forget it. I am massively distracted by this and need to get my game on. I already spent 30m being in thought about this.

First, let me talk about something that occured to me during the night about performance.

CFR is 10x slower than the non optimizing uniform agent.

```
    terminal action = fun {chance_prob player player' id actions next} =>
        inl d = value d actions player.observations
        inl op_prob = log_probm.exp (log_probm.add chance_prob player')
        inl state = state player

        inl next prob a =
            if prob = 0 && op_prob = 0 then r2 0 // the pruning optimization
            else next ((log_probm.from prob,a),state)

        inl weighted_sum policy = am.fold2 (fun s prob a => s +. next prob a *. prob) (r2 0) policy actions
        inl reward,reward_wsum =
            am.mapFold2 (fun s prob a =>
                inl r = next prob a
                r, s +. r *. prob
                ) (r2 0) (regret_match d.regret) actions
        inl ~id = id
        join
            d.regret |> am.mapInplace (fun i x => x + op_prob * (r2_index (index reward i) id - r2_index reward_wsum id))
            inl policy = regret_match d.regret
            inl self_prob = log_probm.exp player.prob
            d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index policy i)
            am.fold2 (fun s prob reward => s +. reward *. prob) (r2 0) policy reward
```

The entirety of the speed difference should be here. This is just a couple of array maps.

```
        inl reward,reward_wsum =
            am.mapFold2 (fun s prob a =>
                inl r = next prob a
                r, s +. r *. prob
                ) (r2 0) (regret_match d.regret) actions
```

One here.

```
d.regret |> am.mapInplace (fun i x => x + op_prob * (r2_index (index reward i) id - r2_index reward_wsum id))
```

```
inl policy = regret_match d.regret
```

```
d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index policy i)
```

Three here.

```
am.fold2 (fun s prob reward => s +. reward *. prob) (r2 0) policy reward
```

Here is a fold.

```
inl d = value d actions player.observations
```

And this is accesing the dict.

If the iteration over the game tree (plus some extras) is 1x, then these lines account for the other 9x of the performance difference.

I said I wanted 2m instead of 200k, but as a thought exercise do I think I could speed up these parts up?

What I am doing here is nothing big, and the arrays being iterated are only a few elements long. And yet that is enough to slow the traversal by 10x.

So maybe I can conclude that the uniform traversal is not as slow as it seems.

11:45am. I certainly can't speed up these pieces. They are just straightforward tail recursive loops.

One thing I should take a look is whether Cython is compiling with optimizations on.

https://stackoverflow.com/questions/16285011/cython-compile-option-o3

```
pyximport.install(language_level=3,setup_args={"include_dirs":np.get_include()},extra_compile_args=["-O3"])
```

Should I do it like this? No I get an error.

```
pyximport.install(language_level=3,setup_args={"include_dirs":np.get_include(),'extra_compile_args': ["-O3"]})
```

Like this then?

```
C:\Users\Marko\anaconda3\lib\distutils\dist.py:274: UserWarning: Unknown distribution option: 'extra_compile_args'
  warnings.warn(msg)
```

I get a warning. This is not right either.

https://cython.readthedocs.io/en/latest/src/userguide/source_files_and_compilation.html#limitations
> pyximport does not use cythonize(). Thus it is not possible to do things like using compiler directives at the top of Cython files or compiling Cython code to C++.

> Pyximport does not give you any control over how your Cython file is compiled. Usually the defaults are fine. You might run into problems if you wanted to write your program in half-C, half-Cython and build them into a single library.

> Pyximport does not hide the setuptools/GCC warnings and errors generated by the import process. Arguably this will give you better feedback if something went wrong and why. And if nothing went wrong it will give you the warm fuzzy feeling that pyximport really did rebuild your module as it was supposed to.

Nevermind this. O2 is good enough for me.

11:55am. At any rate, contrasting the CFR to the uniform agent performance is enough to convince me that the uniform is not that slow. My implementation is fine.

Let me try running the non-optimizing cfr agent.

```
inl main () =
    inl d = dict.empty
    open agent.cfr
    inl p1 = funsTrain PlayCurrentPolicy d
    inl p2 = funsTrain PlayCurrentPolicy d
    loop.for' (from:0u32 nearTo:100) (fun i =>
        open nodes
        leduc.game (nodes.nodes_2p (p1,p2)) (log_probm.from 1,init,init) |> ignore
        )
```

```
7.3339046
```

Hmmm, not as fast as I thought it would be.

```
inl d = value d actions player.observations
```

```
inl weighted_sum policy = am.fold2 (fun s prob a => s +. next prob a *. prob) (r2 0) policy actions
```

I guess there is quite a bit of overhead in accessing the dictionary. Iterating over two arrays should not be that much slower than just one, so I guess that about 6x of the peformance difference is explained by dictionary accesses.

This is nothing unusual. Compared to accessing a regular array, you can expect at least 3x slowdown by using a dictionary.

While getting the key value pair, it does have to access every elements in the bucket until it gets to it. That is a few array accesses right there.

Yeah, all of this is within reasonable limit. I have nothing to complain about. My implementation is sound.

12:10pm. Now what comes next?

```
    cps_test
    ui_leduc_test
    ui_replay_test
    terminal_train_test
    ui_train_test
```

```
inl main () =
    inl p1 = agent.neural.funs(agent.neural.create_net())
    inl p2 = agent.uniform.funs()
    loop.for' (from:0u32 nearTo:100) (fun i =>
        open nodes
        inl r = leduc.game (nodes.cps.nodes_2p (p1, p2)) ((log_probm.from 1,init,init),dyn id)
        $"print(\"Reward for player one at iteration \", !i, \" is \", !r)" : ()
        )
```

Let me get these tests working again.

The main problem is that I've removed the `let`s from leduc and have to think about putting the join points in the nodes themselves.

```
inl sample i pid dist f (p,ret) =
    let f a b = f a b
    inl x = index dist i
    inl p = sample_players_update pid (log_probm.from (1 / f64 (length dist)),x) p
    f x (p,ret)

inl draw i pid dist f (p,ret) =
    let f a b = f a b
    inl x = index dist i
    inl ar = am.removeAtIndex i dist
    inl p = sample_players_update pid (log_probm.from (1 / f64 (length dist)),x) p
    f (x,ar) (p,ret)
```

I think it will be fine if I do the same thing as last time.

```
if i < length dist then one i pid dist f (p,dyn fun r => loop (i+1) (s+r))
```

Won't need the dyn here.

```
    action = fun s pid ar f (chance_prob,p1,p2) =>
        let f (a : a) (b : log_prob * player s1 o a * player s2 o a) : r2 = real real_core.unbox a (fun _ => f a b)
```

In action let me do the same thing as last time. I'll just adjust the type signature.

```
let f (a : a) (b : pl2 s1 s2 o a * (r2 -> ret)) : ret = real real_core.unbox a (fun _ => f a b)
```

This should do nicely. And the compiler is not stack overflowing when I run it. Let me run the example.

12:30pm. Focus me.

```
Error compiling Cython file:
------------------------------------------------------------
...
    cdef bint v3
    cdef unsigned long long v4
    v3 = v2 < v0
    if v3:
        v4 = v2 + 1ull
        v1[v2] = 0.000000f
                        ^
------------------------------------------------------------

c:\Users\Marko\Source\Repos\The Spiral Language\Spiral Compilation Tests\cython_experiments\ui_leduc6\cps_test.pyx:260:25: Syntax error in simple statement list
cps_test.c
```

What is this?

```
v1[v9] = 1.000000f
```

Once I remove that `f` I get an error elsewhere.

Are the literal suffixes the problem here. The performance tests did not use floats anywhere.

12:30pm. Hmmmm, before I start debugging this, just where these floats coming from?

Ah, from the NN agent itself. That has to be it.

But are the float literals really no good. Let me try it in an isolated place.

```
cpdef void main():
    cdef float q
    q = 2.000000f
```

What happens when I try to compile this?

```
Error compiling Cython file:
------------------------------------------------------------
...
cpdef void main():
    cdef float q
    q = 2.000000f               ^
------------------------------------------------------------

c:\Users\Marko\Source\Repos\The Spiral Language\Spiral Compilation Tests\cython_experiments\ui_leduc6\cps_test.pyx:3:16: Syntax error
in simple statement list
cps_test.c
```

Maybe that `f` has special meaning on the Python side. Nevermind this.

Let me hack the compiler again. C literals are pure cancer.

```fs
    | LitInt8 x -> sprintf "(<signed char>%i)" x
    | LitInt16 x -> sprintf "(<signed short>%i)" x
    | LitInt32 x -> sprintf "(<signed long>%i)" x
    | LitInt64 x -> sprintf "(<signed long long>%i)" x
    | LitUInt8 x -> sprintf "(<unsigned char>%i)" x
    | LitUInt16 x -> sprintf "(<unsigned short>%i)" x
    | LitUInt32 x -> sprintf "(<unsigned long>%i)" x
    | LitUInt64 x -> sprintf "(<unsigned long long>%i)" x
    | LitFloat32 x ->
        if x = infinityf then "(<float>float('inf'))"
        elif x = -infinityf then "(<float>float('-inf'))"
        elif Single.IsNaN x then "(<float>float())"
        else sprintf "(<float>%f)" x
    | LitFloat64 x ->
        if x = infinity then "(<double>float('inf'))"
        elif x = -infinity then "(<double>float('-inf'))"
        elif Double.IsNaN x then "(<double>float())"
        else sprintf "(<double>%f)" x
```

I should have done this from the start. No more trouble with literals. I remember having a bug where using f32 would emmit conversion warnings. Now it should be resolved.

12:45pm. Focus me. What is next?

The cps thing works just fine. The rest of the tests should flow as is.

12:55pm. Yeah, they all work just fine.

The ui_train_test runs noticeably faster when I click the run button. Great.

```
C:\Users\Marko\.pyxbld\temp.win-amd64-3.8\Release\pyrex\ui_train_test.c(38696): warning C4018: '<': signed/unsigned mismatch
C:\Users\Marko\.pyxbld\temp.win-amd64-3.8\Release\pyrex\ui_train_test.c(78779): warning C4018: '<': signed/unsigned mismatch
C:\Users\Marko\.pyxbld\temp.win-amd64-3.8\Release\pyrex\ui_train_test.c(78918): warning C4018: '<': signed/unsigned mismatch
```

The only bit of trouble are these mismatch warnings.

VS Code is so good. On a while I went to open file and pasted the above path, and to my surprise it actually accepted the input and opened the file. Let me take a look at the given line.

```
  { Py_ssize_t __pyx_temp;
    for (__pyx_temp=0; __pyx_temp < __pyx_v_v2; __pyx_temp++) {
```

Hmmm...

```
unsigned PY_LONG_LONG __pyx_v_v2;
```

So is `Py_ssize_t` a signed quantity after all?

1:05pm.

```fs
        let length (a,b) =
            match a with
            | YPrim (Int64T | UInt64T as t) -> return' $"<{prim t}>len({tup b})"
            | YPrim a ->
                let l = $"len({tup b})"
                let tmp_i = tmp()
                line defs $"cdef unsigned long long tmp{tmp_i}"
                line s $"tmp{tmp_i} = {l}"
                let a = prim a
                line s $"if <{a}>tmp{tmp_i} != tmp{tmp_i}: raise Exception(\"The conversion to {a} failed.\")"
                return' (tup b)
            | _ -> raise_codegen_error "Compiler error: Expected a primitive type in length."
```

Let me try it like this.

It is no good. I still get the signed unsigned mismatch. Why?

```
    { Py_ssize_t __pyx_temp;
      for (__pyx_temp=0; __pyx_temp < __pyx_v_v12; __pyx_temp++) {
        __Pyx_INCREF(Py_None);
        __Pyx_GIVEREF(Py_None);
        PyList_SET_ITEM(__pyx_t_7, __pyx_temp, Py_None);
      }
    }
```

No, I can't figure out how to fix this. It does not matter. I think it should be fine to just ignore this warning.

```
  /* "ui_train_test.pyx":1178
 *         v1 = method49(v0)
 *         v2 = <unsigned long long>len(v1)
 *         v3 = [None]*v2             # <<<<<<<<<<<<<<
 *         v4 = (<unsigned long long>0)
 *         method53(v2, v1, v3, v4)
 */
  __pyx_t_1 = PyList_New(1 * (__pyx_v_v2)); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 1178, __pyx_L1_error)
  __Pyx_GOTREF(__pyx_t_1);
  { Py_ssize_t __pyx_temp;
    for (__pyx_temp=0; __pyx_temp < __pyx_v_v2; __pyx_temp++) {
```

Ahhh. Cython does in fact optimize this. I somehow missed the comment completely.

1:20pm.

```fs
        let length (a,b) =
            match a with
            | YPrim (Int64T | UInt64T) -> return' $"len({tup b})"
            | YPrim a ->
                let l = $"len({tup b})"
                let tmp_i = tmp()
                line defs $"cdef unsigned long long tmp{tmp_i}"
                line s $"tmp{tmp_i} = {l}"
                let a = prim a
                line s $"if <{a}>tmp{tmp_i} != tmp{tmp_i}: raise Exception(\"The conversion to {a} failed.\")"
                return' (tup b)
            | _ -> raise_codegen_error "Compiler error: Expected a primitive type in length."
```

Let me go back to this...and it is quite late. Let me have breakfast here."

---
## [communistkiro/rice@1022d8b323...](https://github.com/communistkiro/rice/commit/1022d8b323c3e5a3376f6751dcb90711bb053e63)
##### 2021-04-16 16:06:12 by communistkiro

blahblahblah fuck shit
todo rewritten in zsh
yt - based on sb's subs in gh, made better, bc. gh are cunts and can't code properly had to move it to new directory where it will finally be recognized as a file
ffs i fucking hate you fucking kikes, inbred codemonkeys, trannies, simps, and everything else wrong w humanity
fuck me

---
## [mrakgr/The-Spiral-Language@f37cff4e61...](https://github.com/mrakgr/The-Spiral-Language/commit/f37cff4e61fb7766d6ac40d3adeaee6fb1e4ef86)
##### 2021-04-16 17:45:11 by Marko Grdinić

"2:10pm. Done with breakfast. Let me do the chores here.

2:30pm. Done with chores. Now where was I?

2:35pm. Ok, all the tests work now. I see that after the return value change in cfr, the reward chart still look the same.

Obviously the UI version has plenty of closures, and I am definitely not going to bother optimizing that.

2:40pm. Now...

Hmmm, everything is set. It is time to move to sampling CFR.

2:45pm. Let me push a patch. The previous one has broken float literals. Hopefully it should go through, if not I'll leave pushing it for later. Oh, both of them went through. Great.

2:50pm. Made a new project. Let me focus on the sampling cfr in isolation.

I'll get rid of all the tests except the `ui_train` test.

2:55pm. I've cleaned up the the project and got rid of the torch and various other dependencies.

```
packages: |core2-
modules:
    utils-
    log_probm
    sampling
    game-
    nodes/
        utils-
        cps
        main-
    leduc
    agent/
        uniform
        cfr
        human
    ui_train_test

```

Here is how the project file looks. Previously, I needed to make sure that the basic PyTorch player works, but now things are different.

I need to focus on getting the sampling cfr to work.

Let me take a look at the paper again. I need to prepare myself for this mentally. I should also fire an email to Lanctot asking him about the update rule. It is just so nonsensical and is putting pressure on me.

3:10pm.

>  CFR+ works by thresholding the quantity at each round (Tammelin et al. 2015): define Q0 (I, a) = 0 and QT (I, a) = (QT −1 + r T (I, a))+

Ah, CFR+ is not just about alternating the updates, but it also threesholds the regret sums.

I think I noted this back in 2019, but forgot about it.

3:20pm. Variance Reduction in Monte Carlo Counterfactual Regret Minimization (VR-MCCFR) for Extensive Form Games using Baselines

I am reading this paper and I completely do not understand the division by the sampling policy in eq 7. Why not just use a weighted moving average to update the Q values.

Let me fire an email to Lanctot. It is time to get to the bottom of this. I think I will just skip the rule here unless he can convince me that it is right.

https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/

Github is telling me it does not like me using basic password.

3:25pm. Hmmm, is it Visual Studio that is doing this. Forget this for now. Email.

3:55pm. Had to take a break.

```
Hello and sorry for bothering you again. We had some contact 1.5 years ago where I was asking you some CFR related questions, but after not making progress on understanding it, I decided to make a detour to make a new version of the Spiral language instead. But now I am back in the ML game and I've finished implementing regular CFR in it.
I feel that I understand the basic algorithm better now, but

Anyway, I have to ask you about the division
```

Actually, let me not bother the guy just yet. I know how to implement the thing in the paper. There should be an OpenSpiel implementation floating around as well.

How about I first do what the paper says, then do my own and then compare them? Then I can start firing questions.

4:05pm. Let me dig out the algorithm from somewhere.

4:20pm. I can't find it. DREAM I know where it is, but the tabular algorithm I am not sure. I know how to implement my own translation of enumerative CFR into a sampling one. That is what makes sense to me. But DREAM and the VR MCFR paper do many weird things and I do not know what is right or wrong anymore.

http://proceedings.mlr.press/v119/davis20a/davis20a.pdf
Low-Variance and Zero-Variance Baselines for Extensive-Form Games

Let me read this paper. Getting the sampilng CFR figured out will take some work it seems.

> The empirical convergence rate of CFR is greatly improved by the CFR+ variant, which greedily zeroes all negative regrets on every iteration, replacing Rt with an accumulant Qt recursively defined with Q0 (I, a) = 0, Qt (I, a) = max(Qt−1 (I, a) + r t (I, a), 0) (Tammelin et al., 2015).

Maybe I should do this. This update rule is actually close to what the gradient update should be after normalization.

> A sampled utility is then calculated recursively for each prefix of z t as

Yeah, it does divide the utility by the sampling policy.

5pm. Had to take a nap for a bit to think about it. And my conclussion is that the outcome sampling update here makes complete sense.

That sum on the second line of eq 2 seems scary, but with regular outcome sampling that has no baselines, everything but the sampled value ends up being zero.

So what outcome sampling amounts to is doing importance sampling. If I am using a sampling policy, it make sense to divide by it and multiply by the actual policy to correct for that value.

Propagating such values through the tree does make sense.

And remember weighted moving averages? That was my big idea, but it is exactly the same thing as dividing by the sampling policy right away here. It is equivalent.

5:05pm. This is extremely remarkable. Enumerative CFR corrects for the reward sampling error implicitly, but when doing explicit sampling the division by the sampling policy is in fact necessary.

Still when doing a NN based version, instead of this division, I am going to have to reweight the replay buffer appropriately.

Right, I'll have to divide by the sampling probability and the self reach probability. That is the way to go.

5:40pm.

```
let x = (2.0 * 1.0 + 4.0 * 3.0) / (1.0 + 3.0)
let y = 2.0 / 1.0 + 4.0 / 3.0
```

Dividing by the probability and then adding is not at all the same thing as keeping track of weights.

5:45pm. Let me go forward with the paper. I am starting to see what the angle with the update is.

I myself made a mistake by only considering the division by the self reach probability and not the sampled action probability. The policy update only needs the first, but the q values should require both.

Hmmmmm, I think. I mean do they? What would happen if I did not do it?

Hmmmm, I am not sure.

5:50pm. Yeah, this is not going to go quickly. As expected, once you sit down and actually try to understand and code up something all sorts of issues crop up.

But my thinking does seem to be getting more effective.

Hmmm, actually the division would be necessary if you are trying to estimate the outcome sampling sum.

6pm. Eq 4 is the usual update that is puzzling me.

> Equation (4) comes from the application of a control variate, in which we lower the variance of a random variable (X = 1((ha)vz t ) q t(h,a) uˆb((ha)|σ t , zt )) by subtracting another random variable (Y = 1((ha)vz t ) q t(h,a) b t (h, a)) and adding its known expectation (E [Y ] = b t (h, a)), thus keeping the resulting estimate unbiased. If X and Y are positively correlated, then this estimate will have lower variance than X itsel

6:05pm. Wait, this is actually simpler than I thought.

But I don't get it.

If `Y = b / q` then why is `E[Y] = b`?

6:10pm. No, it is not really `q`.

`Y = b h a / (I h a * q h a)` and `E[Y] = b h a`.

Could that `E[Y]` be the known expectation of `Y`...

6:20pm. Well, I can't imagine anything else being the case.

Let me do a little calculation.

Suppose that `q h a` is `0.9`. Then to calculate the mean, what I'd have to do is plug in that indicator function.

```
when I h a = 1 with 0.9 probability.
b h a / (I h a * q h a)
0.9 * b h a / (1 * 0.9)
b h a

when I h a = 0 with 0.1 probability.
b h a / (I h a * q h a)
0.1 * b h a / (0 * 0.9)
0
```

`b h a + 0 = b h a`

Wow. The update in the paper does make sense!

I am in awe.

6:30pm. Ok, but this just proves that particular part of the update make sense in isolation.

It does not mean that it makes sense to combine these values as in eq 4. I get that when all the other trajectories are 0, then that bends up doing importance sampling. But you want to weight the rewards by their action probability. If bootstrapping is done then the division by the sampling probability would offset that.

Some of probabilities being 1 or 0 has special meaning.

7pm. No I was babling in the previous entry. I would not be learning the importance sampled value, but the regular Q one.

7:15pm. Done with lunch.

I was wrong, everything in the paper does make sense after all. I just hadn't understood the motivation behind the various things in it.

Sampled CFR is brilliant. I am in awe all over again by the algorithm. The way it has been done is exactly the way it should be done. Also it is good I did not pester the guy, I said I would not do this, and I managed to stick to my vow. Had I fired the question, I'd just have pestered him with what I could have figured out myself.

7:20pm. In terms of increasing my algorithmic understanding, I am really on fire in April! This is the way to go.

I am going to stop programming for the day here. Tomorrow, I think I'll just implement enumerative CFR+ in the other project. Then I'll do the sampling CFR.

Everything is set. My understanding is excellent. Nothing can stop me from finishing this player.

7:40pm. I have to pry myself off.

The only thing that still bothers me is the scaling in the algorithm. But maybe that concern is only in my mind. I can't really reason out what is bothering me here.

Without a doubt, the way it is currently set up has to be correct, so my issue can only be in my own perception of things.

7:45pm. Enough. Let me play Rance Quest here. Today's work is done."

---
## [xorcare/brainfuck@cecf7f72d4...](https://github.com/xorcare/brainfuck/commit/cecf7f72d43bb05028e7e4b2b3117b7225987738)
##### 2021-04-16 18:44:31 by Vasiliy Vasilyuk

Inception | Your mind is the scene of the crime

The Instructions

Brainfuck is tiny. It consists of eight different instructions. These
instructions can be used to manipulate the state of the Brainfuck
machine:

* `>` - Increment the data pointer by 1.
* `<` - Decrement the data pointer by 1.
* `+` - Increment the value in the current cell (the cell the data
pointer is pointing to).
* `-` - Decrement the value in the current cell.
* `.` - Take the integer in the current cell, treat it as an ASCII char
and print it on the output stream.
* `,` - Read a character from the input stream, convert it to an
integer and save it to the current cell.
* `[` - This always needs to come with a matching `]`. If the current
cell contains a zero, set the instruction pointer to the index of the
instruction after the matching `]`.
* `]` - If the current cell does not contain a zero, set the
instruction pointer to the index of the instruction after the matching
`[`.

That’s all of it, the complete Brainfuck language. Let's do this!

LIFE IS MAGIC, CODING IS ART.

---
## [datalad/datalad@9d2b04e8f9...](https://github.com/datalad/datalad/commit/9d2b04e8f9acdaaace0bb52ff218899f48a4611b)
##### 2021-04-16 20:52:29 by Yaroslav Halchenko

RF: hello java my old friend, I've come to love your kludge again

Making shell-completion into a full featured new type of interface to enjoy all
the levels of decoration for results evaluation, so we could even call it
as

    dl.shell_completion(result_xfm='relpaths')

To be appreciated while listening to https://www.youtube.com/watch?v=u9Dg-g7t2l4

---
## [uoregon-libraries/rais-image-server@51eb62ec4d...](https://github.com/uoregon-libraries/rais-image-server/commit/51eb62ec4de194652dea41d8ba39386922b39f2f)
##### 2021-04-16 21:55:33 by Jeremy Echols

plugins/imagick-decoder: reduce "445" errors

Removes the reuse of ImageMagick data, which caused excessive problems
when writing out some kind of cache to /tmp.

I'm not entirely sure what ImageMagick was doing that became a problem,
but it seems you absolutely *must* free resources immediately after
using them.  Even though the second request instantiated a new C.Image,
it seems whatever ImageMagick does on the filesystem causes it to get
itself confused when the cache is still around and the same file is
requested for processing.

The bad news is that ImageMagick is clearly not threadsafe.  Even with
this fix, problems are less common but not eliminated altogether.  The
good news, however, is that this speeds up info.json requests
dramatically.  Because my first implementation IS TERRIBLE and I should
be very ashamed.  And I am.

Oddly, this only happens with large files, so I suspect ImageMagick has
some kind of limit to the number of bytes it's willing to write to its
filesystem-based cache.

---
## [Skyrat-SS13/Skyrat-tg@da2215fb0d...](https://github.com/Skyrat-SS13/Skyrat-tg/commit/da2215fb0d023a299ad4bcf62184063b57ccbf4e)
##### 2021-04-16 22:41:51 by SkyratBot

[MIRROR] Empty graves can now be spawned (#4755)

* Empty graves can now be spawned (#58200)

* i walked along the no mans road

POV: you got fucked on a previous branch from something stupid

* Update code/modules/ruins/lavalandruin_code/elephantgraveyard.dm

Co-authored-by: Fikou <piotrbryla@ onet.pl>

* Update code/modules/ruins/lavalandruin_code/elephantgraveyard.dm

Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

Co-authored-by: Fikou <piotrbryla@ onet.pl>
Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

* Empty graves can now be spawned

Co-authored-by: ishitbyabullet <deathzombine@outlook.com>
Co-authored-by: Fikou <piotrbryla@ onet.pl>
Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

---
## [mill7079/repo-CSCI5607@9ef019e503...](https://github.com/mill7079/repo-CSCI5607/commit/9ef019e503ee37e1ff3c7ab7428508ffdb218709)
##### 2021-04-16 23:45:55 by Allison Miller

Fix continuous movement
- Oh my god it's so smooth it's like putting lotion on dry hands
- I hadn't realized how annoying the choppy start-stop-go movement was
  until just now
- Slight issue in that if you hold rotate for too long it starts going
  really REALLY fast but I'm not sure why and I have too many other
things to fix

---
## [Bytube/tgstation@e15be702a9...](https://github.com/Bytube/tgstation/commit/e15be702a98f6db5e8e9cfe8fc269a33686a33b0)
##### 2021-04-16 23:46:50 by Mothblocks

Moth tourist bots -- They ask for your hat (#57563)

Adds a rare, once per restaurant venue, chance for a moth tourist bot to show up. Asks for the hat, gloves, or shoes you have on.

Closes #57541 if this is merged first, somehow. It includes the testing fix (since I needed to multiply all the weights to allow for rare bots anyway).

Wings are randomized.

I thought it was funny, and it's infrequent enough for the gag to hopefully not lose its magic.

Also a good test bench for the code to allow more dynamic customers. A lot of supporting code was added to make more customizable customers without influencing the surface area of the venue code too much.

Co-authored-by: ATH1909 <42606352+ATH1909@users.noreply.github.com>

---

# [<](2021-04-15.md) 2021-04-16 [>](2021-04-17.md)

