# [<](2021-05-20.md) 2021-05-21 [>](2021-05-22.md)

2,943,060 events, 1,460,673 push events, 2,357,070 commit messages, 167,831,758 characters


## [rdparker/nix-doom-emacs@7d9a501a1a...](https://github.com/rdparker/nix-doom-emacs/commit/7d9a501a1ae181aacbb8b156e05a54e5380fc178)
##### 2021-05-21 00:27:01 by github-actions[bot]

test/doom.d/init.el: Updating from hlissner/doom-emacs - ef7113d6

### Changes for test/doom.d/init.el

```diff
--- 
+++ 
@@ -111,7 +111,8 @@
 
        :lang
        ;;agda              ; types of types of types of types...
-       ;;cc                ; C/C++/Obj-C madness
+       ;;beancount         ; mind the GAAP
+       ;;cc                ; C > C++ == 1
        ;;clojure           ; java with a lisp
        ;;common-lisp       ; if you've seen one lisp, you've seen them all
        ;;coq               ; proofs-as-programs
@@ -124,6 +125,7 @@
        emacs-lisp        ; drown in parentheses
        ;;erlang            ; an elegant language for a more civilized age
        ;;ess               ; emacs speaks statistics
+       ;;factor
        ;;faust             ; dsp, but you get to keep your soul
        ;;fsharp            ; ML stands for Microsoft's Language
        ;;fstar             ; (dependent) types and (monadic) effects and Z3
@@ -138,9 +140,8 @@
        ;;julia             ; a better, faster MATLAB
        ;;kotlin            ; a better, slicker Java(Script)
        ;;latex             ; writing papers in Emacs has never been so fun
-       ;;lean
-       ;;factor
-       ;;ledger            ; an accounting system in Emacs
+       ;;lean              ; for folks with too much to prove
+       ;;ledger            ; be audit you can be
        ;;lua               ; one-based indices? one-based indices
        markdown          ; writing docs for people to ignore
        ;;nim               ; python + lisp at the speed of c
@@ -159,7 +160,7 @@
        ;;(ruby +rails)     ; 1.step {|i| p "Ruby is #{i.even? ? 'love' : 'life'}"}
        ;;rust              ; Fe2O3.unwrap().unwrap().unwrap().unwrap()
        ;;scala             ; java, but good
-       ;;scheme            ; a fully conniving family of lisps
+       ;;(scheme +guile)   ; a fully conniving family of lisps
        sh                ; she sells {ba,z,fi}sh shells on the C xor
        ;;sml
        ;;solidity          ; do you need a blockchain? No.
@@ -167,6 +168,7 @@
        ;;terra             ; Earth and Moon in alignment for performance.
        ;;web               ; the tubes
        ;;yaml              ; JSON, but readable
+       ;;zig               ; C, but simpler
 
        :email
        ;;(mu4e +gmail)

```

---
## [SatvikR/LCGE@f0db75b714...](https://github.com/SatvikR/LCGE/commit/f0db75b714ed956a958b77f479f41fb8fcb8af65)
##### 2021-05-21 02:20:56 by Satvik Reddy

fix: fixed font data corruption

This was the most annoying bug to find in my life. I was using the
texture() function in my shaders incorrectly. I was supposed to pass in
the Sampler2D (texture slot) but I was just passing in 0 every time. For
some reason all of my tests functioned correctly regardless of this.
Fuck you, OpenGL.

---
## [hannesdonel/My-Portfolio-Site@75e76854ba...](https://github.com/hannesdonel/My-Portfolio-Site/commit/75e76854ba2f7b69c1eff70a9b9d49dec9587e0c)
##### 2021-05-21 08:53:34 by hannesdonel

Enhanced Styling

- "Hello" in index.html bigger
- Color of technologies in work.html less obstrusive
- Rearrange skills => most important on top, non programming on bottom
- More personal things on Hello! screen ("I love music")
- Apply bootstrap on contact form
- Change mail in contact
- Change "Where do you live?" to "Country" in contact form
- Add margin at the bottom of social buttons

---
## [mrakgr/The-Spiral-Language@a2c4a2ec82...](https://github.com/mrakgr/The-Spiral-Language/commit/a2c4a2ec82fed1314a3aab1f26c0de20da8debd8)
##### 2021-05-21 10:00:49 by Marko Grdiniƒá

"10:20am. I got up 10m ago. Let me chill a bit and then I will start.

There is no need to do research anymore. I am not going to beat the transformers anytime soon. I did the equivalent of my PhD in the last six years. Now I just have to make those agents and hope it does not take too long for better chips to get here.

10:55am. Ah, I wish I could see into the future and understand what the algorithms will be like. Imagination will only get you so far in this.

I absolutely loathe leaving important work to others. The smart thing to do would be to kick back while others do the hard work, but I resent it. I keep desiring that I could grasp a bit further and come to an understanding how network distilation could be done. To go beyond a fixed number of steps backwards in time to an infinite amount. It won't be that difficult of a process either.

11am. Now that the secret of how Hopfield nets should be done on a linear layer is out, that will serve as an anchor for how unsupervised learning should be done. I did think of an update such as it, but I could not reason out that it would do what it does. It is really weird that getting rid of that sign and using the softmax has not been figured out earlier. It just goes to show how few eyes are looking into things. Maybe had I been looking into Hopfield nets myself, I could have reached the conclussion.

11:05am. I need the superhuman abilities of self improving AIs if I want to go further. Tech is not like cultivation, it is too impersonal. Power that you can control is always better than the one you can't.

11:10am. Now I need to hype myself up into getting started.

```
    "terminal.integrated.shell.windows": "C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\powershell.exe",
    "terminal.integrated.shellArgs.windows": [
        "-ExecutionPolicy", "ByPass", "-NoExit", "-Command", "& 'C:/Users/Marko/anaconda3/shell/condabin/conda-hook.ps1' ; conda activate 'C:/Users/Marko/anaconda3'"
    ],
```
```
This is deprecated, the new recommended way to configure your default shell is by creating a terminal profile in `#terminal.integrated.profiles.windows#` and setting its profile name as the default in `#terminal.integrated.defaultProfile.windows#`. This will currently take priority over the new profiles settings but that will change in the future.(2)
```

First let me get rid of this. The linter is complaining that these are deprecated.

```
"terminal.integrated.automationShell.windows": "-ExecutionPolicy ByPass -NoExit -Command & 'C:/Users/Marko/anaconda3/shell/condabin/conda-hook.ps1' ; conda activate 'C:/Users/Marko/anaconda3'",
```

Is this how it should go?

```
> Executing task: 'c:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin\node_modules\.bin\tsc.cmd' -p 'c:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin\tsconfig.json' --watch <

The terminal process failed to launch: Path to shell executable "-ExecutionPolicy ByPass -NoExit -Command & 'C:\Users\Marko\anaconda3\shell\condabin\conda-hook.ps1' ; conda activate 'C:\Users\Marko\anaconda3'" does not exist.
```

command 'python.execInTerminal-icon' not found

Now I cannot execute in terminal anymore.

11:15am.

```
"terminal.integrated.defaultProfile.windows": "-ExecutionPolicy ByPass -NoExit -Command & 'C:/Users/Marko/anaconda3/shell/condabin/conda-hook.ps1' ; conda activate 'C:/Users/Marko/anaconda3'"
```

Let me try this.

```
> Executing task: 'c:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin\node_modules\.bin\tsc.cmd' -p 'c:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin\tsconfig.json' --watch <

The terminal process failed to launch: Path to shell executable "-ExecutionPolicy ByPass -NoExit -Command & 'C:\Users\Marko\anaconda3\shell\condabin\conda-hook.ps1' ; conda activate 'C:\Users\Marko\anaconda3'" does not exist.
```

Again I get this. Ah no wait, I forgot to remove the other setting.

11:20am. The internet connection is being dogshit again.

https://code.visualstudio.com/docs/python/environments

Oh, once I select an interpreter I can run the scripts almost.

> Note: conda environments cannot be automatically activated in the integrated terminal if PowerShell is set as the integrated shell. See Integrated terminal - Configuration for how to change the shell.

```
"terminal.integrated.profiles.windows": {
  "PowerShell -NoProfile": {
    "source": "PowerShell",
    "args": ["-NoProfile"]
  }
},
```

Hmmm, let me try this.

```
    "terminal.integrated.profiles.windows": {
        "PowerShell -conda": {
            "source": "PowerShell",
            "args": ["-ExecutionPolicy", "ByPass", "-NoExit", "-Command", "& 'C:/Users/Marko/anaconda3/shell/condabin/conda-hook.ps1' ; conda activate 'C:/Users/Marko/anaconda3'"]
            }
    }
```

Let me try it like this.

Oh, this is good. Now I have this profile. But it is not using it by default.

11:30am. Oh it works. Great. Now it uses the above as default, and I do not have to play that dumb game of temporarily commenting out the args before launching the Typescript build watcher when I start the plugin.

Nice. It is good that I resolved this.

11:35am. Now let me think, what is next?

11:40am. Feeding the net.

11:45am. I need some time to think about it. I'll shut out everything else from my mind and focus on this.

For the past 4 weeks, my concern has been mostly on NNs themselves. But whether it is the weak level of today, or the superhuman level of tomorrow, certain things will be the same. The way these nets are fed and sustained will remain constant.

Right now, if I want to be successful, I need to purposely cultivate a peasant mentality of tending to my field. The peasant is not doing gene editing, he is just maintaining the field.

I've gone through these motions a few times before, but I got distracted by research every time and so that will has been fittered away.

11:55am. I need to remember, ultimately the architecture being used, and the training regime is just a technicality. It is a detail that will change over time. The GPU will make way to neurochips, and neurochips will have their own iterations and differences.

Let me stop here for breakfast and chores. If needed I will step away from the screen for the rest of the day until I get my drive."

---
## [mrakgr/The-Spiral-Language@e48a0af242...](https://github.com/mrakgr/The-Spiral-Language/commit/e48a0af242b20b85c449c66b9bd93e69001495a3)
##### 2021-05-21 11:43:16 by Marko Grdiniƒá

"1:25pm. Done with chores and breakfast, and the Mahoako chapter. I do not feel like programming right now as expected. It is really hard for me to get into the mindset.

Let me turn off the computer and step away from the screne for a while. I need to find the feeling again. Once I do, I will start this and continue going forward without stopping.

I am going to get the agent done, set it lose on online gambling dens and get my success and money. I will integrate new findings as they come along. I will follow the proper steps to get me to the main stage. But I need to get rid of the inertia and overcome the research mindset of the past month. It served me well for a while, but now it is time to go beyond.

Though it did not give me much in terms of algorithms, the Hopfield net paper and the research that I did improved my understanding of transformers significantly. For short term memory, attention is really all that I need.

There are some papers recently showcasing the benefits of feedforward nets, but transformers should have better generalization without the need for regularization. They will make better use of the GPU as well.

Eventually, the way to do Long Term Credit Assignment and proper modules will be discovered. My ideas are interesting pieces that I'll expect to play a role, but I can't see beyond them to the actual solution. I do not think the solution will be hard, I'd expect it to be simple, but regardless, I can't grasp it.

Once the hardware is there, and the understanding of the LTCA is complete, the Singularity itself will be close at hand.

The things I can't do right now do not matter. Ultimately, neuroscience will get its shit together. Or the researchers at large will stumble upon the solution.

I am envious and jealous, so much that I want to cry blood over not being able to go just a step further and deal with this myself. But in this competition, it is not like their position is that dominant. While they hide their thoughts, they are superior, but according to their incentives they have no choice but to release their insights, and once they are out, their work will be internalized by me.

They cannot hide their thoughts either because of the publish or perish culture. The only time hiding insight would make sense is if they stumble on something crucial while would enable them to dominate the whole world in a short amount of time. That kind of fantasy is not going to happen. The improvements are always incremental.

1:40pm. The last month was pretty fun. I learned quite a lot of ML that I did not know a few years ago. It has been a great refresher. But I need to put my own life in order.

I need to internalize the pursuit of the elite agent. The net architectures and the hardware will change, but the pipeline will stay the same. The refining the learning pipeline is something I can get better at and it will give me returns long into the future. The ML algorithms themselves are ultimately a small part of the overall scheme.

I will grow along with the technology itself.

Let me take a nap for a while here."

---
## [STRML/flow@1c218b0844...](https://github.com/STRML/flow/commit/1c218b08441a7d5dcb8ffc874856d18adb303638)
##### 2021-05-21 14:19:06 by Nat Mote

Include sighashes in saved state

Summary:
This diff includes the sighash for each file in the saved state, loads the sighash from saved state if the flag is turned on, and makes the changes needed to take advantage of those loaded sighashes for recheck optimizations.

## Implementation

This change is made up of two main components:
* First, there is the machinery to save the sig hashes to saved state and then load them again (on the condition that the appropriate flowconfig flag is set). This component makes up the bulk of the changes here, but is very simple.
* Second, there are the changes to `Merge_stream` and `Context_heaps` that change the merging logic so that we can actually take advantage of these loaded sighashes. These changes are more interesting, and deserve special attention.

I'll focus on this second component here.

At first blush, it may seem like it is be possible to simply load in the sighashes from saved state, and allow the existing machinery to skip merging and checking whenever the newly-computed sighashes match the old, loaded ones. Unfortunately, this is a fantasy.

The issue is that we may skip merging a component because none of its dependencies changed, which would mean that we still don't have a sig context for it. Then, we may later need to merge or check a downstream component (for example, if another of its dependencies has changed), and to do that we'll need the missing sig context.

To address this issue, it would be possible to carefully track every component that has a missing sig context, and backtrack if it is needed for a downstream file. I may still implement such logic in the future. However, for now I think it makes sense to simply merge every file with a missing sig context, but use the newly-available sighashes to drive skipping in the check phase. This is the approach that I have taken in this diff.

Previously, we didn't distinguish between newly-added sig contexts and those that changed. Now, however, we have the ability to know that a sig context has not changed (based on the sighashes loaded from saved state) even when it is new. So, we need to add a case for that -- if the sighashes are the same, but there wasn't an old sig context, we need to write it, but not add it to the `sig_new_or_changed` set. This is what the changes in `Merge_stream` and `Context_heaps` do.

So, on the initial recheck after a lazy init, we will merge all of the files that we consider for merge, just like we do today. However, we will use the sighashes to gather a set of files that have actually changed, and use that set to determine which files we can skip in the check phase. This should, in theory, reduce the number of files we check in the check phase of an initial recheck after a lazy init, thereby reducing the time spent in these early rechecks.

## Caveats

* There are some performance ramifications (see below).
* There are some serious hash stability issues that hamper the effectiveness of this optimization:
  * When testing on real-world inputs, I found that ~40% of files had sighashes that were unstable from run-to-run. They appear to be stable for the life of a given Flow server. I have not investigated this further. I believe that the hashes computed by types-first 2.0 will be more stable, so I am going to put off a detailed investigation in the hopes that this resolves itself.
  * The hashes include the absolute file paths of various source files, so they will not be stable from machine-to-machine or repo-to-repo unless the absolute path to the repo is identical across invocations. This assumption is unlikely to hold in practice. I plan to address this issue next.
* This may cause us to miss some already-committed errors, similar to how we already can miss already-committed errors in lazy mode. However, the extent to which this is true would increase: today, if you modify a file, Flow will display errors for all of its dependents after the initial lazy recheck. With this change, Flow would only display errors for dependents if your changes might have actually affected those dependents. This could be a plus or a minus depending on your point of view.
* As part of the `flow status` output, when Flow is running in lazy mode, we report the number of files that are currently being checked. Without additional changes, we will now report the number of files that we have at one point considered for checking, even if we ended up skipping them. This might merit some additional thought, but it's unlikely to be a noticeable difference for users in practice.

## Performance

This slows down startup time when turned on (but only barely). It does not appear to affect startup time when turned off. It also slightly increases the size of the saved state blob.

When testing on a benchmark recheck, it was only able to skip checking for less than 1% of the files on an initial recheck, due to the hash stability issues mentioned above. However, tests pass, and this has no measurable performance regression when it's turned off, so I would like to land it and then work on improving hash stability later.

Once the hash stability issues are worked out, we should be able to skip as many files in the check phase of an initial lazy recheck as we would for any other recheck. I believe this will be a significant performance improvement, though we will still have to pay the cost of parsing and merging all of the files. Fortunately, the cost of merging will go down dramatically with types-first 2.0, and the check phase is the most costly even today.

Reviewed By: samwgoldman

Differential Revision: D25137486

fbshipit-source-id: 3e9e5846baec287c880211b13171629668457dde

---
## [mrakgr/The-Spiral-Language@e16a2004d3...](https://github.com/mrakgr/The-Spiral-Language/commit/e16a2004d306f4feb6169bf0ac12d9977dd437a0)
##### 2021-05-21 16:02:56 by Marko Grdiniƒá

"4pm. How did I derive the EqProp update again? I am starting to undertand how long term credit assignment could work.

```py
import torch
import torch.functional
import torch.nn
import torch.nn.functional

s1 = torch.tensor([1,1],torch.float32)
s2 = torch.tensor([2,2],torch.float32)
y = torch.scalar_tensor(0,dtype=torch.float32)
w = torch.scalar_tensor(2,dtype=torch.float32)
def F(s): w * s[0] * s[1]
def E(s): -F(s)
def C(s): torch.abs(F(s) - y)

r = E(s1) + C(s2)
```

This is how I wrote it out, but this is wrong. The forward pass is definitely not supposed to be `w * s[0] * s[1]`, this is just the energy.

I am confused. How are you supposed to take the gradient through the forward pass? I mean, the forward pass does not exist in these kinds of models.

42/119.

> A conceptual difference between s(Œ∏, x) and f(Œ∏, x) is that,
in conventional deep learning, f(Œ∏, x) is usually thought of as the output layer of the model
(i.e. the last layer of the neural network), whereas here s(Œ∏, x) represents the entire state
of the system. Another difference is that f(Œ∏, x) is usually explicitly determined by Œ∏ and
x through an analytical formula, whereas here s(Œ∏, x) is implicitly specified through the
variational equation of Eq. (2.3) and may not be expressible by an analytical formula in
terms of Œ∏ and x. In particular, there exists in general several such states s(Œ∏, x) that satisfy
Eq. (2.3). We further point out that s(Œ∏, x) need not be a minimum of the energy function
E ; it may be a maximum or more generally any saddle point of E.

In the end what I did was lower the energy at the present point and increase it at the target point. But the derivative of the energy should not necessarily be related to the derivative of the state of the system.

43/119.

Here in eq 2.9 the gradient is outright derived as the difference of the gradients of the energy function. `dE(s1)/dW - dE(s2)/dW`. Yes, that makes sense. That is in fact how the update is derived.

```
def F(s): w * s[0] * s[1]
def E(s): -F(s)
def C(s): torch.abs(F(s) - y)

r = E(s1) + C(s2)
```

What does not make sense is that `C` here. The actual cost does not have a damn thing to do with anything. `F` is not the forward pass.

4:20pm. Ah, that cost function is supposed to correspond to clamping. That might be the intent of that.

It is a more elegant way of saying - set the value to this. Energy based models are declarative, so it makes sense to express the need through a cost function.

Hmmmm, but that means not just the output, but the input could be set to a particular value as well through the cost function. Right.

4:25pm.

```py
import torch
import torch.functional
import torch.nn
import torch.nn.functional

s1 = torch.tensor([1,1],torch.float32)
s2 = torch.tensor([2,2],torch.float32)
w = torch.scalar_tensor(2,dtype=torch.float32,requires_grad=True)
def E(s): -w * s[0] * s[1]

r = E(s1) - E(s2)
r.backward()
```

This would give the update in the paper.

Well, either way, you are just pushing the energy around.

4:25pm. Anyway, let me talk a bit. I figured out the essence of long term memory assignment.

It has been giving me a headache to try and wrap my head around this, but now I finally see where I was going wrong. Just like when I was bashing my head against synthetic gradients and then had the great inspiration that instead of predicting the gradients, I should reconstruct the inputs, rigth now I see where I've been going wrong in my assumptions.

I've been like a fish in water not realizing it is wet.

I've been wracking my brain to try and figure out what kind of timescale the higher level modules should have. The naive version of having them be 2x slower than the lower one would not work. Not only because after a certain point no learning would get done, but also because after a certain point they would not be useful to the lower layer modules. With a 2x slowdown the higher level modules would give an improvement for a bit and then become dead weight as they get stacked.

So this is wrong.

Then I've started thinking about NN distilation and data compression. For this line of thought, I've changed my assumption that every module should have the same timescale, but employ data compression in order to have an effectively infinite horizon.

But this does not strike me as being right either. I don't really see how with a fixed timescale, planning and reasoning could be done.

4:40pm. Then I let my thought wonder on poker and realized that by the time you've separated the different phases of the game into discrete steps, the job of intelligence would be 99% done.

It seems really simple, but the way the game is presented to the brain would be duplicate after duplicate data point for the vast majority of its time. By the time you get to actual game steps, the state has already been filtered.

So I thought from the perspective, what if for every data point in a sequence I duplicated it for an arbitrary number of times before feeding it to the net. The current algorithms would not be able to deal with it, but the brain could.

...

So here is the proper assumption - each module in the network should have an arbitrary timescale. Not short term, not long term, but arbitrary.

Lower layers should not be fast, and the higher layers should not be slow. They should be arbitrary.

The notion of time is so ubiquitous in the current day algorithms and my own thinking, that it just never occured to me to wonder and see it as just another feature the brain presents to my consciousness. I took the feature, and ended up assuming it is fundamental to the brain's processing.

This is despite already having an inkling that to get good compression the brain should be aggressively factoring out time. I already got a clue when I realized that predicting the gradients cannot possibly work for long term credit assignment. But I was still thinking about time in discrete steps.

4:50pm. But this notion of arbitrary timescale in modules completely breaks the backpropagation rules. It is impossible, not because of symmetric weights, but because credit assignment cannot possibly work like that in an arbitrary regime. Instead it would be necessary to reconstruct the inputs all the way based on the outputs.

The current notion of replay buffers does not make sense in such a regime, instead the credit assignment would completely be done through associative memories. The memories themselves could store transitions. And those transitions could be arbitrary in time.

4:55pm. This is only something that could be researched on neurochips. GPUs are a dead end as far as this is concerned. I can only discretize the inputs when it comes to them. I can even barely think about this subject when the GPUs are in the picture.

Hmmm...

I think ultimately, the energy based model formulation is correct, as opposed to a probabilistic one.

5pm. ANNs are ultimately abstractions of the real thing, but they ended up abstracting too much in fact.

At their level, I can't easily recover timescale arbitrariness between modules.

5:05pm. Ok, to make my vision work, I am missing two things - the way to do memory distilation. And the way to backprop through it.

I am not sure if GANs can be used for reconstruction, but I'll asume there is an unsupervised method for that. Maybe it would work with GANs as well.

5:10pm. I think I see it. There are ways of achieving what I want if I am willing to think outside the box. I'll fold these insights into my prediction of the future. It does not matter if I cannot reach all the way there. If I can be quarter of the step ahead of the rest, that is fine.

And as a ML practitioner, I should understand all the important parts of intelligence.

Being able to reconstruct is really important when doing credit assignment - this capability is primary to the brain and energy based models, but in constrast I have absolutely no idea how to do a hierarchical associative memory in ANNs. Apart from the modern Hopfield layer, but that thing needs all the past examples in the replay buffer.

GANs can generate great samples, and with the duality gap method could be used to train stably, but can they be used as an associative memory? I have no idea.

Let me do a bit more research.

https://arxiv.org/abs/2011.13553
Association - Remind Your GAN not to Forget

https://arxiv.org/abs/1611.06953
Associative Adversarial Networks

5:25pm. Got a wholesome award for one of my old posts on the RL sub. Let me read these papers quickly.

5:50pm. No forget it. The first paper might have something in it, but it is too complicated for me to dig it out. I don't feel like paying attention to it. The second one is outright not what I am looking for.

None of the current GAN research matters squit.

I think my current ideas are pretty good, but I do not have the architectural pieces to even start thinking about it. I'll put them on the backburner. EqProp makes sense, and I've managed to reason out how credit assignment would work in models that have modules with arbitrary timescales.

It is one thing to say that the inputs should be reconstructed, it is easy enough of you assume a magic energy-based model that could do that, but I have no idea how that could be done with current DL architectures. Having a large Hopfield layer with all the patterns stored in its buffer would be too inefficient. I don't know if GAN's even with their training stabilized via the duality gap method would be good enough for this. I have no idea, the pieces are missing and I won't be able to reason them out. The research community will simply have to get there.

6pm. Let me take a bath and I'll close for the day. If necessary I'll spend the day in bed tomorrow as well. I need to get these thoughts out of my system."

---
## [Perkedel/Kaded-fnf-mods@96ba9ab925...](https://github.com/Perkedel/Kaded-fnf-mods/commit/96ba9ab9255467e74c558e6b05b9a35bb2a669fa)
##### 2021-05-21 20:54:46 by Joel Robert Justiawan

oh man, this is hard to code

We have progressed!

the GF home theater has been further scanned. now we've got all frame that refers the same spot. I think

we've started Rule The World normal JSON chart. I'm sorry, I am too lazy right now, so this likely be the same accross all difficulties with difference maybe scroll speed a chart. why not have it this where difficulty have layers. the base is normal. you can have hard layers that adds more arrows, and easy that remove arrow, stuff like that.

further expanded the  sound for MIDI version SeaNothing. well, the gameover one rendered is the pixel one, not the regular one, but we duplicated it and renamed it as it is.

clean the space up.
PAIN IS TEMPORARY
GLORY IS FOREVER
lol wintergatan!

change the directive for discord rich presence to scan cpp define instead, because when it complains the incompatibility, it says `can't access cpp packages for non cpp build` like neko

unfortunately folks, HaxeFlixel did not list Neko, Haslink, Java, and other weird platforms renderable to. but it show Flash there. so um... maybe... idk... why bother swf anyway? it's already not always good idea these days.

oh man. this lime can only render desktop to same OS as the host is. and my Linux area is tight right now, I can't install Haxelib. Haxeflixel, and Lime to it. damn! we need to do something!

don't forget the `lime test windows -32bit` for Windows render! somehow there's still exist people who still forcedly stick to 32bit Windows or what, due to unforseen financial issues or knowledges. Guys, it's 2021! tbh, it's now shameful to excuse of that these days. are you guys okay? you need help? c'mon, we gotta go 64 bit.
building PC without graphic card is still works! I've seen Linus did it! https://youtu.be/J1z4XqEkSEU

add another health icon for future use. there are Surprise and Warning. Yeah, in the future we will build surprise character song. we may yoink characters from someone's mods. so stay tuned with that. and for those whose mods got yoinked here, congratulations. don't worry, I'll always make sure your gamebanana or whatever link is in `CREDIT.md` under the section `mods that got into this mod` thingy. also your name will be listed too next to it. Basically covering all credits in here and there, everywhere.

let's call this `Last Moment`. let me be honest here. I believe we, gamers, will protest once the FULL ASS GAME released. why? because it's not $0. and many of us will ignore everything and focus on the price like media these days, even explicitly said, mm let's see ...
https://www.kickstarter.com/projects/funkin/friday-night-funkin-the-full-ass-game
mm, Eventual Rollout from the Full Ass to web version, they said... Yeah check out section `WHAT ABOUT WEB GAME`, `Will it still get updates`. They said yes, they'll still got it `as times roll by` and oh! I believe we gamers forcused on this: `Ultimately though you'll only to be able to play the FULL ASS GAME if you buy the FULL ASS GAME.`. um yeah, tbh, I also disagree with that. HOWEVER, we haven't seen the actual outcome yet, so gamers, stay tuned to there, in about next year or so.
And therefore it is. As we protested, we began to leave, and the fandom would die in a matter of months, so **I want you gamers to enjoy** the `Last Moment` right before this game considered *dead* because of still disagreement.
I know I am angery, but let's not take the negative this time. well you don't want to, so I'll do it so you'll be motivated as well.

psst, btw, check out his https://ninja-muffin24.itch.io/ here. he had done something... dirty... üòèüëâ hehe yeah boi. don't tell your parent. oh, you can send me your screenshot that you bought it from n word gaming site. that sure motivates me lmao!

okay sleep

---
## [ilammy/themis@230f8a5d5c...](https://github.com/ilammy/themis/commit/230f8a5d5ca5b34bc5bda67a2ba520d5cff7a4b6)
##### 2021-05-21 21:49:12 by Alexei Lozovsky

Pin Android NDK version to 21.4.7075529

GitHub Actions seems to experience some issues with detecting the right
Android NDK to use. There all sorts of hacks with "side-by-side"
installation and "ndk-bundle" which you can read about elsewhere.

Resolve this predicament by pinning the NDK version we are using for
building our native code, both BoringSSL and Themis JNI stuff.

This means developers will have to install this specific version of NDK
instead of using whatever they are using. But that's not a particular
issue. NDK does not affect compatibility of apps either, unless Google
really screws up and breaks something there. Not in our case though
since we're not really using anything Android-specific in our code.

[debug] run on CI

[debug] disable stuff

Revert "Pin Android NDK version to 21.4.7075529"

This reverts commit 6767e614a9d7dabf74fa23aaf7b81a1a7af82eab.

ANDROID_NDK_HOME

ANDROID_NDK_HOME

no

3.6.4

Revert "Revert "Pin Android NDK version to 21.4.7075529""

This reverts commit ae48ecd55b3045264ae8e223e579bca60675e5d6.

what if?

oh fuck you

Revert "3.6.4"

This reverts commit d369d2e259d29b48904df06f7a60a303413e5c07.

Revert "no"

This reverts commit 9d4774176b01b5308d246e6b0974f8643f0072c7.

Revert "ANDROID_NDK_HOME"

This reverts commit 36857d6c987fdef40e07e60c9ae86eb73225110e.

Revert "ANDROID_NDK_HOME"

This reverts commit f9b7ecb7d9f4d8ac3bf6d96eb5f8814d4c1d481d.

Revert "Revert "Revert "Pin Android NDK version to 21.4.7075529"""

This reverts commit 3122e246e9ae6423766a260bca1a718cebb473a0.

asdasdas

asdsd

asdas

chec it out

asd

do eet

try to unfuck

and build again

oh fuck you too, actions

comments

Pin to macos-10.15 because fuck surpriises

do list first

try to make it faster

what if I just remove these instead

them just do this

and disable this for a moment

yep, seems to be working

rename

---

# [<](2021-05-20.md) 2021-05-21 [>](2021-05-22.md)

