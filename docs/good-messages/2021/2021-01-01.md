# [<](2020-12-31.md) 2021-01-01 [>](2021-01-02.md)

1,667,494 events, 952,274 push events, 1,302,065 commit messages, 89,558,262 characters


## [SpeedSterDevsZ/Speed-s-Modded-Axon@6d7eaa08f4...](https://github.com/SpeedSterDevsZ/Speed-s-Modded-Axon/commit/6d7eaa08f4eaad1af358f1a6de699a5238c4a412)
##### 2021-01-01 01:54:15 by SpeedSterDevs

read me

# Speeds-Modded-Axon

Hi Skid, What is Speed's Modded Axon?

Speed's Modded Axon (SMA) Which is Developed by me, is a way better version of axon.

Why?

It has lots of functions from phantom source (my exploit dll source), and yeah.

This is really powerful, but don't expect this to run scripts like dex or owlhub.

Enjoy My Modded Axon. A Modded Lua Wrapper for Roblox Exploits.

Stuff i will ignore:

- "oh noes the game is kicking me"
- "oh noeas its crashing my roblox"

Those Questions will be ignored, and you will be called skid..
Why did we release this?
A 2021 Gift, Of course lmao.

Developers:
SpeedSterDevs (speedsterkawaii#3823)
Mellon (mellonyt#1488)

Thank you for skidding Speed's Modded Axon.

Add Credits, or i will find out about the exploit and therefore expose you.

This is using mellon addresses (oP).

---
## [KeinR/Danzun@13288fe1fc...](https://github.com/KeinR/Danzun/commit/13288fe1fc46659dfba59c6ebb0d4fbbffecaf69)
##### 2021-01-01 11:30:47 by Orion Musselman

Mmmmmm good first draft of the documentation.
Still need to add like a quickstart tutorial and the build instructions. Not that they're that involved through....
The docs are a little wonky in this draft, but I'll weed out the... weeds. Heh.
It is 2:26 in the AM now though - been a while since I've been up this late.

It's funny: I remember that a year ago, around this time, I was desparing over a quiz making framework.
I had decided that I would make a custom programming language for it, and create a BYTECODE COMPILER for it...
Yah, I've come kinda' far if you think about it.
I also remember that I watched Kimi No Na Wa on that night - it's really been a year since I last watched it...
Damn...

---
## [NetBSD/pkgsrc@6877a1f6d7...](https://github.com/NetBSD/pkgsrc/commit/6877a1f6d718efc1a600939dfe6d15d305e8a01c)
##### 2021-01-01 11:52:16 by mef

(devel/R-usethis) Updated 1.6.3 to 2.0.0, sorry for long log

See also:
 https://www.tidyverse.org/blog/2020/12/usethis-2-0-0/

# usethis 2.0.0

This version is anticipated to be released as usethis v2.0.0.

## Adoption of gert and changes to Git/GitHub credential handling

Usethis has various functions that help with Git-related tasks, which
break down into two categories:

1. Git tasks, such as clone, push, and pull. These are things you
   could do with command line Git.

1. GitHub tasks, such as fork, release, and open an issue or pull
   request. These are things you could do in the browser or with the
   GitHub API.

We've switched from git2r to the gert package for Git operations
(<https://docs.ropensci.org/gert/>). We continue to use the gh package
for GitHub API work (<https://gh.r-lib.org>).

The big news in this area is that these lower-level dependencies are
getting better at finding Git credentials, finding the same
credentials as command line Git (and, therefore, the same as RStudio),
and finding the same credentials as each other. This allows usethis to
shed some of the workarounds we have needed in the past, to serve as a
remedial "credential valet".

Under the hood, both gert and gh are now consulting your local Git
credential store, when they need credentials. At the time of writing,
they are using two different even-lower-level packages to do this:

* gert uses the credentials package (<https://docs.ropensci.org/credentials/>)
* gh uses the gitcreds package (<https://gitcreds.r-lib.org/>)

Even now, gert and gh should discover the same credentials, at least
for github.com. In the future, these two packages may merge into one.

Git/GitHub credential management is covered in a new article:

[Managing Git(Hub)
Credentials](https://usethis.r-lib.org/articles/articles/git-credentials.html)

The main user-facing changes in usethis are:

* usethis should discover and use the same credentials as command line Git.
* usethis should be able to work with any GitHub deployment. While
  github.com is the default, GitHub Enterprise deployments are fully
  supported. The target GitHub host is determined from the current
  project's configured GitHub remotes, whenever possible.

As a result, several functions are deprecated and several other
functions have some deprecated arguments.

* Deprecated functions:
  - `use_git_credentials()`
  - `git_credentials()`
  - `github_token()`
* Functions with (deprecated arguments):
  - `create_from_github()` (`auth_token`, `credentials`)
  - `use_github()` (`auth_token`, `credentials`)
  - `use_github_links()` (`host`, `auth_token`)
  - `use_github_labels()` (`repo_spec`, `host`, `auth_token`)
  - `use_tidy_labels()` (`repo_spec`, `host`, `auth_token`)
  - `use_github_release()` (`host`, `auth_token`)

The switch to gert + credentials should eliminate most
credential-finding fiascos. Gert also takes a different approach to
wrapping libgit2, the underlying C library that does Git
operations. The result is more consistent support for SSH and TLS,
across all operating systems, without requiring special effort at
install time. More users should enjoy Git remote operations that "just
work", for both SSH and HTTPS remotes. There should be fewer
"unsupported protocol" errors.

## GitHub remote configuration

Usethis gains a more formal framework for characterizing a GitHub
remote configuration. We look at:

  * Which GitHub repositories `origin` and `upstream` point to
  * Whether you can push to them
  * How they relate to each other, e.g. fork-parent relationship

This is an internal matter, but users will notice that usethis is more
clear about which configurations are supported by various functions
and which are not. The most common configurations are reviewed in a
[section of Happy
Git](https://happygitwithr.com/common-remote-setups.html).

When working in a fork, there is sometimes a question whether to
target the fork or its parent repository. For example,
`use_github_links()` adds GitHub links to the URL and BugReports
fields of DESCRIPTION. If someone calls `use_github_links()` when
working in a fork, they probably want those links to refer to the
*parent* or *source* repo, not to their fork, because the user is
probably preparing a pull request. Usethis should now have better
default behaviour in these situations and, in some cases, will present
an interactive choice.

## Default branch

There is increasing interest in making the name of a repo's default
branch configurable. Specifically, `main` is emerging as a popular
alternative to `master`. Usethis now discovers the current repo's
default branch and uses that everywhere that, previously, we had
hard-wired `master`.

`git_branch_default()` is a newly exported function that is also what's used internally.

`use_course()`, `use_zip()`, and `create_download_url()` all have some
support for forming the URL to download a `.zip` archive of a repo,
based on a repo specification (e.g. `OWNER/REPO`) or a browser
URL. These helpers now form a URL that targets `HEAD` of the repo,
i.e. the default branch.

## Changes to Git/GitHub functionality

The default Git protocol is now "https" and we no longer provide an
interactive choice, by default, in interactive sessions. As always, a
user can express a preference for "ssh" in individual function calls,
for an R session via `use_git_protocol()`, and for all R sessions via
the `usethis.protocol` option (#1262).

`pr_resume()` is a new function for resuming work on an existing local
PR branch. It can be called argument-less, to select a branch
interactively.

`pr_fetch()` can also be called with no arguments, to select a PR
interactively.  The `owner` argument is replaced by `target`, with a
choice of the source (default) or primary repo.

`pr_forget()` is a new function for abandoning a PR you initiated
locally or fetched from GitHub. It only does local clean up and, for
example, doesn't delete a remote branch or close a PR (#1263).

`pr_view()` can now be called with no arguments. If the current branch
is associated with an open PR, we target that and, otherwise, we offer
an interactive selection.

`pr_finish()` deletes the remote PR branch if the PR has been merged
and the current user has the power to do so, i.e. an external
contributor deleting their own branch or a maintainer deleting a
branch associated with an internal PR (#1150). It no longer errors if
the PR branch has already been deleted (#1196).

`pr_pull_upstream()` is renamed to `pr_merge_main()` to emphasize that
it merges the **main** line of development into the current branch,
where the main line of development is taken to mean the default
branch, as reported by `git_branch_default()`, of the source repo,
which could be either `upstream` or `origin`, depending on the
situation.

`create_from_github()` will only create a read-only clone, due to lack
of a GitHub personal access token, if explicitly directed to do so via
`fork = FALSE`.

`create_from_github()` and `use_tidy_thanks()` accept browser and Git
URLs as the `repo_spec` argument, to be friendlier to copy/paste. When
a URL is passed, the `host` is also extracted from it.

`create_github_token()` is a new name for the function previously
known as `browse_github_token()` and `browse_github_pat()`.

`issue_close_community()` and `issue_reprex_needed()` are two new
functions for maintainers who process lots of GitHub issues. They
automate canned replies and actions, e.g. labelling or closing (#940).

GitHub Actions is the preferred platform for continuous integration,
because that is what the tidyverse team currently uses and
maintains. Functions related to Travis-CI and AppVeyor are
soft-deprecated to raise awareness about this change and to make it
clear that, if substantial maintenance becomes necessary, we may elect
to retire the function (#1169).

`browse_github_actions()` is a new function to open the Actions page
of the respective repo on GitHub, similar to existing `browse_*()`
functions (@pat-s, #1102).

`use_github_pages()` is a new function to activate or reconfigure the
GitHub Pages site associated with a repository (#224).

`use_tidy_pkgdown()` implements the complete pkgdown configuration
used by the tidyverse team (#224).

`pr_sync()` is defunct and can be replicated by calling `pr_pull()`,
`pr_merge_main()`, then `pr_push()`.

## Licensing improvements

All `use_*_license()` functions now work for projects, not just
packages.

`use_apl2_license()` (not `use_apache_license()`) and
`use_gpl3_license()` no longer modify the license text (#1198).

`use_mit_license()` now sets the default copyright holder to
"{package} authors". This makes it more clear that the copyright
holders are the contributors to the package; unless you are using a
CLA there is no one copyright holder of a package (#1207).

New `use_gpl_license()` and `use_agpl_license()` make it easier to
pick specific versions of the GPL and AGPL licenses, and to choose
whether or not you include future versions of the license. Both
default to version 3 (and above).

New `use_proprietary_license()` allows your package to pass R CMD
check while making it clear that your code is not open source
(#1163). Thanks to @atheriel for the blog post suggesting the wording:
https://unconj.ca/blog/copyright-in-closed-source-r-packages-the-right-way.html

`use_lgpl_license()` now uses version 3 (and above), and gains new
`version` and `include_future` argument to control which version is
used.

`use_gpl3_license()`, `use_agpl3_license()` and `use_apl2_license()`
have been deprecated in favour of the new `version` argument to
`use_gpl_license()`, `use_agpl_license()` and `use_apache_license()`.

The `name` argument to `use_mit_license()` has been changed to
`copyright_holder` to make the purpose more clear. The `name` argument
has been removed from all other license functions because it is not
needed; no other license makes an assertion about who the copyright
holder is.

## RStudio preferences

usethis is now fully cognizant of the [changes to RStudio
preferences](https://blog.rstudio.com/2020/02/18/rstudio-1-3-preview-configuration/)
in RStudio 1.3:

`edit_rstudio_snippets()` looks in the new location, and if you have
snippets in the old location, will automatically copy them to the new
location (#1204)

New `edit_rstudio_prefs()` opens RStudio preferences file for editing
(#1148).

`use_blank_slate()` can now configure your global, i.e. user-level,
RStudio preference, in addition to project-level (#1018).

## Other changes

`browse_package()` and `browse_project()` are new functions that let
the user choose from a list of URLs derived from local Git remotes and
DESCRIPTION (local or possibly on CRAN) (#1113).

The legacy `"devtools.desc"` option is no longer consulted when
populating a new DESCRIPTION file. You must use the
`"usethis.description"` now (#1069).

`use_dev_package()` gains a `remote` parameter to allow you to specify
the remote. The existing behaviour, which adds an `OWNER/REPO` GitHub
remote, remains the default (#918, @ijlyttle).

`use_cpp11()` is a new function to set up an R package to use cpp11.

`create_package(roxygen = FALSE)` once again writes a valid NAMESPACE
file (and also has no Roxygen* fields in DESCRIPTION) (#1120).

`create_package()`, `create_project()`, `create_from_github()`, and
`proj_activate()` work better with relative paths, inside and outside
of RStudio (#1122, #954).

`use_testthat()` gains an edition argument to support testthat v3.0.0
  (#1185)

`use_version()` now updates `src/version.c` if it exists and contains
a line matching `PKG_version = "x.y.z";`.

usethis has been re-licensed as MIT (#1252, #1253).

## Dependency changes

New Imports: gert, jsonlite (was already an indirect dependency),
lifecycle, rappdirs

No longer in Imports: git2r, rematch2

---
## [darkhz/revvz_sakura@81a56de5db...](https://github.com/darkhz/revvz_sakura/commit/81a56de5dbf25ee1f52de2fb8668709d32c9673c)
##### 2021-01-01 13:06:57 by George Spelvin

lib/sort: make swap functions more generic

Patch series "lib/sort & lib/list_sort: faster and smaller", v2.

Because CONFIG_RETPOLINE has made indirect calls much more expensive, I
thought I'd try to reduce the number made by the library sort functions.

The first three patches apply to lib/sort.c.

Patch #1 is a simple optimization.  The built-in swap has special cases
for aligned 4- and 8-byte objects.  But those are almost never used;
most calls to sort() work on larger structures, which fall back to the
byte-at-a-time loop.  This generalizes them to aligned *multiples* of 4
and 8 bytes.  (If nothing else, it saves an awful lot of energy by not
thrashing the store buffers as much.)

Patch #2 grabs a juicy piece of low-hanging fruit.  I agree that nice
simple solid heapsort is preferable to more complex algorithms (sorry,
Andrey), but it's possible to implement heapsort with far fewer
comparisons (50% asymptotically, 25-40% reduction for realistic sizes)
than the way it's been done up to now.  And with some care, the code
ends up smaller, as well.  This is the "big win" patch.

Patch #3 adds the same sort of indirect call bypass that has been added
to the net code of late.  The great majority of the callers use the
builtin swap functions, so replace the indirect call to sort_func with a
(highly preditable) series of if() statements.  Rather surprisingly,
this decreased code size, as the swap functions were inlined and their
prologue & epilogue code eliminated.

lib/list_sort.c is a bit trickier, as merge sort is already close to
optimal, and we don't want to introduce triumphs of theory over
practicality like the Ford-Johnson merge-insertion sort.

Patch #4, without changing the algorithm, chops 32% off the code size
and removes the part[MAX_LIST_LENGTH+1] pointer array (and the
corresponding upper limit on efficiently sortable input size).

Patch #5 improves the algorithm.  The previous code is already optimal
for power-of-two (or slightly smaller) size inputs, but when the input
size is just over a power of 2, there's a very unbalanced final merge.

There are, in the literature, several algorithms which solve this, but
they all depend on the "breadth-first" merge order which was replaced by
commit 835cc0c8477f with a more cache-friendly "depth-first" order.
Some hard thinking came up with a depth-first algorithm which defers
merges as little as possible while avoiding bad merges.  This saves
0.2*n compares, averaged over all sizes.

The code size increase is minimal (64 bytes on x86-64, reducing the net
savings to 26%), but the comments expanded significantly to document the
clever algorithm.

TESTING NOTES: I have some ugly user-space benchmarking code which I
used for testing before moving this code into the kernel.  Shout if you
want a copy.

I'm running this code right now, with CONFIG_TEST_SORT and
CONFIG_TEST_LIST_SORT, but I confess I haven't rebooted since the last
round of minor edits to quell checkpatch.  I figure there will be at
least one round of comments and final testing.

This patch (of 5):

Rather than having special-case swap functions for 4- and 8-byte
objects, special-case aligned multiples of 4 or 8 bytes.  This speeds up
most users of sort() by avoiding fallback to the byte copy loop.

Despite what ca96ab859ab4 ("lib/sort: Add 64 bit swap function") claims,
very few users of sort() sort pointers (or pointer-sized objects); most
sort structures containing at least two words.  (E.g.
drivers/acpi/fan.c:acpi_fan_get_fps() sorts an array of 40-byte struct
acpi_fan_fps.)

The functions also got renamed to reflect the fact that they support
multiple words.  In the great tradition of bikeshedding, the names were
by far the most contentious issue during review of this patch series.

x86-64 code size 872 -> 886 bytes (+14)

With feedback from Andy Shevchenko, Rasmus Villemoes and Geert
Uytterhoeven.

Link: http://lkml.kernel.org/r/f24f932df3a7fa1973c1084154f1cea596bcf341.1552704200.git.lkml@sdf.org
Signed-off-by: George Spelvin <lkml@sdf.org>
Acked-by: Andrey Abramov <st5pub@yandex.ru>
Acked-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Reviewed-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Cc: Geert Uytterhoeven <geert@linux-m68k.org>
Cc: Daniel Wagner <daniel.wagner@siemens.com>
Cc: Don Mullis <don.mullis@gmail.com>
Cc: Dave Chinner <dchinner@redhat.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

---
## [mrakgr/The-Spiral-Language@6b997a4e48...](https://github.com/mrakgr/The-Spiral-Language/commit/6b997a4e4863f70a9edbb08b8862eb51f4881a1a)
##### 2021-01-01 18:44:20 by Marko Grdinić

"11:20am. The new year is here. I spent 2020 learning concurrency and then using it in the creation of Spiral v2. This year, I am going to spend programming in it.

11:25am. Let me chill for a while and then I will start.

11:30am. Actually, let me have breakfast and do the chores here. I do not have time to do anything in the morning session. I really slept well tonight.

1:45pm. Let me finally start. First, let me post the review on the Simulacrum blog. I will just put it through a spell checker first. It is really a chore to the do the PL sub review, this, plus this journal, but I'll keep it up unless it starts giving me trouble.

2pm.

///

[Spiral v2](https://github.com/mrakgr/The-Spiral-Language) is out on the VS Code marketplace. Installing the language is as simple as installing the plugin from the extensions tab. It is still in testing phase, and it should have been for a few months more, but I decided to not be timid, and pushed forward the release. My plan right now is to keep fixing the bugs while I do the docs.

Even though I say that, the language is not in a poor shape at all, so feel free to give it a try. By the time January is over both the documentation and the language should be in good shape.

It has been a long time getting to this point. I spent the entire year 2020 on making it happen. Every design mistake from v0.09 has been amended, and the language has a cool, new top-down type system to take the edge off the partial evaluation magic that it does.

After January is done, I intend to finally put my plans into action. In 2018, I put in a lot of effort on making a GPU-based deep learning library using the original Spiral. In 2021 I will do the same thing except much better. I will make use of the language as wedge to get me through the door of [various hardware companies](https://cacm.acm.org/magazines/2020/8/246356-neuromorphic-chips-take-shape/fulltext). Getting them to sponsor me is one aspect of the plan, but the main purpose of the expedition will be to get a handle on these devices and make sure I am continually at bleeding edge of the hardware wave from here on out.

Though I could not accomplish my main goal 2018 which was making a good RL agent, the ML library itself was in my view fairly impressive - I manage to create a fully optimized RNN (similar to the one in the [Nvidia blog post](https://developer.nvidia.com/blog/optimizing-recurrent-neural-networks-cudnn-5/)) and used it as a model in a policy gradient training of a poker agent whose game implemented alongside it. I had to automate serialization between language barriers, and the game's data types and something the NNs could process. In order to implement optimized GPU kernels such as the one in the blog post, I had to implement automatic differentiation on GPU itself. All the technical achievements were not accomplished simply by gritting my teeth and dropping down to the low level, but through innovative high level abstractions. For example, GEMM fusion from that blog post is made much easier by having tensors with named dimensions.

This is the usual habit in my programming - I solve a task, I generalize the solution and then reuse the abstractions elsewhere. The language one is using makes a vital difference - that library and all those optimizations would have been impossibly difficult to do in C for example.

I made some mistakes too. When it comes to making a GPU library, it turns out that .NET's GC is really a poor fit for managing GPU memory. I'd have been better off picking Python as a target platform. Towards the end of 2018 the old Spiral's poor design and implementation made the compile times go up drastically. And having to do type inference programmatically was really an anchor around my neck.

I can go on about my regrets, but even if it weren't for all of those, I do not think I could have accomplished my goal. Towards the end of 2018, I realized one thing - though GPUs are the workhorse of deep learning, they are actually a very poor fit of reinforcement learning. I spent a long time bashing my head how to improve the performance of RL algorithms. I blasted backpropagation. At some point understanding will dawn, but until then I've realized that what I should do is make do with poor algorithms.

Maybe backprop is a poor fit, but the reality is that the task of reinforcement learning is particularly difficult. In 2018 I've challenged myself to improve a single model and got nowhere. Instead of doing that, in 2021 I will take advantage of new hardware to master ensembles.

In 2018, I would train a model, and would get wildly different results depending on random seeds. It is an open secret that RL researchers cherry-pick their results. Instead of getting mad at that, what I have to do is formalize that insight. If training a single model is all up to luck, why not train a 100 of them in a single go?

GPUs are actually a very poor fit for that. They are good for quickly processing batches of data through a single model, but doing something like passing a single vector of data through numerous networks would be the worst case fit for them. It will be different for [neurochips](https://www.youtube.com/watch?v=jhQgElvtb1s). The new language will allow me deal with the hardware, and the hardware will allow me to complete the agents that I could not a few years ago.

Well, that is the plan for when I am done with the docs I plan on striking out and getting in contact with hardware companies themselves when I am ready. If somebody from the hardware side is reading this and is interested having a Spiral backend and an ML library for it please do get in touch.

---

(Continued from the PL monthly sub post.)

If this plan works, I will be able to solve my poverty woes in one fell swoop. The paper I linked says there are 50 startups, so there are plenty of potential sponsors for my services. I plan to price the rental space in the Spiral ecosystem for these companies at 3k per month. Assuming the sponsor is interested in having a permanent backend and library to the point of paying for it, this is the price point that I think will make it rational for the companies to just cash out to me instead of hiring another programmer to fork Spiral and maintain that. Maybe I could go for more, but I will curb my greed and resist trying to squeeze the other parties. Maximizing money is not that important, I want to get through this next part peacefully.

Since the market is ripe, I am hoping to get a bunch of them and build up my monthly income that way.

In Croatia, the average salary is a bit over 1k per month. Local dev salaries are higher than the average, but it is not a rich region by any means. Even just getting single sponsor would put me way above average. And I've been without money for long enough, so I am determined to make this work. I need to get access to chips anyway - instead of paying for them years down the road, isn't it better that I get others to pay me to program them today? I really do need to explore the offerings anyway, I don't have a view on which specific piece will be dominant yet. It would be a mistake to pick one and stick with it; at this point I do need variety.

Unlike v0.09 which was researchy kind of language, and whose libraries I've written in F# strings, v2 has way higher quality across the board, so I am confident on about the quality of my offering.

Still, it is not like my past schemes were successful. But to be fair they were pretty crazy.

This has a fairly rational chance of working out, but I don't have the experience nor the knowledge to tell how things will go. I should be able to convince the leads to take a look at Spiral, but whether the companies will be willing to pay depends on the level of the people working there. I expect them to be mostly using Python + C as their hardware stack. This should have a large amount of friction that Spiral could alleviate, but who knows, maybe the guys there will be the kind that likes C.

I have no idea what the general programmer level of the C crowd is, but there are plenty 'lol GC bad' kiddies there. They are not exactly innovative. If the guys I keep running into don't see a point to a language having first class functions, that will make any kind of negotiations untenable. The ideal team would be the one that has positive experience with other functional programming languages. I can imagine those kinds of people really being impressed with Spiral.

But even though functional programming is in ascendance, it is not mainstream by any means. So if I run into a team like that, it will more likely be an exception than the rule. I might end up being forced to take whatever opportunity I can get.

I can do a good job on Spiral, and I can try to tailor my message, but beyond that things are out of my hands. I'll do my best and let the dice land where they may.

If this were all about money, trying to make a poker agent, let alone a language would make zero sense.

It is not that crushing [these guys](https://www.esportsearnings.com/) would be satisfying monetarily. It is just I can't imagine a successful Singularity run that does not go through that path. Could there exist a programmer too weak to beat online gaming, but strong enough to conquer the universe? I doubt it. Gaming success is as much of a sanity check as it is an opportunity to make money along the way.

When I decided to make Spiral, I set on out on my own path. I really expected there would be more people than just me working on a language to make an ML library. Because I know back then that down the road new hardware will come out, and Theano, Tensorflow - whatever was popular at the time would be scrapped. The hardware companies would not have Google level resources to make those kinds of compilers. Even if they do take the effort to integrate with popular frameworks, what will happen if deep learning gets superseded by new algorithms?

Furthermore, deep learning is not so difficult as to necessitate such huge software investments that Google and Facebook are making. I could not make a GPU library in F#, but it was doable in Spiral v0.09 and it will be easy in v2.

I do feel a certain sense of loneliness that what I expected did not come to pass, but in the software world the isolation I endure today will be competitive moat tomorrow.

It is difficult. In games you command the player character and the action gets executed. If only my own mind had the same kind of calm attitude and disposition that my proxies do. My own mind requires deep planning and elaborate beliefs to embark on a course of action. The action requires effort, but establishing motivation requires just as much effort. It is tiresome.

Do I have anything better to do than programming in this life? No. If I could go back in time 15 years ago, I'd command myself to do programming. If I could get an extra 15 years of work today for free, just how much further along would I be? It is unimaginable.

I can't change the past, so this time I'll be smarter and give this gift to the me of the future. For a person of a sinister character such as myself, programming is a fitting punishment.

Every day I gather inspiration, then I try to manifest it as code. I manifest it as writing. I gather inspiration and let it loose letting my hands act out the office of the mind after that.

This inspiration is an invisible power that I have to contain in raw matter. Otherwise it would escape.

Yet even though I keep doing this exercise, I haven't attained even a bit of what I want yet. I have no intention of stopping.

I will find my dream again, at the end of the human era.

///

LanguageTools are good, but they fucked up my formatting. Now I have to fix it by hand. Just how is it inserting those newlines?

Furthermore, deep learning is not so difficult as to necessitate such huge software investments that Google and Facebook are making.

2:10pm.

```fs
let s =  "Furthermore, deep learning is not so difficult as to necessitate such huge software investments that Google and Facebook are making."
let s' = "Furthermore, deep learning is not so difficult as to necessitate such huge software investments that Google and Facebook are making."

let x = s.ToCharArray()
let x' = s'.ToCharArray()
Array.iteri2 (fun i x x' ->
    if x <> x' then printfn "at %i, %i <> %i" i (int x) (int x')
    ) x x'
```

This is what I am doing now. When printed as chars, all these spaces look the same to me.

```
at 17, 160 <> 32
at 26, 160 <> 32
```

2:15pm. Ok, I see it. Even in the editor I can just select the empty space at is will show me what is going on. Language tools for some reason is using a different word (160) than the regular space (32) and that is what is causing trouble. Easy fix.

2:20pm. Ok, let me post this thing.

2:25pm. This is beyond troublesome. How do I switch to the old editor so I can just paste the text as it is?

2:30pm. The wordpress interface is so convoluted. I figured out how to switch the classic editor, but now it is not letting me do the change due to it shadowing the one I trashed.

2:40pm. Finally posted the thing. I feel like me trying to remind myself how to switch to the classic editor is going to be a recurring skit every six months.

Note: To switch to the Classic Editor go to WP Admin -> Posts and edit the draft from there.

I'll add this note to the top of this journal file. That way I will be reminded where the instructions are.

2:50pm. Ok, I got that out of the way. I am just taking a short breather. This always takes me a full hour for some reason. The PL thread is not up yet.

2:55pm. It is time I start work on the docs. As expected, when I am fresh, the inspiration comes much more easily. And I did have some time to think about it during the night. Today I will try keeping it up till 8pm. Given that I've been skipping the morning session and getting up late, 6pm is really enough time in the day to do all that I want.

3:30pm. Had to take a little break. I realized that I forgot to make the `:` parser indentation sensitive.

```fs
            (indent i (<=) (opt (skip_op ":" >>. root_type_annot)))
```

Let me just go with this.

3:35pm. Since it is not really important I'll leave this change for a later patch. Let me just focus on the documentation for the time being.

No doubt later, maybe soon, I'll have to do something to allow `print_static` in the editor. It would be best if I could open the plugin in a terminal. There was something about that in the samples.

I'll also need to look into language extension options. There was stuff in the sample for that.

3:40pm. Focus me. I need to focus on the docs for now. I'll deal with the extension configuration and terminal launching later.

4:30pm. Good thing I haven't just published the thing without testing it. What I had there broke the parser.

```
(opt (indent i (<=) (skip_op ":" >>. root_type_annot)))
```

Let me try this.

5:30pm.

```
// Asserts an expression. If the conditional and the message are literals it raises a type error instead.
inl assert c msg =
    inl raise =
        if lit_is c && lit_is msg then error_type
        else failwith `(())

    if c = false then raise msg
```

Fixed an error in assert. I was not passing the type as the first argument.

```
inl assert c msg =
    if c = false then
        if lit_is c && lit_is msg then error_type msg
        else failwith `(()) msg
```

Let me implement it like this. What I had previously was too complicated.

...Let me actually test this.

6:05pm. Done with lunch. Let me resume. I can keep going for a while longer.

```
Error trace on line: 4, column: 5 in module: c:\Users\Marko\Source\Repos\The Spiral Language\Spiral Compilation Tests\compilation_tests\tutorial1\a.spir.
    assert_less_than_ten 11
    ^
Error trace on line: 3, column: 34 in module: c:\Users\Marko\Source\Repos\The Spiral Language\Spiral Compilation Tests\compilation_tests\tutorial1\a.spir.
    inl assert_less_than_ten x = assert (x < 10) "The argument must be less than 10."
                                 ^
Error trace on line: 128, column: 5 in module: c:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin\core\real_core.spir.
    if c = false then
    ^
Error trace on line: 129, column: 9 in module: c:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin\core\real_core.spir.
        if lit_is c && lit_is msg then error_type msg
        ^
Error trace on line: 129, column: 40 in module: c:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin\core\real_core.spir.
        if lit_is c && lit_is msg then error_type msg
                                       ^
The argument must be less than 10.
```

Yeah, it all works.

6:20pm. Right now I am at 2k lines. I've exceeded my previous high.

But I am starting to feel pinched. I really do need the Spiral compiler to have the capability of showing itself somewhere. Other compilers are fine, but Spiral programs can cause the compiler to stack overflow at compile time. I need to warn the user about that somehow. I need to enable print_static to be used from the plugin. I am going to make that my priority tomorrow.

6:30pm. Hmmm, I need to think a little what comes next. Let me take a break.

I want to cover type inference in the bottom up segment.

6:45pm. I am back. I should be able to do this for another burst. Let me give it a try.

7:25pm. No, let me stop here at 2.08k. I just feel too unmotivated to do this next part right now. I need my sloth after all. Let me publish the patch. Done.

Let me also see if the PL thread is up.

...Nope. I'll post the monthly rev tomorrow then.

7:30pm. Tomorrow I will take a break from the docs to upgrade editor support. I am actually up for doing this since it is related to user experience. This is what I want to do the most right now. The thing I least want do right now is hack on the language itself, such as when doing that typecase fix. I want to put working on the language firmly behind me.

I paid my dues to the PL gods with v0.09, the two prototypes and now v2.

Let me close here.

I am going to put another dent in this tomorrow. 2.1k is not too bad. 30 lines is a single page, so right now I am at 70 pages. I won't make any hard limits on how low the number of lines should be. As long as the language is covered in depth sufficiently enough it will be fine.

Struct of array tensors and serialization I'll do as separate chapters from the two main ones. For reverse serialization, I do not think the language has enough built in ops yet. I'll have to extend it more.

I'll use opportunity January is giving me to trully finish the language. Then come the chips."

---
## [hjhornbeck/bayes_speedrun_cheating@cc0766a375...](https://github.com/hjhornbeck/bayes_speedrun_cheating/commit/cc0766a37555c4850e7fd6bca1d59eb434876f34)
##### 2021-01-01 21:13:01 by hjhornbeck

- Tidied up the dataset a bit, seperating Dream's runs made "before" his
  break from those made "after."
- Added a bit more to the intro section.

NOTES TO FUTURE SELF:
- Make sure to add the big brain and galaxy brain cheaters to the
  analysis, and run the numbers. Your past self thinks that's very cool.
- Also make sure to write the section translating the Negative Binomial
  into the Binomial according to my original analysis, but add a part
where I hint at the correct answer. Honestly, the fact that the
resulting function no longer normalized to one should have been a big
clue that the multiplicative factor shouldn't have been p^c, but instead
1.
- Oh yeah, and the current draft doesn't do much to justify the
  normalization constant in the final metric. Add a graph that "fans"
across all possible weightings, to show it really is the maximal value
the numerator can take.

---

# [<](2020-12-31.md) 2021-01-01 [>](2021-01-02.md)

