# [<](2021-04-01.md) 2021-04-02 [>](2021-04-03.md)

2,612,160 events, 1,357,414 push events, 2,052,064 commit messages, 152,720,084 characters


## [Robort32/ProjectTwoMain](https://github.com/Robort32/ProjectTwoMain)@[5fa454b4ae...](https://github.com/Robort32/ProjectTwoMain/commit/5fa454b4ae96f26f8371c777733e39d8df9d4fb2)
#### Friday 2021-04-02 01:09:41 by Matthew-Whitely

Merge pull request #22 from Robort32/Matt

FUCK YOU SASS

---
## [postgres/postgres](https://github.com/postgres/postgres)@[9eacee2e62...](https://github.com/postgres/postgres/commit/9eacee2e62d89cab7b004f97c206c4fba4f1d745)
#### Friday 2021-04-02 01:11:44 by David Rowley

Add Result Cache executor node (take 2)

Here we add a new executor node type named "Result Cache".  The planner
can include this node type in the plan to have the executor cache the
results from the inner side of parameterized nested loop joins.  This
allows caching of tuples for sets of parameters so that in the event that
the node sees the same parameter values again, it can just return the
cached tuples instead of rescanning the inner side of the join all over
again.  Internally, result cache uses a hash table in order to quickly
find tuples that have been previously cached.

For certain data sets, this can significantly improve the performance of
joins.  The best cases for using this new node type are for join problems
where a large portion of the tuples from the inner side of the join have
no join partner on the outer side of the join.  In such cases, hash join
would have to hash values that are never looked up, thus bloating the hash
table and possibly causing it to multi-batch.  Merge joins would have to
skip over all of the unmatched rows.  If we use a nested loop join with a
result cache, then we only cache tuples that have at least one join
partner on the outer side of the join.  The benefits of using a
parameterized nested loop with a result cache increase when there are
fewer distinct values being looked up and the number of lookups of each
value is large.  Also, hash probes to lookup the cache can be much faster
than the hash probe in a hash join as it's common that the result cache's
hash table is much smaller than the hash join's due to result cache only
caching useful tuples rather than all tuples from the inner side of the
join.  This variation in hash probe performance is more significant when
the hash join's hash table no longer fits into the CPU's L3 cache, but the
result cache's hash table does.  The apparent "random" access of hash
buckets with each hash probe can cause a poor L3 cache hit ratio for large
hash tables.  Smaller hash tables generally perform better.

The hash table used for the cache limits itself to not exceeding work_mem
* hash_mem_multiplier in size.  We maintain a dlist of keys for this cache
and when we're adding new tuples and realize we've exceeded the memory
budget, we evict cache entries starting with the least recently used ones
until we have enough memory to add the new tuples to the cache.

For parameterized nested loop joins, we now consider using one of these
result cache nodes in between the nested loop node and its inner node.  We
determine when this might be useful based on cost, which is primarily
driven off of what the expected cache hit ratio will be.  Estimating the
cache hit ratio relies on having good distinct estimates on the nested
loop's parameters.

For now, the planner will only consider using a result cache for
parameterized nested loop joins.  This works for both normal joins and
also for LATERAL type joins to subqueries.  It is possible to use this new
node for other uses in the future.  For example, to cache results from
correlated subqueries.  However, that's not done here due to some
difficulties obtaining a distinct estimation on the outer plan to
calculate the estimated cache hit ratio.  Currently we plan the inner plan
before planning the outer plan so there is no good way to know if a result
cache would be useful or not since we can't estimate the number of times
the subplan will be called until the outer plan is generated.

The functionality being added here is newly introducing a dependency on
the return value of estimate_num_groups() during the join search.
Previously, during the join search, we only ever needed to perform
selectivity estimations.  With this commit, we need to use
estimate_num_groups() in order to estimate what the hit ratio on the
result cache will be.   In simple terms, if we expect 10 distinct values
and we expect 1000 outer rows, then we'll estimate the hit ratio to be
99%.  Since cache hits are very cheap compared to scanning the underlying
nodes on the inner side of the nested loop join, then this will
significantly reduce the planner's cost for the join.   However, it's
fairly easy to see here that things will go bad when estimate_num_groups()
incorrectly returns a value that's significantly lower than the actual
number of distinct values.  If this happens then that may cause us to make
use of a nested loop join with a result cache instead of some other join
type, such as a merge or hash join.  Our distinct estimations have been
known to be a source of trouble in the past, so the extra reliance on them
here could cause the planner to choose slower plans than it did previous
to having this feature.  Distinct estimations are also fairly hard to
estimate accurately when several tables have been joined already or when a
WHERE clause filters out a set of values that are correlated to the
expressions we're estimating the number of distinct value for.

For now, the costing we perform during query planning for result caches
does put quite a bit of faith in the distinct estimations being accurate.
When these are accurate then we should generally see faster execution
times for plans containing a result cache.  However, in the real world, we
may find that we need to either change the costings to put less trust in
the distinct estimations being accurate or perhaps even disable this
feature by default.  There's always an element of risk when we teach the
query planner to do new tricks that it decides to use that new trick at
the wrong time and causes a regression.  Users may opt to get the old
behavior by turning the feature off using the enable_resultcache GUC.
Currently, this is enabled by default.  It remains to be seen if we'll
maintain that setting for the release.

Additionally, the name "Result Cache" is the best name I could think of
for this new node at the time I started writing the patch.  Nobody seems
to strongly dislike the name. A few people did suggest other names but no
other name seemed to dominate in the brief discussion that there was about
names. Let's allow the beta period to see if the current name pleases
enough people.  If there's some consensus on a better name, then we can
change it before the release.  Please see the 2nd discussion link below
for the discussion on the "Result Cache" name.

Author: David Rowley
Reviewed-by: Andy Fan, Justin Pryzby, Zhihong Yu, Hou Zhijie
Tested-By: Konstantin Knizhnik
Discussion: https://postgr.es/m/CAApHDvrPcQyQdWERGYWx8J%2B2DLUNgXu%2BfOSbQ1UscxrunyXyrQ%40mail.gmail.com
Discussion: https://postgr.es/m/CAApHDvq=yQXr5kqhRviT2RhNKwToaWr9JAN5t+5_PzhuRJ3wvg@mail.gmail.com

---
## [ccodwg/Covid19Canada](https://github.com/ccodwg/Covid19Canada)@[bf05fd2d41...](https://github.com/ccodwg/Covid19Canada/commit/bf05fd2d41b7612cbf611cb23f6b6ee25992e763)
#### Friday 2021-04-02 03:00:35 by Jean-Paul R. Soucy

New data: 2021-04-01. See data notes.

Revise historical data: cases (MB, ON, SK, YT).

Note regarding deaths added in QC today: “9 new deaths, for a total of 10,676 deaths: 1 death in the last 24 hours, 5 deaths between March 25 and March 30, 3 deaths at an unknown date.” We report deaths such that our cumulative regional totals match today’s values. This sometimes results in extra deaths with today’s date when older deaths are removed.

Recent changes:

2021-01-27: Due to the limit on file sizes in GitHub, we implemented some changes to the datasets today, mostly impacting individual-level data (cases and mortality). Changes below:

1) Individual-level data (cases.csv and mortality.csv) have been moved to a new directory in the root directory entitled “individual_level”. These files have been split by calendar year and named as follows: cases_2020.csv, cases_2021.csv, mortality_2020.csv, mortality_2021.csv. The directories “other/cases_extra” and “other/mortality_extra” have been moved into the “individual_level” directory.
2) Redundant datasets have been removed from the root directory. These files include: recovered_cumulative.csv, testing_cumulative.csv, vaccine_administration_cumulative.csv, vaccine_distribution_cumulative.csv, vaccine_completion_cumulative.csv. All of these datasets are currently available as time series in the directory “timeseries_prov”.
3) The file codebook.csv has been moved to the directory “other”.

We appreciate your patience and hope these changes cause minimal disruption. We do not anticipate making any other breaking changes to the datasets in the near future. If you have any further questions, please open an issue on GitHub or reach out to us by email at ccodwg [at] gmail [dot] com. Thank you for using the COVID-19 Canada Open Data Working Group datasets.

- 2021-01-24: The columns "additional_info" and "additional_source" in cases.csv and mortality.csv have been abbreviated similar to "case_source" and "death_source". See note in README.md from 2021-11-27 and 2021-01-08.

Vaccine datasets:

- 2021-01-19: Fully vaccinated data have been added (vaccine_completion_cumulative.csv, timeseries_prov/vaccine_completion_timeseries_prov.csv, timeseries_canada/vaccine_completion_timeseries_canada.csv). Note that this value is not currently reported by all provinces (some provinces have all 0s).
- 2021-01-11: Our Ontario vaccine dataset has changed. Previously, we used two datasets: the MoH Daily Situation Report (https://www.oha.com/news/updates-on-the-novel-coronavirus), which is released weekdays in the evenings, and the “COVID-19 Vaccine Data in Ontario” dataset (https://data.ontario.ca/dataset/covid-19-vaccine-data-in-ontario), which is released every day in the mornings. Because the Daily Situation Report is released later in the day, it has more up-to-date numbers. However, since it is not available on weekends, this leads to an artificial “dip” in numbers on Saturday and “jump” on Monday due to the transition between data sources. We will now exclusively use the daily “COVID-19 Vaccine Data in Ontario” dataset. Although our numbers will be slightly less timely, the daily values will be consistent. We have replaced our historical dataset with “COVID-19 Vaccine Data in Ontario” as far back as they are available.
- 2020-12-17: Vaccination data have been added as time series in timeseries_prov and timeseries_hr.
- 2020-12-15: We have added two vaccine datasets to the repository, vaccine_administration_cumulative.csv and vaccine_distribution_cumulative.csv. These data should be considered preliminary and are subject to change and revision. The format of these new datasets may also change at any time as the data situation evolves.

https://www.quebec.ca/en/health/health-issues/a-z/2019-coronavirus/situation-coronavirus-in-quebec/#c47900

Note about SK data: As of 2020-12-14, we are providing a daily version of the official SK dataset that is compatible with the rest of our dataset in the folder official_datasets/sk. See below for information about our regular updates.

SK transitioned to reporting according to a new, expanded set of health regions on 2020-09-14. Unfortunately, the new health regions do not correspond exactly to the old health regions. Additionally, the provided case time series using the new boundaries do not exist for dates earlier than August 4, making providing a time series using the new boundaries impossible.

For now, we are adding new cases according to the list of new cases given in the “highlights” section of the SK government website (https://dashboard.saskatchewan.ca/health-wellness/covid-19/cases). These new cases are roughly grouped according to the old boundaries. However, health region totals were redistributed when the new boundaries were instituted on 2020-09-14, so while our daily case numbers match the numbers given in this section, our cumulative totals do not. We have reached out to the SK government to determine how this issue can be resolved. We will rectify our SK health region time series as soon it becomes possible to do so.

---
## [aviatesk/Cthulhu.jl](https://github.com/aviatesk/Cthulhu.jl)@[c49f8c3d7f...](https://github.com/aviatesk/Cthulhu.jl/commit/c49f8c3d7f662b781e0a46eb370de9989a33165a)
#### Friday 2021-04-02 03:57:21 by Shuhei Kadowaki

fix #142

So the problem was kinda deeply nested.

The first thing to notice is that there are cases when we can't find a
cache in `CthulhuInterpreter.opt`
AFAIU this is mainly because of limited frames; the native interpreter
doesn't cache them but there are still statement infos about them and
so the problem happens if a Cthulhu user tries to inspect it.

When trying to inspect uncached frames, the next problem is how to
handle them at pre-optimization phase. The `slottypes` of them would be
`nothing` at that point, and so we need to handle that like
[the external `OptimizationState` constructor 
does](https://github.com/JuliaLang/julia/blob/878e1cd52ebf14e8666d8d54a54a95ccee4405d3/base/compiler/optimize.jl#L69-L71).
The another problem is the return types of those frames can be annotated
as `LimitedAccuracy` at that point, and it contains recursive frame 
chain
and thus the printer can fail into infinite loop. The same issue happens
when Cthulhu tries to show `CodeInfo` object, which may also contain 
`LimitedAccuracy`.

Now this commit addresses the issues outlined above:
- check if a callsite is cached or not, and wrap it into 
`UncachedCallInfo` if not,
  and Cthulhu doesn't recurse into the `UncachedCallInfo` (or we can 
even
  forcibly recurse into, but I decided not to do that, since it's really
  different behaviour from what the native interpreter did).
  Note that `UncachedCallInfo` won't happen when working on a 
pre-optimization state,
  since Cthulhu will cache unoptimized frames by hooking into
  `finish(::InferenceState, ::CthulhuInterpreter)` in that case,
  which caches a frame no matter whether it is limited or not.
- when working on a pre-optimization state, if the return type of a 
callsite
  is annotated as `LimitedAccuracy`, we widen it and wrap it into 
`LimitedCallInfo`.
  When printing `LimitedCallInfo`, Cthulhu only indicates the call is 
limited,
  but won't try to print the whole `LimitedAccuracy` object
- when working on pre-optimization state, we make sure we don't print 
`LimitedAccuracy`
  included in `CodeInfo`. Currently I wrote an naive hack to just copy
  the original `CodeInfo` and widen `LimitedAccuracy`s in 
`ssavaluetypes` and `rettype`.

---
## [TTSWarhammer40k/Battleforged-Workshop-Mod-Compilation](https://github.com/TTSWarhammer40k/Battleforged-Workshop-Mod-Compilation)@[e4b69cb9d9...](https://github.com/TTSWarhammer40k/Battleforged-Workshop-Mod-Compilation/commit/e4b69cb9d91e77873d78ccb71234acd011b48b57)
#### Friday 2021-04-02 04:00:34 by Mothman-Zack

4/2//21 Massive Moth AoS Update

AGE OF SIGMAR

MODELS: ORDER

Lumineth Realm-lords
- Vanari Bannerblade
- Scinari Calligrave
- Hurakan Spirit of the Wind
- Sevireth, Lord of the Seventh Wind
- Hurakan Windchargers
- Hurakan Windmage
- Vanari Starshard Ballista
- Scinari Loreseeker
- Lyrior Uthralle, Warden of Ymetrica
- Vanari Bladelords
- Ellania and Ellathor, Eclipsian Warsages
- Vanari Lord Regent
- Myari Lightcaller
- Myari's Purifiers (3x models)
- Shrine Luminor

Seraphon
- Substantial update with new units, improved models and additional weapon option loadouts.

Kharadron Overlords
- Improved Arkanaut Frigate
- Improved Arkanaut Ironclad

Daughters of Khaine
- Updated All Warscrolls
- Improved Morathi-Khaine
- Improved Shadow Queen
- Slaughter Queen
- Slaughter Queen on Cauldron of Blood
- Hag Queen
- Hag Queen on Cauldron of Blood
- Avatar of Khaine
- Bloodwrack Shrine
- 2x Witch Aelf w/ Paired Sacrificial Knives
- Witch Aelf Standard Bearer
- Sister of Slaughter
- Sister of Slaughter Handmaiden
- Sister of Slaughter Banner Bearer
- Doomfire Master of Warlocks
- Doomfire Warlock
- Khinerai Lifetaker Harridynn
- 2x Khinerai Lifetaker
- Khinerai Heartrenders Shryke
- 2x Khinerai Heartrenders
- Melusai Blood Stalker Krone
- 2x Melusai Blood Stalker
- Melusai Blood Sister Gorgai
- Melusai Blood Sister
- Khainite Shadowstalker - Shroud Queen
- Khainite Shadowstalker - Shroudblade w/ Cursed Sword
- Morgwaeth's Blade-coven - Morgwaeth The Bloodied
- Morgwaeth's Blade-coven - Kyrae
- Morgwaeth's Blade-coven - Khamyss
- Morgwaeth's Blade-coven - Kyrssa (- Proxy)
- Morgwaeth's Blade-coven - Lethyr
- Endless Spell - Heart of Fury
- Endless Spell - Bloodwrack Viper
- Endless Spell - Bladewind

MODELS: CHAOS

Maggotkin of Nurgle
- Exalted Greater Daemon of Nurgle
- Great Unclean One w/ Doomsday Bell
- Orghotts Daemonspew
- Morbidex Twiceborn
- Bloab Rotspawned
- Harbinger Of Decay
- Putrid Blightking
- Putrid Blightking w/ Sonorous Tocsin
- Lord Of Plagues
- Gutrot Spume
- Lord of Blights
- Rotbringers Sorcerer
- Festus the Leechlord
- Pusgoyle Blightlord
- Pusgoyle Blightlord w/ Dolorous Tocsin
- Plaguebearer - Plagueridden
- Plaguebearer - Piper
- Plaguebearer

Blades of Khorne
- Bloodthirster of Unfettered Fury
- Bloodreaver Icon Bearer
- Bloodreaver w/ Meatripper Axe
- Skullreaper
- Skullreaper Icon Bearer
- Blood Warrior w/ Goreglaive
- Blood Warrior Chaos Champion
- Skarr Bloodwrath
- Improved Valkia The Bloody
- Wrathmonger
- Exalted Deathbringer w/ Impaling Spear
- Exalted Deathbringer w/ Bloodbite Axe
- Bloodstoker
- Aspiring Deathbringer w/ Bloodaxe + Wrath-hammer

Beasts of Chaos
- Ghorgon
- Cygor
- Dragon Ogor w/ Paired Ancient Weapons
- Doombull
- Doomblast Dirgehorn
- All models updated with proper labels

MODELS: DEATH

Nighthaunt
- Tomb Banshee

Legions of Nagash
- Tomb Banshee
- Coven Throne
- Bloodseeker Palanquin
- Mourngul

---
## [aviatesk/Cthulhu.jl](https://github.com/aviatesk/Cthulhu.jl)@[2c17476a02...](https://github.com/aviatesk/Cthulhu.jl/commit/2c17476a0263545425c7c0b0352968423e2a9e27)
#### Friday 2021-04-02 04:15:43 by Shuhei Kadowaki

fix #142

So the problem was kinda deeply nested than I expected.

The first thing to notice is that there are cases when we can't find a
cache in `CthulhuInterpreter.opt`.
AFAIU this is mainly because of limited frames; the native interpreter
doesn't cache them but there are still statement infos about them and
so the problem happens if a Cthulhu user tries to inspect it.

When trying to inspect uncached frames, the next problem is how to
handle them at pre-optimization phase. The `slottypes` of them would be
`nothing` at that point, and so we need to do something like
[what the external `OptimizationState` constructor does](https://github.com/JuliaLang/julia/blob/878e1cd52ebf14e8666d8d54a54a95ccee4405d3/base/compiler/optimize.jl#L69-L71).
The another problem is the return types of those frames can be annotated
as `LimitedAccuracy` at that point, and it contains recursive frame chain
and thus the printer can fail into infinite loop. The same issue happens
when Cthulhu tries to show `CodeInfo` object, which may also contain
`LimitedAccuracy`.

Now this commit addresses the issues outlined above:
- check if a callsite is cached or not, and wrap it into `UncachedCallInfo` if not,
  and Cthulhu doesn't recurse into the `UncachedCallInfo` (or we can even
  forcibly recurse into, but I decided not to do that, since it's really
  different behaviour from what the native interpreter did).
  Note that `UncachedCallInfo` won't happen when working on a pre-optimization state,
  since Cthulhu will cache unoptimized frames by hooking into
  `finish(::InferenceState, ::CthulhuInterpreter)` in that case,
  which caches a frame no matter whether it is limited or not.
- when working on a pre-optimization state, if the return type of a callsite
  is annotated as `LimitedAccuracy`, we widen it and wrap it into
`LimitedCallInfo`.
  When printing `LimitedCallInfo`, Cthulhu only indicates the call is limited,
  but won't try to print the whole `LimitedAccuracy` object.
- when working on pre-optimization state, we make sure we don't print `LimitedAccuracy`
  included in `CodeInfo`. Currently I wrote an naive hack to just copy
  the original `CodeInfo` and widen `LimitedAccuracy`s in `ssavaluetypes` and `rettype`.

---
## [aviatesk/Cthulhu.jl](https://github.com/aviatesk/Cthulhu.jl)@[deb0ea482a...](https://github.com/aviatesk/Cthulhu.jl/commit/deb0ea482a1b3ec432d81d97c98e81a994fd66b8)
#### Friday 2021-04-02 04:23:48 by Shuhei Kadowaki

fix #142

So the problem was kinda deeply nested than I expected.

The first thing to notice is that there are cases when we can't find a
cache in `CthulhuInterpreter.opt`.
AFAIU this is mainly because of limited frames; the native interpreter
doesn't cache them but there are still statement infos about them and
so the problem happens if a Cthulhu user tries to inspect it.

When trying to inspect uncached frames, the next problem is how to
handle them at pre-optimization phase. The `slottypes` of them would be
`nothing` at that point, and so we need to do something like
[what the external `OptimizationState` constructor 
does](https://github.com/JuliaLang/julia/blob/878e1cd52ebf14e8666d8d54a54a95ccee4405d3/base/compiler/optimize.jl#L69-L71).
The another problem is the return types of those frames can be annotated
as `LimitedAccuracy` at that point, and it contains recursive frame 
chain
and thus the printer can fail into infinite loop. The same issue happens
when Cthulhu tries to show `CodeInfo` object, which may also contain
`LimitedAccuracy`.

Now this commit addresses the issues outlined above:
- check if a callsite is cached or not, and wrap it into 
`UncachedCallInfo` if not,
  and Cthulhu doesn't recurse into the `UncachedCallInfo` (or we can 
even
  forcibly recurse into, but I decided not to do that, since it's really
  different behaviour from what the native interpreter did).
  Note that `UncachedCallInfo` won't happen when working on a 
pre-optimization state,
  since Cthulhu will cache unoptimized frames by hooking into
  `finish(::InferenceState, ::CthulhuInterpreter)` in that case,
  which caches a frame no matter whether it is limited or not.
- when working on a pre-optimization state, if the return type of a 
callsite
  is annotated as `LimitedAccuracy`, we widen it and wrap it into 
`LimitedCallInfo`.
  When printing `LimitedCallInfo`, Cthulhu only indicates the call is 
limited,
  but won't try to print the whole `LimitedAccuracy` object.
- when working on pre-optimization state, we make sure we don't print 
`LimitedAccuracy`
  included in `CodeInfo`. Currently I wrote an naive hack to just copy
  the original `CodeInfo` and widen `LimitedAccuracy`s in 
`ssavaluetypes` and `rettype`.

---
## [p3ol/buddy](https://github.com/p3ol/buddy)@[fb526f2265...](https://github.com/p3ol/buddy/commit/fb526f226575cc05148d249a77185ad0480dd854)
#### Friday 2021-04-02 08:00:49 by Ugo Stephant

docs: fix readme

that's what happens when you copy/paste shit like the lazy ass fuck you are

---
## [LichKing112/lichking112.github.io](https://github.com/LichKing112/lichking112.github.io)@[cbac4d7566...](https://github.com/LichKing112/lichking112.github.io/commit/cbac4d75662108f7729a09edf1ceff9947cc435e)
#### Friday 2021-04-02 09:21:54 by Liz

go fuck youself responsive design and <select> :tired_face:

---
## [mneumann/thin-edge.io](https://github.com/mneumann/thin-edge.io)@[fbf2bcea25...](https://github.com/mneumann/thin-edge.io/commit/fbf2bcea254fc237131545acc42e09b6aa9be4d4)
#### Friday 2021-04-02 10:13:18 by Michael Neumann

WIP - Use privilege separation for `tedge`

- OpenBSD folks will love you!!! :)

- IF security is not the primary objective, read SECTION BELOW.

- https://en.wikipedia.org/wiki/Privilege_separation

- This makes running `sudo tedge` much more secure!

- Get rid of UserManager. This is way too complicated and is not really
  "thread-safe".

- Running `tedge` as root will immediatly lower privileges to
  `tedge:tedge`, right after forking the privleged process `tedge_priv`.

- `tedge_priv` will continue running as `root`.

- Any privleged command will be executed from the very stripped down
  `tedge_priv` binary. Communication is done via a pipe.

- We will only fork when running as `root`. If not running as root, we
  do not need to fork `tedge_priv` and instead use a dummy privileged
  command executor which will always fail with InsufficientPermissions.
  That means that there are zero performance penalties when running
  not as root.

- This is a **much more secure** approach, as the code that runs as `root`
  is very much limited. You only have to audit `tedge_priv` for
  vulnerabilities and the first lines of `tedge` up to the point where
  it irrecoverably gives up root permissions.

- `tedge` can really drop root permissions permanently. There is no way
  to recover `root` permissions. `UserManager` instead only switches the
  effective uid/gid, which is NOT SECURE.

- Much much easier to test, as you can really mock things that need root
  privileges.

- This is just an example implementation to show the general concept.
  There are crates like `privsep-rs` or `sandbox-ipc` that do similar
  things, but are way more complex.

- It's also possible to `fork(2)` the `tedge` executable in the
  beginning. I just went with a separate binary because it was easier to
  implement. Having a separate binary (ideally as a completely separate
  crate with almost no dependencies) has a lot of advantages. Mainly the
  reduced dependencies required to build it which reduces the potential
  for exploits.

-----------------------------------------
IF security is not the primary objective:
-----------------------------------------

- To run a command as a different user, just use:

  "su -m mosquitto -c systemctl  ..."

  See `su(1)` for details.

- To change file permissions, `chown(2)` is your friend.

- Both of them work as you are running as `root`.

- You do not need to switch the effective user permissions of the
  running process! Only change the user permissions of the command
  that you want to execute!

- If you don't like `su(1)`, just write your own `Process::spawn`, that
  will call `setuid(2)` between `fork(2)` and `execl(3)`.

---
## [Galentina/CodeWars](https://github.com/Galentina/CodeWars)@[9bbb638eeb...](https://github.com/Galentina/CodeWars/commit/9bbb638eeb4dbeff4564547ecf7ee1d0c1880279)
#### Friday 2021-04-02 10:29:42 by Galina Malareva

Create  Blood-Alcohol Content

Bob drinks too much, and he gets in trouble for it a lot. He drinks so much, in fact, that he has broken the local law enforcement's breathalizer with his alcoholic breath! Bob feels simply aweful, so he wants to make up for it by creating a function that will calculate his blood-alcohol level for them. Unfortunately, Bob has gotten too inebriated to do any programming today, so he needs your help!
He did manage to research the formula for blood-alcohol content before getting too drunk, which he describes below.
BAC calculator Formula:
BAC% = (A × 5.14 / W × r) - .015 × H 

A: Total alcohol consumed, in ounces (oz)
W: Body weight, in pounds (lbs)
r: The alcohol distribution ratio, 0.73 for man, and 0.66 for women
H: Time passed since drinking, in hours
Source:
http://www.endmemo.com/medical/bac.php
Alcohol consumed will be passed as a drinks object with two properties: ounces (the total volume of beverage consumed in ounces), and abv (the % of alcohol by volume of the beverage as a floating point number--such as 0.05 for 5% abv beer or 0.4 for 40% abv whisky). For simplicity, Bob assures us that he drinks the same kind of beverage each time he drinks.
The gender will be passed as a string, either "male" or "female".
Output must be returned as a number data-type, greater than or equal to 0, with up to 4 decimal places. No error handling will be required.
Using these parameters, create the function that will calculate Bob's and other partier's BAC.

---
## [Benjaminz1000/Benjaminz1000](https://github.com/Benjaminz1000/Benjaminz1000)@[64eb0b0a51...](https://github.com/Benjaminz1000/Benjaminz1000/commit/64eb0b0a51f54cb7bcaf42e0ae6bbf089d77ddb9)
#### Friday 2021-04-02 11:12:54 by Bitcoin

Explicitasset.net

explicitasset.net
explicitasset.net
WELCOME TO EXPLICITASSET.NET


Welcome to explicitasset.net


To grow your bitcoin with us in the best, safe and most convenient way one can demand

explicitasset.net is a young and legit bitcoin management company.⚖️

The company is dedicated in crypto currencies industry, with outstanding technology of bitcoin trading and bitcoin mining. Since the company was born, our role is to offer each person the chance to attend the bitcoin related business and earn profit easily with our powerful sytem. 💯

There are over a 100(hundred) thousand antminers working unbehalf of you tirelessly.🖥️🖥️🖥️

The current members of our team come from different scientific disciplines, but our common faith in crypto currencies has brought us together. Our company has successfully earned huge number of positive reviews and feedback from clients across the world wide.Join us and grow your bitcoin💰💰💳💳


The best investment platform with high experienced in trading 
explicitasset.net
👇👇👇👇👇👇👇
We invite you to the world of digital currencies. with the company explicitasset.net earnings on cryptocurrency trading is available to everyone. 
explicitasset.net
👉🏻👉🏻👉🏻This is our investment plans 
👇👇👇👇👇👇👇👇
🚀1st plan minimum of $30 after 1 day you earn 20% of your investment 

🚀🚀2nd plan minimum of $500 after 1 day you earn 50% of your investment 

🚀🚀3rd plan minimum of $700 after 2days you earn 65% of your investment

🚀🚀🚀4th plan  minimum of $1000 after 3 days you earn 80% of your investment 

🚀🚀🚀5th plan  minimum of $2000 after 3 days you earn 100% of your investment 

🚀🚀🚀🚀6th plan minimum of $5000 after 4 days you earn 150% of your investment

☎☎24/7 LIVE SUPPORT📞📞

🎛🎛NO WITHDRAWAL FEE 📟


♻REINVESTMENT FROM my ACCOUNT BALANCE IS  ALLOWED 

🧤REGISTER WITH THE SITE LINK 
https://explicitasset.net/?a=signup

---
## [cortex-command-community/Cortex-Command-Community-Project-Source](https://github.com/cortex-command-community/Cortex-Command-Community-Project-Source)@[66d8fccbff...](https://github.com/cortex-command-community/Cortex-Command-Community-Project-Source/commit/66d8fccbff9fcc0c24d472207a9c09ee92b7a947)
#### Friday 2021-04-02 12:06:04 by MaximDude

Add option to ignore GUI keyboard events on GUI control manager Update - helps resolve stupid input conflicts when custom keyboard handling is present for GUI elements, instead of figuring out in what goddamn order to update things to avoid said conflicts. Screw you GUI lib

Fix bug where selected list item jumps erratically between what ObjectPickerGUI sets it to and what the GUILib thinks it should be (the audacity smh) when up/down are held for repeated input after a list is clicked and something somewhere enables keyboard events for it (not just focus)

---
## [Clemens85/runningdinner](https://github.com/Clemens85/runningdinner)@[396e17ac3b...](https://github.com/Clemens85/runningdinner/commit/396e17ac3bb86a70df6f40b81ab03bb9150da119)
#### Friday 2021-04-02 12:27:52 by Clemens Stich

- Fuck you material ui.. why do i not get away the !%$§ hover effect?!

---
## [ihavezerohealth/dankgrinder](https://github.com/ihavezerohealth/dankgrinder)@[70b0813c99...](https://github.com/ihavezerohealth/dankgrinder/commit/70b0813c99cd32fe0f75a569a184f73e3c151ab2)
#### Friday 2021-04-02 13:53:21 by ihavezerohealth

Attempted to change CustomCommands

Yeah i prolly fucked it haha. sorry grind95 ;(

---
## [SgtHunk/fulpstation-1](https://github.com/SgtHunk/fulpstation-1)@[248727fe11...](https://github.com/SgtHunk/fulpstation-1/commit/248727fe11559541360a8e7a381bad9b64793bce)
#### Friday 2021-04-02 15:01:44 by SgtHunk

'cause you're the only one who'd understand (The meaning of this!)

Oh My God
I try and I try and I try to make you listen to me
I try to call you every day
I'm rehearsing what to say when the truth comes out (Of my very own mouth)
I've been working on a unified theory
If I make it through tonight everybody's gonna hear me out
'Cause I'm the right one
On my touch-tone, touch-tone telephone
I'm the only one, hey!
On your A.M., A.M. radio
Don't hang up yet, I'm not done
I'm an expert, I'm the one
The one who was right all along
Better to be laughed at than wrong
I'm an expert in my field
UFOlogy, yes, it's all real
Ancient aliens, it's all true
I'm an expert just like you
And like you, I'm a genius before my time
Disbelieving, that's the real crime
Pretty soon they'll discover me in the Super-Sargasso Sea

---
## [JefC-94/messaging-app-server](https://github.com/JefC-94/messaging-app-server)@[a3e9a86b85...](https://github.com/JefC-94/messaging-app-server/commit/a3e9a86b850aaa467f9ebc8b3a2c6a83de936172)
#### Friday 2021-04-02 15:32:24 by Jef Ceuppens

Merge branch 'main' of https://github.com/JefC-94/messaging-app-server into main
This is necessary because fuck you.

---
## [microsoft/terminal](https://github.com/microsoft/terminal)@[fb597ed304...](https://github.com/microsoft/terminal/commit/fb597ed304ec6eef245405c9652e9b8a029b821f)
#### Friday 2021-04-02 16:00:09 by Mike Griese

Add support for renaming windows (#9662)

## Summary of the Pull Request

This PR adds support for renaming windows.

![window-renaming-000](https://user-images.githubusercontent.com/18356694/113034344-9a30be00-9157-11eb-9443-975f3c294f56.gif)
![window-renaming-001](https://user-images.githubusercontent.com/18356694/113034452-b5033280-9157-11eb-9e35-e5ac80fef0bc.gif)


It does so through two new actions:
* `renameWindow` takes a `name` parameter, and attempts to set the window's name
  to the provided name. This is useful if you always want to hit <kbd>F3</kbd>
  and rename a window to "foo" (READ: probably not that useful)
* `openWindowRenamer` is more interesting: it opens a `TeachingTip` with a
  `TextBox`. When the user hits Ok, it'll request a rename for the provided
  value. This lets the user pick a new name for the window at runtime.

In both cases, if there's already a window with that name, then the monarch will
reject the rename, and pop a `Toast` in the window informing the user that the
rename failed. Nifty!

## References
* Builds on the toasts from #9523
* #5000 - process model megathread

## PR Checklist
* [x] Closes https://github.com/microsoft/terminal/projects/5#card-50771747
* [x] I work here
* [x] Tests addded (and pass with the help of #9660)
* [ ] Requires documentation to be updated

## Detailed Description of the Pull Request / Additional comments

I'm sending this PR while finishing up the tests. I figured I'll have time to sneak them in before I get the necessary reviews.

> PAIN: We can't immediately focus the textbox in the TeachingTip. It's
> not technically focusable until it is opened. However, it doesn't
> provide an even tto tell us when it is opened. That's tracked in
> microsoft/microsoft-ui-xaml#1607. So for now, the user _needs_ to
> click on the text box manually.
> We're also not using a ContentDialog for this, because in Xaml
> Islands a text box in a ContentDialog won't recieve _any_ keypresses.
> Fun!

## Validation Steps Performed

I've been playing with 

```json
        { "keys": "f1", "command": "identifyWindow" },
        { "keys": "f2", "command": "identifyWindows" },
        { "keys": "f3", "command": "openWindowRenamer" },
        { "keys": "f4", "command": { "action": "renameWindow", "name": "foo" } },
        { "keys": "f5", "command": { "action": "renameWindow", "name": "bar" } },
```

and they seem to work as expected

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[7f60a9e655...](https://github.com/mrakgr/The-Spiral-Language/commit/7f60a9e655ce7324031970b274bf0abda0fa10cd)
#### Friday 2021-04-02 16:44:52 by Marko Grdinić

"11:10am. Let me chill a bit and then I will start. I got up too late again today.

11:45am. Let me start for the day. What I want to do right now, before anything else, is study the Deep CFR and the DREAM papers.

It is time to get this shit going. Before I start work on dictionaries or PG or whatever else, it is time to famiarize myself with deep learning version of CFR.

Let me close that issue from yesterday. No way do I feel like checking out the master of Kivy. Why is the author goading me into wasting my time?

...Let me start with the DREAM paper.

Though I do nto understand CFR, what I do not understand will not prevent me from using it effectively.

...For fuck's sake. Let me reset the router.

I should have the paper on my hard drive so let me go for that in the meantime.

12:05pm. 4/14. The paper has been rather clear so far. Let me get the variance reduction parts.

12:10pm. The equations in VR OC part is something I do not understand. What is that p(h',T...) thing. Over what is the Sum h' summing over, don't tell me all possible histories.

> Single Deep CFR (SD-CFR) is a modification of Deep CFR that instead stores all value networks from each CFR iteration to disk and mimics the average policy exactly during play by sampling one of them and using its policy for the entire game. This is mathematically equivalent to sampling actions from the average policy. SD-CFR eliminates the approximation error in Deep CFR resulting from training a network to predict the average policy, at the minor cost of using extra disk space to store the models from each CFR iteration (or a random sample of CFR iterations).

This is interesting.

6/14. These equations are actually easy to understand.

12:35pm. 6/14. I get this stuff almost completely.

12:40pm. Hmmm, no, actually I don't how do it make sense to divide v - Q by the policy probability.

`Q - (v - Q) / e`. And I know that e is a probability.

Is it trying to emulate uniform exploration's values while sampling from the policy? That has to be it.

12:50pm. Ah, right. Eq 7 is the policy update. The multiplication by the opponent's probability is implicit, but it has to correct for its own probability. I've been imagining how I would do this, and it is eactly as in the paper.

If the paper is innovative in something, that would be that it propagated v back...ah, no wait. The values gets corrected in eq 8. So the range of the value will be properly scoped. It makes sense.

12:55pm. Damn it, if only I could understand why ommiting out the self probability during updates makes sense, I could understand CFR.

3/14. I've been hung up on trying to understand the average policy update. Why does it multiply only by self, but not by the opponent probability?

But according to eq 2, the average policy is literally just the average policy. I can consider the thing in the actual algorithm a hack.

Maybe tabular CFR that I implemented would make more sense if I made it a separate pass.

Wait wait...

Maybe I am thinking about this wrong. If the goal is to get the average policy...

Consider the policy update. The oppontent probability is implicit. And the self probability is reversed.

If the goal is to get the average policy, then the reason why it is not necessary to multiply by the opponent probability in the average policy update, because that is already the case.

The policy update has already been multed by the opponent probability! Doing that in the average as well would just double count it.

And the reason why the average policy update multiplies by own probability is to reverse what has been done in the policy update itself!

I see.

2:20pm. Done with lunch.

https://www.reddit.com/r/reinforcementlearning/comments/miac4q/back_to_square_one_superhuman_performance_in/

This made me lol, but no way do I feel like reading all of it.

2:40pm. Let me resume. Focus me. I've gotten a lot from the paper. I can visualize in its entirety. I won't have trouble implementing the sampling CFR when the time comes.

2:45pm. Let me finish the paper. I'll move to the deep CFR one after that.

I really gained a lot from reading the DREAM paper. I finally understand the average policy updates. And I understand the policy update as well.

3pm. Hmmm, it is expecting me to train D. I thought that values can be calculated just from Qs. This is confusing. I am not sure what D is. I really should try all of this in the tabular case first.

> Moreover, DREAM’s Q network requires the input of both player’s private cards and thus has a slightly adjusted input layer

This was in the appendix as the very last sentence. What the hell?

Why cheat like this?

3:20pm. Nevermind this. Let me take a look at the other CFR papers.

Let me go for the Deep CFR paper. Then I'll go for the tabular VR-MCFR paper.

3:30pm. My will to read it is low. it seems a lot of my energy went into that first one, and now I am exhausted mentally. Let me just skim it.

> Model-free policy gradient algorithms have been shown to minimize regret when parameters are tuned appropriately [26] but the performance of these algorithms is comparable to NFSP.

Hmmm...

3:40pm. Focus me. Focus on the paper. Slacking can come later.

4:05pm. Had to take a break after all. Let me resume.

> The neural network model begins with separate branches for the cards and bets, with three and two layers respectively. Features from the two branches are combined and three additional fully connected layers are applied. Each fully-connected layer consists of xi+1 = ReLU(Ax[+x]). The optional skip connection [+x] is applied only on layers that have equal input and output dimension. Normalization (to zero mean and unit variance) is applied to the last-layer features of each position. The network architecture was not highly tuned, but normalization and skip connections were used because they were found to be important to encourage fast convergence when running preliminary experiments on pre-computed equilibrium strategies in FHP.

This is what I was planning to do, so it is nice to see confirmation that it helps.

4:15pm. Let me go for the tabular MCCFR paper. After that I should decide what I want to do. I am leaning towards doing whatever is necessary to get tabular CFR to work on these toy games. After that I will plug in NNs.

4:30pm. No wait, what I thought was the division by the current probability can't possibly have been the case in the previous paper.

Ah, in eq 4 this is the division by the current policy. But if we are doing sampling where does the divison by the current player probability come in?

4:35pm. No nevermind. I do not get it once again.

4:45pm. I get it again.

The value propagation makes perfect sense. So does the strategy averaging, if you ignore that CFR itself does not exactly do it. The average strategy update rule is more of a hack than anything else. I can't reduce the rule to doing explicit averaging.

In order to get that, I'd have to modify CFR so it buffers the policy updates until the end of the traversal (similarly to how gradients are accumulated) and then apply them all at once. The update the average strategy without that self prob multiplication hack.

Anyway, this will be good as it will prevent the strategies from oscilating so much.

No, my reasoning about average strategy being multied by the strategy path probability is bogus.

4:55pm. But damn it, I understand exactly how the average strategy update rule came about.

Imagine you are doing regular CFR on Rock Paper Scissors.

If you are player 1, that is one update for the average strategy.

If you are player 2, that comes down to three updates for the average strategy.

That causes an imbalance. Then to resolve, you can add the rule to multiply the average strategy update by the path prob of ... no wait.

The self update in that case would be 1 in both cases.

But suppose you have a chance node instead of player two. Then it would make sense.

5:05pm. Yeah, if the goal is strategy averaging, the current CFR formulation is bogus. It is missing a regret accumulation step. It is not at all compatible with sampling old strategies.

5:40pm. There is also one more troublesome consideration.

I am wracking my brain over it right now.

I know the strategy/policy update is `prob_op * (v_cur - v_all)`. But is it the right choice to simply leave up the `prob_self`. Might it not be better to importance sample it.

In the tabular case (and depending on the game) it might now matter since all the states are sparse, but in the dense base whether I simply ommit the prob_self or importance sample the uniform distribution will affect the shape of the replay buffer.

But ways can't possibly be right!

The average strategy rule is definitely wrong, but could this rule be wrong as well.

5:55pm. I am wracking my brains over this, but there is no point in thinking about it.

I should just test it out and confirm my suspicions with my own hands.

6pm. There is also another point of suspicion. I've been folding the chance node probabilities into both of the players. This would work when it is just for updating the strategy regularly, but in a sampling scheme, if I divide by the self probability to correct the sampling bias, it won't match the update. It will correct for both players chance node path probs.

I should redesign the games so that chance node probs are separate from action probs.

This is something the papers did not talk about it at all, but it is important.

6:05pm. No. The uniform distribution is correct.

Imagine I was sampling using self uniform probs and op policy probs.

In that case would I adjust the buffer so that self probs are all one?

Hmmmm...I am imagining adding some fake player nodes.

6:10pm. Surprisngly, the CFR rules are right. Adding fake 0.5 change nodes that lead to terminal states with 0 rewards would wreck things with uniform sampling. But with hard sampling like CFR does, the results would be proper.

6:15pm. I need to think about the shape of the data in the replay buffer, both for the policy and the value nets carefully.

I think I gained quite a bit from today's session - I am definitely going to separate chance probabilities from action probabilities. Otherwise the replay buffer will be broken by the time I try NN training.

6:20pm. Hard sampling that CFR does is quite profound now that I think about it. At first it does not make sense to just switch the target distribution to whatever has been picked - it seems like it would make more sense to just pick a distribution and stick to that, but that approach would be too inefficient. If the target distribution is 1 at one category and 0 everywhere else, then the program would just be burning clock cycles.

But looking at it most broadly, if one iterates through all the combinations of hard distributions and filters out the 0 probability nodes, one would get exactly the same result as if one did importance sampling with switching to the target hard distribution.

This is the right approach. The CFR paper is right in this.

6:30pm. I did quite a bit of thinking today, enough to get a headache. The question right now that I should be asking myself is - what is next?

Hrmmmm...I am strongly leaning towards implementing the tabular version of various CFR algorithms.

Unless I do this, I won't have a baseline to compare the neural ones against.

The problem with neural algos is that if I implement them, they will work. But I won't be able to know whether they are truly strong or weak.

Not unless I want to spent my own time playing against the agents.

Leduc poker is one thing, but I should do what the paper does and implement Flop Holdem. That will still be viable as a game, but it has a much larger number of states than Leduc does.

That game should be the ultimate test for tabular algorithms. It is what I should be pitting the NN agents against.

After I do that, I'll be ready for HU NL Holdem with large stacks.

6:40pm. I'll stop here for the day. Time for lunch.

Tomorrow, I'll get the drudge work out of the way and implement that dictionary.

TODO: Separate the action from chance probabilities.

Let me also not forget about this."

---
## [Geadalu/NoName](https://github.com/Geadalu/NoName)@[79437d4172...](https://github.com/Geadalu/NoName/commit/79437d417216b311d153183ee3349de2c7e2fb26)
#### Friday 2021-04-02 18:00:11 by Geadalu

Real shit loading Alumnos.
- Class Alumno filled with DB connections to READ.
- Added ControladorAlumnos for the multilayer architecture. Now, MainWindow calls ControladorAlumnos.
- Added NuevaTareaWindow, which at the moment is empty.
- Changed all code in MainWindow -> btnCargarTabla to fit new multilayer changes.
- Honestly I think I've changed something else but I can't remember.

TODO next time: fill Alumno with DB connections to SAVE aaaand fill new DB's to get example data for the main table.

---
## [maborak/iemaddon-installer](https://github.com/maborak/iemaddon-installer)@[4258348167...](https://github.com/maborak/iemaddon-installer/commit/42583481675d69e43450a949f02674274039a2c7)
#### Friday 2021-04-02 21:48:14 by Wilmer Adalid (Alienware)

Updates for: ... eighty years later he could still recall with the young pang of his
original joy his falling in love with Ada.
		-- Nabokov

---
## [LunarWatcher/dotfiles](https://github.com/LunarWatcher/dotfiles)@[0a4c4722e5...](https://github.com/LunarWatcher/dotfiles/commit/0a4c4722e54c076974f5ebe4db4080e09c41d8ed)
#### Friday 2021-04-02 22:25:39 by Olivia

Vimrc refactor

FZF QOL update; removing preview, updating window in
general to better fit my style. reverse layout, remove sink on the
custom file lists so I can use custom actions.
Also adjusts the window so the border is pink (becase PINK <3), and make
the border rounded instead of pure ascii (ew)
And the window is some different dimensions now.

Terminal update: adding necessary mappings

Airline: remove redundant or annoying plugins
The coc.nvim plugin is great in theory, but the implementation is SO GOD
DAMN ANNOYING! It updates whenever the cursor moves with some new,
redundant state. I just wanna see that it's running, not what it's doing
at any given time.
I can monitor the current status with a coc.nvim command anyway, so it's
not necessary for normal use.

Polyglot: don't load sensible (cuts down load time by 126ms)

Clear out remnants of nvim

Add plugin for showing colors

Add matchit for extended % support

Remove comments containing unused plugins

Add AsyncTask fzf integration

---
## [imperialbin/imperial](https://github.com/imperialbin/imperial)@[99cf59ce5f...](https://github.com/imperialbin/imperial/commit/99cf59ce5f0538f0632cfe3941921cb1c48d5af2)
#### Friday 2021-04-02 23:58:19 by devlooskie

wegrbthwbeghtrbwegrq  egwr3qhu908 -erqw gfuhopwegr hujofuck you

---

# [<](2021-04-01.md) 2021-04-02 [>](2021-04-03.md)

