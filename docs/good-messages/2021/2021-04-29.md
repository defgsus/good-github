# [<](2021-04-28.md) 2021-04-29 [>](2021-04-30.md)

3,311,857 events, 1,519,609 push events, 2,474,426 commit messages, 194,654,787 characters


## [LeoColomb/perfectmotherfuckingwebsite](https://github.com/LeoColomb/perfectmotherfuckingwebsite)@[bd4598b456...](https://github.com/LeoColomb/perfectmotherfuckingwebsite/commit/bd4598b456b9c100db8ec762b5883176813c6da3)
#### Thursday 2021-04-29 00:51:13 by Ryan Crosby

Fix fucking darkmode (#6)

* Make darkmode background black so users don't die in low-contrast grey shittiness

OLED users thank me later

* Burn the eyeballs less with a nice light-grey text colour

Co-authored-by: Léo Colombaro <git@colombaro.fr>

---
## [XOSP-Reborn/build](https://github.com/XOSP-Reborn/build)@[46140f0e87...](https://github.com/XOSP-Reborn/build/commit/46140f0e875713a732f201559514299ae4fe8f19)
#### Thursday 2021-04-29 01:43:01 by JamieHoSzeYui

core: Makefile: shut the fuck up

* silence wedge, I just wanna copy a prebuilt apk

Silences annoying shit like
build/make/core/Makefile:49: warning: Prebuilt apk found in PRODUCT_COPY_FILES: vendor/xperia/proprietary/system/priv-app/Contacts/Contacts.apk:system/priv-app/Contacts/Contacts.apk, use BUILD_PREBUILT instead!

---
## [Astronomy487/astronomy487.github.io](https://github.com/Astronomy487/astronomy487.github.io)@[6834caec13...](https://github.com/Astronomy487/astronomy487.github.io/commit/6834caec133ed833b0e74376ea481abe0e8a656b)
#### Thursday 2021-04-29 02:19:02 by Astronomy487

big update of how my fractal thing renders stuff

i love thinking abt fractals they are so cool

---
## [flopingbird/ya-mom-bot](https://github.com/flopingbird/ya-mom-bot)@[0e02ccdcd5...](https://github.com/flopingbird/ya-mom-bot/commit/0e02ccdcd5df07674f4e254afa3ad0ab26569acb)
#### Thursday 2021-04-29 02:54:41 by flopingbird

WHY DOES THIS SHIT KEEP CRASHINg

GOD HELP ME WHAT THE FUCK

---
## [GavinLeavitt/UVU-Surgery-Project](https://github.com/GavinLeavitt/UVU-Surgery-Project)@[f94d05c843...](https://github.com/GavinLeavitt/UVU-Surgery-Project/commit/f94d05c843aa1869b78ffeeb0a9c0a562ac1a3fa)
#### Thursday 2021-04-29 04:22:52 by Brandon

the one with all the changes

In this episode of Friends, Joey get in trouble when he buys himself a cordless drill and tries to perform surgery on Chandelier. Meanwhile, Phoebe is trying to break up with her boyfriend who thinks ACLs don't actually exist.

---
## [halostatue/kx](https://github.com/halostatue/kx)@[1f50703365...](https://github.com/halostatue/kx/commit/1f50703365f60b7398b962c976690c3d33b427f3)
#### Thursday 2021-04-29 04:43:36 by Austin Ziegler

Fork as kx 1.0.0

- Forked from [taylor/kiex]. See the rationale below.
- Formatted with `shfmt` and checked for issues with `shellcheck`.
- Removed the _generic_ startup script in favour of being explicit about
  loading the fish, bash, or csh scripts directly.
- Only Elixir 1.0 or later can be installed with kx.
- The Elixir version is associated associated with the exact Erlang/OTP
  release used at build time.

Why Fork kiex?
==============

I thought it was the _better_ option. The journey to that point is
convoluted.

I switched to the [Fish shell] a couple of years ago and I created
[halostatue/fish-kiex] to automatically load kiex rather than having to
remember to add it to my Fish configuration. I was recently (April 2021)
working through some improvements to it to add command-line completion
and some other quality of life improvements. In looking at that,
I started looking deeper into the implementation of kiex to see what
I could use to make those happen.

This led to the start of some consideration on what would be changed if
I ran [shfmt] (a lot) and what would be required to change if I ran
[shellcheck] (a lot more). So I started working on what would become
a PR or a series of PRs on [kiex]…and it kept growing. And growing.

Breaking the changes down would have resulted in _many_ PRs or a single
all-in-one PR replacing 75% or more of the original implementation…and
it wouldn't have helped with any of the original problem I had…making it
easier to use kiex to improve `halostatue/fish-kiex`.

So I started reworking the code. I applied my idiomatic style of bash
scripting. I applied shfmt and shellcheck ruthlessly. I changed
dependencies. I dropped all support for Elixir versions less than 1.0.
I dropped commands that I've never used and changed how a number of the
commands work. I changed the help messages and added a _lot_ more help
messages. I made it so that kx has knowledge of the underlying
Erlang/OTP version that was used to install each Elixir‡. I added
support for other repositories and for release tarballs. I added
caching.

> ‡ I have run into at least two instances where I tried to run an
> Elixir built with Erlang 22 on Erlang 21.

I've kept the ease of kiex. I've kept the principle of being
self-contained.

kx (1,593 lines) is larger than kiex (1,124 lines) and has _different_
functionality. `git diff --stat` tells me that there's 1,383 insertions
and 914 deletions. I can't consider myself a good citizen by dropping
a diff like that on a project that last saw a merged patch 13 months
ago. So I've forked kiex as kx.

I'm perfectly willing to contribute kx back to kiex; without kiex, kx
would not exist. I don't know that it's necessary for there to be two
"main" tools that do almost the same as each other. I do think that the
improvements that I've made along the way are worthwhile and I'll keep
maintaining kx as long as I keep using it.

[fish shell]: https://fishshell.com 'friendly interactive shell'
[halostatue/fish-kiex]: https://github.com/halostatue/fish-kiex
[shfmt]: https://github.com/mvdan/sh/tree/master/cmd/shfmt
[shellcheck]: https://github.com/koalaman/shellcheck
[kiex]: https://github.com/taylor/kiex
[taylor/kiex]: https://github.com/taylor/kiex

---
## [spences10/strapi-starter-gatsby-blog-v2](https://github.com/spences10/strapi-starter-gatsby-blog-v2)@[75be4d39eb...](https://github.com/spences10/strapi-starter-gatsby-blog-v2/commit/75be4d39eb7231e34a7b4b4a261339d30712512a)
#### Thursday 2021-04-29 05:36:12 by ImgBotApp

[ImgBot] Optimize images

*Total -- 3,571.62kb -> 3,433.14kb (3.88%)

/backend/data/uploads/favicon.png -- 5.14kb -> 3.44kb (33.17%)
/backend/data/uploads/default-image.png -- 356.09kb -> 292.72kb (17.8%)
/backend/data/uploads/daviddoe@strapi.io.jpg -- 841.68kb -> 804.20kb (4.45%)
/backend/data/uploads/beautiful-picture.jpg -- 571.40kb -> 559.72kb (2.04%)
/backend/data/uploads/this-shrimp-is-awesome.jpg -- 93.32kb -> 91.79kb (1.63%)
/backend/data/uploads/sarahbaker@strapi.io.jpg -- 681.91kb -> 672.52kb (1.38%)
/backend/data/uploads/we-love-pizza.jpg -- 932.63kb -> 919.90kb (1.37%)
/backend/data/uploads/the-internet-s-own-boy.jpg -- 89.45kb -> 88.84kb (0.68%)

Signed-off-by: ImgBotApp <ImgBotHelp@gmail.com>

---
## [Amirbapiri/peace](https://github.com/Amirbapiri/peace)@[69f3cb3afd...](https://github.com/Amirbapiri/peace/commit/69f3cb3afdf189805e19e2468084445506c7e01c)
#### Thursday 2021-04-29 08:29:33 by amir

feat: Probably the last commit to this project

I learned a lot from this project which I don't remember any of them.
I don't feel good at this stage of my bloody life.
I will do my best to make this commit a funny diary in the future.
Bye!

---
## [WatIsDeze/Nail-Crescent](https://github.com/WatIsDeze/Nail-Crescent)@[224f61eee3...](https://github.com/WatIsDeze/Nail-Crescent/commit/224f61eee369e4bbdc03c04d17fd5d14fe020c7b)
#### Thursday 2021-04-29 11:59:48 by WatIsDeze

Fixed that ugly bug, I seriously hate touching msg.cpp..... FUCK YOU MSG.

---
## [Dalekamistoso/parches-ips-msx](https://github.com/Dalekamistoso/parches-ips-msx)@[6c8dafd638...](https://github.com/Dalekamistoso/parches-ips-msx/commit/6c8dafd638b3e7e82b9ffe11f3b92d4cf12e58d2)
#### Thursday 2021-04-29 14:03:14 by Dalekamistoso (DrWh0)

Fixes not booting on Turbo R / 2+

Fixes Disk Mystery #4 -The man I love (Thinking Rabbit) (1988)  (MSX2+ needs to boot with "1" pressed at boot)

---
## [xamarin/xamarin-android](https://github.com/xamarin/xamarin-android)@[0e74266820...](https://github.com/xamarin/xamarin-android/commit/0e74266820ad5dd11a0ffe9b8e999e5572205309)
#### Thursday 2021-04-29 15:52:16 by Jonathan Peppers

[Xamarin.Android.Build.Tasks] Build should update designtime/Resource.designer.cs (#5884)

When testing .NET 6 in the latest Internal Preview builds of Visual
Studio, we are now in a place where the Android designer is working
and rendering layouts.

Unfortunately if you add a new `android:id="@+id/foo"`, Intellisense
does not update to show that `Resource.Id.foo` is available.

After enabling build logging with `Project System Tools`:

https://marketplace.visualstudio.com/items?itemName=VisualStudioProductTeam.ProjectSystemTools

I found the following is happening:

* I hit Ctrl+S to save the `.xml` file
* `UpdateGeneratedFiles` runs, which updates
  `obj/Debug/*/designtime/Resource.designer.cs`. This runs in a
  design-time context where we use the `ManagedResourceParser`, it
  uses an `R.txt` if found on disk. In this case, it is an old `R.txt`
  from a previous build.
* `UpdateAndroidResources` runs, which updates `R.txt` and
  `obj/Debug/*/Resource.designer.cs`. It runs in a context where we
  run `aapt2` instead of the `ManagedResourceParser`.

Unfortunately, at this point the `designtime/Resource.designer.cs` is
out of date and does not have the new `android:id` available.

This problem seems to happen in "legacy" Xamarin.Android as well.
However, doing a regular build appears to update Intellisense. I think
developers are currently doing a regular build here, which is not
ideal? While in .NET 6, doing a `Build` does *not* update the
Intellisense. The only way I could get .NET 6 to update was to force a
design-time build by changing a random setting in `Project Options`.

To solve the issue, we can make both `UpdateAndroidResources` and
regular `Build` update the `designtime/Resource.designer.cs` file.
These files should stay in sync if we have the proper values from
`aapt2` anyway. I think this will solve the problem in .NET 6 as well
as improve the experience in "legacy" Xamarin.Android.

Down the road, I think we should rethink how design-time builds work,
in general here.

I updated some tests around this scenario to be sure the
`designtime/Resource.designer.cs` get updated with the same contents.

---
## [emilk/egui](https://github.com/emilk/egui)@[02a62d1986...](https://github.com/emilk/egui/commit/02a62d198622eeb179f70e96ef67f421ff6e2286)
#### Thursday 2021-04-29 17:49:51 by David Pedersen

Replace `impl Into<String>` with `impl ToString` (#302)

* Replace `impl Into<String>` with `impl ToString`

This is something I ran into today. Types that implement
`std::fmt::Display` cannot be passed to functions that take `impl
Into<String>`. You have to call `display_thing.to_string()`. Its a small
thing but would be fixed by instead taking `impl ToString`.

Afaik `impl ToString` is a superset of `impl Into<String>`, unless users
manually implement `Into<String> for T` (or `From<T> for String`) for
their own types. However I think its more common to implement `Display`
as that works with `println` and friends. The main difference is that
`Display::fmt` can return errors but thats also quite rare in my
experience.

I did some testing in a [playground] and seems to work.

[playground]: https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=1111e071f6ae416ae2688d58d2e9b575

* Silence warnings

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[1bf6457d63...](https://github.com/mrakgr/The-Spiral-Language/commit/1bf6457d63c75fefadcae734f20df6476126e289)
#### Thursday 2021-04-29 17:58:25 by Marko Grdinić

"9:50am. Uagh, I am suffering again. AdaBelief is out. I am 80-90% sure that I am on the wrong track with it. The paper sounds nice, but I need to focus on the reality what I am optimizing, not some abstract thing.

And the reality is that sparsity is killer. The longer training proceeds and the deeper and the more parameters there are, the sparser the gradients for any particular parameter.

Momentum and keeping an exponential moving average of variance (or centered version) sounds good, but what if those terms just end up tracking the sparsity factors of the entire system?

There are good reason to be concerned about this!

After all after I modulate the inputs and the gradients, they will become very sparse, the bigger the layer, the sparser they will get! Since they will be very sparse, the mean will be zero, AdaBelief will degenerate into Adam. And the sparser the are, the stronger the counterbalancing factor.

It does me no good for one parameter to get a gradient of 1, only for the counterbalancing factor to push it up by a factor 10,000.

There is reason to be concerned about this! I mean, going from the video, to the first paper, to the latest one, the epsilon parameter - the one which controls the uncertainty in the parameter estimate is what keeps changing places!

10:15am. Maybe I really am better than I was in the past. If this was 2015 or 2016, I'd look at the epsilon parameter, see that it is 10^-8 and say pfft, it does not matter. It is too small to affect anything. But right now my attention is completely dominated by it.

These sparsity concerns are not even the slightest bit of a problem on toy examples where you always have gradients everywhere.

I think I now know the reason why for example the OpenAI agents needed a batch size of like a million.

Yes, indeed, if you use a large enough batch size to lower the degree of sparsity, then Adam, AdaBelief would start working again.

10:25am. I bought into the AdaBelief paper, but in the Crowded Valley one, it did not particularly stand out. That means that the concerns in the toy example are just that.

10:45am. I need to think about this.

12:45pm. I've been sitting here as waves and waves of inspiration roll over me.

I can see so much more than I used to.

I've realized something huge.

Consider the way Adam evaluates the variance parameter.

It just uses a exponential moving average of squared gradients.

How many samples would it need to get a good estimate of that?

Consider a linear net without bias whose matrix is of size 1x1.

In other words during the backprop phase the update rule will just be input * gradient.

Now consider what would happen if the input distribution is 1 with 1/100 probability and 0 with 99/100 probability everywhere else uniformly. Assume that the gradient also has the same distribution.

This means that the Adam style variance estimator will need squared the number of samples, meaning 100 * 100 to estimate the variance correctly.

On the other hand estimating the input and the gradient variance independently would need only 100 samples each.

I am fucking shocked, shocked that I did not realize this before!

When I estimate them separately the uncertainty is additive, but when I estimate them together the uncertainty is multiplicative. It is just like multiplying two gaussians, the standard deviations get multiplied as well.

1:05pm. Let me have breakfast here. I have other things I want to talk about.

2:40pm. I am in thought.

https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance

Ahhh...so that is what that damn bias correction was.

3:40pm. I've figured out a bunch of stuff. Let me do the chores.

5:20pm. This just won't end. I never spend my days deep in thought in such intensity for such prolonged periods. This last week has to be a new record.

I am trying to figure some things out, but I can't figure out how to leave the AdaBelief update in.

Replacing var with centered variance has appeal, but it would cost me the reward scaling property. Still, there is a variant that could potentially work.

I agree that centered variance is a better measure of uncertainty that regular variance. I could use the regular variance to maintain gradient scaling and the centered variance to reweight the gradients intra layer.

Two different algorithms are fighting it out in my mind.

In both cases for the inputs, only use the fast variance to rescale the input magnitudes.

For the outputs, use the fast variance (which is a scalar) to maintain the gradient magnitudes.

This is under the theory that estimating the general scale can be done much more easily that the individual variables.

For the individual variables, maintain slow moving averages of window length that is 3 times the size fanout for both the means and the variances.

Then use the relative variances to rescale the weights.

5:35pm. Ugh, my mind is intense but the words are coming out with difficulty. Let me code it up just to demonstrate.

5:55pm.

```fs
let alg1 means vars =
    let len = float (List.length vars)
    let stds = List.map2 (fun mean var -> var - mean * mean |> sqrt) means vars
    let average_std = List.sum stds / len
    let eps = 2.0 ** -7.0
    List.map (fun x ->
        if average_std = 0.0 then 0.0 else
        average_std / (eps * average_std + (1.0 - eps) * x)
        ) stds

let r = alg1 [1.0;2.0;3.0] [1.0; 12.0; 15.0]
```
```
val r : float list = [128.0; 0.6238507359; 0.7198180488]
```

Here is what I had in mind. What this does is make those entries that have low variance, in other words, high certainty relative to the rest of the bunch learn faster. Algorithm 2, would be the same as above, but it would ignore the means when doing the reweighting.

```fs
let alg2 vars =
    let len = float (List.length vars)
    let stds = List.map sqrt vars
    let average_std = List.sum stds / len
    let eps = 2.0 ** -7.0
    List.map (fun x ->
        if average_std = 0.0 then 0.0 else
        average_std / (eps * average_std + (1.0 - eps) * x)
        ) stds
```

This algorithm has interesting property when combined with the reward scaling modulator.

```fs
    let stds = List.map sqrt vars
    let average_std = List.sum stds / len
```

When there is enough data for example, when doing full batch learning, this quantity would be the same as the modulator and would cancel out.

```fs
    List.map (fun x ->
        if average_std = 0.0 then 0.0 else
        average_std / (eps * average_std + (1.0 - eps) * x)
        ) stds
```

Then this part would be equivalent to...

```fs
    List.map (fun x ->
        if average_std = 0.0 then 0.0 else
        1.0 / (eps * average_std + (1.0 - eps) * x)
        ) stds
```

In other words, it would be the same thing as taking a moving average of just the particular random variable in the first place and using that.

But it is not the same thing.

Different things are easier and harder to estimate depending on where you do it. Just like I shown earlier...

If you have two random variables X and Y, once you multiply them, it becomes much harder to get a proper estimate. And this is what Adam does instead of the more sensible thing.

If you have a list of a single one and 99 zeroes, and you pairwise multiply every element with another, you get a list with single one and 9999 zeroes. That is much harder to sample from. And this is the common case in deep learning.

It is much easier to have a moving average of size 100 than of size 10,000.

6:35pm. Done with lunch. Forget that first algorithm. AdaBelief made me chase a mirage.

```fs
let alg2 vars =
    let len = float (List.length vars)
    let stds = List.map sqrt vars
    let average_std = List.sum stds / len
    let eps = 2.0 ** -7.0
    List.map (fun x ->
        if average_std = 0.0 then 0.0 else
        average_std / (eps * average_std + (1.0 - eps) * x)
        ) stds
```

What I should do is collect the fast variance, and then collect the slow variance and use this weighting scheme to get me what I want. I can't figure out what centered variance is suppose to converge to. But I suppose that both might be worth trying out. I wouldn't want to try `alg1` on the inputs, but `alg2` would do well. The outputs I will just have to try out.

6:40pm. Oh yes, the reason why I would not want alg1 on the inputs is because I do not want the bias term to have infinite certainty.

If I take the means, eventually the mean of the bias will be 1, and the variance will be one, meaning that once it is centered it will be zero. For outputs, I am strugling to think of a reason why I should not use `alg1`. If it wasn't for AdaBelief I would not even consider it, but now...

Actually, I was wrong what alg2 should be. Instead of the `average_std`, that should be `average_variance`. Then the fast and the slow would even out.

But when I started writing it, summing up the standard deviations started feeling more natural.

7pm. No, alg 1 is wrong. It is definitely wrong.

I've been fooled by AdaBelief. Guh, why do I even read those outside papers?

It is easy to imagine a situation. Suppose the layer has two outputs with gradient variances [100;10;1]. Do I want in any situation ever to replace those with centered variance? Suppose after subtracting the sequare of the means, I get...[1;1;1].

Do I want the 3 units to use the same constant factor during learning even though the first unit is 10x larger than the rest?

Of course not! I want to even these out!

I want the one which has low variance to learn faster and the one which are too fast to slow down.

The way I imagine it, if the output units are paths, I want to even them out so they are getting sampled uniformly rather than whatever screams the loudest. The mean is the wrong thing to consider here!

7:10pm. I am obsessed with these means in gradient propagation. I just keep trying to find a way to do it even if it does not make sense. I completely lost sight of the principles of why I am even doing this.

Adaptive momentum is right and AdaBelief is wrong. Though I accept that it is a better certainty measure, it is not what I need. I should not ever do shifting.

If AdaBelief crushed Adam in that paper, I'd consider it.

7:10pm. At any rate, I had some insights regarding TD learing.

I think I now understand completely the purpose behind Zap Q learning. I've been confused by all the matrix inversion, but all it does is take the state's reach probability into consideration and inverts it.

The tabular case is just the linear case where all the vector elements are one hot. If you built a covariance matrix out of the inputs, it would be really easy to invert as only the diagonals would have variances. Zap Q goes a bit further and considers the transitions as well.

Consider the way regular TD learning works.

You'd have the values in some dictionary, you'd use a learning rate of 2 ** -7 or something like that, but if the problem is big enough and if some states are hard enough to reach, that means that they'd very rarely get updated.

A state that gets visited 0.01% of the time would get updated 1000x less than the one that gets visited 10% of the time. What would be a reasonable learning rate for a node close too the root would be very unreasonable for something deeper.

7:25pm. I'd set a learning rate of 2 ** -7 and see that it works, but that is the wrong way to think. If the estimator is good enough, even 2 ** -2 would be fine. In some situations even only a few samples would be fine.

```fs
let alg2 vars =
    let len = float (List.length vars)
    let stds = List.map sqrt vars
    let average_std = List.sum stds / len
    let eps = 2.0 ** -7.0
    List.map (fun x ->
        if average_std = 0.0 then 0.0 else
        average_std / (eps * average_std + (1.0 - eps) * x)
        ) stds
```

No wait, I was wrong again. What I said originally was right. The `average_std` here would become the same as the fast `average_std` that I'd use to module the entire layer in the limit of data. I started thinking that the fast modulator was variance and confused myself.

7:30pm. Wow, I am tired.

But I figured it all out. In April, I've attained a deep appreciation for the art of averaging. There is so much depth to this.

My understanding is on an entirely different level now.

7:35pm. I'll be able to do it this time around.

But damn I need a break. I am exhausted. Even regular programming does not take it as much out of me as this does. Today I literally just sat in this chain in my room thinking, while the screen had gone blank hours ago.

That was a common scene today.

I thought after yesterday I'd be able to take a break today, but hopefully what I have now will have been enough to come to a resolution. I really need a day to not think about deep learning issues. I need to slack at this.

The last week cannot come close to qualifying as a vacation.

Tomorrow, at the very least I'll do that PL sub review."

---
## [NextGenSoftwareUK/Our-World-OASIS-API-HoloNET-HoloUnity-And-.NET-HDK](https://github.com/NextGenSoftwareUK/Our-World-OASIS-API-HoloNET-HoloUnity-And-.NET-HDK)@[27ba3bcd98...](https://github.com/NextGenSoftwareUK/Our-World-OASIS-API-HoloNET-HoloUnity-And-.NET-HDK/commit/27ba3bcd989b97e4c569d40bcef8cb247c652409)
#### Thursday 2021-04-29 18:14:57 by David Ellams

STAR ODK - Continued improving CLI (can now create a new avatar along with your favourite colour!) ;-)

- Can now set the colour you wish to use for the CLI (which is saved to your avatar so the next time you login back in STAR will remember your choice).

- Had a crazy OCD James Halliday essentric day today! Got a bit carried away with the CLI! ;-) lol Reminds me of the text based advnture game I coded on the Amiga as my 1st game I wrote back when I was around 10... :) Love my retro! ;-)

---
## [HectorDiazJr/react-portfolio](https://github.com/HectorDiazJr/react-portfolio)@[048a4677c4...](https://github.com/HectorDiazJr/react-portfolio/commit/048a4677c4e82f7f8fc56eafb5f5be4bc6d7efeb)
#### Thursday 2021-04-29 18:22:14 by Hector Diaz

still can't get images to render... I hate my life

---
## [EaW-Team/equestria_dev](https://github.com/EaW-Team/equestria_dev)@[51eae28b97...](https://github.com/EaW-Team/equestria_dev/commit/51eae28b973fe9862aff504971c5afd004d8cffd)
#### Thursday 2021-04-29 18:46:28 by ValenceWubs1738

18 Naked Bats at Crab Ranch

Fuck you, Cyrus lmao

Co-Authored-By: Cyrus1234 <32648244+Cyrus1234@users.noreply.github.com>

---
## [leanprover-community/mathlib](https://github.com/leanprover-community/mathlib)@[8ce8cef525...](https://github.com/leanprover-community/mathlib/commit/8ce8cef525438ada685843af5c8858c7bd3b3e7d)
#### Thursday 2021-04-29 18:49:21 by Bhavik Mehta

feat(category_theory/sites): Sheaves of structures (#5927)

Define sheaves on a site taking values in an arbitrary category.

Joint with @kbuzzard. cc: @jcommelin, this is a step towards condensed abelian groups.

I don't love the names here, design advice (particularly from those who'll use this) more than appreciated.

The main points are:

- An `A`-valued presheaf `P : C^op => A` is defined to be a sheaf (for the topology J) iff for every `X : A`, the type-valued presheaves of sets given by sending `U : C^op` to `Hom_{A}(X, P U)` are all sheaves of sets.
- When `A = Type`, this recovers the basic definition of sheaves of sets.
- An alternate definition when `C` is small, has pullbacks and `A` has products is given by an equalizer condition `is_sheaf'`.
- This is equivalent to the earlier definition.
- When `A = Type`, this is definitionally equal to the equalizer condition for presieves in sheaf_of_types.lean
- When `A` has limits and there is a functor `s : A => Type` which is faithful, reflects isomorphisms and preserves limits, then `P : C^op => A` is a sheaf iff the underlying presheaf of types `P >>> s : C^op => Type` is a sheaf. (cf https://stacks.math.columbia.edu/tag/0073, which is a weaker version of this statement (it's only over spaces, not sites) and https://stacks.math.columbia.edu/tag/00YR (a), which additionally assumes filtered colimits).

A couple of questions for reviewers:
- We've now got a ton of equivalent ways of showing something's a sheaf, and it's not the easiest to navigate between them. Is there a nice way around this? I think it's still valuable to have all the ways, since each can be helpful in different contexts but it makes the API a bit opaque. There's also a bit of inconsistency - there's a definition `yoneda_sheaf_condition` which is the same as `is_sheaf_for` but the equalizer conditions at the bottom of sheaf_of_types aren't named, they're just some `nonempty (is_limit (fork.of_ι _ (w P R)))` even though they're also equivalent.
- The name `presieve.is_sheaf` is stupid, I think I was just lazy with namespaces. I think `presieve.family_of_elements` and `presieve.is_sheaf_for` are still sensible, since they are relative to a presieve, but `is_sheaf` doesn't have any reference to presieves in its type. 
- The equalizer condition of sheaves of types is definitionally the same as the equalizer condition for sheaves of structures, so is there any point in having the former version in the library - the latter is just more general (the same doesn't apply to the actual def of sheaves of structures since that's defined in terms of sheaves of types). The main downside I can see is that it might make the proofs of `equalizer_sheaf_condition` a bit trickier, but that's about it

Co-authored-by: Kevin Buzzard <k.buzzard@imperial.ac.uk>

---
## [mody199610/Countries](https://github.com/mody199610/Countries)@[a5cfbb8739...](https://github.com/mody199610/Countries/commit/a5cfbb8739ce415b10a3144eaa5568f855cf2804)
#### Thursday 2021-04-29 19:29:39 by mody199610

Update DE01_NOTHING.m3u

I'm sorry for reaching out to you this way and of course i do see that there's some idiots whom are trying to steal your work and act as if they're the one who made it which is wrong and stupid of them at least they could somehow give you credit for it at least , Sorry again if this pull request bothers you tho but may it be possible if i could have access to your private repo please?

---
## [manen/dungeoneer](https://github.com/manen/dungeoneer)@[ab14bc26ae...](https://github.com/manen/dungeoneer/commit/ab14bc26ae3a2d4958f0a741c0bfc59961f2655f)
#### Thursday 2021-04-29 20:11:49 by manen

Fix math/rand
you know go is the kind of language that you really love, and you want to love it
but there are just some pain points that hurt
same thing with V btw

---
## [troglobit/watchdogd](https://github.com/troglobit/watchdogd)@[90f69d2ff1...](https://github.com/troglobit/watchdogd/commit/90f69d2ff1c14758e67813ac49500b5bf809c47d)
#### Thursday 2021-04-29 20:40:31 by Joachim Nilsson

Clarify nomenclature; cause is from kernel, reason from watchdogd

Richard Alpe brought to my attention the confusing nomenclature used in
watchdogd today.  Reset *cause* and reset *reason* were sloppily, and
often interchangeably, used.  It was not clear to a user of watchdogd
what was the status from the kernel WDT, and what was the extra info
that watchdogd had added.

After a brief discussion he convinced me it was a good idea to clean
up this mess and we arreived at the following conclusion;

  - Reset cause should always be the `WDIOF_` flags returned from the
    kernel `WDIOC_GETBOOTSTATUS` ioctl.
  - Reset reason is the extra value watchdogd adds.  This, at its core,
    is a `code` that can be set by any of the supervisor or monitor
    plugins supported; e.g. a monitored process fails to meet its
    deadline, a system memory leak, or CPU overload (loadavg plugin)

95% of this patch is a search-and-replace of:

  - cause --> reason
  - cause --> code
  - etc.

The remaining 5% are adaptations, comments, and updated documentation.

The biggest implication of this change is the change in the libwdog API.
The wdog_reason_t::cause rename to ::code.  Hence the ABI version bump.
We could of course add an ugly union to support both ::cause and ::code,
but that seems silly since we might also want to add the kernel reset
cause to that struct later.  So, in the immortal words of a good friend
of mine;

>  "I'd rather be consistent than please everyone."

Also, I'd be very surpised if this really breaks anyone's code, because
that would mean someone other than Westermo actually uses libwdog ...

Signed-off-by: Joachim Nilsson <troglobit@gmail.com>

---
## [linkerd/linkerd2-proxy](https://github.com/linkerd/linkerd2-proxy)@[479b46ddda...](https://github.com/linkerd/linkerd2-proxy/commit/479b46ddda3e4a3efafe65c9edd2af91947debd5)
#### Thursday 2021-04-29 21:41:38 by Eliza Weisman

apparently theres a magic setting that FIXES IT

aaghghghgh i want the last two goddamn hours of my life back!!!!!!

Signed-off-by: Eliza Weisman <eliza@buoyant.io>

---
## [arceuss/simply-love-oat-fork](https://github.com/arceuss/simply-love-oat-fork)@[2700e1962d...](https://github.com/arceuss/simply-love-oat-fork/commit/2700e1962dca61bb977e2283bd99ebb91ad393d5)
#### Thursday 2021-04-29 22:19:08 by arceuss

make noteskin selection thing easier to use or some shit

i just leeched it off joses simply love visual modification tho

---
## [vdonato/streamlit](https://github.com/vdonato/streamlit)@[bbafe9391b...](https://github.com/vdonato/streamlit/commit/bbafe9391b44bc7a155b7ff94f128d6687bb76dd)
#### Thursday 2021-04-29 23:16:01 by Vincent Donato

Revive the docker image used to take cypress snapshots

This should make it possible to generate and update snapshots without relying
on CI in some way (either by deleting snapshots to have CI regenerate them or
having CI generate diffs to be processed by the cropping script)

After this PR goes in, I'll spend some time reworking our wiki pages to include
instructions on how to use this image as well as an overview of all of the
available options for updating snapshots.

Unfortunately, this isn't a perfect cure for our snapshot woes for a few
reasons.

* it takes *forever* to build image, mostly due to the length of time it
  takes to install Python dependencies. As far as I can tell, this is
  naturally quite slow, and there's not much we can do about it.
* for whatever reason I'm having trouble getting component-template
  tests to work, but this is just a minor annoyance for now as those
  don't change much. This does mean that you can't run all e2e tests
  easily since they'll fail midway through due to these tests blowing
  up, but doing this takes so long that it seems unlikely that it'll be
  done often anyway.

---

# [<](2021-04-28.md) 2021-04-29 [>](2021-04-30.md)

