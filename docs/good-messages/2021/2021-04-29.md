# [<](2021-04-28.md) 2021-04-29 [>](2021-04-30.md)

3,311,857 events, 1,519,609 push events, 2,474,426 commit messages, 194,654,787 characters


## [frawhst/goonstation@24f6cd7530...](https://github.com/frawhst/goonstation/commit/24f6cd75306650a172e69ebe2a7a79c3a1824150)
##### 2021-04-29 06:08:02 by BatElite

Resprites building materials & standardises their code a bunch (#4086)

* Sprites & functions & oh god this needs unfucking
* Do rods, mostly. We have procs for this shit y'all
* Tile stacking/taking cleanup
* Sheet consumption stuff
* Addresses review, enlargens and changes sprites

Co-authored-by: ZeWaka <zewakagamer@gmail.com>

---
## [Amirbapiri/peace@69f3cb3afd...](https://github.com/Amirbapiri/peace/commit/69f3cb3afdf189805e19e2468084445506c7e01c)
##### 2021-04-29 08:29:33 by amir

feat: Probably the last commit to this project

I learned a lot from this project which I don't remember any of them.
I don't feel good at this stage of my bloody life.
I will do my best to make this commit a funny diary in the future.
Bye!

---
## [WatIsDeze/Nail-Crescent@224f61eee3...](https://github.com/WatIsDeze/Nail-Crescent/commit/224f61eee369e4bbdc03c04d17fd5d14fe020c7b)
##### 2021-04-29 11:59:48 by WatIsDeze

Fixed that ugly bug, I seriously hate touching msg.cpp..... FUCK YOU MSG.

---
## [techytoes/cockroach@f2c2f2e3db...](https://github.com/techytoes/cockroach/commit/f2c2f2e3dbbb52ff08fd30d4c533695c491103d8)
##### 2021-04-29 15:37:22 by craig[bot]

Merge #64060

64060: kvserver: fix delaying of splits with uninitialized followers r=erikgrinaker a=tbg

Bursts of splits (i.e. a sequence of splits for which each split splits
the right-hand side of a prior split) can cause issues. This is because
splitting a range in which a replica needs a snapshot results in two
ranges in which a replica needs a snapshot where additionally there
needs to be a sequencing between the two snapshots (one cannot apply
a snapshot for the post-split replica until the pre-split replica has
moved out of the way). The result of a long burst of splits such as
occurring in RESTORE and IMPORT operations is then an overload of the
snapshot queue with lots of wasted work, unavailable followers with
operations hanging on them, and general mayhem in the logs. Since
bulk operations also write a lot of data to the raft logs, log
truncations then create an additional snapshot burden; in short,
everything will be unhappy for a few hours and the cluster may
effectively be unavailable.

This isn't news to us and in fact was a big problem "back in 2018".
When we first started to understand the issue, we introduced a mechanism
that would delay splits (#32594) with the desired effect of ensuring
that, all followers had caught up to ~all of the previous splits.
This helped, but didn't explain why we were seeing snapshots in the
first place.

Investigating more, we realized that snapshots were sometimes spuriously
requested by an uninitialized replica on the right-hand side which was
contacted by another member of the right-hand side that had already been
initialized by the split executing on the left-hand side; this snapshot
was almost always unnecessary since the local left-hand side would
usually initialize the right-hand side moments later.  To address this,
in #32594 we started unconditionally dropping the first ~seconds worth
of requests to an uninitialized range, and the mechanism was improved in
 #32782 and will now only do this if a local neighboring replica is
expected to perform the split soon.

With all this in place, you would've expected us to have all bases
covered but it turns out that we are still running into issues prior
to this PR.

Concretely, whenever the aforementioned mechanism drops a message from
the leader (a MsgApp), the leader will only contact the replica every
second until it responds. It responds when it has been initialized via
its left neighbor's splits and the leader reaches out again, i.e.  an
average of ~500ms after being initialized. However, by that time, it is
itself already at the bottom of a long chain of splits, and the 500ms
delay is delaying how long it takes for the rest of the chain to get
initialized.  Since the delay compounds on each link of the chain, the
depth of the chain effectively determines the total delay experienced at
the end. This would eventually exceed the patience of the mechanism that
would suppress the snapshots, and snapshots would be requested. We would
descend into madness similar to that experienced in the absence of the
mechanism in the first place.

The mechanism in #32594 could have helped here, but unfortunately it
did not, as it routinely missed the fact that followers were not
initialized yet. This is because during a split burst, the replica
orchestrating the split was typically only created an instant before,
and its raft group hadn't properly transitioned to leader status yet.
This meant that in effect it wasn't delaying the splits at all.

This commit adjusts the logic to delay splits to avoid this problem.
While clamoring for leadership, the delay is upheld. Once collapsed
into a definite state, the existing logic pretty much did the right
thing, as it waited for the right-hand side to be in initialized.

Closes #61396.

cc @cockroachdb/kv

Release note (bug fix): Fixed a scenario in which a rapid sequence
of splits could trigger a storm of Raft snapshots. This would be
accompanied by log messages of the form "would have dropped incoming
MsgApp, but allowing due to ..." and tended to occur as part of
RESTORE/IMPORT operations.


Co-authored-by: Tobias Grieger <tobias.b.grieger@gmail.com>

---
## [mrakgr/The-Spiral-Language@1bf6457d63...](https://github.com/mrakgr/The-Spiral-Language/commit/1bf6457d63c75fefadcae734f20df6476126e289)
##### 2021-04-29 17:58:25 by Marko GrdiniÄ‡

"9:50am. Uagh, I am suffering again. AdaBelief is out. I am 80-90% sure that I am on the wrong track with it. The paper sounds nice, but I need to focus on the reality what I am optimizing, not some abstract thing.

And the reality is that sparsity is killer. The longer training proceeds and the deeper and the more parameters there are, the sparser the gradients for any particular parameter.

Momentum and keeping an exponential moving average of variance (or centered version) sounds good, but what if those terms just end up tracking the sparsity factors of the entire system?

There are good reason to be concerned about this!

After all after I modulate the inputs and the gradients, they will become very sparse, the bigger the layer, the sparser they will get! Since they will be very sparse, the mean will be zero, AdaBelief will degenerate into Adam. And the sparser the are, the stronger the counterbalancing factor.

It does me no good for one parameter to get a gradient of 1, only for the counterbalancing factor to push it up by a factor 10,000.

There is reason to be concerned about this! I mean, going from the video, to the first paper, to the latest one, the epsilon parameter - the one which controls the uncertainty in the parameter estimate is what keeps changing places!

10:15am. Maybe I really am better than I was in the past. If this was 2015 or 2016, I'd look at the epsilon parameter, see that it is 10^-8 and say pfft, it does not matter. It is too small to affect anything. But right now my attention is completely dominated by it.

These sparsity concerns are not even the slightest bit of a problem on toy examples where you always have gradients everywhere.

I think I now know the reason why for example the OpenAI agents needed a batch size of like a million.

Yes, indeed, if you use a large enough batch size to lower the degree of sparsity, then Adam, AdaBelief would start working again.

10:25am. I bought into the AdaBelief paper, but in the Crowded Valley one, it did not particularly stand out. That means that the concerns in the toy example are just that.

10:45am. I need to think about this.

12:45pm. I've been sitting here as waves and waves of inspiration roll over me.

I can see so much more than I used to.

I've realized something huge.

Consider the way Adam evaluates the variance parameter.

It just uses a exponential moving average of squared gradients.

How many samples would it need to get a good estimate of that?

Consider a linear net without bias whose matrix is of size 1x1.

In other words during the backprop phase the update rule will just be input * gradient.

Now consider what would happen if the input distribution is 1 with 1/100 probability and 0 with 99/100 probability everywhere else uniformly. Assume that the gradient also has the same distribution.

This means that the Adam style variance estimator will need squared the number of samples, meaning 100 * 100 to estimate the variance correctly.

On the other hand estimating the input and the gradient variance independently would need only 100 samples each.

I am fucking shocked, shocked that I did not realize this before!

When I estimate them separately the uncertainty is additive, but when I estimate them together the uncertainty is multiplicative. It is just like multiplying two gaussians, the standard deviations get multiplied as well.

1:05pm. Let me have breakfast here. I have other things I want to talk about.

2:40pm. I am in thought.

https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance

Ahhh...so that is what that damn bias correction was.

3:40pm. I've figured out a bunch of stuff. Let me do the chores.

5:20pm. This just won't end. I never spend my days deep in thought in such intensity for such prolonged periods. This last week has to be a new record.

I am trying to figure some things out, but I can't figure out how to leave the AdaBelief update in.

Replacing var with centered variance has appeal, but it would cost me the reward scaling property. Still, there is a variant that could potentially work.

I agree that centered variance is a better measure of uncertainty that regular variance. I could use the regular variance to maintain gradient scaling and the centered variance to reweight the gradients intra layer.

Two different algorithms are fighting it out in my mind.

In both cases for the inputs, only use the fast variance to rescale the input magnitudes.

For the outputs, use the fast variance (which is a scalar) to maintain the gradient magnitudes.

This is under the theory that estimating the general scale can be done much more easily that the individual variables.

For the individual variables, maintain slow moving averages of window length that is 3 times the size fanout for both the means and the variances.

Then use the relative variances to rescale the weights.

5:35pm. Ugh, my mind is intense but the words are coming out with difficulty. Let me code it up just to demonstrate.

5:55pm.

```fs
let alg1 means vars =
    let len = float (List.length vars)
    let stds = List.map2 (fun mean var -> var - mean * mean |> sqrt) means vars
    let average_std = List.sum stds / len
    let eps = 2.0 ** -7.0
    List.map (fun x ->
        if average_std = 0.0 then 0.0 else
        average_std / (eps * average_std + (1.0 - eps) * x)
        ) stds

let r = alg1 [1.0;2.0;3.0] [1.0; 12.0; 15.0]
```
```
val r : float list = [128.0; 0.6238507359; 0.7198180488]
```

Here is what I had in mind. What this does is make those entries that have low variance, in other words, high certainty relative to the rest of the bunch learn faster. Algorithm 2, would be the same as above, but it would ignore the means when doing the reweighting.

```fs
let alg2 vars =
    let len = float (List.length vars)
    let stds = List.map sqrt vars
    let average_std = List.sum stds / len
    let eps = 2.0 ** -7.0
    List.map (fun x ->
        if average_std = 0.0 then 0.0 else
        average_std / (eps * average_std + (1.0 - eps) * x)
        ) stds
```

This algorithm has interesting property when combined with the reward scaling modulator.

```fs
    let stds = List.map sqrt vars
    let average_std = List.sum stds / len
```

When there is enough data for example, when doing full batch learning, this quantity would be the same as the modulator and would cancel out.

```fs
    List.map (fun x ->
        if average_std = 0.0 then 0.0 else
        average_std / (eps * average_std + (1.0 - eps) * x)
        ) stds
```

Then this part would be equivalent to...

```fs
    List.map (fun x ->
        if average_std = 0.0 then 0.0 else
        1.0 / (eps * average_std + (1.0 - eps) * x)
        ) stds
```

In other words, it would be the same thing as taking a moving average of just the particular random variable in the first place and using that.

But it is not the same thing.

Different things are easier and harder to estimate depending on where you do it. Just like I shown earlier...

If you have two random variables X and Y, once you multiply them, it becomes much harder to get a proper estimate. And this is what Adam does instead of the more sensible thing.

If you have a list of a single one and 99 zeroes, and you pairwise multiply every element with another, you get a list with single one and 9999 zeroes. That is much harder to sample from. And this is the common case in deep learning.

It is much easier to have a moving average of size 100 than of size 10,000.

6:35pm. Done with lunch. Forget that first algorithm. AdaBelief made me chase a mirage.

```fs
let alg2 vars =
    let len = float (List.length vars)
    let stds = List.map sqrt vars
    let average_std = List.sum stds / len
    let eps = 2.0 ** -7.0
    List.map (fun x ->
        if average_std = 0.0 then 0.0 else
        average_std / (eps * average_std + (1.0 - eps) * x)
        ) stds
```

What I should do is collect the fast variance, and then collect the slow variance and use this weighting scheme to get me what I want. I can't figure out what centered variance is suppose to converge to. But I suppose that both might be worth trying out. I wouldn't want to try `alg1` on the inputs, but `alg2` would do well. The outputs I will just have to try out.

6:40pm. Oh yes, the reason why I would not want alg1 on the inputs is because I do not want the bias term to have infinite certainty.

If I take the means, eventually the mean of the bias will be 1, and the variance will be one, meaning that once it is centered it will be zero. For outputs, I am strugling to think of a reason why I should not use `alg1`. If it wasn't for AdaBelief I would not even consider it, but now...

Actually, I was wrong what alg2 should be. Instead of the `average_std`, that should be `average_variance`. Then the fast and the slow would even out.

But when I started writing it, summing up the standard deviations started feeling more natural.

7pm. No, alg 1 is wrong. It is definitely wrong.

I've been fooled by AdaBelief. Guh, why do I even read those outside papers?

It is easy to imagine a situation. Suppose the layer has two outputs with gradient variances [100;10;1]. Do I want in any situation ever to replace those with centered variance? Suppose after subtracting the sequare of the means, I get...[1;1;1].

Do I want the 3 units to use the same constant factor during learning even though the first unit is 10x larger than the rest?

Of course not! I want to even these out!

I want the one which has low variance to learn faster and the one which are too fast to slow down.

The way I imagine it, if the output units are paths, I want to even them out so they are getting sampled uniformly rather than whatever screams the loudest. The mean is the wrong thing to consider here!

7:10pm. I am obsessed with these means in gradient propagation. I just keep trying to find a way to do it even if it does not make sense. I completely lost sight of the principles of why I am even doing this.

Adaptive momentum is right and AdaBelief is wrong. Though I accept that it is a better certainty measure, it is not what I need. I should not ever do shifting.

If AdaBelief crushed Adam in that paper, I'd consider it.

7:10pm. At any rate, I had some insights regarding TD learing.

I think I now understand completely the purpose behind Zap Q learning. I've been confused by all the matrix inversion, but all it does is take the state's reach probability into consideration and inverts it.

The tabular case is just the linear case where all the vector elements are one hot. If you built a covariance matrix out of the inputs, it would be really easy to invert as only the diagonals would have variances. Zap Q goes a bit further and considers the transitions as well.

Consider the way regular TD learning works.

You'd have the values in some dictionary, you'd use a learning rate of 2 ** -7 or something like that, but if the problem is big enough and if some states are hard enough to reach, that means that they'd very rarely get updated.

A state that gets visited 0.01% of the time would get updated 1000x less than the one that gets visited 10% of the time. What would be a reasonable learning rate for a node close too the root would be very unreasonable for something deeper.

7:25pm. I'd set a learning rate of 2 ** -7 and see that it works, but that is the wrong way to think. If the estimator is good enough, even 2 ** -2 would be fine. In some situations even only a few samples would be fine.

```fs
let alg2 vars =
    let len = float (List.length vars)
    let stds = List.map sqrt vars
    let average_std = List.sum stds / len
    let eps = 2.0 ** -7.0
    List.map (fun x ->
        if average_std = 0.0 then 0.0 else
        average_std / (eps * average_std + (1.0 - eps) * x)
        ) stds
```

No wait, I was wrong again. What I said originally was right. The `average_std` here would become the same as the fast `average_std` that I'd use to module the entire layer in the limit of data. I started thinking that the fast modulator was variance and confused myself.

7:30pm. Wow, I am tired.

But I figured it all out. In April, I've attained a deep appreciation for the art of averaging. There is so much depth to this.

My understanding is on an entirely different level now.

7:35pm. I'll be able to do it this time around.

But damn I need a break. I am exhausted. Even regular programming does not take it as much out of me as this does. Today I literally just sat in this chain in my room thinking, while the screen had gone blank hours ago.

That was a common scene today.

I thought after yesterday I'd be able to take a break today, but hopefully what I have now will have been enough to come to a resolution. I really need a day to not think about deep learning issues. I need to slack at this.

The last week cannot come close to qualifying as a vacation.

Tomorrow, at the very least I'll do that PL sub review."

---
## [Codecademy/client-modules@c4643db64d...](https://github.com/Codecademy/client-modules/commit/c4643db64d3684319f8d5fc6f68d05197ec1f61e)
##### 2021-04-29 20:14:14 by Aaron Robb

feat(Buttons): ColorModes + Props + Refactor + Refs Oh My! [GM-211] [REACH-856] [GM-192]

* Buttons now use `ButtonBase` to match with `Anchor`.
* Removes extra tagged templates to reduce Emotion Label spam and reduces internal style overhead.
* Button Colors are now serialized as variables inside of the base component depending on which mode they are.
* Buttons will now default to the current ColorMode if they have not been passed a prop.
* Also fixes a prop forwarding bug with CTAButton where inner component was not getting props forwarded.
* Adds all button types to the ColorMode example (small and large).
* Buttons now will use the current mode in context unless specified otherwise. Since all dark mode buttons are specifically configured this way there should be no changes to ANY existing button colors.
* Buttons now come with a few system props to make life easier including spacing layout and positioning. See prop reference here https://gamut.codecademy.com/storybook/?path=/docs/foundations-system-props--layout. Please note that these will cause issues with behavior if used incorrectly. Changes to display and padding may break button styles in some cases.
* All Buttons now accept a ref!

---
## [troglobit/watchdogd@90f69d2ff1...](https://github.com/troglobit/watchdogd/commit/90f69d2ff1c14758e67813ac49500b5bf809c47d)
##### 2021-04-29 20:40:31 by Joachim Nilsson

Clarify nomenclature; cause is from kernel, reason from watchdogd

Richard Alpe brought to my attention the confusing nomenclature used in
watchdogd today.  Reset *cause* and reset *reason* were sloppily, and
often interchangeably, used.  It was not clear to a user of watchdogd
what was the status from the kernel WDT, and what was the extra info
that watchdogd had added.

After a brief discussion he convinced me it was a good idea to clean
up this mess and we arreived at the following conclusion;

  - Reset cause should always be the `WDIOF_` flags returned from the
    kernel `WDIOC_GETBOOTSTATUS` ioctl.
  - Reset reason is the extra value watchdogd adds.  This, at its core,
    is a `code` that can be set by any of the supervisor or monitor
    plugins supported; e.g. a monitored process fails to meet its
    deadline, a system memory leak, or CPU overload (loadavg plugin)

95% of this patch is a search-and-replace of:

  - cause --> reason
  - cause --> code
  - etc.

The remaining 5% are adaptations, comments, and updated documentation.

The biggest implication of this change is the change in the libwdog API.
The wdog_reason_t::cause rename to ::code.  Hence the ABI version bump.
We could of course add an ugly union to support both ::cause and ::code,
but that seems silly since we might also want to add the kernel reset
cause to that struct later.  So, in the immortal words of a good friend
of mine;

>  "I'd rather be consistent than please everyone."

Also, I'd be very surpised if this really breaks anyone's code, because
that would mean someone other than Westermo actually uses libwdog ...

Signed-off-by: Joachim Nilsson <troglobit@gmail.com>

---
## [linkerd/linkerd2-proxy@479b46ddda...](https://github.com/linkerd/linkerd2-proxy/commit/479b46ddda3e4a3efafe65c9edd2af91947debd5)
##### 2021-04-29 21:41:38 by Eliza Weisman

apparently theres a magic setting that FIXES IT

aaghghghgh i want the last two goddamn hours of my life back!!!!!!

Signed-off-by: Eliza Weisman <eliza@buoyant.io>

---

# [<](2021-04-28.md) 2021-04-29 [>](2021-04-30.md)

