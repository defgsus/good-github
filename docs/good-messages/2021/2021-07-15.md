# [<](2021-07-14.md) 2021-07-15 [>](2021-07-16.md)

2,932,904 events, 1,463,475 push events, 2,431,402 commit messages, 272,281,747 characters


## [tauds/DataCamp-Analyzing-TV-Data](https://github.com/tauds/DataCamp-Analyzing-TV-Data)@[b7d971969f...](https://github.com/tauds/DataCamp-Analyzing-TV-Data/commit/b7d971969f08cf7a3755b074668190ddb02f2bc8)
#### Thursday 2021-07-15 00:39:29 by tauds

Create 5. Do blowouts translate to lost viewers?

The vast majority of Super Bowls are close games. Makes sense. Both teams are likely to be deserving if they've made it this far. The closest game ever was when the Buffalo Bills lost to the New York Giants by 1 point in 1991, which was best remembered for Scott Norwood's last-second missed field goal attempt that went wide right, kicking off four Bills Super Bowl losses in a row. Poor Scott. The biggest point discrepancy ever was 45 points (!) where Hall of Famer Joe Montana's led the San Francisco 49ers to victory in 1990, one year before the closest game ever.
I remember watching the Seahawks crush the Broncos by 35 points (43-8) in 2014, which was a boring experience in my opinion. The game was never really close. I'm pretty sure we changed the channel at the end of the third quarter. Let's combine our game data and TV to see if this is a universal phenomenon. Do large point differences translate to lost viewers? We can plot household share (average percentage of U.S. households with a TV in use that were watching for the entire broadcast) vs. point difference to find out.

---
## [apollographql/apollo-server](https://github.com/apollographql/apollo-server)@[adbbd3eb5e...](https://github.com/apollographql/apollo-server/commit/adbbd3eb5e0d164f91d0b0a48adf9f17d63ab6d9)
#### Thursday 2021-07-15 06:39:14 by David Glasser

Require `prettier` formatting again

Prior to 553122891 in August 2019, this repository required all code to be
formatted with the tool Prettier. We decided to remove this requirement;
@abernix included a compelling explanation in that commit's message.

The state of prettier in the repository has been somewhat mixed recently. I'm
currently the lead maintainer of Apollo Server and I've kept all the files I
work on most actively (including most of apollo-server-core) prettier-clean when
feasible. In my personal experience I've found that the ability to completely
turn off any part of my brain that thinks about the details of formatting
outweighs the drawbacks described in the above message. I've also found that
many contributors do run prettier accidentally as part of their commits; telling
people to revert changes (which often increase readability!) feels like as much
of a waste of time as forcing folks who may have never encountered the quite
popular Prettier tool to run it. We do use enforced Prettier on several of
Apollo's internal projects and it does seem to work well.

As an experiment, we're going to try turning Prettier enforcement back on. If
this proves to be a hurdle for contributors, we can revert it. I recognize
there's some hubris in ignoring @abernix's well-reasoned plea from two years
ago, and there's a very good chance that in a few months I'll have learned the
hard way that his judgment two years ago was better than my judgment today.

Note that since we've upgraded to Prettier 2 since the last time we enforced
Prettier, so as the comment in the old prettierrc suggests, we no longer need
its requirePragma hack.

---
## [RichardBerlin345/RichardBerlin345](https://github.com/RichardBerlin345/RichardBerlin345)@[9329d60020...](https://github.com/RichardBerlin345/RichardBerlin345/commit/9329d60020618ce62a6b63a47c9679f02dd0c80a)
#### Thursday 2021-07-15 10:33:49 by RichardBerlin345

Benefits Of Couples Massage Tukwila

Your body releases the "love hormone oxytocin", which gives the massage a feeling of love and affection. This means that when you massage couples, Tukwila's love is in the air (and in the bloodstream). Increased appreciation is one of the main benefits of multiple massages. Many couples want to have a good time together during their hectic times, and multiple massage sessions can be a deliberate and deliberate reunion. Massage can also improve well-being and intimacy by releasing oxytocin, serotonin, and dopamine. According to experts, such statistics suggest that couples may have frequent massage sessions.

For more: 
https://sites.google.com/view/bluelotusspa/blog/benefits-of-couples-massage-tukwila

---
## [RichardBerlin345/RichardBerlin345](https://github.com/RichardBerlin345/RichardBerlin345)@[274e2294ec...](https://github.com/RichardBerlin345/RichardBerlin345/commit/274e2294ec9761e83c545b746afc04dda9beeda4)
#### Thursday 2021-07-15 10:57:19 by RichardBerlin345

Couples Massage Tukwila

Your body releases the 'love hormone oxytocin,' which gives your massages a sense of love and affection. It implies that when you get a Couple Massage, Tukwila love is in the air (and the bloodstream). Increased appreciation is one of the main advantages of a few massages. Many couples want to spend quality time together in their hectic times, and a few massages may be deliberate and deliberate reconnection. Massage treatment may also help enhance the condition and vicinity by producing oxytocin’s, serotonin, and body dopamine. According to specialists, such statistics suggest that couples may have frequent massages.

For more: https://tananet.net/couple-massage-tukwila/

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[62418d329c...](https://github.com/mrakgr/The-Spiral-Language/commit/62418d329c6e4e6793e6c47336dde8ebcb4e242b)
#### Thursday 2021-07-15 11:18:18 by Marko Grdinić

"11:05am. I went to bed a bit earlier, but I failed at moving the internal wake up clock. Only got up 10m ago and I feel groggy right now. Well, either way today is the programming day. I am not going to waste time waiting for my grogginess to pass. Instead, I am going to get started on the transformer and make my way from there.

I've made up my resolve. To break into the 4/5 realm where I can exert heavy pressure on the real world, I need energy based models, algos for them and neurochips. I am not going to force deep learning into the mold. It won't be able to handle composition of independently trained modules nor long term credit assignment.

Instead I'll aim to reach the pinnacle of 3/5. The current deep learning wave is here and I should just ride it out to its end. It is time to master it thoroughly.

I've spent way too much time chasing down empty leads. It is time to get things done.

11:10am. I know I say this often, but once I implement the SAU trick, there will be no way to exceed the current formulation very easily. Not without some complicated GAN prediction training. What I have in mind now is as easy as it will get. Low hanging fruit like SAU was truly a furtuitous encounter.

11:25am.

```py
import torch
from torch.nn import Module,Linear

class BatchLinear(Linear):
    def forward(self,x):
        self.weight.bmm

class SimpleTransformer(Module):
    def __init__(self,dim_in,dim_out):
        super().__init__()
        self.k = Linear(dim_in,dim_out)
        self.q = Linear(dim_in,dim_out)
        self.v = Linear(dim_in,dim_out)

    def forward(self,x):
        k,q,v = self.k(x), self.q(x), self.v(x)
```

No, not like this. I need to check out how the batch matrix multiply works.

```py
import torch
from torch.nn import Module,Linear

w = torch.rand(3,2)
x = torch.rand(2,4,2)
r = x @ w.t()
r.shape
```

Actually I am confused now. Why isn't the regular mm good enough?

https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html

```py
        attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 encoder_outputs.unsqueeze(0))
```

I am confused. Even with multiple heads it should not be necessary.

11:40am. ...Ah, right. I see. The values should be using bmm. No doubt about it.

11:50am.

```
    def forward(self,x):
        q,k,v = self.q(x), self.k(x), self.v(x)
        z = torch.einsum('bsa,bsa->bs',q,k)
        z = z / torch.sqrt(q.shape[-1])
```

I am a bit confused about the division by sqrt d. If the purpose is for a linear layer to come up to 1, then it would be better to use the mean.

https://paperswithcode.com/method/scaled

Ah, they are trying to scale the expected variance. I see.

12:15pm.

```py
import torch
from torch import Tensor
from torch.nn import Module,Linear
from torch.nn.init import xavier_normal_
from torch.nn.parameter import Parameter

class SimpleTransformer(Module):
    def __init__(self,dim_in,dim_out):
        super().__init__()
        self.q, self.k, self.q = [Parameter(xavier_normal_(torch.empty(dim_out,dim_in))) in range(3)]

    def forward(self,x,mask):
        q,k,v = self.q(x), self.k(x), self.v(x)
        z : Tensor = torch.einsum('qsa,qsa->qs',q,k) / torch.sqrt(self.k.shape[-1])
        z = z.masked_fill(mask,float('-inf'))
        z = z.softmax(-1)
        torch.einsum('qs,qsa->qa',z,v)
```

Is this right. It feels wrong somehow.

```py
z : Tensor = q.view(dim_batch*dim_seq,dim_el) @ k.view(dim_batch*dim_seq,dim_el).t() / dim_el
```

This is so complicated.

```py
z : Tensor = torch.einsum('qsa,qsa->qsqs',q,k) / dim_el
```

No this was not how I imagined it.

```py
    def forward(self,x,mask):
        q,k,v = self.q(x), self.k(x), self.v(x)
        dim_batch, dim_seq, dim_el = k.shape
        z : Tensor = torch.einsum('qsa,qsa->qss',q,k) / dim_el
```

Yeah, this is where the batch matrix multiply would come into play.

```py
z : Tensor = torch.einsum('qca,qba->qcb',q,k) / dim_el
```

Maybe I could do it like this in order to disambiguate the dimension ordering.

```py
    def forward(self,x,mask):
        q,k,v = self.q(x), self.k(x), self.v(x)
        dim_batch, dim_seq, dim_el = k.shape
        z : Tensor = torch.einsum('qca,qba->qcb',q,k) / dim_el
        z = z.masked_fill(mask,float('-inf'))
        z = z.softmax(-1)
        torch.einsum('qs,qsa->qa',z,v)
```

I know how to deal with the mask, but I am confused about the softmax dimension.

12:50pm.

```py
class Attention(Module):
    def __init__(self,dim_in,dim_out):
        super().__init__()
        self.q, self.k, self.q = [Parameter(xavier_normal_(torch.empty(dim_out,dim_in))) in range(3)]

    def forward(self,x,mask=None):
        q,k,v = self.q(x), self.k(x), self.v(x)
        dz, dk, da = k.shape
        z : Tensor = torch.einsum('zqa,zka->zqk',q,k) / da
        if mask is not None:
            mask = torch.logical_or(mask.view(dz,dk,1),mask.view(dz,1,dk))
            z = z.masked_fill(mask,float('-inf'))
        torch.einsum('zqk,zka->zqa',z.softmax(-1),v)
```

Let me set it like this. Einsum really makes dealing with dimensions a lot easier.

```py
return torch.einsum('zqk,zka->zqa',z.softmax(-1),v)
```

Can't forget the return.

Ah, yes. What will softmax do to an array of -infs?

```
>>> q.softmax(-1)
tensor([nan, nan, nan, nan])
```

1pm.

```py
return torch.einsum('zqk,zka->zqa',z.softmax(-1).nan_to_num(),v)
```

So I should fix it using something like this...

1:10pm.

```py
class Attention(Module):
    def __init__(self,dim_in,dim_v,dim_qk=None):
        super().__init__()
        dim_proj = dim_v if dim_qk is None else dim_qk
        self.q, self.k, self.v = Linear(dim_in,dim_proj,bias=False), Linear(dim_in,dim_proj,bias=False), Linear(dim_in,dim_v)

    def forward(self,x,mask=None):
        q,k,v = self.q.forward(x), self.k.forward(x), self.v.forward(x)
        dz, dk, da = k.shape
        z : Tensor = torch.einsum('zqa,zka->zqk',q,k) / da
        if mask is not None:
            mask = torch.logical_or(mask.view(dz,dk,1),mask.view(dz,1,dk))
            z = z.masked_fill(mask,float('-inf'))
        return torch.einsum('zqk,zka->zqa',z.softmax(-1).nan_to_num(),v)
```

Let me set it up like this. This will serve nicely.

Let me take a break here.

```py
z : Tensor = torch.einsum('zqa,zka->zqk',q,k) / sqrt(da)
```

Ah, I forgot the sqrt.

```py
        if mask is not None:
            z = z.masked_fill(torch.logical_or(mask.view(dz,dk,1),mask.view(dz,1,dk)),float('-inf'))
```

I also remember that assigning to the params will change the defaults. Let me do it like this then."

---
## [Ruega/kernel_asus_sdm660](https://github.com/Ruega/kernel_asus_sdm660)@[078b090e41...](https://github.com/Ruega/kernel_asus_sdm660/commit/078b090e41a47fb93b3beba0e4b5aabcc97e6392)
#### Thursday 2021-07-15 12:14:37 by Petr Mladek

kthread: add kthread_create_worker*()

Kthread workers are currently created using the classic kthread API,
namely kthread_run().  kthread_worker_fn() is passed as the @threadfn
parameter.

This patch defines kthread_create_worker() and
kthread_create_worker_on_cpu() functions that hide implementation details.

They enforce using kthread_worker_fn() for the main thread.  But I doubt
that there are any plans to create any alternative.  In fact, I think that
we do not want any alternative main thread because it would be hard to
support consistency with the rest of the kthread worker API.

The naming and function of kthread_create_worker() is inspired by the
workqueues API like the rest of the kthread worker API.

The kthread_create_worker_on_cpu() variant is motivated by the original
kthread_create_on_cpu().  Note that we need to bind per-CPU kthread
workers already when they are created.  It makes the life easier.
kthread_bind() could not be used later for an already running worker.

This patch does _not_ convert existing kthread workers.  The kthread
worker API need more improvements first, e.g.  a function to destroy the
worker.

IMPORTANT:

kthread_create_worker_on_cpu() allows to use any format of the worker
name, in compare with kthread_create_on_cpu().  The good thing is that it
is more generic.  The bad thing is that most users will need to pass the
cpu number in two parameters, e.g.  kthread_create_worker_on_cpu(cpu,
"helper/%d", cpu).

To be honest, the main motivation was to avoid the need for an empty
va_list.  The only legal way was to create a helper function that would be
called with an empty list.  Other attempts caused compilation warnings or
even errors on different architectures.

There were also other alternatives, for example, using #define or
splitting __kthread_create_worker().  The used solution looked like the
least ugly.

Link: http://lkml.kernel.org/r/1470754545-17632-6-git-send-email-pmladek@suse.com
Signed-off-by: Petr Mladek <pmladek@suse.com>
Acked-by: Tejun Heo <tj@kernel.org>
Cc: Oleg Nesterov <oleg@redhat.com>
Cc: Ingo Molnar <mingo@redhat.com>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Steven Rostedt <rostedt@goodmis.org>
Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
Cc: Josh Triplett <josh@joshtriplett.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: Jiri Kosina <jkosina@suse.cz>
Cc: Borislav Petkov <bp@suse.de>
Cc: Michal Hocko <mhocko@suse.cz>
Cc: Vlastimil Babka <vbabka@suse.cz>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: celtare21 <celtare21@gmail.com>
Signed-off-by: YousefAlgadri <yusufgadrie@gmail.com>

---
## [hmpf/easydmp](https://github.com/hmpf/easydmp)@[a8cf52e54d...](https://github.com/hmpf/easydmp/commit/a8cf52e54d73fa663bd02136df7aa4dc49e9b9e8)
#### Thursday 2021-07-15 12:37:22 by Hanne Moa

Migrate: Add an identifier to each answerset

When a section can have more than one AnswerSet per Plan, the end-user
needs to be able to refer to each AnswerSet. We could use use the
primary key, but having a dedicated text-field makes it possible for the
end-user to use their own preferred identifiers, like a DOI or a nick
name.

TODO: Have a question input type that sets this identifier. This will
most likely lead to the information being stored in two locations.
However: not storing anything in AnswerSet.data would lead to ugly hacks
regarding validity and branching. Not storing it on a direct field on
the AnswerSet would lead to other ugly hacks, as in: having to check if
the section has a question with the magical input in order to find the
question id, then look up the answer. The added complexity would not be
worth it.

---
## [linux-chenxing/linux](https://github.com/linux-chenxing/linux)@[514798d365...](https://github.com/linux-chenxing/linux/commit/514798d36572fb8eba6ccff3de10c9615063a7f5)
#### Thursday 2021-07-15 13:17:41 by Linus Torvalds

Merge tag 'clk-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/clk/linux

Pull clk updates from Stephen Boyd:
 "This round has a diffstat dominated by Qualcomm clk drivers. Honestly
  though that's just a bunch of data so the diffstat reflects that.
  Looking beyond that there's just a bunch of updates all around in
  various clk drivers. Renesas and NXP (for i.MX) are two SoC vendors
  that have a lot of patches in here.

  Overall the driver changes look to be mostly enabling more clks and
  non-critical fixes that we could hold until the next merge window.

  I'm especially excited about the series from Arnd that graduates
  clkdev to be the only implementation of clk_get() and clk_put().
  That's a good step in the right direction to migreate eveerything over
  to the common clk framework. Now we don't have to worry about clkdev
  specific details, they're just part of the clk API now.

  Core:
   - clkdev is now the only option, i.e. clk_get()/clk_put() is
     implemented in only one place in the kernel instead of in
     drivers/clk/clkdev.c and in architectures that want their own
     implementation

  New Drivers:
   - Texas Instruments' LMK04832 Ultra Low-Noise JESD204B Compliant
     Clock Jitter Cleaner With Dual Loop PLLs
   - Qualcomm MDM9607 GCC
   - Qualcomm SC8180X display clks
   - Qualcomm SM6125 GCC
   - Qualcomm SM8250 CAMCC (camera)
   - Renesas RZ/G2L SoC
   - Hisilicon hi3559A SoC

  Updates:
   - Stop using clock-output-names in ST clk drivers (yay!)
   - Support secure mode of STM32MP1 SoCs
   - Improve clock support for Actions S500 SoC
   - duty cycle setting support on qcom clks
   - Add TI am33xx spread spectrum clock support
   - Use determine_rate() for the Amlogic pll ops instead of
     round_rate()
   - Restrict Amlogic gp0/1 and audio plls range on g12a/sm1
   - Improve Amlogic axg-audio controller error on deferral
   - Add NNA clocks on Amlogic g12a
   - Reduce memory footprint of Rockchip PLL rate tables
   - A fix for the newly added Rockchip rk3568 clk driver
   - Exported clock for the newly added Rockchip video decoder
   - Remove audio ipg clock from i.MX8MP
   - Remove deprecated legacy clock binding for i.MX SCU clock driver
   - Use common clk-imx8qxp for both i.MX8QXP and i.MX8QM
   - Add multiple clocks to clk-imx8qxp driver (enet, hdmi, lcdif,
     audio, parallel interface)
   - Add dedicated clock ops for i.MX paralel interface
   - Different fixes for clocks controlled by ATF on i.MX SoCs
   - Add A53/A72 frequency scaling support i.MX clk-scu driver
   - Add special case for DCSS clock on suspend for i.MX clk-scu driver
   - Add parent save/restore on suspend/resume to i.MX clk-scu driver
   - Skip runtime PM enablement for CPU clocks in i.MX clk-scu driver
   - Remove the sys1_pll/sys2_pll clock gates for i.MX8MQ and their
     bindings
   - Tegra clk driver no longer deasserts resets on clk_enable as it
     gets in the way of certain power-up sequences
   - Fix compile testing for Tegra clk driver
   - One patch to fix a divider on the Allwinner v3s Audio PLL
   - Add support for CPU core clock boost modes on Renesas R-Car Gen3
   - Add ISPCS (Image Signal Processor) clocks on Renesas R-Car V3U
   - Switch SH/R-Mobile and R-Car "DIV6" clocks to .determine_rate() and
     improve support for multiple parents
   - Switch Renesas RZ/N1 divider clocks to .determine_rate()
   - Add ZA2 (Audio Clock Generator) clock on Renesas R-Car D3
   - Convert ar7 to common clk framework
   - Convert ralink to common clk framework"

* tag 'clk-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/clk/linux: (161 commits)
  clk: zynqmp: Handle divider specific read only flag
  clk: zynqmp: Use firmware specific mux clock flags
  clk: zynqmp: Use firmware specific divider clock flags
  clk: zynqmp: Use firmware specific common clock flags
  clk: lmk04832: Use of match table
  clk: lmk04832: Depend on SPI
  clk: stm32mp1: new compatible for secure RCC support
  dt-bindings: clock: stm32mp1 new compatible for secure rcc
  dt-bindings: reset: add MCU HOLD BOOT ID for SCMI reset domains on stm32mp15
  dt-bindings: reset: add IDs for SCMI reset domains on stm32mp15
  dt-bindings: clock: add IDs for SCMI clocks on stm32mp15
  reset: stm32mp1: remove stm32mp1 reset
  clk: hisilicon: Add clock driver for hi3559A SoC
  dt-bindings: Document the hi3559a clock bindings
  clk: si5341: Add sysfs properties to allow checking/resetting device faults
  clk: si5341: Add silabs,iovdd-33 property
  clk: si5341: Add silabs,xaxb-ext-clk property
  clk: si5341: Allow different output VDD_SEL values
  clk: si5341: Update initialization magic
  clk: si5341: Check for input clock presence and PLL lock on startup
  ...

---
## [Chris-plus-alphanumericgibberish/dNAO](https://github.com/Chris-plus-alphanumericgibberish/dNAO)@[48423d1013...](https://github.com/Chris-plus-alphanumericgibberish/dNAO/commit/48423d1013a75f7d5a7c7ff27d96c646633f5a9b)
#### Thursday 2021-07-15 15:53:20 by ChrisANG

More vault treasures (and also a couple of Nudzirath temple ones that got roped in)

Magic-blocking items
Antimagic rift: Blocks most types (any types not covered by the other two) of monster spelcasting, blocks your spellcasting if you cast with Int.
Catapsi vortex: Blocks monster psionic spellcasting and mind blasts. Blocks your spellcasting if you cast using Cha (which Madmen do) and blocks your psychic powers.
Misotheistic pyramids: Blocks monster divine spellcasting. Blocks prayer, sacrificing, altar curse-testing, and priest services
-mirrored fragments: Also causes monsters to fracture on death (this is pretty bad). Reading or using it will teach any character Nudzirath's seal.
--Includes improvement to Nudzirath; mirrors are shown as options for 't'.
-black-stone: Grants dark vision and blocks all other forms of vision.
--Includes a modification to Align2gangr such that it will always return the GA_s for your pantheon. Introduces AltarAlign2gangr to handle altars. Probably indroduces some bugs in the process :(
-All durations increase exponentially (1.1x) with repeated uses. It is possible to permanently turn off a given type of magic by using enough of a given type of item. The math should be handled to not overflow (eventually stops incrementing the number used, the cutoff is much lower than both the total size of a long int and the max value of a float)
--Duration incrementing functions changed to take long durations instead of int durations (these functions then cap what they're given appropriately).

Regen-up items
Vital soulstone: Increases health regen while in inventory (stacks), use for burst healing. More effective when cursed. Uses soul coin rules.
Spiritual soulstone: Inccreases energy regen while in inventory (stacks), use for burst recovery. More effective when cursed. Uses soul coin rules.

Rod of Force: Treasure for zigguraut chests
-2x lightsaber-type weapon using longsword or two-handed sword skill
-zap for force bolt + knockback beam (uses 10000 turns of charge/100 hits of charge)
-When used unlit, suppresses the special abilities of any monster with spellcasting powers. More effective vs. clerical casters. May cancel clerical casters. Gains 100 charge (1 hit) per turn of suppression damage inflicted, or all charge when cancelling a clerical caster.
-When wielded, absorbs force bolts and wand of striking blasts and converts them to charge (10,000 turns of charge/1 zap)
--In order to make a weapon that can be zapped, separates oc_dir and dtype into separate fields rather than overloading oc_dir (oc_dtyp introduced. oc_dtyp still overloaded with armor coverage info ): )
--Zap is handled for MvYou, but monster AI doesn't actually do this.

Slightly re-works how hell vault items are created, in prep for placing them explicitly in .des files.

---
## [KennyThrug/KGLGE](https://github.com/KennyThrug/KGLGE)@[e094c1c340...](https://github.com/KennyThrug/KGLGE/commit/e094c1c340c0c982687a334477e5bde6aaef963f)
#### Thursday 2021-07-15 15:56:23 by KennyThrug

Multiple Texture slots

Fucking Hell, I feel like I'm an idiot? The issue was GameLoop.cpp line 74, where it sets the active texture. Everything I was reading online said you need to do it after loading the texture, but you have to do it before... UGH. Now I get to rewrite my Texture Atlas Class. 3rd times a charm

---
## [testgram/cli](https://github.com/testgram/cli)@[a51ccddb41...](https://github.com/testgram/cli/commit/a51ccddb415a57c6a0df250e40e65e75a4596219)
#### Thursday 2021-07-15 16:39:07 by Sejal Patel

Add support for doctor examination

Yes folks you read that correctly. There is now a doctor in the house.

No need to be afraid, this is great as it will help identify potential
complications that might exist and at least identify various symptons
that exist in order to help us understand the health of your integration
with our services.

And of course in the spirit of all things we do here, we kept it as a
simple, pleasant, painless experience.

More importantly, we have as a result of the doctors examinations,
discovered and cured a large number of issues for an overall vastly
improved end user experience.

Can I get warm welcome for our newest addition, the `tg doctor` ^_^


Co-authored-by: Animesh Koratana <animesh.koratana@gmail.com>

---
## [fastly/Viceroy](https://github.com/fastly/Viceroy)@[53dfe50374...](https://github.com/fastly/Viceroy/commit/53dfe5037404fc333e7b934f35007fca8b8d5299)
#### Thursday 2021-07-15 19:26:53 by Michael Gattozzi

Add a Code of Conduct to Viceroy

While this commit could really just be the title I think it's important
to expand on the "why have a Code of Conduct?" question that gets asked.
Sometimes in good faith, but unfortunately often in bad faith. For
those who think a Code of Conduct is bad and try to open up issues on
the issue tracker about this addition we'll kindly ask you to leave and
lock them. For those who are legitimately curious do read on as to the
why.

Often times tech is a pretty hostile space for many people, especially
with Open Source Software (OSS), but also at companies themselves. Now
you might be thinking "Well I've never experienced harrassment or rude
people in tech" and that's great. Honestly I'd hope no one would be
subject to that kind of treatment. That being said it's annecdotal and
if you spend time talking to minorities in tech (whether it's sex, race,
gender identity or something else) you'll find this is often not the
case. Personally I've barely experienced anything at the level of
harrasment that coworkers or friends in the industry have experienced.
We want a Code of Conduct so that we have a mechanism to enforce safety
within our community. While we'd like to be open for everyone we don't
want to be open at the expense of the well being of others. Working on
code just isn't fun if all you're going to recieve is angry threats from
developers and worst of all if we tolerate that kind of behavior people
will leave and they won't make a fuss of it they'll just go.

Worse, people see this behavior being tolerated and won't want to be
part of that community or participate in it. By tolerating intolerance
by others, a community either diminishes or becomes intolerant itself.
This is where the paradox of intolerance comes in, in order to have a
tolerant and welcoming community we must be intolerant of intolerance.
This is what the Code of Conduct is for! We don't want people who are
rude or make people feel unwelcome. We want others who are kind, caring,
and make others want to be here and feel supported. A Code of Conduct is
by no means perfect and only one part of this, but it's the bare minimum
needed to have a healthy community.

If you feel excluded because of the Code of Conduct, I'd ask you to
think why. Is it because you can't be rude towards someone you probably
don't know online? Is it because in a public professional space you
can't swear? Is it so bad that you can't do these things? The Code of
Conduct doesn't ask for a lot and while you might say "We're all adults
here, we don't need one", consistently that just hasn't been the case in
this industry. We're always open to contributions we just want to have a
certain level of professionalism.

At Fastly we also have our own values and code of conduct and part of
this includes the values "We are trustworthy" and "We are good people"
and if we want others to trust us and if we want to be good people
looking out for our community's well being over the hurt feelings of an
individual who feels excluded because they can't be rude is a tradeoff
we'll make in a heartbeat.

In summary while a Code of Conduct does exclude people, it excludes the
kinds of people we'd rather not interact with and instead invites the
people who we would want to interact with into the community. We want
people who care and who are kind and compassionate. If you've read this
long do come join us. A better world of software where we are nice to
each other, where we build each other up, and build some really neat
things is possible.

[fastly-coc]: https://www.fastly.com/code-of-business-conduct-and-ethics/

---
## [eficode/cso-github-postgres](https://github.com/eficode/cso-github-postgres)@[8114e4478e...](https://github.com/eficode/cso-github-postgres/commit/8114e4478e672393d107f78fe341977b761eef40)
#### Thursday 2021-07-15 19:34:36 by Tom Lane

Further fix ALTER COLUMN TYPE's handling of indexes and index constraints.

This patch reverts all the code changes of commit e76de8861, which turns
out to have been seriously misguided.  We can't wait till later to compute
the definition string for an index; we must capture that before applying
the data type change for any column it depends on, else ruleutils.c will
deliverr wrong/misleading results.  (This fine point was documented
nowhere, of course.)

I'd also managed to forget that ATExecAlterColumnType executes once per
ALTER COLUMN TYPE clause, not once per statement; which resulted in the
code being basically completely broken for any case in which multiple ALTER
COLUMN TYPE clauses are applied to a table having non-constraint indexes
that must be rebuilt.  Through very bad luck, none of the existing test
cases nor the ones added by e76de8861 caught that, but of course it was
soon found in the field.

The previous patch also had an implicit assumption that if a constraint's
index had a dependency on a table column, so would the constraint --- but
that isn't actually true, so it didn't fix such cases.

Instead of trying to delete unneeded index dependencies later, do the
is-there-a-constraint lookup immediately on seeing an index dependency,
and switch to remembering the constraint if so.  In the unusual case of
multiple column dependencies for a constraint index, this will result in
duplicate constraint lookups, but that's not that horrible compared to all
the other work that happens here.  Besides, such cases did not work at all
before, so it's hard to argue that they're performance-critical for anyone.

Per bug #15865 from Keith Fiske.  As before, back-patch to all supported
branches.

Discussion: https://postgr.es/m/15865-17940eacc8f8b081@postgresql.org

---
## [Chris-plus-alphanumericgibberish/dNAO](https://github.com/Chris-plus-alphanumericgibberish/dNAO)@[a4a06a03ed...](https://github.com/Chris-plus-alphanumericgibberish/dNAO/commit/a4a06a03ed62d0409b9aff9e7cdd56a3a2855f8b)
#### Thursday 2021-07-15 22:12:12 by ChrisANG

Gehennom treasure upgrades

Valley of the Dead: one of each coin type, randomly placed.

Juiblex (demon lord): 6x wage of gluttony, 12x demon vault loot.

Zuggtmoy (demon lady): 6x wage of gluttony, 12 demon vault loot.

Yeenoghu (demon lord): 3x wage of gluttony, 3x wage of wrath, 12x demon vault loot.

Baphomet (demon lord): 3x wage of pride, 3x wage of wrath, 12x demon vault loot

Kostchtchie (demon lord): 3x wage of wrath, 3x wage of sloth, 12x demon vault loot

Malcanthet (demon prince): 3x wage of lust, 3x wage of envy, 18x demon vault loot

Graz'zt (demon prince): 3x wage of pride, 3x wage of lust, 18x demon vault loot

Avatar of Lolth (demon prince): 2x wage of pride, 2x wage of lust, 2x wage of envy, 18x lolth vault loot

Pale Night (demon prince): up to 36x tannin vault loot at various probabilities
-Pale Night buffed with a summon tanninim spell (1/36 casts, summons shalosh or teraphim.
-Pale Night gains acid resistance (so her own shalosh can't kill her)

Orcus (demon prince): 3x wage of pride, 3x wage of sloth, 18x demon vault loot.

Dagon (tannin prince): up to 36x tannin vault loot at various probabilities scattered in the water.
-Dagon buffed with khaamnun pull and drain level attacks

Demogorgon (prince of demons): 3x wage of pride, 3x wage of envy, 6x ancient vault loot, 6x angel vault loot, 6x tannin vault loot, 6x devil vault loot, 12x demon vault loot.

Lamashtu (prince of demons): 3x wage of envy, 3x wage of greed, 12x angel vault loot, 12x tannin vault loot, 12x demon vault loot.
-Lamashtu armies supplemented with angels.

Bael (archdevil): 6x wage of wrath, 3x wage of envy, 9x angel vault loot, 9x devil vault loot, 9x demon vault loot.

Dispater (archdevil): 3x wage of greed, 3x wage of envy, 3x wage of pride, 6x ancient vault loot, 3x angel vault loot, 18x devil vault loot.

Mammon (archdevil): a whole mess of wages of greed, 27x devil vault loot.

Belial/Fierna/Naome (archdevils): 3x wage of lust, 3x wage of pride, 3x wage of wrath, 27x devil vault loot, 9x75% devil vault loot.

Leviathan has no loot at present. Loot should be included when "opening" the creature in the ice.

Lilith (archdevils and court): 6x wage of envy, 6x wage of lust, 6x wage of pride, 27x devil vault loot.

Baalzebub (archdevil): 6x wage of pride, 3x wage of sloth, 18x angel vault loot, 9x devil vault loot.

Mephistopheles (archdevil): 3x wage of pride, 2x wage of greed, 2x wage of envy, 2x wage of wrath, 27x devil vault loot, 9x75% devil vault loot.
Baalphegor (ancient of hell): 4x antimagic rift, 4x catapsi vortex, 24x ancient vault loot, 8x75% ancient vault loot.

Asmodeus (archdevil): 3x antimagic rift, 3x catapsi vortex, 3x wage of pride, 27x devil vault loot, 27x90% devil vault loot, 27x45% devil vault loot, random soul coins with the treasury golems.

---
## [juansirai/aboutMe](https://github.com/juansirai/aboutMe)@[576ef57ac4...](https://github.com/juansirai/aboutMe/commit/576ef57ac427a47a98baa8a0539c8c1b4c847275)
#### Thursday 2021-07-15 23:53:52 by Juan B. Sirai

README.md

# 🙌 Hi everyone!
*My name is Juan, I'm 28 years old, and I live in La Plata, Buenos Aires (ARG).*

* 🎓I have a degree in Bussiness, and I am studying IT. Yes, I love learning! 
* 💡 I complemented my background with **Data Analytics** and **Data Science** studies.
* 📈 Currently working as an analyst of planing and control, at EDELAP (energy industry).
* 📫 If you want to contact me: juanbsirai664@gmail.com

---

# [<](2021-07-14.md) 2021-07-15 [>](2021-07-16.md)

