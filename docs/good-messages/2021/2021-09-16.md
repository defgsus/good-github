# [<](2021-09-15.md) 2021-09-16 [>](2021-09-17.md)

3,193,813 events, 1,649,279 push events, 2,481,132 commit messages, 183,154,729 characters


## [aozbolt/OOSE_HW2_temp](https://github.com/aozbolt/OOSE_HW2_temp)@[4e772a0a6f...](https://github.com/aozbolt/OOSE_HW2_temp/commit/4e772a0a6fc1c96847f03af4756030dacbbba987)
#### Thursday 2021-09-16 01:25:00 by aozbolt

Merge pull request #2 from aozbolt/feature3

FUCK YOU

---
## [Jrchintu/CDN](https://github.com/Jrchintu/CDN)@[51e84cd378...](https://github.com/Jrchintu/CDN/commit/51e84cd378eb1422893bcbb3a2c17a7b8499e618)
#### Thursday 2021-09-16 07:48:41 by CHINTU

Fuck you shell

https://bbs.archlinux.org/viewtopic.php?id=197349

---
## [MemedHams/Shiptest](https://github.com/MemedHams/Shiptest)@[2743030b17...](https://github.com/MemedHams/Shiptest/commit/2743030b17b8fa1354375c975218e1ff46d69c63)
#### Thursday 2021-09-16 09:32:39 by MemedHams

Miners are a menace to society, and are fundamentally dangerous to our cultural stability. In today's essay I will

Contents:
-Loot pool adjusted to remove some currently-broken items, and to add new items.
-Guardian spawner now has a 25% chance to spawn a selectable version. (spin the wheeel)
-Immortality 2 has finally been released, and now it fits into pockets and has a modular funny scream. 
With the power of rebalance, not only can you now use your hands to pray for forgiveness while bubblegum waits patiently outside your void patch, you can also directly use the ability from your hotbar, hands free.
(Cooldown and active time have both been increased by 1.5 of their original value, pending possible increase to 2.0 if abusable.)

---
## [odoo/odoo](https://github.com/odoo/odoo)@[d2953b58dd...](https://github.com/odoo/odoo/commit/d2953b58dd8de1c65690d3d89ea426be7ae20267)
#### Thursday 2021-09-16 10:00:57 by Yannick Tivisse

[IMP] hr_work_entry: Improve work entries generation perf

Purpose
=======

The work entries generation is not scaling properly
for large employee datasets. A lot of improvements
have been made to reduce the number of query to the
database. However, one of the remaining bottleneck
was the new records insertion, made 1 by 1 on the
database.

Due to the fact that the model doens't have a lot
of columns, and doens't have columns containing
too much data (HTML fields for example), we could
imagine inserting the records by batch, for example
1000 by 1000, as it is done with SELECT.

The results weren't satisfying enough, so we prefered
using the new cron triggers mechanism, to re-trigger
the same cron at the end of its execution in another
transaction, to avoid exceeding the non return point
from which posgreSQL is pedaling in the semoule.

With this commit, it takes less than 30 seconds to
generate the work entries for 1000 employees, instead
of 3 min, and allows to generate the work entries in
a linear execution time instead of a exponential one.

Create method analysis:
=======================

Note, that only the call to the create method is
tracked, not the preprocessing time to retrieve
the work entries values.

When inserting the records 1 by 1:
----------------------------------

records   - Elapsed Time (s)   - AVG Time per record (s)
10        - 0.0045862197875976 - 0.00045862197875976
432       - 0.1101946830749511 - 0.00025508028489572
192       - 0.0525383949279785 - 0.00027363747358322
522       - 0.1418645381927490 - 0.00027177114596312
892       - 0.2803149223327636 - 0.00031425439723404
8800      - 3.4928441047668457 - 0.00039691410281441
88000     - 188.82338452339172 - 0.00214572027867490
176000    - 1172.4313135147095 - 0.00666154155406084

We observe that from 10.000 new record, the create
method is not scaling anymore before this commit.

For a company with 1000 employees, the mean work
entries by month is 1000*2*21=42.000 work entries,
and the time to create the records is not acceptable.

When inserting the records 1000 by 1000
---------------------------------------

records   - Elapsed Time (s)   - AVG Time per record (s)
10        - 0.003406763076782  - 0.0003406763076782
432       - 0.075028181076049  - 0.0001736763450834
192       - 0.030718326568603  - 0.0001599912842114
522       - 0.084228038787841  - 0.0001613563961452
892       - 0.145947217941284  - 0.0001636179573332
8800      - 2.204506397247314  - 0.0002505120905962
88000     - 181.9736533164978  - 0.0020678824240511
176000    - 1146.928646564483  - 0.0065166400372981

We observe a sligh improvement, but cleary not enough
and not worth the complexity of inserting the records
in batch on the database.

Note: The following explanation is speculative and could
be validated with a real analysis, but according to the
results we obtained with the cron, this is most likely
to be true.

In fact, this is a limitation of PosgreSQL. In the
implementation that manages the transactions, the number
of inserts becomes greater than certain memory limits,
suddenly it falls on a slower alternative storage (this
is the problem with SELECT and tuples of ids ).

When we inject 100,000 ids into a query, PosgreSQL parses
the query (already there it could have some trouble), then
it stores the ids in a data structure: hash if not too
large, other (on the file system) otherwise. Then it
executes the query and uses the data structure to check
the validity (membership).

Specification
=============

Instead of inserting the record in batch in the same
transaction, it would be more interesting to split the
transaction into several transactions:
Each transaction process N employees who have not yet been
processed, until there are no more. With the new cron
trigger mechanism, it is possible to:
- When the cron runs, it processes N employees
  (N to be determined)
- If there is still some, he retriggers itself at the end
  of his transaction
Like that, the cron is scheduled 1x / day, but it is
retriggered as many times as necessary each month.

Regarding the number of employees to process, with 100
employees, we can expect:

100 * 2 (morning / evening) * 21 (working days) = 4200

work entries to generate, which is manageable given
the above measures.

TaskID: 2646056

---
## [Lamella-0587/Skyrat-tg](https://github.com/Lamella-0587/Skyrat-tg)@[6a1231406b...](https://github.com/Lamella-0587/Skyrat-tg/commit/6a1231406b8bac8996bf232a5ea249a8df70c71d)
#### Thursday 2021-09-16 10:08:26 by Lamella-0587

fuck you, i removed these items because they are copying new ones. And i will do it again and again. THIS IS WAR!!!

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[ad62a65b42...](https://github.com/mrakgr/The-Spiral-Language/commit/ad62a65b42f1577e4ae13193ac2239d22742bdbd)
#### Thursday 2021-09-16 10:37:38 by Marko GrdiniÄ‡

"10:10am. The gaming session was too intense, but I am almost done with Ender Lilies and should get back to the old habit soon.

10:20am. Let me start. Any mail? SO asking me about my job status. Updated to open, but not looking.

Let me get going. What I want to do right now is look into the nested inference papers. ProbProg is no good if it cannot match tabular CFR in its chosen domain. It should be able to express it.

https://arxiv.org/abs/2106.11302
Nested Variational Inference

> We develop nested variational inference (NVI), a family of methods that learn proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate densities, which can serve as heuristics to guide the sampler. Our experiments apply NVI to (a) sample from a multimodal distribution using a learned annealing path (b) learn heuristics that approximate the likelihood of future observations in a hidden Markov model and (c) to perform amortized inference in hierarchical deep generative models. We observe that optimizing nested objectives leads to improved sample quality in terms of log average weight and effective sample size.

http://auai.org/uai2018/proceedings/papers/92.pdf
Nesting Probabilistic Programs

> We formalize the notion of nesting probabilistic programming queries and investigate the resulting statistical implications. We demonstrate that while query nesting allows the definition of models which could not otherwise be expressed, such as those involving agents reasoning about other agents, existing systems take approaches which lead to inconsistent estimates. We show how to correct this by delineating possible ways one might want to nest queries and asserting the respective conditions required for convergence. We further introduce a new online nested Monte Carlo estimator that makes it substantially easier to ensure these conditions are met, thereby providing a simple framework for designing statistically correct inference engines. We prove the correctness of this online estimator and show that, when using the recommended setup, its asymptotic variance is always better than that of the equivalent fixed estimator, while its bias is always within a factor of two.

Let me start with the last one.

> For example, one might use such nesting to model a poker player reasoning about another player

I think I read this paper in 2019.

> Before jumping into a full formalization of nested inference, we first consider the motivating example of modeling a poker player who reasons about another player. Here each player has access to information the other does not, namely the cards in their hand, and they must perform their own inference to deal with the resulting uncertainty

11:10am. 8/10. I can't follow what this 5 line algorithm is supposed to be doing.

11:15am. Nevermind. If it is important I'll look up the implementation somewhere.

Let me read the NVI paper.

> NVI is particularly useful in the context of deep probabilistic programming systems. Because NVI can be applied to learn proposals for a wide variety of importance samplers, it can be combinedwith methods for inference programming that allow users to tailor sampling strategies to a particular probabilistic program. Concretely, NVI can be applied to nested importance samplers that are defined using a grammar of composable inference combinators [Stites et al., 2021], functions that implement primitive operations which preserve proper weighting, such as using samples from one program as a proposal to another program, sequential composition of programs, and importance resampling.

No I do not get it. And there isn't an algorithms section. Is there ref code provided? Ah, I see there is a large appendix.

11:45am. This paper has no code provided. I guess I'll just assume it is like regular VI except nested. Let me watch the 5m video on it.

https://www.youtube.com/watch?v=1mukAGMEYNQ
AABI 2021 - Nested Variational Inference

11:55am. The 5m video is a decent bit clearer. They just define intermediate objective and densities and optimize them locally.

https://arxiv.org/abs/2109.00301
âˆž-former: Infinite Memory Transformer

This caught my eye on the sidebar. Let me take a look.

12pm. No forget this, I do not have the mental energy to study random things. I'll keep it in mind for the future.

12:05pm. A goal is forming in my mind.

If I am really going in the right direction, I should be able to express CFR in a probabilistic language.

This is the way to do it. If you want to create a man, you must first create the basic mammal and work your way up. The reason why I failed is because I leapt from tabular CFR to deep RL and fell into the chasm instead. There wasn't a bridge so of course I just try to make the leap.

There is a veritable jungle of methods and papers out there? How am I to make my way around? Step by step.

12:10pm. Based on how things are, I can immediatelly eliminate Turing, Soss and Omega as implementation libraries since they do not have explicit addressing. Gen and Pyro are in the maybe zone. For all I know they might only accept string addresses.

It gives me a concrete goal.

Right now, if you look at the tutorials it is all boring statistics shit with low scale models. The reason why I ignored PPLs up to now is because I could not make a connection between them and RL. If I saw even a single example of them being used for RL instead of statistics geekery I would have looked at it much more closely.

12:25pm. It is time to start getting ready for the next step. Since I haven't heard from Z for this long I am starting to wonder what will come out of this. I guess if I do not hear from him by the end of the month, I'll fire an email and close the thread if he is not interested.

People should do paid work only if they really want to. What I did in the last month was awkward, I could not cut it in RL so I slinked away like a loser into the safety of mediocrity.

There is only one way that I want to win, and that is by making agents. It does not matter to me that I have to endure this poverty until I get to them. There is no path towards AI that does not involve me making agents in some way or form. If I am doing arbitrary work then I am not traversing the path itself.

12:30pm. This is good. My mindset is starting to adjust back to what it was earlier in the year when I was hopeful about making the agent. I am getting the feeling of determination back.

This is the best.

12:35pm. I can win this!

What I'll do next is try out Pyro and Gen and see whether they can accept int and tuples of them as arguments. If they can I'll have cleared the first hurdle.

For now let me have breakfast."

---
## [Rust405/WADAssignment](https://github.com/Rust405/WADAssignment)@[8ed5fec7e6...](https://github.com/Rust405/WADAssignment/commit/8ed5fec7e661d23585d9a82a6ab386f4ec836309)
#### Thursday 2021-09-16 12:55:53 by russelllen405@outlook.com

Included sql database into git cuz fuck you visual studio

---
## [mquevill/sgtpuzzles](https://github.com/mquevill/sgtpuzzles)@[c0da615a93...](https://github.com/mquevill/sgtpuzzles/commit/c0da615a933a6676e2c6b957368067ca1bc10abd)
#### Thursday 2021-09-16 13:00:34 by Simon Tatham

Centralise initial clearing of the puzzle window.

I don't know how I've never thought of this before! Pretty much every
game in this collection has to have a mechanism for noticing when
game_redraw is called for the first time on a new drawstate, and if
so, start by covering the whole window with a filled rectangle of the
background colour. This is a pain for implementers, and also awkward
because the drawstate often has to _work out_ its own pixel size (or
else remember it from when its size method was called).

The backends all do that so that the frontends don't have to guarantee
anything about the initial window contents. But that's a silly
tradeoff to begin with (there are way more backends than frontends, so
this _adds_ work rather than saving it), and also, in this code base
there's a standard way to handle things you don't want to have to do
in every backend _or_ every frontend: do them just once in the midend!

So now that rectangle-drawing operation happens in midend_redraw, and
I've been able to remove it from almost every puzzle. (A couple of
puzzles have other approaches: Slant didn't have a rectangle-draw
because it handles even the game borders using its per-tile redraw
function, and Untangle clears the whole window on every redraw
_anyway_ because it would just be too confusing not to.)

In some cases I've also been able to remove the 'started' flag from
the drawstate. But in many cases that has to stay because it also
triggers drawing of static display furniture other than the
background.

---
## [JosephJomama/tgstation](https://github.com/JosephJomama/tgstation)@[e8c25e495e...](https://github.com/JosephJomama/tgstation/commit/e8c25e495e08065e3349f5d4719b58710c11fd0b)
#### Thursday 2021-09-16 14:27:36 by JosephJomama

I'm really fucking stupid and added gunner jellyfish as an afterthought without building afterwards

god fucking forgive me please

---
## [JosephJomama/tgstation](https://github.com/JosephJomama/tgstation)@[1a82194fe7...](https://github.com/JosephJomama/tgstation/commit/1a82194fe7b6677467e0639f6471bfb4ef194fd3)
#### Thursday 2021-09-16 14:27:36 by JosephJomama

Unfucks the typo, undocumented changes that I left in by accident removed

god forgive my soul

---
## [goonstation/goonstation](https://github.com/goonstation/goonstation)@[bce68a709f...](https://github.com/goonstation/goonstation/commit/bce68a709f30e39fffee5caaf14dd2e09e2178da)
#### Thursday 2021-09-16 15:01:34 by Azrun

QM Special Orders - Minor Event (#6024)

* Stamps Stamps Stamps Stamps Stamps

* Pizza, Blood, and Weedz

* Organ swaperoo

* Chef Orders: Breakfast, Lunch, Dinner and Snack Orders!

* Fix name

* Drug 'em and take off useful gear

---
## [rpilcrow/Citadel-Station-13-RP](https://github.com/rpilcrow/Citadel-Station-13-RP)@[f141da2de5...](https://github.com/rpilcrow/Citadel-Station-13-RP/commit/f141da2de5e111aa226dce4206edb93ad9384239)
#### Thursday 2021-09-16 15:11:15 by nik707

Merge pull request #2906 from Citadel-Station-13/silicons-patch-38

i guess i have to review voremob prs from now on because mimics just broke my goddamn linter and people need to remember that just because the last guy didn't code well doesn't mean there's an excuse to copy awful code

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[303db5023f...](https://github.com/mrakgr/The-Spiral-Language/commit/303db5023f0ba74a4b12375db6c3309472657a2c)
#### Thursday 2021-09-16 17:35:54 by Marko GrdiniÄ‡

"1:50pm. Let me do a bit more chilling.

2:20pm. Let me start. Right now I am thinking about neuro symbolic programming.

https://www.youtube.com/watch?v=4PuuziOgSU4
MIT 6.S191 (2020): Neurosymbolic AI

I posted this before. Z mentioned it during the meet and it caught my interest.

http://introtodeeplearning.com/

This is supposed to be lecture 7 from here.

https://www.youtube.com/watch?v=toTcf7tZK8c&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=7
MIT 6.S191: Evidential Deep Learning and Uncertainty

I guess they changed up the course work.

https://www.youtube.com/results?search_query=neuro+symbolic

There are plenty of interesting videos out here.

https://www.youtube.com/watch?v=vNOTDn3D_RI
Gary Marcus: Toward a Hybrid of Deep Learning and Symbolic AI | Lex Fridman Podcast #43

Let me watch this interview.

https://youtu.be/vNOTDn3D_RI?t=2087
> LF: There is a huge amount of limitations which you've outlined with deep learning, but the nice feature of deep learning is whatever it is they are to accomplish, it does a lot of stuff automatically without human interwention.

> GM: That is part of why people love it. I always think of this quote from Bertrand Russell: "It has all the advantage of theft over honest toil."

https://youtu.be/vNOTDn3D_RI?t=2181
> GM: I've never seen anybody build a system that in a roboust way can actually watch videos and predict exactly which containers would leak and which ones wouldn't.

This reliability problem is exactly what I was worried about in poker.

https://youtu.be/vNOTDn3D_RI?t=2193
> GM: So Yann Lecunn who is my colleague at NYU for many years think that the hard work should go into defining an unsupervised learning algorithm. It will watch videos, use the next frame in order to tell it what is going on, and he thinks that is the royal road and he is will to put in the work in devising that algorithm. Then he wants the machine to do the rest. And again, I understand the impulse. My intuition based on years of watching this stuff and making predictions 20 years ago that still hold even though there is a lot more computation and so forth is that we have to do a different kind of hard work, which is more like building a design specification for what we want the system to do. Doing hard engineering work like Yann did for convolutions in order to figure out how to encode complex knowledge into the systems. Current systems do not have that much knowledge other than convolutions.

> GM: People do not want to do that work. Then do not want to see how to naturally fit one within the other.

https://youtu.be/vNOTDn3D_RI?t=2377
> LF: One possibility just I wondered what you think about is - deep neural networks do form abstraction but they are not accessible to use humans. In terms of, we can't...

> GM: There is some truth in that.

> LF: Is it possible that either current or future neural networks form very high level abstractions which are as powerful as our human abstractions of common sense? We just can't get a hold of them and so the problem is essentially we need to make them explainable.

> GM: It is an astute question, but I think the answer is at least partly no. One of the kinds of classical neural network architectures is what we call an auto associator. It just tries to take an input, goes through a set of hidden layers and comes out with an output and it is supposed to learn the identity function. That your input is the same as your output. You can think of it as binary numbers, you've got 1,2,4,8 the 16 and so forth. And if you want to input 24, you turn on the 16 and you turn on the 8. It like the binary 11 and bunch of 0s. So, I did a bunch of experiments in 1998 with the precursors of contemporary deep learning. And what I showed was that, you can train these networks on all the even numbers and it would never generalize to the odd numbers. A lot of people thought that I was an idiot or faking the experiment or wasn't true, or whatever. But it is true that with the class of networks we had in the day they would never make those generalizations. It is not that the networks were stupid, it is that they see the world in a different way than we do. They were basically concerned with what is the probability that the rightmost output node is going to be one. And as far as they were concerned on everything they've been trained on it was a zero. The node had never been turned on so why turn it on now?

> GM: Whereas a person would look at at the same problem and say it is obvious - we just doing the thing that corresponds. The latin word for that is mutatis mutandis. What changed, what needs to be changed. And we do this, this is what algebra is. I can do f(x)=x+2 and I can do it for a couple of values, I can tell you if x is 3 then f(x) is 5, x is 2 then f(x) is 4 and now I can do it for some totally different number like a million because now obviously it is a million and 2. You have an algebraic operation that you are applying to a variable. And deep learning systems kind of emulate that, but they don't actually do it. The particular example...you can fudge a solution to a particular problem, but the general form of that problem remains. What they learn is really correlations between different input and output nodes. They are complex correlations, with multiple nodes involved and so forth, but ultimately they are correlative, they are not structured over these operations over variables. Some day people would do a new form of deep learning that incorporates that stuff, and I think it will help a lot and there is some tenative work on things like differentiable programming right now that falls into that category. But the sort of classic stuff that people use for Imagenet doesn't have it and you have people like Hinton going around saying symbol manipulation like what Marcus - what I advocate is like the gasoline engine. It is obsolete. We should just use this cool electric power that we've got with that deep learning. And that is really destructive. Cuz we really do need to have the gasoline engine stuff that represents - I do not think it is a good analogy (Lex laughs), we do need to have the stuff that represents symbols.

Shots fired. It is worth trying out. I am sure I won't get anywhere with the classical stuff.

4:35pm.

https://youtu.be/vNOTDn3D_RI?t=3483
> GM: ...But just because you have the innate stuff doesn't mean you don't learn anything. So many people get that wrong, including in the field. The people think if I work in machine learning, the learning side, I must not be allowed to work on the innate side. That would be cheating. People said that to me and it's just absurd.

https://youtu.be/vNOTDn3D_RI?t=3717
> LF: How efficient is do you think is evolution?
> GM: Oh it is terribly inefficient except that once it gets a good idea it runs with it.

5:10pm. https://www.youtube.com/watch?v=kAaKs_04dEk
Logical Neural Networks: Toward Unifying Statistical and Symbolic AI by Alexander Gray, Ph.D.

Maybe I should have skipped the GM interview as it spent a lot of my time. Let me watch this next for a bit.

> Recently there has been renewed interest in the long-standing goal of somehow unifying the capabilities of both statistical AI (learning and prediction) and symbolic AI (knowledge representation and reasoning). We introduce Logical Neural Networks, a new neuro-symbolic framework which identifies and leverages a 1-to-1 correspondence between an artificial neuron and a logic gate in a weighted form of real-valued logic. With a few key modifications of the standard modern neural network, we construct a model which performs the equivalent of logical inference rules such as modus ponens within the message-passing paradigm of neural networks, and utilizes a new form of loss, contradiction loss, which maximizes logical consistency in the face of imperfect and inconsistent knowledge. The result differs significantly from other neuro-symbolic ideas in that 1) the model is fully disentangled and understandable since every neuron has a meaning, 2) the model can perform both classical logical deduction and its real-valued generalization (which allows for the representation and propagation of uncertainty) exactly, as special cases, as opposed to approximately as in nearly all other approaches, and 3) the model is compositional and modular, allowing for fully reusable knowledge across talks. The framework has already enabled state-of-the-art results in several problems, including question answering.

This is the abstract. Reading things like this, I am reminded how I am 99% effort and 1% inspiration when it comes to AI. I am yet to attain anything and am just barely hanging on.

5:25pm. https://youtu.be/kAaKs_04dEk?t=549

This is giving me flashbacks to the prob theory book by Jaynes where he argued that rules of probability are just an extension of logic. I am going to have to go through the rest of that book at some point. I might as well since I am going down this path.

https://youtu.be/kAaKs_04dEk?t=844

Real value logics are completely new to me. I am just a frog in a well.

Ah, I remember this being refered to being as unnormalized Bayesian stuff.

https://youtu.be/kAaKs_04dEk?t=1042

Hmmmm, instead of thinking this are as a replacement for the Bayesian stuff, what would happen if I used that in conjuction with probabilistic programming?

https://youtu.be/kAaKs_04dEk?t=1169

I am guessing MILP here stands for Mixed Integer Linear Programming.

This is an amazing talk. I should have watched it weeks ago. I am glad that I decided to it today on a whim.

5:35pm. https://youtu.be/kAaKs_04dEk?t=1755

I should be studying this. I swear I am going to get it right the third time around.

6:15pm. https://youtu.be/kAaKs_04dEk?t=2527

Yeah, this is interesting. It actually touches RL.

https://youtu.be/kAaKs_04dEk?t=2633
> Reinforcement learning
    - Generally massive number of trials needed
    - Generally no knowledge
> Goal: Use knowledge to dramatically reduce the number of trials needed.

This is new stuff only from last year. If I looked at it in 2018 or 2019 I would not have found it. It is not in any of the courses which are tabular RL -> deep RL.

https://youtu.be/kAaKs_04dEk?t=2719

It learns interpretable rules? That could be huge.

It seems I am going to have to adjust my plans again. I have no idea what the hell is going on anymore. One day I am Bayesian, and now I am into (L)NNs.

https://youtu.be/kAaKs_04dEk?t=3378
> Foundations of mathematics
- Reformulating the foundations of mathematics through the lens of a bit rather than a set. What if qubit is a foundational mathematical construct.

6:55pm. Forgot about lunch. Let me have pizza while I finish the video. Less that 20m left.

7:30pm. Done with lunch and the video. Let me close here. This is classical me. I was slacking during the day, and then I only got interested towards the end so I put in overtime. The LNN paper is 40 pages. I'll read it tomorrow.

I am going to find a way to put this all together. LNNs and probabilistic programming and RL. That will give me the victory that I seek.

Right now, let me do some gaming. I only have one stone tablet left to find in Ender Lilies and then I will be done with the game. So far it has been very good, I am having the time of my life with it."

---
## [DarviL82/PBar](https://github.com/DarviL82/PBar)@[c4454e0172...](https://github.com/DarviL82/PBar/commit/c4454e0172df9a54fd2bbb64907f1ca844599c41)
#### Thursday 2021-09-16 18:57:08 by DarviL82

god this needs a fucking rewrite... pain. :trollface:

---
## [volkova-lisa/Codelab3](https://github.com/volkova-lisa/Codelab3)@[8e14900199...](https://github.com/volkova-lisa/Codelab3/commit/8e14900199eefb43f39cd4faea1372a90bfe236b)
#### Thursday 2021-09-16 19:25:17 by Your Name

such a bullshit codelab omg spent 10 hours because i'm stupid and THERE IS NO FREAKING INFO about how make this shit work

---
## [JoshCheek/open-api-mocker](https://github.com/JoshCheek/open-api-mocker)@[d17ecdf44b...](https://github.com/JoshCheek/open-api-mocker/commit/d17ecdf44b5f8a65221ea72d3ac9fbfbf3fe1916)
#### Thursday 2021-09-16 21:42:14 by Josh Cheek

Uhm... whatevz

* Passed the path down to the response generator because we wanted
  to be able to reference it (eg /users/123 should return a user
  whose ID is 123)... but then that didn't work for some reason
  that I forgot, but was really annoying and then we gave up.
* Then we came back here because `x-faker: random.arrayElement(["a", "b", "c"])`
  wasn't working on enums. Then I got nerd-sniped and rearranged
  it so that it always checked for faker before it checked any of
  the other ones, so that it would work on enums, too.
* Then just refactored a bit
* And then realized that they use tabs and I was using spaces, but
  my tabstop was set to 2, so I didn't notice... So that's
  embarrassing, but I'm not really planning on upstreaming it,
  and don't feel like taking the time to go back and reindent shit,
  so here it lies in its wonky ass state.

Co-authored-by: Marissa Biesecker <marissa.biesecker@gmail.com>

---
## [newstools/2021-iol](https://github.com/newstools/2021-iol)@[fb632a9f2f...](https://github.com/newstools/2021-iol/commit/fb632a9f2f38931e82b3b720aaa8febab70e881d)
#### Thursday 2021-09-16 23:17:58 by Billy Einkamerer

Created Text For URL [www.iol.co.za/news/south-africa/gauteng/love-triangle-ends-in-bloodshed-as-taxi-owner-shoots-ex-and-her-alleged-boyfriend-3dcec1bf-aa19-402c-ad0f-67ceaf54cf75]

---

# [<](2021-09-15.md) 2021-09-16 [>](2021-09-17.md)

