# [<](2021-05-15.md) 2021-05-16 [>](2021-05-17.md)

2,102,345 events, 1,164,747 push events, 1,648,700 commit messages, 88,416,282 characters


## [LemonInTheDark/tgstation](https://github.com/LemonInTheDark/tgstation)@[c1e2ba748a...](https://github.com/LemonInTheDark/tgstation/commit/c1e2ba748a642f096ce892f8b665905619f4dee9)
#### Sunday 2021-05-16 01:47:43 by LemonInTheDark

Prevents players from pressing the haha funny HELL BUTTONS WHAT THE FUCK OLD ME WHY DIDN'T YOU CATCH THIS

---
## [GrantRoberts/Colour-Is-Everything](https://github.com/GrantRoberts/Colour-Is-Everything)@[193d92c9dc...](https://github.com/GrantRoberts/Colour-Is-Everything/commit/193d92c9dc5ab045fc1c2051b4a61f271ceecac9)
#### Sunday 2021-05-16 03:35:22 by Grant

Initial Unity commit, with URP default crap, yeah pushd on main fuck you

---
## [Danny7007/Trivia-Murder-Party](https://github.com/Danny7007/Trivia-Murder-Party)@[5f6d35f43c...](https://github.com/Danny7007/Trivia-Murder-Party/commit/5f6d35f43c7f1d95f33a02ab1432b75b7076a803)
#### Sunday 2021-05-16 05:45:25 by Danny7007

Added Scratch-Off

Akihabara, the electric town--hub for state-of-the-art technology, fashion, otaku goods, and the cutest boys in skirts you will ever see. Pop into the Girls! Girls! Girls!? Cafe and fall in love with one of four charming "ladies..." or maybe even all of them? Jump into a story full of fluff, romance, friendship, and crossdressing that will keep you coming back for more!

---
## [nikitaanand16/Medizin-by-Crazy-Coders](https://github.com/nikitaanand16/Medizin-by-Crazy-Coders)@[e3c9807cba...](https://github.com/nikitaanand16/Medizin-by-Crazy-Coders/commit/e3c9807cba0f83155141a22178561c7d452351ae)
#### Sunday 2021-05-16 06:28:25 by nikitaanand16

Update README.md

Problem : Today, in this pandemic situation all over the world, our lives have become difficult. People are afraid to go out of their homes and they especially don’t want to go to hospitals as there is a high chance of disease spread. This situation is even worst during complete Lockdown situations. Also, on other hand, doctors are facing problems in diagnosing the disease at an early stage. They diagnose based on their knowledge and their experience. But, when a new disease breaks out, it is difficult for them to find out the relation between medical data they get from a healthy person and a patient victimized by the disease. Besides these problems, many people go to hospitals and crowd the place for very small health issues which can be cleared out in a call or chat. And sometimes during an emergency, people always face difficulty finding out doctor available at service to help them out. All these problems call out a need to come up with a technology that would connect both categories of people: medical help seekers and providers together all-time 24/7.

Solution : The Three Main Objective of this idea are: • Providing Medizin Chatbot Service • Providing effective E-consultancy Service • Providing AI Expert to Doctors We have brainstormed our ideas to connect Doctors and Patients who are separated by distance and current pandemic situations. Through this website, Patients can ask their queries to Doctors at any time. Their query will be posted in the doctors circle and the doctor is provided with options to answer the query, report the query and see the medical history of a patient who posted the query etc... The patient can also find out details of doctors who are currently available to help them via call and they can check out the doctor replies to all cases posted on the website publicly and filter replies for their case alone using their unique patient ID. The admin will be given a feature to authorize the patients and doctors on the website.

The AI Expert and Medizin Chatbot module of this idea will add widen the use case of this idea. Using AI Expert, doctors would predict diseases like heart disease, corona based on X-ray scans, etc. based on Machine Learning Models and pre-defined datasets. The Medizin Chatbot will be developed using NLP which would assist patients by providing details required by them like Hospital address, contact, website, and recommend best hospitals near them for EyeCare, etc. on request. It would allow the patient to request a live chat with a doctor from any hospital who is registered on this website. Doctors can accept and answer multiple patients' queries in this Live chat 24*7. They will be paid based on patients review and the number of cases solved in a month. All these features add good value to our Medizin website and it is more likely to eliminate the need of going to hospitals in many families – thus solving the problem defined.

Technologies will be used for implementing this idea : • Html • CSS • Bootstrap • Django • Flask • MongoDB • MySQL • PHP • JavaScript • Machine Learning • Data Visualization

How we Implemented our idea : Our Idea consists of three different modules. Both of us together worked on each module for one-third of the hackathon duration and with our best, we finished our idea implementation on time.

Project Progress: ~ 90% completed

Business Plan: 1. Patients pay consultancy fees for doctors after each V-check-up. Only after they pay, they can continue with the next check-up. (rs. 160 per e-consultancy) 2. Doctors get paid based on the review they get from patients and the number of cases they solved in a particular month. (rs. 135 per e-consultancy) 3. We charge a small and affordable percent of pay on doctors and as well as patients for our profit. (rs.25 per e-consultancy).

With further improvements in the business plan and technical features, we believe that this idea will be a successful product in the market, solving all defined problems.

---
## [unit0016/Skyrat-tg](https://github.com/unit0016/Skyrat-tg)@[66bf090fbe...](https://github.com/unit0016/Skyrat-tg/commit/66bf090fbe817520903e614ac43d90d043851bca)
#### Sunday 2021-05-16 07:39:00 by Unit0016

Revert "go fuck yourself"

This reverts commit 8b90946491335f71130597a36d8c183d32425aff.

---
## [PARAdoxiBLE/Democracy4KoreanTranslation](https://github.com/PARAdoxiBLE/Democracy4KoreanTranslation)@[14e60094c9...](https://github.com/PARAdoxiBLE/Democracy4KoreanTranslation/commit/14e60094c9598c7c37984ece1b121b9ae467d7bc)
#### Sunday 2021-05-16 09:01:24 by PARAdoxiBLE

Update media.csv

0. 아래 각각 armedpolice_lack, caremissions_lack 발췌
"His wife <FEMALENAME> and daughter <FEMALENAME> talk to us tonight about what they think of the government."
"Tonight, angry classic car salesman <MALENAME>, Explains how the this governments disastrous <POLICYNAME> policy is wrecking their once thriving business. <MALENAME> may have to close his 25 year old business after government policy has made the cars he sells virtually worthless."
전자는 FEMALENAME, 후자는 MALENAME이 두 번 나오는데 별다른 표시 없이도 전자는 아내와 딸 (미치지 않고서야 당연히 별개의 인물), 후자는 인터뷰 대상자 (별일이 없으면 당연히 동일인물) 이름을 표시하는데 사용하는 모양인데, 테스트를 좀 해보는게 맞지 않나? 이걸 테스트를 어케하지? 아무 뉴스나 뜨는 테스트 세이브파일 하나 가져다놓고 해당하는 텍스트 덮어씌우는 방식을 써볼까?

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[969039e6e6...](https://github.com/mrakgr/The-Spiral-Language/commit/969039e6e6ca8e3daedc29264789027865ea1b79)
#### Sunday 2021-05-16 11:02:30 by Marko Grdinić

"10:20am. I ended up going to bed at 2am, so it is no wonder I am waking up this late today.

Rather than programming I want to do some more research on Hebbian learning. I doubt I'll resolve anything, but that is what I want to do.

The way Miconi does it or that fast weights paper by Hinton is not the answer. It does not feel like it.

Now that I've solved some of the other things, Hebbian learning is probably the most mysterious part of the brain. One thing I can understand is that having Hebbian learning would allow easy reversal learning. For regular NNs, I do not think that reversing the output would get inputs that resemble the ones from the input distribution, but Hebbian learning would allow it.

10:25am. Hmmm...how would using Hebbian learning for doing the majority of the learning, and using something like backprop for just learning the scale of the weights work? And I do not need to update the Hebbian weights at every timestep like in Miconi's work, it can be done in a separate phase...

No, actually it is fine if I mutate it. If the goal is to do exact backprop it would be wrong. But if the method is about reconstructing inputs then taking the weights as they are to determine the score is the correct answer.

https://www.sciencedirect.com/science/article/pii/S2352154621000280
Hebbian learning revisited and its inference underlying cognitive function

> Despite the recent success of deep learning in artificial intelligence, the lack of biological plausibility and labeled data in natural learning poses a challenge in understanding biological learning. At the other extreme lies Hebbian learning, the simplest local and unsupervised one, yet considered to be computationally less efficient. The recent development of Hebbian learning re-evaluates its contribution to natural learning and memory association. It highlights the efficiency of Hebbian learning combined with supervised learning in forming a low-dimensional and coarse representation, and its role in many cognitive tasks by providing a basis activity patterns and dynamics. Deriving the form of Hebbian learning from in-vivo data bridges the neural and behavioral data and requires utilizing both spatial and temporal activity changes.

Let me find the pdf for this.

...I can't find it.

https://www.nature.com/articles/nn.4158?proof=t
Inferring learning rules from distributions of firing rates in cortical neurons

https://www.nature.com/articles/srep28073
A Local Learning Rule for Independent Component Analysis

10:45am. I am going to have to try Libgen for Sukbin Lim's papers.

10:50am. No luck. Forget it. There are plenty of other papers on this subject though.

https://scholar.google.com/scholar?as_ylo=2021&q=hebbian+learning&hl=hr&as_sdt=0,5

10:55am. I guess I'll do reading today again. Let me just finish the article by Moldbug so I can clear that tab.

11:10am. https://graymirror.substack.com/p/there-is-no-ai-risk

I think that Moldbug's IQ really did fall at some point in his journey. Seriously dude, even in the worse case, a super AI could make millions a month by doing a thousand lowly paid programming jobs at once which it could use to buy more hardware to do even more work. It boogles my mind how he thinks Rennaisance is the top limit on what superintelligence could do. Endless capacity for work and endless inspiration would allow it to corner any market it decides to enter fairly quickly.

And yes, he is right that there is an IQ gap limit between a leader and a follower, but it does not remotely apply in the context of AI since it would have powerful self control. I might be a weirdo myself, but I have the awareness to know that my internal systems are fucking me over in social situations by making me bored. And AI could deal with that sort of internal issue easily. And moreover, an AI would not need to lead or control humans, just other AI agents. Alternatively, an IQ 200 AI could create an IQ 120 AI which could lead the 100 IQ humans if it does not feel like doing the work on its own.

Just who is he writing these articles for? 90 IQ proles?

11:25am. Let me go back to deep learning.

https://arxiv.org/abs/2102.00428
PyTorch-Hebbian: facilitating local learning in a deep learning framework

https://journals.physiology.org/doi/abs/10.1152/jn.00712.2020
Classic Hebbian learning endows feed-forward networks with sufficient adaptability in challenging reinforcement learning tasks

https://ieeexplore.ieee.org/abstract/document/9321229
Bidirectional Associative Memories: Unsupervised Hebbian Learning to Bidirectional Backpropagation

https://arxiv.org/abs/2103.09002
Hebbian Semi-Supervised Learning in a Sample Efficiency Setting

https://arxiv.org/abs/2104.07959
Evolving and Merging Hebbian Learning Rules: Increasing Generalization by Decreasing the Number of Rules

A lot of paper on Hebbian learning came out. Strange that there is nothing in 2018 and 2019. Did Scholar drop the data from those years?

11:40am. That networks trained with Hebbian learning would be easy to reverse is important insight. I am not sure if anybody is aware of it, but I'd bet the brain takes great advantage of it. It is more realistic than training GANs in the way I've suggested.

https://arxiv.org/abs/2104.07959
Evolving and Merging Hebbian Learning Rules: Increasing Generalization by Decreasing the Number of Rules

This paper was not much.

https://arxiv.org/abs/2103.09002
Hebbian Semi-Supervised Learning in a Sample Efficiency Setting

Let me go for this one next. I want to see some papers with combine HL with backprop for things like learning the scale and the like.

12pm. It is nothing much.

https://ieeexplore.ieee.org/abstract/document/9321229
Bidirectional Associative Memories: Unsupervised Hebbian Learning to Bidirectional Backpropagation

http://sipi.usc.edu/~kosko/BAM-SMC-January-2021.pdf
> Bidirectional backpropagation lets users run deep classifiers and regressors in reverse as well as forward. Bidirectional training exploits pattern and synaptic information that forward-only running ignores.

Bidirectional? This might be something. Let me take a break here.

12:10pm.

> Every real matrix is bidirectionally stable.

What does this mean.

http://sipi.usc.edu/~kosko/B-BP-SMC-Revised-13January2018.pdf
Bidirectional Backpropagation

Oh what is this?

12:20pm. The BAM paper is complicated and I am just skimming it right now.

> Backpropagation is a special case of the generalized EM algorithm for maximum-likelihood estimation of parameters [5], [21].

There is a lot in the BAM paper.

What is backpropagation invariance?

12:25pm. I'll leave the bidi paper for later.

Classic Hebbian learning endows feed-forward networks with sufficient adaptability in challenging reinforcement learning tasks

Let me go for this next.

No, I can't find the pdf for it. Forget it.

https://arxiv.org/abs/2103.10252
Augmenting Supervised Learning by Meta-learning Unsupervised Local Rules

> The brain performs unsupervised learning and (perhaps) simultaneous supervised learning. This raises the question as to whether a hybrid of supervised and unsupervised methods will produce better learning. Inspired by the rich space of Hebbian learning rules, we set out to directly learn the unsupervised learning rule on local information that best augments a supervised signal. We present the Hebbian-augmented training algorithm (HAT) for combining gradient-based learning with an unsupervised rule on pre-synpatic activity, post-synaptic activities, and current weights. We test HAT's effect on a simple problem (Fashion-MNIST) and find consistently higher performance than supervised learning alone. This finding provides empirical evidence that unsupervised learning on synaptic activities provides a strong signal that can be used to augment gradient-based methods.

> We further find that the meta-learned update rule is a time-varying function; thus, it is difficult to pinpoint an interpretable Hebbian update rule that aids in training. We do find that the meta-learner eventually degenerates into a non-Hebbian rule that preserves important weights so as not to disturb the learner's convergence.

This is pretty promising.

...No, it is nothing much.

12:45pm. https://direct.mit.edu/neco/article/33/5/1300/97484/Contrastive-Similarity-Matching-for-Supervised

Huh, actually I hadn't made the connection between contrastive learning and Hebbian learning. Maybe I should think of them as being the same.

> This is because our weight updates solve a minimax problem.

This is promising. I'd expect any unsupervised scheme to do this. What I cannot connect is how the brain does it so quickly.

1pm. https://arxiv.org/abs/2103.09985
A deep learning theory for neural networks grounded in physics

https://arxiv.org/abs/2104.01677
A contrastive rule for meta-learning

https://arxiv.org/abs/2104.04657
Meta-Learning Bidirectional Update Rules

1pm. Let me have breakfast here."

---
## [Tunguso4ka/RFU-.NET-Framework](https://github.com/Tunguso4ka/RFU-.NET-Framework)@[465aee2c81...](https://github.com/Tunguso4ka/RFU-.NET-Framework/commit/465aee2c81fc3b3ab9d2b202c8bb5350b8218686)
#### Sunday 2021-05-16 15:01:09 by Tunguso4ka

0.1.7 / new icon, about crash fix / I love Stef and i feel pain in my heart after his "arm incident"...

---
## [dastrukar/zre4pghd](https://github.com/dastrukar/zre4pghd)@[5e25880eaf...](https://github.com/dastrukar/zre4pghd/commit/5e25880eaf51de8a890eb1770caa4d6424d4b612)
#### Sunday 2021-05-16 15:36:47 by dastrukar

Fix player items not glowing after a level

Holy shit, this was a pain in the ass to fix.
It seems like it would be easy to fix, but nothing ever comes easy now
does it?

A few things that I've learned from this experience:
* Player doesn't spawn in when moving to the next level(?)
* The Inventory is weird as shit
* Player's Inventory doesn't get loaded at the same time as the player
spawns

Also, removed some unneccessary Destroy() calls, since EventHandler only
lasts for a level. (found that out the hard way)

---
## [cashapp/sqldelight](https://github.com/cashapp/sqldelight)@[e9075da800...](https://github.com/cashapp/sqldelight/commit/e9075da800eb96b3d35863c752a02435ebbc7896)
#### Sunday 2021-05-16 16:04:42 by Alexander Perfilyev

Qualify column name intention (#2405)

hell yea, love that it shows you which tables have that column name

---
## [bajajfiinance/Amazon-customer-care.-09064017013](https://github.com/bajajfiinance/Amazon-customer-care.-09064017013)@[65e5898703...](https://github.com/bajajfiinance/Amazon-customer-care.-09064017013/commit/65e589870319934e321cd514fa1559adbb99ebf1)
#### Sunday 2021-05-16 16:06:35 by bajajfiinance

Amazon customer care. 9064017013

Amazon.com
E-commerce company

OverviewPeople also search for
amazon.in
Amazon.com, Inc. is an American multinational technology company based in Seattle, Washington, which focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence. Wikipedia
CEO: Jeff Bezos (May 1996–) Trending
Customer service: 1800 3000 9009
Contact.09064017013
Stock price: AMZN (NASDAQ) $3,222.90 +61.43 (+1.94%)
14 May, 4:00 pm GMT-4 - Disclaimer
Founder: Jeff Bezos
Founded: 5 July 1994, Bellevue, Washington, United States
Headquarters: Seattle, Washington, United States
Subsidiaries: Audible, Zappos, Whole Foods Market, AbeBooks, Amazon Fresh, Amazon.ae, Ring
Learning Center>Startup Stories>Amazon Startup Story
Amazon Startup Story
Introduction
This startup story features Jeffrey P. Bezos, the innovative founder of Amazon. The company, which now generates over $61 Billion in Revenue and holds the title as the world’s largest  online retailer, was started out of Bezos’s garage at 30 years old.

Amazon Stats:
Industry: Online Retailing
Annual Revenue: $61.09 Billion
# of Employees: 97,000
Famous For: Being the world’s largest online retailer
How Amazon Got Started
The year was 94′ and Bezos was working diligently on Wall Street. At 30 years old, he began to see the internet revolution take place, and made the decision to quit his job and start an internet company.

“The wake up call was finding this startling statistic that web usage in the spring of 1994 was growing at 2,300 percent a year. You know, things just don’t grow that fast. It’s highly unusual, and that started me about thinking, “What kind of business plan might make sense in the context of that growth?” 

After making a list of the ‘top 20’ products that he could potentially sell on the internet, he decided on books because of their low cost and universal demand. It turns out, it was just the beginning…..

The Founder’s Start
As a child, he spent summers at his grandfather’s ranch in southern Texas, “laying pipe, vaccinating cattle and fixing windmills”. The 18-year-old Bezos “said he wanted to build space hotels, amusement parks and colonies for 2 million or 3 million people who would be in orbit. ‘The whole idea is to preserve the earth’ he told the newspaper …. The goal was to be able to evacuate humans. The planet would become a park”. 

Amazon’s Funding
The initial startup capital came from his parent’s personal savings.

From an interview with Jeff Bezos, for the Academy of Achievement:

“The first initial start-up capital for Amazon.com came primarily from my parents, and they invested a large fraction of their life savings in what became Amazon.com. And you know, that was a very bold and trusting thing for them to do because they didn’t know. My dad’s first question was, “What’s the Internet?” Okay. So he wasn’t making a bet on this company or this concept. He was making a bet on his son, as was my mother. So, I told them that I thought there was a 70 percent chance that they would lose their whole investment, which was a few hundred thousand dollars, and they did it anyway.”

Follow on Funding
Amazon raised a series A of $8M from Kleiner Perkins Caufield & Byers in 1995. In 1997, Amazon went public to raise additional capital. By 1999, the value of the Kleiner Perkins Caufield & Byers investment in Amazon created returns of over 55,000%.

Years to profitability
Within two months, Amazon’s sales were up to $20,000/week. However, the company has continued to plow their revenue back into growth. The chart below depicts Amazon’s continued focus on long-term growth, with profit remaining near $0 or below, and revenue rising.

Amazon Profitability

Important Amazon Milestones:
1994: Jeff Bezos quits his job and launches Amazon out of his garage.
             Within 30 Days, it is doing $20,000 per week in sales.

1995: Bezos raises an $8 Million round of funding from Kleiner Perkins.

1997: Amazon goes public at $18 per share.

1999: Bezos is named Time Magazine’s “Person of the Year” for popularizing online shopping.

2009: Bezos acquires Tony Tsieh’s Zappos through a stock swap.

2013: Bezos acquires the Washington Post.

Companies Amazon Has Acquired:
Amazon has made over 44 notable company acquisitions over the years. It’s first Acquisition was in 1998.

1998: PlanetAll, Junglee, Bookpages.co.uk (later became Amazon UK).
1999: Internet Movie Database (IMDb), Alexa, Accept.com, and Exchange.com
2003: CDNow (Defunct)
2004: Joyo.com, an e-commerce site in China.
2005: BookSurge, Mobipocket.com, and CreateSpace.com.
2006: Shopbop, a women’s luxury retailer.
2007: DPReview.com and Brilliance Audio.
2008: Audible.com, Fabric.com, Box Office Mojo, AbeBooks, Shelfari, and Reflexive Entertainment.
2009: Zappos,  Lexcycle, SnapTell,  Stanza (Kindle Rival).
2010: Touchco., Woot, Quidsi, BuyVIP, and Amie Street.
2010: Toby Press
2011: LoveFilm, The Book Depository, Pushbutton, and Yap
2012: Kiva Systems, Teachstreet, and Evi
2013: IVONA Software, GoodReads, and Liquavista
Jeff Bezos Startup Advice
“We are stubborn on vision. We are flexible on details…. We don’t give up on things easily. Our third-party seller business is an example of that. It took us three tries to get the third-party seller business to work. We didn’t give up.”

“If you’re not stubborn, you’ll give up on experiments too soon. And if you’re not flexible, you’ll pound your head against the wall and you won’t see a different solution to a problem you’re trying to solve.”

COMMENTS
No comments yet.
GET OUR WEEKLY NEWSLETTER
Join over 40,000 of your peers who receive regular updates on Fundable, crowdfunding, and starting a business.

you@email.com
SUBSCRIBE
RECENT STARTUP STORIES
March 27, 2014
Patron Tequila Founder - John Paul DeJoria
January 20, 2014
DC Shoes Startup Story
January 20, 2014
Pom Wonderful Startup Story
January 20, 2014
Samuel Adams Startup Story
January 20, 2014
Spanx Startup Story
Ready to Start Raising Capital?

GET STARTED 9
GET STARTED
Start a Fundraise
Investor Signup
Account Login
BROWSE
Trending
Recently Funded
New & Noteworthy
Communities
RESOURCES
FAQ
Guidelines
Contact Us
LEGAL
Privacy Policy
Terms of Service
FOLLOW
the startups.com platform
Startups Education
Startup Planning
Access Mentors
Secure Funding
Reach Customers
Virtual Assistants
Copyright © 2021 Startups.com. All rights reserved.
Fundable is a software as a service crowdfunding platform. Fundable is not a registered broker-dealer and does not offer investment advice or advise on the raising of capital through securities offerings. Fundable does not recommend or otherwise suggest that any investor make an investment in a particular company, or that any company offer securities to a particular investor. Fundable takes no part in the negotiation or execution of transactions for the purchase or sale of securities, and at no time has possession of funds or securities. No securities transactions are executed or negotiated on or through the Fundable platform. Fundable receives no compensation in connection with the purchase or sale of securities.

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[76104e7286...](https://github.com/mrakgr/The-Spiral-Language/commit/76104e7286511157bfe5879f66c3b88a045c0ef4)
#### Sunday 2021-05-16 16:32:15 by Marko Grdinić

"1:25pm. Done with breakfast. Let me read the Hellrage MTL for a bit and then I will resume.

1:40pm. Let me resume my reading. I'll do the chores later.

1:50pm. Using HL with backprop learning the scale is too obvious. It is impossible that nobody else tried it before. This is unlike the GAN idea.

So it has to be something else. Right now my understanding of HL is too simple. Being able to reverse inputs is an important principle, but it is not enough.

1:55pm. Actually, let me take a break for a while longer. These papers are complex and right now I feel distracted. I should do the chores.

2:35pm. Done with chores and the bidi backprop paper. I am not sure what to think of it.

I understand that there is zero chance of me figuring out how to fit Hebbian learning into the current framework of deep learning. That talk by Simon Thorpe did shake me up a bit. I started to get confident after figuring out the GAN trick, but the talk reminded me just how far I need to go.

There is no point in playing around with Hebbian learning on the GPUs. If people still haven't figured it out by then, I'll gave it a shot myself once I get those neurochips.

https://arxiv.org/abs/2104.04657
Meta-Learning Bidirectional Update Rules

Let me go for this paper next.

2:40pm. This paper is interesting. It links to many of the references which were of interest to me. I wonder what the proposals will be.

2:55pm. It was a timewaster.

https://arxiv.org/abs/2104.01677
A contrastive rule for meta-learning

Just having some complicated rules and doing benchmarks is not doing me any favors. Let me just go through these.

> When posed with a sequence of tasks that bear some relation to one another, a learning system that is capable of meta-learning will be able to improve its performance as the number of tasks increases. A successful meta-learner discovers shared structure across tasks and modifies its own inductive bias accordingly [1, 2]. Eventually, the system is able to generalize to new problem instances given little additional data. Such ability is believed to be a hallmark of human intelligence [3], and it has been observed in non-human primates as well as in other animal species [4]. Determining which mechanisms support this form of learning in the brain is a fundamental question in neuroscience [5].

I wish neuroscience contributed some actual algorithms rather than being a prophet.

> When applied to learn the weights of a Hopfield network, equilibrium propagation gives rise to a variant of the well-known contrastive Hebbian learning rule [17–21]. Here, we exploit an equivalence between the training of energy-based neural networks and implicit metalearning and show that, when applied to meta-learning problems, equilibrium propagation gives rise to a new meta-parameter learning rule. We term this rule contrastive meta-learning.

I actually do not know much about EP. I should look it up.

3:20pm. https://www.youtube.com/results?search_query=equilibrium+propagation

Done with the paper. It is not something I need. There is a simple rule, plus a lot of math. I do not really understand how it works, but that does not matter. Let me move to the next paper, and then I'll take a look at the EP talk.

https://arxiv.org/abs/2103.09985
A deep learning theory for neural networks grounded in physics

This one is next.

https://arxiv.org/abs/2102.00428
PyTorch-Hebbian: facilitating local learning in a deep learning framework

After that it is this one. After that I'll take a look at the 20' papers. Then I'll take a look at the EP talks.

Oh, that paper is a 110 page PhD.

...No I do not have time for it. Though it is all about equilibrium propagation so maybe the method will be developed to fruition in the context of neurochips in the future. It seems that right now the training is hard and does not match up to backprop.

3:30pm. Not much in the second paper.

https://www.biorxiv.org/content/10.1101/2020.12.30.424888v1.full.pdf
Orchestrated Excitatory and Inhibitory Learning Rules Lead to the Unsupervised Emergence of Up-states and Balanced Network Dynamics

Let me take a look at this next.

It is some biology thing. Forget it.

3:35pm. https://arxiv.org/abs/2104.04132
Replay in Deep Learning - Current Approaches and Missing Biological Elements

> Replay is the reactivation of one or more neural patterns, which are similar to the activation patterns experienced during past waking experiences. Replay was first observed in biological neural networks during sleep, and it is now thought to play a critical role in memory formation, retrieval, and consolidation. Replay-like mechanisms have been incorporated into deep artificial neural networks that learn over time to avoid catastrophic forgetting of previous knowledge. Replay algorithms have been successfully used in a wide range of deep learning methods within supervised, unsupervised, and reinforcement learning paradigms. In this paper, we provide the first comprehensive comparison between replay in the mammalian brain and replay in artificial neural networks. We identify multiple aspects of biological replay that are missing in deep learning systems and hypothesize how they could be utilized to improve artificial neural networks.

This seems really promising.

3:45pm. I'll actually save this paper for later. Most of my own innovation too are related to the use of the replay buffer whether that be GANs or RL methods. So far they are independent of the architectures and training algorithms used.

4:10pm. https://arxiv.org/abs/2101.05848
Unveiling the role of plasticity rules in reservoir computing

Let me go for this paper next.

> While most reservoir computing approaches consider a reservoir with fixed internal connection weights, plasticity was rediscovered as an unsupervised, biologically inspired adaptation to implement an adaptive reservoir. It appeared first as a type of Hebbian synaptic plasticity to modify the reservoir weights [11], but soon the ideas of nonsynaptic plasticity that inspired the first Intrinsic Plasticity (IP) rule [12] were also implemented in an Echo State Network [13]. After that, many different models of plasticity rules have been implemented in RC networks with promising results [14, 15, 16]. Today, the fact that biologically meaningful learning algorithms have a place in these models, together with recent discoveries suggesting that biological neural networks display RCs’ properties [17, 18], make reservoir computing a field of machine learning in continuous growth.

This is interesting. I had no idea that HL played a role in these kinds of nets.

4:20pm. Hmmm, I had an idea. Remember that data dependent initialization method I came up with which pushes the L1 norm of the activations to 1?

Isn't that similar to Hebbian learning?

I never worked out the rules exactly, but it would come to a variant of that wouldn't it?

4:30pm. https://arxiv.org/abs/2007.02686
Meta-Learning through Hebbian Plasticity in Random Networks

Forget the reservior computing paper. Hebbian learning is data dependent intialization isn't it? It is really foolish of me to not have made this connection before.

https://arxiv.org/abs/1511.06856
Data-dependent Initializations of Convolutional Neural Networks

Let me take a look at this old paper before I move on to the rest of them...It is nothing much.

Let me move to the one I linked.

4:45pm. https://arxiv.org/abs/2006.16558
Enabling Continual Learning with Differentiable Hebbian Plasticity

Let me make this the last paper. I am going down the wrong path with Hebbian learning. It is possible to conceptually split the mechnisms into local and more global objectives.

Locally, you can only constraint the layer to have activations that are on average of norm 1. You can give objectives to the individual actionations or it can be the whole layer. Working backwards from that, you arrive at the variants of the Hebbian learning rule depending on the norm used.

There is nothing magical about this. But now that I understand what HL is doing from this perspective, I can see that the neuroscience community is just geeking out about the only thing that they can see. They don't know the more important mechanisms so they are glued to this.

Let me read this more recent paper by Miconi and then I will move to the eq prop papers. After that I'll watch my hands of the HL business. Wasting one day on it is enough.

5:05pm. In constrast to his perivous papers this one is a head layer hack to improve contrinual learning. It does not seem to be that big of a deal on its own, but it performs best when combined with some other methods.

5:10pm. https://arxiv.org/abs/1711.09601
Memory Aware Synapses - Learning what (not) to forget

> Humans can learn in a continuous manner. Old rarely utilized knowledge can be overwritten by new incoming information while important, frequently used knowledge is prevented from being erased. In artificial learning systems, lifelong learning so far has focused mainly on accumulating knowledge over tasks and overcoming catastrophic forgetting. In this paper, we argue that, given the limited model capacity and the unlimited new information to be learned, knowledge has to be preserved or erased selectively. Inspired by neuroplasticity, we propose a novel approach for lifelong learning, coined Memory Aware Synapses (MAS). It computes the importance of the parameters of a neural network in an unsupervised and online manner. Given a new sample which is fed to the network, MAS accumulates an importance measure for each parameter of the network, based on how sensitive the predicted output function is to a change in this parameter. When learning a new task, changes to important parameters can then be penalized, effectively preventing important knowledge related to previous tasks from being overwritten. Further, we show an interesting connection between a local version of our method and Hebb's rule,which is a model for the learning process in the brain. We test our method on a sequence of object recognition tasks and on the challenging problem of learning an embedding for predicting <subject, predicate, object> triplets. We show state-of-the-art performance and, for the first time, the ability to adapt the importance of the parameters based on unlabeled data towards what the network needs (not) to forget, which may vary depending on test conditions.

Is this just learning the variance of the inputs and the gradients?

Before this I saw a paper that suggests what is essentially KFAC to improve continual learning.

5:20pm. > In the Elastic Weight Consolidation work [12], this is done based on an approximation of the diagonal of the Fisher information matrix. In the Synaptic Intelligence work [39], importance weights are computed during training in an online manner. To this end, they record how much the loss would change due to a change in a specific parameter and accumulate this information over the training trajectory. However, also this method has some drawbacks: 1) Relying on the weight changes in a batch gradient descent might overestimate the importance of the weights, as noted by the authors. 2) When starting from a pretrained network, as in most practical computer vision applications, some weights might be used without big changes. As a result, their importance will be underestimated. 3) The computation of the importance is done during training and fixed later. In contrast, we believe the importance of the weights should be able to adapt to the test data where the system is applied to. In contrast to the above two methods, we propose to look at the sensitivity of the learned function, rather than the loss. This simplifies the setup considerably since, unlike the loss, the learned function is not in a local minimum, so complications with gradients being close to zero are avoided

Sensivity of the learned function? So something like a layer compared to a norm loss. It seems reasonable. This is not bad insight.

...Ah no not that. It would just be the same as the regular gradient, except not optimized for some label.

...Ahhh, that gives me some insight!

In my own scheme I will be replacing the outright loss with -1 and 1 weighted implicitly by the sampling probability.

That will actually end up measuring the actual sensitivity, so I'll get the method in this paper automatically.

Mhhh, this is quite good!

Well, it is not exactly the same thing...I'd have to measure it against the class labels that aren't used.

Agh, nevermind.

No, the recommend the gradients of the squared l2 norm of the outputs.

5:35pm. They test the local version of the rule as well which corresponds to Hebbian learning and find that it helps. Yes, this is it. Besides initialization, this is what HL could be useful for.

> In this paper, we argued that, given a limited model capacity and unlimited evolving tasks, it is not possible to preserve all the previous knowledge. Instead, agents should learn what (not) to forget. Forgetting should relate to the rate at which a specific piece of knowledge is used. This is similar to how biological systems are learning. In the absence of error signals, synapses connecting biological neurons strengthen or weaken based on the concurrence of the connected neurons activations.

I liked this paper. It had some clean insight and it improved my understanding of ML.

Let me take a break here.

5:55pm. I am back.

https://brainscan.uwo.ca/research/cores/computational_core/uploads/11May2020-Lillicrap_NatNeuroRev_2020.pdf
Backpropagation and the brain

Let me finish things off today with this. I'll skip EqProp.

6pm. Neuroscience is too glued on Hebbian learning just because they can see it, and I let the video by Simon Thorpe unsettle me. It does not matter that the brain can find repeating patterns which HL is good at. ANNs can't do this easily. Just optimizing for the average layer norm is not enough to give insights for memory.

Once I get good enough hardware, I swear I will solve this mystery. But let me not fall into a hole.

To optimize, you need an objective function. Doing local norm optimization will not make learning magically happen at the grand scale the brain is capable of.

6:10pm. Ok, enough. I will stop for the day on time for once.

The long term goal is to get order of magnitude better neuromorphic hardware which will allow me to implement self improving agents.

I should not be unsettled by the things I can't do yet. The things Simon Thorpe showed in the video is definitely due to salience. Repeating patterns are due to salience. That is what makes them interested enough for the brain to store in long term memory.

The stacked GAN ideas I came up with are closer to that principle than anything else that exists today so I should not give up yet.

Who is to say that in the future I won't come up with even better memory systems? I should not assume that just because the brain does HL that it is the principle behind its memory capabilities.

6:15pm. Coming up with better memory systems should be left for the future.

Tomorrow, I will focus on programming. I will focus on accomplishing small goals that will eventually get me to the finish line. I've hit the limits of what is possible with my own research. I already can't try the GAN ideas without my GPU going up in flames.

6:20pm. Ahhh, it is wonderful. These research excursions have been fun. I've learned quite a bit.

But I should not forget where the power comes from. In the present, from implementing my own ideas and bringing those agents to life. In the future, from getting my hands on better hardware.

I should hold those two dreams close to my heart.

It is possible to get to the brain level efficiency. Once those memristor devices get below 10nm they should be 4 orders of magnitude potential improvement. Spikes should add 2 orders to that. That is what I have to look forward to in the 30s.

6:25pm. Though my ML skills have gone up an entire level in the last month, my programming has gone astray. I need to get back to the fundamentals. Small goals, one after another. I should not worry about how much work there is in total. I should worry about making the next step.

Surely I can do the uniform player? I should focus on that and then think about the linear player. I should make one step after the other and that will get me to where I want to go.

6:30pm. Right now, let me have fun. I am still fresh, and instead of beating myself to get better at ML, I should have fun reading Hellrage while in the background I prime myself to get into programming again. From ML at least, I should relax from learning new things for a year or two.

Time for lunch."

---
## [ActualMandM/Funkin](https://github.com/ActualMandM/Funkin)@[a9fdeedc02...](https://github.com/ActualMandM/Funkin/commit/a9fdeedc0212ab24e08256fb64f6e3c63e1794fe)
#### Sunday 2021-05-16 17:15:41 by ddavader

autoplay improvement (i think)

if it crashes then fuck you

---
## [phanthetue2309/Python-Fun-Family](https://github.com/phanthetue2309/Python-Fun-Family)@[b9ec59e38b...](https://github.com/phanthetue2309/Python-Fun-Family/commit/b9ec59e38b39927c383db0a9c1e59e392a46ae81)
#### Sunday 2021-05-16 17:18:01 by Phan Thế Tuệ

Somthing funny and memories

Code valentine  for my love ,HPBD my sister

---
## [stercoris/vk-tpcol-bot](https://github.com/stercoris/vk-tpcol-bot)@[9988692720...](https://github.com/stercoris/vk-tpcol-bot/commit/998869272097abb90155bf11d1b32302709be72c)
#### Sunday 2021-05-16 19:35:24 by Leeeeeeeeeeeeeeeeeeeee

fuck functional programming!
all my homies use OOP
o - oh my god
o - only thing i want
p - peace

---
## [DistroTEAM/to](https://github.com/DistroTEAM/to)@[9a81e78627...](https://github.com/DistroTEAM/to/commit/9a81e78627a855cdadb11c17d2c32879488c51e9)
#### Sunday 2021-05-16 19:48:57 by shabane

1- again task's are added, we shoud do them.
2- save the stable state.

i fealing love with the first angel of my life.
i hade to say this to some one. so who better than my self :)

love is too good. i like it.

---

# [<](2021-05-15.md) 2021-05-16 [>](2021-05-17.md)

