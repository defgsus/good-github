# [<](2021-04-17.md) 2021-04-18 [>](2021-04-19.md)

2,129,352 events, 1,235,390 push events, 1,700,181 commit messages, 100,307,926 characters


## [Shota5749/Palace-of-Pooches-Series@cc5e32561a...](https://github.com/Shota5749/Palace-of-Pooches-Series/commit/cc5e32561aa652c98c19a953ace680641f069d96)
##### 2021-04-18 06:24:20 by Shota

Update Girlfriend Who Absolutely Doesn't Want to Take a Bath vs Boyfriend Who Absolutely Wants Her to Take a Bath

---
## [Chamiu/web-blacklist@4ef8a5d036...](https://github.com/Chamiu/web-blacklist/commit/4ef8a5d036d352d8e610dd8275c7a2e999e8b0e1)
##### 2021-04-18 08:03:41 by Chamiu_IT

add 30 domain

以下を追加した。
- 774route.com
- applech2.com
- aurora-aurum.com
- bbhylife.com
- ereoh.com
- glen-air.com
- hachiblog-fan.com
- happier-days.com
- happy-enjoy-life-blog.com
- happyheart92.com
- hanahana-23.com
- healthy-life-blog.com
- hidekun-blog.com
- ishiishiishi.com
- kanazawa-ambi.com
- lifesimplelive88.com
- mamanochiwatashi.com
- marublog.biz
- matsu-journal.com
- niterune.com
- pedia-jp.com
- saolifegetwell.com
- science-festa.jp
- shinchan-social.jp
- shrine-carine.com
- tv-rider.jp
- urauraplus.com
- warsuksdo.com
- youkosatou0727.com
- yukoblog.net

---
## [mrakgr/The-Spiral-Language@dc64e93aab...](https://github.com/mrakgr/The-Spiral-Language/commit/dc64e93aabd3146c755f6050d7584b1416af3b5f)
##### 2021-04-18 11:37:30 by Marko Grdinić

"11:20am. Yesterday I went to bed and became obsessed. If it was the usual 2am, I'd be screwed right now, but I went a bit before 11am just as I said I would. These few extra hours to lounge around and digest my thoughts make a large difference in my restfulness levels. I think I'll exchange my game time for sleep from here on out.

Until the agents are training on full HU NL Holdem, I know what needs to be done.

At any rate, I had a lot of inspiration during the night.

Still, despite everything it is weird that it does not work.

```
d.regret |> am.mapInplace (fun i x => max 0 <| x + regret_prob * (index u i - us))
```

This definitely needs to be done like this.

Still it is strange that it is not working...

```
            inl ({sampled policy} as p) = {sampled=index sample_policy i; policy=index policy i}
            inl reward = r2_index (next (log_probm.from p,index actions i)) id
            inl u,us =
                am.initFold num_actions (fun s i' =>
                    inl q = index d.qs i'
                    inl r = if i = i' then (reward - q) / sampled + q else q
                    r, s + r * policy
                    ) 0
```

Hmmmm, what is this mult by `policy` here. That has to be wrong.

```
            inl ({sampled} as p) = {sampled=index sample_policy i; policy=index policy i}
            inl reward = r2_index (next (log_probm.from p,index actions i)) id
            inl u,us =
                am.initFold num_actions (fun s i' =>
                    inl q = index d.qs i'
                    inl r = if i = i' then (reward - q) / sampled + q else q
                    r, s + r * index policy i
                    ) 0
```

Yeah, it is much easier to detect these kinds of mistakes when you are not dead tired. When you are tired you subconciously hold back on spending your mental energy.

I have a lot of ideas on how to go beyond this, but the kind of underperformance I've been seeing is consistent with there being too much variance in the system. I've been suspicious of q values from the start, and I see that this was the right choice to make.

```
inl Alpha = 2 ** -2
```

Let me increase the alpha to something like this.

```
-0.5610703300594555
-0.6344380817983737
-0.31715759113673425
-0.27148150402704047
-0.3187543649780006
-0.35069259155225413
-0.3735738436749507
-0.37723432633420223
-0.3698352972916449
-0.3950146106655472
-0.39407466019689974
-0.40028191615902636
-0.4194888164392181
-0.44374797339999233
-0.45833883672833065
-0.46164837609531917
-0.4678660090508558
-0.46012899441229127
-0.45281219153225916
-0.4509295552518016
-0.4548415864866367
-0.4635267151240994
-0.46832436534614336
-0.47268056372531886
-0.4739774320685466
-0.4740602924656687
-0.47227454593963913
-0.47247597866269253
-0.4718230766351366
-0.47190353284438125
-0.471144835921283
-0.4705916127680415
-0.4664427359397114
```

No it is still not good enough.

KCRRCKR....Oh, it corrected the calling habit for this. The regret sums always raise now. It took a while for this to happen. Previously it had a bit of calling.

QRCQR

Avg strategy

F: 700
C: 52179
R: 448517

Regret

F: 0
C: 0
R: 38

Yes, the regrets are right, but the Avg Strategy has been corrupted.

11:40am. No, it is still not right. No matter how I slice it, the regret sums should not be so large in places. This is telling me that the system still has a problem with variance. I am going to print out the q values in the buffer. That should have been my priority yesterday.

```
inl regret_prob = exp (-(self_prob.sampled) + (chance_prob.policy - chance_prob.sampled) + (op_prob.policy - op_prob.sampled))
```

Let me reorder these operations just a bit.

11:45am. Ok, now focus me. Let me extend the UI.

11:55am. I don't know.

Take a look at this.

Trace:

QRCQR

Average Policy:

F: 0
C: 1477
R: 1682

Regret:

F: 0
C: 331
R: 29

Q:

F: 0
C: 3.05
R: 5.8

The Qs feels extremely reasonable here. Given what the Qs are, why aren't the regret sums being pushed in its direction?

There is still something wrong here.

On the next iteration Qs are `C: 5.35` and `R: 5.8`. The regrets though have been pushed entirely towards calling.

12pm.

On the next iteration `C: 6.7` and `R: 5.8`. This is not good.

12:05pm. Oh, it in this situation as it gets more data, it corrects. It is not a problem.

What about...

Trace:

JCCQCRR

Avg Policy:

F: 3162
C: 21964

Regret:

F: 211
C: 221

Q:

F: -4.1
C: -6.95

Why is it calling down with the weakest possible hand on a board like this.

Once again, the Q values seem reasonable. The policy itself not so much.

12:15pm.

Trace:

JCCJC

Avg Strategy:

C: 173758
R: 831663

Regret:

C: 1
R: 1.85

Q:

C: 1
R: 1.2

The Avg Strategy for this key has been developing at a predicatble pace. Previously R was 3, now it has gone closer to 1 presumably in response to the opponent's actions.

12:25pm. Let me study the regular CFR's replay buffer for comparison.

12:35pm. Ugh, sorting the buffer by the trace would make things a lot easier on me. Let me do it.

```py
            buffer = self.cb_show_buffer()
            buffer.sort(key=lambda x: x.trace)
            self.buffer_view.data = buffer
```

Let me do it like this.

```py
buffer.sort(key=lambda x: x['trace'])
```

Ah, whops, let me do it like this.

Yeah, now it is much easier to find `JCCJC`.

12:45pm. Let me stop updating the qs. I'll see what happens then.

Let me just say I know I am doing this inefficiently. The cause is almost certainly the variance of the updates. I have an idea how to do this a lot better. But there is an inkling that what I have now should work better, and I want to get all the sanity checking out of the way.

```
// set d.qs i ((1 - Alpha) * index d.qs i + Alpha * reward)
```

Let me get rid of this line. Now all the qs should be zero. I want to test my understanding of what should happen to regret sums once I do this.

12:55pm.

The training the program develops is quite predicable. The average strategy ratios do get maintained.

With the latest changes JCCJC is different.

C: 26511
R: 414259

JCCJCR

C: 142237
R: 100146

Ok, this last one boggles my mind. What the fuck?

Under straightforward accumulation of reward, it is impossible that C should be higher than R. Raising should always make what call does, plus extra when the other guy calls or reraises.

This is why I am going through this finely rather than just putting in a method with less variance right off the bat.

I need to verify that the update itself makes sense.

```
d.regret |> am.mapInplace (fun i x => max 0 <| x + regret_prob * (index u i - us))
```

Let me remove that `-us`. That will make the regret perfectly accumulative.

1:05pm. Right now JCCJCR is better, it calls 1 for every 1.3 times it raises. But the overall score is a lot worse than it was previously. So that variance reduction thing had some effect.

```
-1.6453137552176127
-1.8399172777029937
-2.1376050558318003
-1.983358752257276
-1.9137073695523097
-1.757066176621184
-1.6789213318085996
-1.5953536722442678
-1.5238552108291554
-1.4982876991509209
-1.4675517593619896
-1.4325132149330746
-1.3994194271801395
-1.3793457120893127
-1.3553392717616344
-1.3420378013538878
-1.3137334123109041
-1.3035952646045974
-1.3011785392980673
-1.2992089772547717
-1.2999946531936164
-1.3027948043053046
-1.2923498442376844
```

Take a look at this run.

JCCJCR

Avg Policy:

F: 148579
C: 707
R: 339

Regret:

F: 100
C: 0
R: 0

Somehow it got a large positive reward as the starting regret and it is not correcting it.

Ah, whops, I accidentally put in the id from the opponent player. Nevermind.

1:30pm. After running it for enough iteration I see that it starts adjusting the average strategy for `JCCJCR`. It is accumulating regrets and going towards raising.

```
-0.29565441715734037
-0.7091504516018011
-0.32747188673428307
-0.31645956947805076
-0.3672305720289004
-0.4352897911399727
-0.46459915946859487
-0.48095632220390305
-0.47031607486952354
-0.48070610294787397
-0.4783930197472404
-0.4902956567664199
-0.5073680273024885
-0.5195593000759695
-0.5170000542925999
-0.5124704749616762
-0.5125029166577972
-0.5032814870374764
-0.4943771232006149
-0.4914907387334547
-0.4930835979208264
-0.4960958317771228
-0.49513414316361964
-0.49219469441045255
-0.4894455225397292
-0.48578690444099937
-0.4798865097574413
-0.4729614466575853
-0.4704706974535122
-0.46902459446384565
-0.4652580713779778
-0.4634850707474814
-0.45942903529665724
-0.4546474366507698
-0.45119096376177437
-0.44982553979294965
-0.4500840635589592
-0.4485994444489899
-0.4470406483570333
-0.4432415924758418
-0.4402708540564878
-0.4382936220882635
-0.43625332455738175
-0.4334529752801502
-0.4314126126415988
-0.4283753252618038
-0.42552805768915913
-0.4208228541162967
-0.4186973057535357
-0.4168698154955106
-0.4156346586244369
-0.4141950345642328
-0.4123772985690197
-0.408616640739659
-0.40721549071864704
-0.4049817213475292
-0.40394215393285854
-0.4011619966424019
-0.39912426409452656
-0.3971530383068993
-0.3947418297631693
-0.3927090433357564
-0.3905497634026552
-0.38750210441565125
-0.38520015300861177
-0.38322807251526314
-0.38151252892421406
-0.379576681781424
-0.3777135520916157
-0.37635847655093324
-0.37477470671003205
-0.37302689880790985
-0.37219101333844684
-0.3710874708549448
-0.36926855614476956
-0.3666127246563982
-0.3654028395781559
-0.3641368643245511
-0.3633623290721416
-0.3624124500320811
-0.36104515650188296
-0.36007188021111186
-0.3588930588842163
```

It is not that good, but you can see from this that it is steadily improving.

```
            inl self_prob = player.prob
            inl op_prob = player'
            inl regret_prob = exp (-(self_prob.sampled) + (chance_prob.policy - chance_prob.sampled) + (op_prob.policy - op_prob.sampled))
            d.regret |> am.mapInplace (fun i x => max 0 <| x + regret_prob * (index u i - us))
            inl policy = regret_match d.regret
            inl avg_policy_prob = exp ((self_prob.policy - self_prob.sampled) - (chance_prob.sampled + op_prob.sampled))
            d.avg_policy |> am.mapInplace (fun i x => x + avg_policy_prob * index policy i)
            inl r = am.fold2 (fun s prob u => s + u * prob) 0 policy u
            if id = 0 then r2 r else r2 -r
```

I am convinced. There isn't anything wrong with what I have here.

```
            inl u,us =
                am.initFold num_actions (fun s i' =>
                    inl q = index d.qs i'
                    inl r = if i = i' then (reward - q) / sampled + q else q
                    r, s + r * index policy i
                    ) 0
```

I fixed the bug in the morning and this should be fine as well.

```
            inl policy = regret_match d.regret
            inl sample_policy = am.map (fun x => Epsilon * (1 / f64 num_actions) + (1 - Epsilon) * x) policy
            inl i : u64 = $"numpy.random.choice(!num_actions,p=!sample_policy)"
            inl ({sampled} as p) = {sampled=index sample_policy i; policy=index policy i}
            inl reward = r2_index (next (log_probm.from p,index actions i)) id
            inl u,us =
                am.initFold num_actions (fun s i' =>
                    inl q = index d.qs i'
                    inl r = if i = i' then (reward - q) / sampled + q else q
                    r, s + r * index policy i
                    ) 0
            // set d.qs i ((1 - Alpha) * index d.qs i + Alpha * reward)
```

Instead what I need are better q functions to reduce variance pure and simple.

1:35pm. Enough. Let me get breakfast."

---
## [ranvis/putty@c19e7215dd...](https://github.com/ranvis/putty/commit/c19e7215ddd1d6a890fdb94d89bc5ccb46151363)
##### 2021-04-18 15:00:08 by Simon Tatham

Replace mkfiles.pl with a CMake build system.

This brings various concrete advantages over the previous system:

 - consistent support for out-of-tree builds on all platforms

 - more thorough support for Visual Studio IDE project files

 - support for Ninja-based builds, which is particularly useful on
   Windows where the alternative nmake has no parallel option

 - a really simple set of build instructions that work the same way on
   all the major platforms (look how much shorter README is!)

 - better decoupling of the project configuration from the toolchain
   configuration, so that my Windows cross-building doesn't need
   (much) special treatment in CMakeLists.txt

 - configure-time tests on Windows as well as Linux, so that a lot of
   ad-hoc #ifdefs second-guessing a particular feature's presence from
   the compiler version can now be replaced by tests of the feature
   itself

Also some longer-term software-engineering advantages:

 - other people have actually heard of CMake, so they'll be able to
   produce patches to the new build setup more easily

 - unlike the old mkfiles.pl, CMake is not my personal problem to
   maintain

 - most importantly, mkfiles.pl was just a horrible pile of
   unmaintainable cruft, which even I found it painful to make changes
   to or to use, and desperately needed throwing in the bin. I've
   already thrown away all the variants of it I had in other projects
   of mine, and was only delaying this one so we could make the 0.75
   release branch first.

This change comes with a noticeable build-level restructuring. The
previous Recipe worked by compiling every object file exactly once,
and then making each executable by linking a precisely specified
subset of the same object files. But in CMake, that's not the natural
way to work - if you write the obvious command that puts the same
source file into two executable targets, CMake generates a makefile
that compiles it once per target. That can be an advantage, because it
gives you the freedom to compile it differently in each case (e.g.
with a #define telling it which program it's part of). But in a
project that has many executable targets and had carefully contrived
to _never_ need to build any module more than once, all it does is
bloat the build time pointlessly!

To avoid slowing down the build by a large factor, I've put most of
the modules of the code base into a collection of static libraries
organised vaguely thematically (SSH, other backends, crypto, network,
...). That means all those modules can still be compiled just once
each, because once each library is built it's reused unchanged for all
the executable targets.

One upside of this library-based structure is that now I don't have to
manually specify exactly which objects go into which programs any more
- it's enough to specify which libraries are needed, and the linker
will figure out the fine detail automatically. So there's less
maintenance to do in CMakeLists.txt when the source code changes.

But that reorganisation also adds fragility, because of the trad Unix
linker semantics of walking along the library list once each, so that
cyclic references between your libraries will provoke link errors. The
current setup builds successfully, but I suspect it only just manages
it.

(In particular, I've found that MinGW is the most finicky on this
score of the Windows compilers I've tried building with. So I've
included a MinGW test build in the new-look Buildscr, because
otherwise I think there'd be a significant risk of introducing
MinGW-only build failures due to library search order, which wasn't a
risk in the previous library-free build organisation.)

In the longer term I hope to be able to reduce the risk of that, via
gradual reorganisation (in particular, breaking up too-monolithic
modules, to reduce the risk of knock-on references when you included a
module for function A and it also contains function B with an
unsatisfied dependency you didn't really need). Ideally I want to
reach a state in which the libraries all have sensibly described
purposes, a clearly documented (partial) order in which they're
permitted to depend on each other, and a specification of what stubs
you have to put where if you're leaving one of them out (e.g.
nocrypto) and what callbacks you have to define in your non-library
objects to satisfy dependencies from things low in the stack (e.g.
out_of_memory()).

One thing that's gone completely missing in this migration,
unfortunately, is the unfinished MacOS port linked against Quartz GTK.
That's because it turned out that I can't currently build it myself,
on my own Mac: my previous installation of GTK had bit-rotted as a
side effect of an Xcode upgrade, and I haven't yet been able to
persuade jhbuild to make me a new one. So I can't even build the MacOS
port with the _old_ makefiles, and hence, I have no way of checking
that the new ones also work. I hope to bring that port back to life at
some point, but I don't want it to block the rest of this change.

---
## [mrakgr/The-Spiral-Language@93eb8da937...](https://github.com/mrakgr/The-Spiral-Language/commit/93eb8da9377671f21d7ccb8327b4461a15be3fcf)
##### 2021-04-18 17:35:00 by Marko Grdinić

"2:25pm. Done with breakfast. Let me do some reading, chores and then I will resume, just as usual. Recently gaming has been taking up my mental energy.

3:25pm. Done with chores. Let me resume. Before I go forward with predictive baselines, let me check out the learned infoset baseline section in that paper again.

I don't want to give up on the current way of doing things too soon. If possible I'd like to make it work as well. Do I just need a low alpha or do I need something other as well?

3:40pm. No lowering the alpha does nothing. I give up. I can't make the learned infoset version work as well as the original version. The error with these learned biases is not particularly lower than the alternative.

Let me go back to regular CFR for the moment.

JRRCJR

Average Policy:

F: 4
C: 5
R: 2642

One thing that strikes out at me is that regular CFR sometimes has these small positive values to fold and call the nuts.

I need investigate this anomaly.

```
am.fold2 (fun s prob reward => s +. reward *. prob) (r2 0) policy reward
```

Let me try returning the regular reward.

Ah, it happens even with that, and the effect is even more pronounced. I think what happens is that these values appear near the start of training when the policy is random. I don't entirely get it, but that is how it is.

Let me try simultaneous training next.

3:50pm. Cython sure takes a while to compile.

```
-0.3395432308075824
-0.2533599941779393
-0.22357341044814538
-0.17717199528748645
-0.16040157989521134
-0.13518814753114156
-0.12001452886851877
-0.11763171566456901
-0.10782204053922298
-0.09939194709745293
-0.09899111228458246
-0.09070187373657404
-0.08696618115847801
-0.08509719414513373
-0.08479629888002374
-0.084239434595478
-0.08257227901357497
-0.08229556783748593
-0.08173697444883012
-0.08009222404262709
```

Huh not what I expected. Even with regular rewards, the optimization here is a lot better than last time. Huh?

4pm. Maybe I am judging these nets wrong. Maybe I should train a net specifically to try and exploit the trained one and see how far it can get instead of pitting the average strategy against itself.

Nevermind that. Let me move on to predictive baselines for sampling agents. I'll leave this as it is.

4:20pm.

```
            inl qs : a _ _ = dict.toArray v.qs
            am.generic.map (fun (card, q) =>
                inl card = show_card card
                inl actions =
                    am.generic.map2 (fun k v =>
                        inl k,v = show_action k, pstr v in $"f'{!k}: {!v}'" : string
                        ) v.actions q
                    |> rm.join' ", "
                $"f'{!card} - {!actions}'"
                ) qs
            |> rm.join' "\n"
```

Had to open a new project. So far, fancy type systems might be good for refactoring, but they such when it comes to abstracting different algorithm. Just to get this to work, I needed to add a dependency on the game state so the algorithm is no longer generic. Maybe it would make more sense if instead of getting the card from the hidden state I instead return them along with the reward, but that is too fancy. Let me just go with this.

This kind of evolution is fine. Copy paste is fine.

Making changes with the help of the type system is a lot easier than in bottom up Spiral or regular Python.

4:25pm. It really takes Cython a shitload of time to finish compilation. I hate it.

```
-0.24985030697528868
-0.5297683960931245
-0.29955691269374596
-0.20538310500578483
-0.25347200896272687
-0.3564634096197068
-0.3673478740812035
-0.36785526337658514
-0.3720664432386738
-0.4046568398583635
-0.3946928213754751
-0.4010499968684864
-0.4170369125813405
-0.43044728219900535
-0.43966891227517924
-0.44344778648565486
-0.4479076014463991
-0.43772366964486836
-0.42803244375788
-0.42245095869443944
-0.4218485533659685
-0.42576311251129617
-0.42665293601435056
-0.4293559704201275
```

Can this even be considered better than what I had last time?

```
-0.24985030697528868
-0.5297683960931245
-0.29955691269374596
-0.20538310500578483
-0.25347200896272687
-0.3564634096197068
-0.3673478740812035
-0.36785526337658514
-0.3720664432386738
-0.4046568398583635
-0.3946928213754751
-0.4010499968684864
-0.4170369125813405
-0.43044728219900535
-0.43966891227517924
-0.44344778648565486
-0.4479076014463991
-0.43772366964486836
-0.42803244375788
-0.42245095869443944
-0.4218485533659685
-0.42576311251129617
-0.42665293601435056
-0.4293559704201275
-0.42861628114221006
-0.42613248963415495
-0.42289550603599535
-0.4220324115128709
-0.420761029595264
-0.42105641155768003
-0.4206826562365321
-0.42109162826641033
-0.41777609907263186
-0.4132777517880636
-0.4116206950644162
-0.41335259283105386
-0.4158231912643662
-0.416531835014454
-0.4178809066197949
-0.41613363808177717
-0.4133153416204787
-0.41142317482737933
-0.4109494518132317
-0.4105194516454972
-0.4096056164321112
-0.40822472284326883
-0.40663285824232204
-0.40358030307500803
-0.4026968701323125
-0.4015853238697169
-0.4007827725326575
-0.3993817432847223
-0.3979784067908261
-0.39621411520615935
-0.39548976262319574
-0.39443968364669424
-0.3939961147272076
-0.3932184247852337
-0.39240739907724415
-0.3912418977814896
-0.3904029826217338
-0.39005379537111906
-0.38909084341749145
-0.38823516233811006
```

My ideas are not working.

Let me move to the next idea. I considered letting the variance reduced rewards fall from the play actors. That would not be hard to implement. But, let me just do simultaneous training instead. That will give me the effect that I want.

```
0.017448824903161426
-0.6020023380754602
-0.445017886294411
-0.40276457869740856
-0.4281032329268171
-0.47603000344755764
-0.5060380553869251
-0.5220680937734106
-0.5256126073285112
-0.5481668679446376
-0.5366508565679076
-0.5438694222529797
-0.5585289426197309
-0.575361804287303
-0.5751289840774316
-0.5820690160333355
-0.5822122937652026
-0.5747685865601084
-0.5744449304063693
-0.5730775425921784
-0.5758594564588804
-0.5798485324633227
-0.5804462788648392
-0.5811505662621098
-0.5814299083107547
-0.5814523173169452
-0.5793221731226081
-0.5762788279503379
-0.574940729988424
-0.573388067947581
-0.5718290473601058
-0.5704419384348549
-0.567548854915559
-0.5626269688815173
-0.5606230755304181
-0.5608518187932621
-0.5629952552769387
-0.5646075966317599
-0.5663641923603613
-0.564439991367058
-0.5622414131221066
```

Simultaneous updating works even worse. Let me try simultaneous training for regular CFR.

```
-0.5185801492706632
-0.6332198926181151
-0.6100004112025169
-0.5886621631453562
-0.5643392101252186
-0.5508446043278278
-0.5323601351735765
-0.5193314432139406
-0.509049549150599
-0.5013967884775421
-0.4966938798583847
-0.4944338669791456
-0.49466546521875854
-0.49650631971712605
-0.501076951362041
-0.5076397445174856
-0.5120317347206857
-0.5160780864736452
-0.5199060104388914
-0.5237959259483089
-0.5266838487874135
-0.5286575685388746
-0.5304652840356651
-0.5311964588578318
-0.5328717675434951
-0.5337763153907666
-0.5339653827382869
-0.5336100619106019
-0.5333684887509709
-0.5331386720530027
-0.5326013122806179
-0.5318760537090518
-0.5309258473670383
-0.529941040656144
-0.528896981616053
-0.5279418874579223
-0.5270750391472212
-0.5265416051674353
-0.5259821736269742
-0.5258206706673221
```

The sampling player performs a tad better than regular CFR, but it could be trick of the draw.

5:05pm. I am thinking. Let me put a few comments into the package file.

5:15pm. Yeah, this is a complete failure.

I think the problem is that updating the policy based on the current sample is too much variance. What I should do is average out the updates. Maybe update the policy and the average strategy after the node has been sampled 100 times. Something like that.

The papers taught me a lot, but they haven't taught me the most important things to get this to work it seems.

Yeah, I still do not understand the algorithm.

This is going to take effort. I did the straightforward approach. I did my imagination training. But the next step would be to dig deeper. I cannot give up here.

Let me take a look at the papers again and then I will try to dig up the code. If the later fails, I might want to try my own ideas.

Even though it is not working for me right now, I do believe in the ideas in the paper.

What this failure means though is that I am going to have to study things more in depth.

6:05pm. Done with lunch.

https://github.com/rggibson/open-pure-cfr
> Pure CFR is a time and memory-efficient variant of the Counterfactual Regret Minimization (CFR) algorithm algorithm for computing strategies for an extensive-form game with imperfect information, such as poker. Unlike Monte Carlo CFR (MCCFR), Pure CFR samples a pure strategy profile (exactly one action assigned probability 1 at every state) on each iteration and performs an update assuming the players and chance follow the sampled pure profile. In practice, strategies improve faster over time than the original CFR algorithm. In addition, while previous CFR algorithms require floating-point values (doubles), Pure CFR uses integer values (ints) and thus uses approximately half the memory of other common variants. Furthermore, Pure CFR retains the theoretical guarantees of MCCFR, and thus converges (probabilistically) to an equilibrium profile in two-player zero-sum games. More details regarding Pure CFR can be found in my PhD dissertation. Credit goes to Oskari Tammelin for inventing the algorithm.

I am trying to dig up some of these new versions of MC CFR, but am not having any luck. I thought of doing something like sampling pure strategies before, back when I thought of doing Q updates by taking the max instead of threesholding towards the median, but changed my mind.

(Edit: I can't find anything. Everything on Github is vanilla CFR and regular outcome sampling before the latest variance reduction techniques.)

6:30pm. I am thinking.

At this point I am completely out of ideas how I could make the sampling algorithm work through sheer programming.

It does not mean that I am out of options though.

What I need to do is a thorough study every aspect of the algorithm and match it step by step with regular CFR.

6:55pm. Let me stop here. My morale is down now that my sampling attempt failed, but I am only getting started here. I am making plans for how to figure this out.

I have various things in mind, which I will start work on starting tomorrow. The main thing I'll want to do is measure how distant the updates are from regular CFR. I'll make sure for myself just how having baselines reduces the variance.

In the UI I'll add a few more buttons, and I'll radically expand the table so I can compared the entries to enumerative estimates.

Also instead of changing baselines willy nilly, I might do a step where I calculate them in advance.

7:20pm. I am still thinking about it, once I make the tests, what will I learn from the tests.

The authors of the papers should know this well enough, but I myself should take those rules and make sure that sampling the game with the optimal baselines actually improves variance compared to the original.

I should calculate the reward using enumeration like CFR does plugging in the baseline values along the way. Then I should sample 100k times with and without baselines and plot a moving average of the difference between the perfect and sampled estimate.

7:25pm. Who knows, maybe the way I am doing it now has some bugs and the testing will uncover it.

After I get a sense for how things work, I'll use this insight to decide how to threeshold the policy update. Like accumulating gradients, I'll move from online learning to requiring a certain amount of samples before doing an update.

The benefit these variance reduction techniques is that they will lower how much samples I need to get a good estimate.

7:30pm. Great programming skills aren't what I need here. What I need to do is some simple science. That is what will show me the truth.

Let me stop here. It is time for some fun."

---

# [<](2021-04-17.md) 2021-04-18 [>](2021-04-19.md)

