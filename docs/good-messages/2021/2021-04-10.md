# [<](2021-04-09.md) 2021-04-10 [>](2021-04-11.md)

2,195,242 events, 1,265,987 push events, 2,130,271 commit messages, 123,157,129 characters


## [newstools/2021-the-daily-sun@f4befc55d6...](https://github.com/newstools/2021-the-daily-sun/commit/f4befc55d67a510123e23307834898fea960cbf3)
##### 2021-04-10 02:24:10 by NewsTools

Created Text For URL [www.dailysun.co.za/LIfestyle/im-attracted-to-my-friends-younger-brother-20210409]

---
## [mrakgr/The-Spiral-Language@770ee34e40...](https://github.com/mrakgr/The-Spiral-Language/commit/770ee34e40c72bb6d4e3c1a9bf85f6b66e1d8d2e)
##### 2021-04-10 11:12:21 by Marko Grdinić

"11:20am.

> Play Rance Quest
> H scene plays
> Think about how CFR works for half an hour instead while My Glorious Days plays in the background

That is an episode of my life from last night.

At any rate, I've figured something out. In fact, I understand CFR about 80% and I doubt I'll be able to move past that without understanding the specifics of how regret matching interacts with the rest of the system. This is most likely impossible since it involves interacting nonlinearities.

My suspicions from yesterday were right.

```
inl self_prob = exp_log_prob (add_log_prob chance_prob player.prob)
```

I've proven it to myself on a toy example mentally. To optimize the whole path you need to optimize `self_prob * chance_prob * op_prob`, those kinds of paths. I thought it was suspicious that right now, the graph has been factored into `self_prob * chance_prob` and `chance_prob * op_prob`, instead of `self_prob` and `chance_prob * op_prob` respectively. This later way of doing it is right after all.

PG optimize `softmax (self_prob * chance_prob * op_prob * r)`. CFR optimizes `self_prob * (norm << relu) (chance_prob * op_prob * r)`. So the whole path across the sums of probability factors is there.

The paper was right that it would be possible to sample old policies, I mean enough samples are taken `self_prob` average out across training runs.

Though this kind averaging would make the optimization biased though. There is no choice though in the NN case, you can't keep a net after literally every single update. Neither can I update one sample at a time.

The best way to resolve this is to use memory - make sure that the infostates are neatly separated and things will work fine.

11:40am. I am not sure why I made a mistake in strategy averaging. I am pretty sure that `x_i` and `x_-i` were both intended to use the path probs in the paper, but I could have missed it. The reason why this was never a problem when I tested it is because on all of the games the chance nodes were uniform and at the start of the game so they evened out.

It might have not been a problem for optimizing those games, but it sure was a problem when it comes to optimizing my own understanding of the algorithm.

```
inl self_prob = exp_log_prob player.prob
```

This should be like so, no doubt about it.

11:45am. Oh yeah...

```
        // This one might give better results during training.
        // let actions_prob () = regret_match (if is_update then d.regret else d.avg_policy)
```

I've also been wondering whether it would be better to use the average policy, but now that I understand what is being done, I see that using the current policy is just sampling the latest policy net from the ensemble. This is actually the best choice for the sake of driving exploration. I do not need to drive old policies forward and load up the buffer with stale data.

11:55am. Hrmmm...I really am going to have to do some reasearch and see if I can figure out how to average policies rather than keeping old nets around. Keeping the old nets around would be too inefficient in terms of memory.

...Though I wonder. Imagine if the learning rate was something like 0.01. In that case, I can assume that this would decay the old data samples at this rate anyway. Meaning it would keep around the information of the last 100 training runs at most.

12pm. If it was something like a 64 or 128 nets, I could afford to keep them around. It is no biggie. It might be better pay the memory cost up front rather than having to deal with having to optimize another net, and then heaving to deal with limitation imposed by having a limited memory buffer.

12:10pm. Hmmm, if I went the route of training a separate net to average policies, instead of doing that, what I could do is train a much slimmer net to predict the targets generated by the main training net. The main net could be 100mb, but it could generate the targets for a 1mb student net. I could afford to keep those nets around.

12:15pm. ...Forget that, it does not matter. I'll do the simple thing and keep old policy nets around. I'll figure out how to optimize that step later.

Let me read the Single Deep CFR paper.

https://arxiv.org/abs/1901.07621
Single Deep Counterfactual Regret Minimization

This guy also wrote the DREAM paper.

12:45pm. Actually, rather than training a separate slim net, what I should do after an initial period of training for the large one is freeze the lower layers and train just the head. I should then create an ensemble of heads. That will absolutely crush the size down while not reducing the network capacity in any way.

This is the ideal way. I'll actually go with this.

1pm. Let me stop here. I haven't even started, but I am already tired. Let me have breakfast here.

CFR is absolutely remarkable and profound. What did I learn from it? How to optimize probability distributions by averaging them. That strategy averaging works is nothing short of miraculous.

I could hypthesize it on my own, but whether I'd have the courage to actually try it out is a completely different thing.

It also has philosophical implications. My having a model of the self and factoring it out, one can get remarkable performance increases. Ultimately, self modeling is a large part of CFR. Right now, CFR only has two levels of optimization, but it might turn out to be the case that it would work for a multitude of factors, rather than just the self and the environment.

Who knows, maybe not. The self and the env are the two most important factors one can think of.

1:10pm. I can already see my victory. Let me stop here so I can eat."

---
## [mrakgr/The-Spiral-Language@b3b5dc5167...](https://github.com/mrakgr/The-Spiral-Language/commit/b3b5dc516771ac7ef5b14ef65932efc6a9df5344)
##### 2021-04-10 14:01:18 by Marko Grdinić

"2:10pm. Done with breakfast and chores. Let me chill a little and I will start.

2:35pm. Let me start. I really need to catch up with Aiki S at some point, the latest chapter was fun. I have so much on my backlog. Gone are the days where I could just watch and read everything. There is just not enough time. Maybe I'll go back to them once I get a galaxy brain.

Forget about that.

Let me put the finishing touches on my thinking. I've been obsessing about CFR for so long, and in the last few days, I've finally pushed my understanding from 40% where it was stuck for months, to about 80%.

I now understand all of its most salient aspects and can derive a sampling version directly from the original one. This includes the neural version of it.

The simplifications that I've arrived are remarkable. I won't have any need for policy gradients anymore and will be able to derive the policy from value nets instead. This is something I've thought about even before 2018, but never figured out how to do it.

In fact the way I will do sampling CFR will be novel not just to myself, but the field at large.

2:45pm. Yesterday I started out the day wanting to work on the UIs, but around this time just went to bed so I can think about CFR.

```
inl funsTrain (d : state _) = player_funs {
    action = dyn fun {chance_prob player player' actions next} =>
        inl num_actions = length actions
        inl d = value d num_actions player.observations
        inl self_prob = exp_log_prob player.prob
        inl op_prob = exp_log_prob (add_log_prob chance_prob player')
        inl reward,reward_wsum =
            inl state = state player
            am.mapFold2 (fun s prob a =>
                inl r =
                    if prob = 0 && op_prob = 0 then 0 // the pruning optimization
                    else next ((to_log_prob prob,a),state)
                r, s + prob * r
                ) 0 (regret_match d.regret) actions
        d.regret |> am.mapInplace (fun i x => x + op_prob * (index reward i - reward_wsum))
        inl actions_prob = regret_match d.regret
        d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
        am.fold2 (fun s prob r => s + prob * r) 0 actions_prob reward
    terminal = fun _ => ()
    }
```

Let me make this like so. And I will make a version just for evaling the game tree.

```
union cfr_mode = PlayAvgPolicy | PlayCurrentPolicy | TrainCurrentPolicy
```

Let me make this.

```
inl funsPlayEnum (d : state _) = player_funs {
    action = dyn fun {chance_prob player player' actions next} =>
        inl num_actions = length actions
        inl d = value d num_actions player.observations
        inl op_prob = log_probm.exp (log_probm.add chance_prob player')
        inl state = state player
        am.fold2 (fun s prob a =>
            inl r =
                if prob = 0 && op_prob = 0 then 0 // the pruning optimization
                else next ((log_probm.from prob,a),state)
            s + prob * r
            ) 0 (regret_match d.avg_policy) actions
    terminal = fun _ => ()
    }

inl funsTrain (d : state _) = player_funs {
    action = dyn fun {chance_prob player player' actions next} =>
        inl num_actions = length actions
        inl d = value d num_actions player.observations
        inl self_prob = log_probm.exp player.prob
        inl op_prob = log_probm.exp (log_probm.add chance_prob player')
        inl reward,reward_wsum =
            inl state = state player
            am.mapFold2 (fun s prob a =>
                inl r =
                    if prob = 0 && op_prob = 0 then 0 // the pruning optimization
                    else next ((log_probm.from prob,a),state)
                r, s + prob * r
                ) 0 (regret_match d.regret) actions
        d.regret |> am.mapInplace (fun i x => x + op_prob * (index reward i - reward_wsum))
        inl actions_prob = regret_match d.regret
        d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
        am.fold2 (fun s prob r => s + prob * r) 0 actions_prob reward
    terminal = fun _ => ()
    }
```

Let me back this up.

3:40pm.

```
union cfr_mode = PlayAvgPolicy | PlayCurrentPolicy | TrainCurrentPolicy
inl funsTrain mode (d : state _) = player_funs {
    action = dyn fun {chance_prob player player' actions next} =>
        inl d = value d (length actions) player.observations
        inl op_prob = log_probm.exp (log_probm.add chance_prob player')
        inl state = state player

        inl r prob a =
            if prob = 0 && op_prob = 0 then 0 // the pruning optimization
            else next ((log_probm.from prob,a),state)

        let weighted_sum actions_prob = am.fold2 (fun s prob a => s + prob * r prob a) 0 actions_prob actions

        match mode with
        // For evaling the overall quality of the policy whose agents are done training.
        | PlayAvgPolicy => weighted_sum (regret_match d.avg_policy)
        // Alternating updates between the players works better than training both simultaneously.
        // For the sake of training it is better to play the latest policy and get the freshest data instead of
        // sampling the old ones, which in effect would happen if the avg_policy was used for training.
        | PlayCurrentPolicy => weighted_sum (regret_match d.regret)
        // Trains the current policy. The old F# version in the `v0.2` branch of the Spiral repo has some bugs.
        // The way it is done currently reflects my current understanding of the way the algorithm is supposed to be implemented.
        // For example, I am decently sure that `self_prob` should not have the chance probability like the paper suggests.
        // Also the latest policy should be added to the average one rather than the stale one.
        // To speed up learning I also reweight the rewards using the updated policy.
        | TrainCurrentPolicy =>
            inl reward,reward_wsum =
                am.mapFold2 (fun s prob a =>
                    inl r = r prob a in r, s + prob * r
                    ) 0 (regret_match d.regret) actions
            d.regret |> am.mapInplace (fun i x => x + op_prob * (index reward i - reward_wsum))
            inl actions_prob = regret_match d.regret
            inl self_prob = log_probm.exp player.prob
            d.avg_policy |> am.mapInplace (fun i x => x + self_prob * index actions_prob i)
            weighted_sum actions_prob
    terminal = fun _ => ()
    }
```

Yeah, this is. This is the definitive CFR implementation. Linear CFR reweighting might make things work better, but that aspect is not important in comparison.

I've run it so many time through my mind. I've literally bashed my head against the CFR wall for months trying to understand the algorithm. And now I finally understand it well enough to make my own modifications and derive the sampling versions on my own.

3:45pm. Ah, I want a break. Right now I am too hung up on this. Let me think, do I finally have enough to focus on the UIs?

Can I shift my goal to working on them? The learning algorithm itself is finally clear.

3:55pm. Let me take a break here. I do not feel like it.

4pm. Yesterday I literally pried myself off from work at 8pm, and I was still interrupted by the subjects after that.

Let me chill a bit now."

---
## [Skyrat-SS13/Skyrat-tg@72f32ade24...](https://github.com/Skyrat-SS13/Skyrat-tg/commit/72f32ade24843803f65d434293f14c1518a8b68e)
##### 2021-04-10 17:09:41 by death and coding

[modular][ready] adds a uniform in the loadout for the superstar cops (#4779)

* the expression

* its speaking again

* its still talking

* WORK

* Mazovian Socio-Economics

People think Communism was some crazy idea that had its comeuppance 40 years ago. A fever that shook the world, never to return again. They were right. Until *he* woke up today – a spiritual corpse responsive only to the call of Commodore Red, prostitutes, and Kras Mazov. For him, Communism is still a *thing*. He will single-handedly raise the Commune of '02 from the oceanic trench where it has been resting, covered in ghosts and seaweed! He is the Big Communism Builder. Come, witness his attempt to rebuild Communism in the year '51!

0.000% of Communism has been built. Evil child-murdering billionaires still rule the world with a shit-eating grin. All he has managed to do is make himself *sad*. He is starting to suspect Kras Mazov *fucked him over* personally with his socio-economic theory. It has, however, made him into a very, very smart boy with something like a university degree in Truth. Instead of building Communism, he now builds a precise model of this grotesque, duplicitous world.

* work

* Delete accessories.dm

there is nothing, only darkness

* FUCKFUCKFUCK

* RIGHT FIXEDA AAAA

---
## [Skyrat-SS13/Skyrat-tg@da2215fb0d...](https://github.com/Skyrat-SS13/Skyrat-tg/commit/da2215fb0d023a299ad4bcf62184063b57ccbf4e)
##### 2021-04-10 17:18:38 by SkyratBot

[MIRROR] Empty graves can now be spawned (#4755)

* Empty graves can now be spawned (#58200)

* i walked along the no mans road

POV: you got fucked on a previous branch from something stupid

* Update code/modules/ruins/lavalandruin_code/elephantgraveyard.dm

Co-authored-by: Fikou <piotrbryla@ onet.pl>

* Update code/modules/ruins/lavalandruin_code/elephantgraveyard.dm

Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

Co-authored-by: Fikou <piotrbryla@ onet.pl>
Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

* Empty graves can now be spawned

Co-authored-by: ishitbyabullet <deathzombine@outlook.com>
Co-authored-by: Fikou <piotrbryla@ onet.pl>
Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

---
## [AllyTally/VVVVVV@300f1b7919...](https://github.com/AllyTally/VVVVVV/commit/300f1b791956f864a6d4333c0c2edb6b56097ab5)
##### 2021-04-10 18:30:35 by Misa

Switch assets mounting to dedicated directory

This fixes an issue where you would be able to mount things other than
custom assets in per-level custom asset directories and zips.

To be fair, the effects of this issue were fairly limited - about the
only thing I could do with it was to override a user-made quicksave of a
custom level with one of my own. However, since the quicksave check
happens before assets are mounted, if the user didn't have an existing
quicksave then they wouldn't be able load my quicksave. Furthermore,
mounting things like settings.vvv simply doesn't work because assets
only get mounted when the level gets loaded, but the game only reads
from settings.vvv on startup.

Still, this is an issue, and just because it only has one effect doesn't
mean we should single-case patch that one effect only. So what can we
do?

I was thinking that we should (1) mount custom assets in a dedicated
directory, and then from there (2) mount each specific asset directly -
namely, mount the graphics/ and sounds/ folders, and mount the
vvvvvvmusic.vvv and mmmmmm.vvv files. For (1), assets are now mounted at
a (non-existent) location named .vvv-mnt/assets/. However, (2) doesn't
fully work due to how PhysFS works.

What DOES work is being able to mount the graphics/ and sounds/ folders,
but only if the custom assets directory is a directory. And, you
actually have to use the real directory where those graphics/ and
sounds/ folders are located, and not the mounted directory, because
PHYSFS_mount() only accepts real directories. (In which case why bother
mounting the directory in the first place if we have to use real
directories anyway?) So already this seems like having different
directory and zip mounting paths, which I don't want...

I tried to unify the directory and zip paths and get around the real
directory limitation. So for mounting each individual asset (i.e.
graphics/, sounds/, but especially vvvvvvmusic.vvv and mmmmmm.vvv), I
tried doing PHYSFS_openRead() followed by PHYSFS_mountHandle() with that
PHYSFS_File, but this simply doesn't work, because PHYSFS_mountHandle()
will always create a PHYSFS_Io object, and pass it to a PhysFS internal
helper function named openDirectory() which will only attempt to treat
it as a directory if the PHYSFS_Io* passed is NULL. Since
PHYSFS_mountHandle() always passes a non-NULL PHYSFS_Io*,
openDirectory() will always treat it like a zip file and never as a
directory - in contrast, PHYSFS_mount() will always pass a NULL
PHYSFS_Io* to openDirectory(), so PHYSFS_mount() is the only function
that works for mounting directories.

(And even if this did work, having to keep the file open (because of the
PHYSFS_openRead()) results in the user being unable to touch the file on
Windows until it gets closed, which I also don't want.)

As for zip files, PHYSFS_mount() works just fine on them, but then we
run into the issue of accessing the individual assets inside it. As
covered above, PHYSFS_mount() only accepts real directories, so we can't
use it to access the assets inside, but then if we do the
PHYSFS_openRead() and PHYSFS_mountHandle() approach,
PHYSFS_mountHandle() will treat the assets inside as zip files instead
of just mounting them normally!

So in short, PhysFS only seems to be able to mount directories and zip
files, and not any loose individual files (like vvvvvvmusic.vvv and
mmmmmm.vvv). Furthermore, directories inside directories works, but
directories inside zip files doesn't (only zip files inside zip files
work).

It seems like our asset paths don't really work well with PhysFS's
design. Currently, graphics/, sounds/, vvvvvvmusic.vvv, and mmmmmm.vvv
all live at the root directory of the VVVVVV folder. But what would work
better is if all of those items were organized into a subfolder, for
example, a folder named assets/. So the previous assets mounting system
before this patch would just have mounted assets/ and be done with it,
and there would be no risk of mounting extraneous files that could do
bad things. However, due to our unorganized asset paths, the previous
system has to mount assets at the root of the VVVVVV folder, which
invites the possibility of those extraneous bad files being mounted.

Well, we can't change the asset paths now, that would be a pretty big
API break (maybe it should be a 2.4 thing). So what can we do?

What I've done is, after mounting the assets at .vvv-mnt/assets/, when
the game loads an asset, it checks if there's an override available
inside .vvv-mnt/assets/, and if so, the game will load that asset
instead of the regular one. This is basically reimplementing what PhysFS
SHOULD be able to do for us, but can't. This fixes the issue of being
able to mount a quicksave for a custom level inside its asset directory.

I should also note, the unorganized asset paths issue also means that
for .zip files (which contain the level file), the level file itself is
also technically mounted at .vvv-mnt/assets/. This is harmless (because
when we load a level file, we never load it as an asset) but it's still
a bit ugly. Changing the asset paths now seems more and more like a good
thing to do...

---
## [LDR-Siren/EmilyC-SamanthaPrater-EruzaArto@80a0ff66f6...](https://github.com/LDR-Siren/EmilyC-SamanthaPrater-EruzaArto/commit/80a0ff66f6f709672e0b62d5a1e098a01c34a39c)
##### 2021-04-10 23:19:56 by LDR

Just why? No one is talking to her

I haven't uploaded in over 20+ days and she is still rattling on. No one is talking to her. No one is even contacting her, and yet here she is, going off like a crazy person. 

Lets talk about the MRN(Medical Records Number) she willy nilly gave out on April 10th 2021. Yup! One of the screen grabbers caught that one. Like how dumb do you have to be to not cover that up? As one of the folks in my discord put it "All someone would have to do is take that number, her information that she has given out, and tada, they have all her medical history and who only knows what else."  She screams and screams about all sorts of shit and here she is DOXXING herself, again. 
On another note, some of us have been contacted by individuals on Reddit about what the hell is wrong with her, and if she is crazy. You can see some of the convo I had with them on 04092021. The person is nice, but I warned them heavily that she is crazy.

Lets talk about Lamby. I love the fact she is outright talking shit about my favorite artist. No joke. I have commissioned a piece from her, and seeing how her other work has been, will likely commission more. I also love the fact that Lamby is probably the most patient, loving and sadistic one out of all of us. She unblocked the TURD and has been letting her go crazy. You can see a few chunks starting about March 25th to recently. Lets make something clear, Lamby has no control of how other people react, or don't react to Emily. What Lamby has done, is give people the information that is publicly out there, so others can make an informed decision whether or not they want to deal with Emily's form of nutjob. If they choose to not interact with her, its because of her, not Lamby.

Aislinn, has been AWOL for a while now, thanks to a messed up shoulder and a remodel of I want to say her kitchen. Yup. Emily is not the first and last thing any of us think of during the day or much at all to be precise. 

Her snotty remarks of Video taping herself showering is disgusting and has made a few of us gag. I personally think it would solve several of our problems, because that can be reported for pornography. Go Nuts turdlet! None of us believe she showers on the regular or often enough to keep from being disgusting. She has shown enough images of herself to prove she is not maintaining a respectable level of bathing of her person. And I am sorry, not really, her house is filthy. My own home, may be well loved in, but is always clean, there are no chicken bones on my floor, unlike hers.

Anyways, No one is talking to her. No one cares to. And those that do, when they find us, we tell them the truth. Simple as that.

---

# [<](2021-04-09.md) 2021-04-10 [>](2021-04-11.md)

