# [<](2021-05-01.md) 2021-05-02 [>](2021-05-03.md)

2,069,345 events, 1,214,867 push events, 1,731,229 commit messages, 100,672,597 characters


## [KornFlaks/UnityBad](https://github.com/KornFlaks/UnityBad)@[6eb2a9935d...](https://github.com/KornFlaks/UnityBad/commit/6eb2a9935d2bea5f1125c0132078fa9928e8dac4)
#### Sunday 2021-05-02 00:43:06 by Jeffrey Wang

Upgraded to Unity 2021.2 for better UI Builder. Fuck you Unity.

---
## [IgiariValkyr/Citadel-Station-13](https://github.com/IgiariValkyr/Citadel-Station-13)@[786877cdb7...](https://github.com/IgiariValkyr/Citadel-Station-13/commit/786877cdb7a883a9e4bcbb62d9b675104471c678)
#### Sunday 2021-05-02 09:58:17 by Igiari

And here he be.

So. Basically someone said "haha roleplay dragon", so I said fuck it. I did this once before for another server, i can do it again. And hopefully be less possessive this time.

*clicks tongue* Yeah, I didn't expect to sit here for three fucking hours coding it all over again.

Dragon can't swoop, which means no slams, fire rain, or lava arena.
Dragon fire only goes in a straight line for 8 tiles.
Dragon can penetrate armor much better, but deals less overall damage.
Dragon fast as fuck boi, but if you get 11 tiles away, loses interest. *Good luck.*
Dragon can see in the dark with a meson/thermal combo. Can't see items outside its "technical" vision cone, as no x-ray.
Dragon has a unique GPS marker to be found on the map if it exists through admin or random spawn.
I have no fucking clue if it can random spawn.

---
## [MyObliviousDeveloper/mom-release-monthly](https://github.com/MyObliviousDeveloper/mom-release-monthly)@[b4788fe7d6...](https://github.com/MyObliviousDeveloper/mom-release-monthly/commit/b4788fe7d6b25b0b13013f53e90e4b44421c0535)
#### Sunday 2021-05-02 11:55:40 by MyObliviousDeveloper

v 0.30.0

- You can buy estrogen pills on the Market
- You can spike $landlord's drink with them at dinner, repeat 5 times total for feminization (first chapter about it in next release)(monthly poll)
- Night > Master Bedroom > Use her pussy (ranked poll)
- Hotkeys! You can now use the NumPad to navigate through your options, and forget about the mouse. This is a kind contribution of TacoVengeance.
- Dinner Kitchen > Comment on his ass > Stealth Check (Distracted based)(Feminization ON)
- Dinner Kitchen > Comment on his ass > Cunning Check (Gullible based)(Feminization ON)
- You can skip to a period of your choice, option in the living room, when there is no event
- You can find 1, 2 or 3 bucks every day in the dirty laundry
- More gifs!

---
## [mxriverlynn/guide-to-allyship](https://github.com/mxriverlynn/guide-to-allyship)@[0150ad4719...](https://github.com/mxriverlynn/guide-to-allyship/commit/0150ad4719131527bd67d89280da4c234447b347)
#### Sunday 2021-05-02 13:06:18 by River Lynn Bailey

replace the bit about "remove politics"

hello - i noticed in the section on apologies that there was a mention of removing politics. i think i understand the intent here, to let go of politics for a moment when thinking about why we apologize. but the idea of removing politics bothers me for a lot of reasons - especially when considering recent events with Basecamp declaring their business as a no-politics area, and the massive problems this creates. 

i'd love to see the wording about removing politics removed from the guide and replaced with something less ambiguous. my suggestion in this PR is an idea that i had on what the wording may sound like, but i'm definitely open to any and all feedback around this and changes to the wording.

---
## [ShanmugaGanesh1999/LeetCodeDailyChallenge](https://github.com/ShanmugaGanesh1999/LeetCodeDailyChallenge)@[1a8b6067f0...](https://github.com/ShanmugaGanesh1999/LeetCodeDailyChallenge/commit/1a8b6067f02bebab55ada5c59a012eb2ca2c9a27)
#### Sunday 2021-05-02 13:53:10 by Shanmuga Ganesh T

Course Schedule III

There are n different online courses numbered from 1 to n. You are given an array courses where courses[i] = [durationi, lastDayi] indicate that the ith course should be taken continuously for durationi days and must be finished before or on lastDayi.

You will start on the 1st day and you cannot take two or more courses simultaneously.

Return the maximum number of courses that you can take.

 

Example 1:

Input: courses = [[100,200],[200,1300],[1000,1250],[2000,3200]]
Output: 3
Explanation: 
There are totally 4 courses, but you can take 3 courses at most:
First, take the 1st course, it costs 100 days so you will finish it on the 100th day, and ready to take the next course on the 101st day.
Second, take the 3rd course, it costs 1000 days so you will finish it on the 1100th day, and ready to take the next course on the 1101st day. 
Third, take the 2nd course, it costs 200 days so you will finish it on the 1300th day. 
The 4th course cannot be taken now, since you will finish it on the 3300th day, which exceeds the closed date.

Example 2:

Input: courses = [[1,2]]
Output: 1

Example 3:

Input: courses = [[3,2],[4,3]]
Output: 0

 

Constraints:

    1 <= courses.length <= 104
    1 <= durationi, lastDayi <= 104

   Hide Hint #1  
During iteration, say I want to add the current course, currentTotalTime being total time of all courses taken till now, but adding the current course might exceed my deadline or it doesn’t.

1. If it doesn’t, then I have added one new course. Increment the currentTotalTime with duration of current course.
   Hide Hint #2  
2. If it exceeds deadline, I can swap current course with current courses that has biggest duration.
* No harm done and I might have just reduced the currentTotalTime, right?
* What preprocessing do I need to do on my course processing order so that this swap is always legal?

Idea:

If we think of this problem in a larger sense, we can envision a slightly more simplistic situation without the issues of the last day cutoff for each course. In that scenario, we could quite easily add up all the course durations, then selectively remove the courses with the longest remaining duration until we've found the ideal number of courses that will fit into our desired timeframe.

The trouble here, of course, is that we do have cutoffs for each course, which means we can no longer fill the entire time before removing courses. Instead, we'll have to selectively backtrack and remove courses as we iterate through the input array (C).

As is often the case in a scheduling-type problem, this leads to a particular issue: we'll want to sort the data in two distinctly different ways. Since we'll be progressing through C as if we're progressing through time, we'll want to sort C based on the courses' cutoffs (end), but when we backtrack to potentially remove a course, we'll want to sort the courses we've accepted by their duration (dur).

The need for a data structure that will maintain its sort through insertions and max value removals means that we're looking for a max priority queue or max-heap.

Once we've sorted C and set up our max priority queue or heap (pq/heap), it's simply a matter of iterating through C, adding the courses to pq/heap, and then removing the max duration course as necessary to stay underneath the current end value with our accumulated duration (total).

In order to minimize unnecessary insertions/removals, we can perform a few basic conditional checks to tell if they're necessary. If the current course will already fit, we can just add it, or if the current course is a better fit than our longest course, then we can swap them.

Then, once we reach the end of C, pq/heap should contain all the non-discarded courses, so we can return its size as our answer.

    Time Complexity: O(N * log N) where N is the length of C, due to both the sort and the priority queue / heap implementation
    Space Complexity: O(N) due to the priority queue / heap data

Implementation:

In this instance, the MaxPriorityQueue() npm for Javascript was actually competitively performant compared to a custom max-heap structure.

To avoid having to use a custom comparator for Python, which defaults to a min heap, we can just switch the sign before insertion and after extraction to mimic a max heap.

---
## [ceccopierangiolieugenio/pyTermTk](https://github.com/ceccopierangiolieugenio/pyTermTk)@[b2bef78c90...](https://github.com/ceccopierangiolieugenio/pyTermTk/commit/b2bef78c9060f60981f8d4d8fc267aab1bddabbb)
#### Sunday 2021-05-02 15:43:50 by Eugenio Parodi

Started to using the Fucking Obnoxious Sphinx (Fuck YOU!!!)

---
## [crawl/crawl](https://github.com/crawl/crawl)@[4d8f0be3d4...](https://github.com/crawl/crawl/commit/4d8f0be3d4e71952106442d068d0cf89802c7219)
#### Sunday 2021-05-02 15:51:24 by advil

feat: allow setting tiles window width by ratio

On the previous defaults, the tiles window width was screen edge - 90 in
both dimensions. This works fine on many screen ratios, but is kind of
annoying (in my opinion) on an ultrawide screen. This commit provides an
option to override the width setting using a ratio of the height, and
makes it activated by default. When activated, the width setting is
instead used as a maximum value. This has no impact on full screen mode.
This can be disabled by just setting `tile_window_ratio` to 0 or a
negative value, in which case the behavior is the same as before
(including if you had custom values set here).

I've used the golden ratio as the default value, so on a 4:3 screen
you'd typically have the same default results as before, but on a wider
screen the window default width will be capped. This looks (again, IMO)
a lot better on an ultrawide ratio like 2:1 or greater.

It would be ideal, eventually, to actually remember positioning and
size.

---
## [bemp-research/bemp-site](https://github.com/bemp-research/bemp-site)@[7138751051...](https://github.com/bemp-research/bemp-site/commit/7138751051f7238f60734fc4c7a7ade1ae8ddc9a)
#### Sunday 2021-05-02 19:07:08 by Sang Dang

oi vcl dcm 3 days just to have a blank repo, fuck you gatsby

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[28bed32447...](https://github.com/mrakgr/The-Spiral-Language/commit/28bed324472699b94447a83c7c13e935fd27f651)
#### Sunday 2021-05-02 20:01:27 by Marko Grdinić

"5pm. Reinforcement Learning, Bit by Bit

I got this paper a week ago and it has been sitting there. But it talks about exploration and even mentions posterior sampling and information directed sampling. I had not heard about that last one. It seems like it would be worth reading. Oh Osband is one of the authors.

I am not sure what to think about China, but 50% of the authors of any paper of interest seem to be from there these days. China hasn't had any notable successes in AI like Deepmind and OpenAI have, but their foundation seems to be good.

5:10pm. Oh the paper is 74 pages. Wow. I'll spread out my reading activity over several days then. Let me read 10 pages or so.

6pm. Forget this paper. What is information directed sampling?

6:10pm. I found a Youtube video that describes it. It optimizes reward / information gain. How do I get that later quantity? I have no idea.

https://willdabney.com/

I am looking at this guy's work.

https://willdabney.com/publication/hca/
Hindsight Credit Assignment

I'll read this later.

6:20pm. Ok, I get the concept of information gain. The way I would do this in a policy optimization setting is to train an ensemble, calculate the centered variance of the action probabilities, then multiply the action probs by the square root of that, then normalize the vector.

This is not exactly the same thing, but information gain can only really come from reducing epistemic uncertainty. Where is the greatest epistemic uncertainty - where there is difference in the models. So it could serve well as a proxy.

I admit, this did inspire me a little. I never though of just multiplying the actions by the sqrt var until I was introduced to the idea. Actually, using variance of actions between different policy nets in the ensemble would not be good. Consider the two action case. The variances would always be the same. Instead it might be better to use the variances of the value functions in order to mutiply the actions. It has to be the epistemic variance, not aleatoric. The way to get it is to train an ensemble.

https://machinelearningmastery.com/information-gain-and-mutual-information/

I do not feel like studying trees.

Let me take a look at the HCA paper.

https://willdabney.com/publication/iqn/

I saw this paper years ago.

6:45pm. Forget this. Let me get lunch. I'll leave distributional learning for some other time.

6:55pm. But this idea of multiplying actions by the value variance is a tremendous gain. I won't need it for Leduc, but larger games it will be worth it to do it. It will give me deep exploration the way it is meant to be done.

In the reversibility paper I had the idea of sampling from a Gaussian, but that one would just give me variance. Multiplying by sqrt variance and renormalizing the policy is perfect! I have no idea why I did not think of it before.

The last 1.5 weeks have been amazing. It has been just one good idea after another.

7:10pm. I think that at this point my grasp of low level RL is excellent.

8:30pm. https://arxiv.org/abs/2103.12685
Generative Minimization Networks - Training GANs Without Competition

If this works, it could be amazing. It is a really interesting idea. I'll keep it in mind if the minimax optimization of the RL agent turns into trouble...

But it should not, not of the kind of trouble found in GANs.

What the above paper does is given the weight `u` and `v` for the generator and the discriminator first optimize them to generate temporary `u'` `v'`. That is it achors the originals to get the `u'` and `v'`. Then it uses the `u'` and `v'` as achors itself to do regular GAN objective training for `u` and `v` for the next timestep.

I actually had some unfinished ideas along these lines, but I had no idea whether it would work.

There really is all sort of interesting work being done. For all I know, this might solve the issue of GAN training. If this turns GAN training as stable as regular supervised training that means that GANs becomes a viable options for unsupervised representation learning in real life systems!

Actually my description was bad. Let me do it in pseudocode.

```fs
// lr, lr' - learning rates
// grad_u, grad_v - gradient functions
// M - GAN objective
// u, v - discriminator & generator parameters
let minimize_approx_duality_gap lr lr' grad_u grad_v k M u v =
    let mutable u' = u
    let mutable v' = v
    for i=1 to k do
        u' <- u' - lr * grad_u M u' v
        v' <- v' + lr * grad_v M u v'

    let DG u v = M u v' - M u' v
    let u_new = u - lr' * grad_u DG u v
    let v_new = v - lr' * grad_v DG u v
    u_new, v_new
```

I find this algorithm very elegant. It is worth keeping in mind.

9:40pm. If my noise idea fails, I'll take the above, modify the `-` in `let DG u v = M u v' - M u' v` into a `+` and train the RL players using that objective. The noise idea is too obvious to work, maybe it will make the network unstable. But applying the above algorithm should get me what I need.

9:45pm. Looking at this, I have no doubt I've had the right idea to wait. The the longer the time goes on, the luckier I get in algorithmic terms.

Still, I think there might be something to the noisy key idea.

GANs are unstable not just because they are minimax problems, but because there are two competing networks within the same model. It is like an extreme case of schizophrenia.

In fact, given the kind of toy problems that SGD fails when doing minimax training it is nothing short of a miracle that large GANs can be trained at all.

9:55pm. I admit, I am having fun. Whenever I see algorithms like this or the ideas like information driven sampling, it feels like I gain an ability that I did not have before.

10pm. Let me get back to reading. I'll try to do at least a little programming tomorrow. Today was a windfall of luck once more."

---
## [SherRao/Chaterest](https://github.com/SherRao/Chaterest)@[b50c88b3b8...](https://github.com/SherRao/Chaterest/commit/b50c88b3b8eaaa2d20cc373859173c4e92c4bfa7)
#### Sunday 2021-05-02 21:02:50 by Nausher Rao

SO MANY FAT CHANGES BRO HOLY SHIT CHECK THE MESSAGES

trackEvent command now fully works
Made getDuration() output print seconds
Did some bullshit with the health check shit good job ben
The bot now always increments global sentiment
Fixed percentages in info.js
Made the compare command and it works fully after fixing so many errors holy shit
Created and organised embeds for bot
Embeds now take the bot profile picture as the author icon

---
## [nobility-suite/Nobility](https://github.com/nobility-suite/Nobility)@[bfa3d0b6eb...](https://github.com/nobility-suite/Nobility/commit/bfa3d0b6eb66d77da812716527863f7058b21de1)
#### Sunday 2021-05-02 21:35:42 by Kayla Saint

Merge pull request #20 from nobility-suite/main

Unfuck for now (FUCK YOU ORINNARI)

---
## [CS411-Calendar/App](https://github.com/CS411-Calendar/App)@[8f5bfd8795...](https://github.com/CS411-Calendar/App/commit/8f5bfd8795a9ac90d136d35259e867878e583a12)
#### Sunday 2021-05-02 21:49:56 by Takamitsu Shirono

convert all single quote to double-quote (i'm sorry my formatter is kinda annoying

---
## [07th-mod/higurashi-console-arcs](https://github.com/07th-mod/higurashi-console-arcs)@[a2ae38abb4...](https://github.com/07th-mod/higurashi-console-arcs/commit/a2ae38abb4e3251ec3c070d2d2cdb3d8b30bef06)
#### Sunday 2021-05-02 23:38:37 by msksk

TL improvement (#382)

* Update omot_026.txt

Line 53
Literal meaning of "バチが当たる" is "punished by Heaven."

line 357
くっ is a part of くっそー ! or くっそお! Shion tried to say "damn it!" or "shit!"

line 527
I think 興宮航空(Okinomiya Airlines) is a fictitious airline company.

line 802
みたく is the same as みたいに(like). This is not 見たく(want to see).

line 953
Probably "who made me realize that" was Mion.

* Update omot_026.txt

These are imported TLs from Matsuribayashi official TL(_mats_023.txt) except line 1296, 1312, 1444, 1454, 1668, 1735.

地区 is officially translated as "Section."

line 1735 "頼みの綱(の)" is an idiom.

* Apply suggestions from code review

---

# [<](2021-05-01.md) 2021-05-02 [>](2021-05-03.md)

