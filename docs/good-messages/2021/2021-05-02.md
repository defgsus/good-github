# [<](2021-05-01.md) 2021-05-02 [>](2021-05-03.md)

2,069,345 events, 1,214,867 push events, 1,731,229 commit messages, 100,672,597 characters


## [mrakgr/The-Spiral-Language@28bed32447...](https://github.com/mrakgr/The-Spiral-Language/commit/28bed324472699b94447a83c7c13e935fd27f651)
##### 2021-05-02 20:01:27 by Marko Grdinić

"5pm. Reinforcement Learning, Bit by Bit

I got this paper a week ago and it has been sitting there. But it talks about exploration and even mentions posterior sampling and information directed sampling. I had not heard about that last one. It seems like it would be worth reading. Oh Osband is one of the authors.

I am not sure what to think about China, but 50% of the authors of any paper of interest seem to be from there these days. China hasn't had any notable successes in AI like Deepmind and OpenAI have, but their foundation seems to be good.

5:10pm. Oh the paper is 74 pages. Wow. I'll spread out my reading activity over several days then. Let me read 10 pages or so.

6pm. Forget this paper. What is information directed sampling?

6:10pm. I found a Youtube video that describes it. It optimizes reward / information gain. How do I get that later quantity? I have no idea.

https://willdabney.com/

I am looking at this guy's work.

https://willdabney.com/publication/hca/
Hindsight Credit Assignment

I'll read this later.

6:20pm. Ok, I get the concept of information gain. The way I would do this in a policy optimization setting is to train an ensemble, calculate the centered variance of the action probabilities, then multiply the action probs by the square root of that, then normalize the vector.

This is not exactly the same thing, but information gain can only really come from reducing epistemic uncertainty. Where is the greatest epistemic uncertainty - where there is difference in the models. So it could serve well as a proxy.

I admit, this did inspire me a little. I never though of just multiplying the actions by the sqrt var until I was introduced to the idea. Actually, using variance of actions between different policy nets in the ensemble would not be good. Consider the two action case. The variances would always be the same. Instead it might be better to use the variances of the value functions in order to mutiply the actions. It has to be the epistemic variance, not aleatoric. The way to get it is to train an ensemble.

https://machinelearningmastery.com/information-gain-and-mutual-information/

I do not feel like studying trees.

Let me take a look at the HCA paper.

https://willdabney.com/publication/iqn/

I saw this paper years ago.

6:45pm. Forget this. Let me get lunch. I'll leave distributional learning for some other time.

6:55pm. But this idea of multiplying actions by the value variance is a tremendous gain. I won't need it for Leduc, but larger games it will be worth it to do it. It will give me deep exploration the way it is meant to be done.

In the reversibility paper I had the idea of sampling from a Gaussian, but that one would just give me variance. Multiplying by sqrt variance and renormalizing the policy is perfect! I have no idea why I did not think of it before.

The last 1.5 weeks have been amazing. It has been just one good idea after another.

7:10pm. I think that at this point my grasp of low level RL is excellent.

8:30pm. https://arxiv.org/abs/2103.12685
Generative Minimization Networks - Training GANs Without Competition

If this works, it could be amazing. It is a really interesting idea. I'll keep it in mind if the minimax optimization of the RL agent turns into trouble...

But it should not, not of the kind of trouble found in GANs.

What the above paper does is given the weight `u` and `v` for the generator and the discriminator first optimize them to generate temporary `u'` `v'`. That is it achors the originals to get the `u'` and `v'`. Then it uses the `u'` and `v'` as achors itself to do regular GAN objective training for `u` and `v` for the next timestep.

I actually had some unfinished ideas along these lines, but I had no idea whether it would work.

There really is all sort of interesting work being done. For all I know, this might solve the issue of GAN training. If this turns GAN training as stable as regular supervised training that means that GANs becomes a viable options for unsupervised representation learning in real life systems!

Actually my description was bad. Let me do it in pseudocode.

```fs
// lr, lr' - learning rates
// grad_u, grad_v - gradient functions
// M - GAN objective
// u, v - discriminator & generator parameters
let minimize_approx_duality_gap lr lr' grad_u grad_v k M u v =
    let mutable u' = u
    let mutable v' = v
    for i=1 to k do
        u' <- u' - lr * grad_u M u' v
        v' <- v' + lr * grad_v M u v'

    let DG u v = M u v' - M u' v
    let u_new = u - lr' * grad_u DG u v
    let v_new = v - lr' * grad_v DG u v
    u_new, v_new
```

I find this algorithm very elegant. It is worth keeping in mind.

9:40pm. If my noise idea fails, I'll take the above, modify the `-` in `let DG u v = M u v' - M u' v` into a `+` and train the RL players using that objective. The noise idea is too obvious to work, maybe it will make the network unstable. But applying the above algorithm should get me what I need.

9:45pm. Looking at this, I have no doubt I've had the right idea to wait. The the longer the time goes on, the luckier I get in algorithmic terms.

Still, I think there might be something to the noisy key idea.

GANs are unstable not just because they are minimax problems, but because there are two competing networks within the same model. It is like an extreme case of schizophrenia.

In fact, given the kind of toy problems that SGD fails when doing minimax training it is nothing short of a miracle that large GANs can be trained at all.

9:55pm. I admit, I am having fun. Whenever I see algorithms like this or the ideas like information driven sampling, it feels like I gain an ability that I did not have before.

10pm. Let me get back to reading. I'll try to do at least a little programming tomorrow. Today was a windfall of luck once more."

---
## [07th-mod/higurashi-console-arcs@a2ae38abb4...](https://github.com/07th-mod/higurashi-console-arcs/commit/a2ae38abb4e3251ec3c070d2d2cdb3d8b30bef06)
##### 2021-05-02 23:38:37 by msksk

TL improvement (#382)

* Update omot_026.txt

Line 53
Literal meaning of "バチが当たる" is "punished by Heaven."

line 357
くっ is a part of くっそー ! or くっそお! Shion tried to say "damn it!" or "shit!"

line 527
I think 興宮航空(Okinomiya Airlines) is a fictitious airline company.

line 802
みたく is the same as みたいに(like). This is not 見たく(want to see).

line 953
Probably "who made me realize that" was Mion.

* Update omot_026.txt

These are imported TLs from Matsuribayashi official TL(_mats_023.txt) except line 1296, 1312, 1444, 1454, 1668, 1735.

地区 is officially translated as "Section."

line 1735 "頼みの綱(の)" is an idiom.

* Apply suggestions from code review

---

# [<](2021-05-01.md) 2021-05-02 [>](2021-05-03.md)

