# [<](2021-06-15.md) 2021-06-16 [>](2021-06-17.md)

3,632,673 events, 1,570,468 push events, 2,530,659 commit messages, 192,734,929 characters


## [gitster/git@22d16ca2f1...](https://github.com/gitster/git/commit/22d16ca2f1b5a7db6f16e577c20de5251847ab06)
##### 2021-06-16 06:46:42 by Derrick Stolee

CodingGuidelines: recommend singular they

Technical writing seeks to convey information with minimal friction. One
way that a reader can experience friction is if they encounter a
description of "a user" that is later simplified using a gendered
pronoun. If the reader does not consider that pronoun to apply to them,
then they can experience cognitive dissonance that removes focus from
the information.

When choosing a gendered pronoun, that pronoun no longer applies to
nearly half of possible readers. Even if we alternated between "he/him"
and "she/her" perfectly evenly, we would still expect male and female
readers to experience an incorrect pronoun half the time. However, some
readers will not prescribe to either of these binary genders. Those
readers hence suffer an incorrect pronoun the entire time.

To make our documentation more inclusive, add recommendations to the
CodingGuidelines document. We can refer to this section when a
contributor submits a patch with a gendered pronoun and these
recommendations apply. The examples can assist in producing a new patch
with adjusted language.

As noted in the guidelines, removing an example person can make the
writing clearer and more concise. Other techniques such as singular
"you" and plural "they" are widely accepted ways to adjust the noun and
avoid gendered pronouns. Finally, an author can resort to singluar
"they" if absolutely necessary, but this can be difficult for readers
who learned English in a way that dictated "they" as always plural.

If we refer to a specific person, then using a gendered pronoun is
appropriate. There can also be other cases where it is inappropriate for
us to update the existing examples within the Git codebase, such as:

* References to real people (e.g. Linus Torvalds, "the Git maintainer").
  Do not misgender real people. If there is any doubt to the gender of a
  person, then avoid using pronouns.

* References to fictional people with clear genders (e.g. Alice and
  Bob).

* Sample text used in test cases (e.g t3702, t6432).

* The official text of the GPL license contains uses of "he or she", but
  modifying the license this way is not within the scope of the Git
  project.

* Literal email messages in Documentation/howto/ should not be edited
  for grammatical concerns such as this, unless we update the entire
  document to fit the standard documentation format. If such an effort is
  taken on, then the authorship would change and no longer refer to the
  exact mail message.

* External projects consumed in contrib/ should not deviate solely for
  style reasons. Recommended edits should be contributed to those
  projects directly.

Other cases within the Git project were cleaned up by the previous
changes.

Co-authored-by: Junio C Hamano <gitster@pobox.com>
Signed-off-by: Derrick Stolee <dstolee@microsoft.com>
Signed-off-by: Junio C Hamano <gitster@pobox.com>

---
## [mrakgr/The-Spiral-Language@824bbb239d...](https://github.com/mrakgr/The-Spiral-Language/commit/824bbb239dde2ae0df18be40341ae14f37e90cda)
##### 2021-06-16 10:16:26 by Marko Grdinić

"9:45am. Maybe I lounged in bed a bit too long. Let me chill just a bit and then I will start. Today it is time that I start dealing with the UI. Once I have that I will be able test the game.

10:15am. Let me start. It is time to get some things done.

If I am going to fail somewhere, it will be in trying to interface with online sites. The sooner I get to that point, the sooner I can find out the truth. Is it the slightest bit possible to live my life with my own power, or am I a NPC in a Nightmare mode game just responding to the stimulus in my brain at random? Is there meaning to my inspiraiton or am I just another lunatic being fooled by my own subconscious?

10:30am.

```py
init_data = {'my_card': ' ', 'op_card': ' ', 'my_pot': 0, 'community_card': ' ', 'op_pot': 0}
```

To get into it, let me replace this with named tuples.

10:30am.

```
init_data = namedtuple("Data",['my_stack','my_pot','my_card','op_stack','op_pot','op_card','community_card'])(' ',' ',)
```

Wow, it is impressive that Intellisense is giving me the field names even as I write this.

10:35am.

```py
init_data = namedtuple("GameData",['my_stack','my_pot','my_card','op_stack','op_pot','op_card','community_card'])('0','0','  ','0','0','  ','')
```

Doing a bit of cleaning up is not bad as it will allow me to familiarize myself with the codebase.

```
init_actions = {'call': False, 'fold': False, 'raise': False}
class Actions(BoxLayout):
    actions = ObjectProperty(init_actions)
```

Let me change this as well. This time for the raises I'll want a slider.

```
init_data = namedtuple("GameData",['my_stack','my_pot','my_card','op_stack','op_pot','op_card','community_card'])('0','0','  ','0','0','  ','')
class Table(FloatLayout):
    data = ObjectProperty(init_data)

init_actions = namedtuple("GameActions",['is_call','is_fold','is_raise','raise_min','raise_max'])(False,False,False,0,0)
class Actions(BoxLayout):
    actions = ObjectProperty(init_actions)
```

Now both of these should be fine.

Er, should the chips be numeric?

Yes they should.

```
    Card:
        text: root.data['my_card']
        pos_hint: {'x': 0.075, 'y': 0.075}
    SwitchCard:
        shown: False
        text: root.data['op_card'] if self.shown else ' '
        pos_hint: {'right': 0.925, 'top': 0.925}
```

Now let me change the out.

Ah, actually I forgot that the actions should be closures and not just booleans.

10:50am. Focus me.

11am. Stop surfing /biz/. Do the task in front of you.

11:10am.

```
<Actions>:
    canvas:
        Line:
            rectangle: self.x, self.y, self.width, self.height
    Action:
        action: root.actions.fold
        text: 'Fold'
        size_hint_x: 0.3
    Action:
        action: root.actions.call
        text: 'Call'
        size_hint_x: 0.3
    Action:
        action: root.actions.raise_
        text: 'Raise ' + str(round(slider.value))
        size_hint_x: 0.6
    Slider:
        id: slider
        min: 2
        max: 210
        step: 1
        value: 5
```

This works quite nicely. I'll go with this. Now where was I? I have the UI, but now I think the time is to start making use of it.

```
inl amount_min_raise = amount_call + min 1 (amount_call - min p1.pot p2.pot)
```

Agh, this should be a max.

11:35am. I am thinking. There isn't much I can do with the UI right now. In order to make use of it, I need the training function. Let me think about that for a while.

11:45am.

```
inl vs_self game (batch_size, p) =
    let rec loop (l : a u64 _) =
        inl rewards : ra u64 _ = am.empty
        inl actions_indices : ra u64 _ = am.empty
        inl data : ra u64 _ = am.empty
        inl nexts : ra u64 _ = am.empty
        l |> am.iteri fun i => function
            | Action: player_state,game_state,pid,actions,next =>
                rm.add actions_indices i
                rm.add data (player_state,game_state,pid,actions)
                rm.add nexts next
            | Terminal: x =>
                rm.add rewards (i, x)
        inl rewards_actions =
            if 0 < length data then
                inl cs,update = p data
                am.generic.map2 (<|) nexts cs |> loop |> update
            else am.empty
        inl rewards_all : a _ r2 = create (length l)
        am.generic.iter2 (set rewards_all) actions_indices rewards_actions
        am.iter (fun (i,_,_,r) => set rewards_all i r) rewards
        rewards_all
    loop (am.init batch_size fun _ => game pl2_init)
```

I am just looking at this and thinking.

...Instead of just doing that, how about I give it a try.

11:55am.

```
inl vs_human game human_pid p =
    let rec loop = function
        | Terminal: _ as g => g
        | Action: player_state,game_state,pid,actions,next as g =>
            if pid = human_pid then g
            else
                inl cs,_ = p (am.singleton (player_state,game_state,pid,actions))
                next (index cs 0)
    loop game
```

Hmmm, yes. I do not need much more than this.

12:15pm. Right now I am just reving up my mind for the next step. Let me have breakfast here. Then I will bring in the uniform player knock out the UI interface. This is going to be done today. Then I'll thoroughly test the game before moving on to training the agents on it."

---
## [ingdongdung/RunAndBuild@1dcc5d5248...](https://github.com/ingdongdung/RunAndBuild/commit/1dcc5d5248c8f15758a2ea98a03e7e5c1e513b29)
##### 2021-06-16 11:08:02 by ouoaz

보스 작업 중

enemy script 기반으로 boss script 만들고, 3개의 보스에 대한 script 따로 설계하는 것이 목표

보스몹
Mage Female 03
Necromancer Female 01
Warrior Male 07

보스몹 이펙트 mage - necromancer - warrior 순
CFX_Explosion_B_Smoke+Text

CFX2_EnemyDeathSkull
CFX3_Skull_Explosion
CFX3_MagicAura_D_Runic

CFX_Hit_A Red+RandomText
CFX2_Blood
으로 설정 후 진행할 예정

first, mid boss는 원거리라 평타 공격에 필요한 프리팹 필요

---
## [jupyterkat/BeeStation-Hornet@1405821172...](https://github.com/jupyterkat/BeeStation-Hornet/commit/1405821172a86f948711881af641b9d384719ab2)
##### 2021-06-16 12:44:02 by bluezorua

adds a new clown mask, the madman, a madness combat reference (#4356)

* funni reference

* fixes bug

* oh fuck oh shit im stupid

---
## [PIOTRFRACZEK/Class-Projects@c3a64c8317...](https://github.com/PIOTRFRACZEK/Class-Projects/commit/c3a64c8317ff3349c0b973f1af28c4bb3c63612e)
##### 2021-06-16 14:52:43 by PIOTRFRACZEK

Add files via upload

CODING EXERCISE 3

Your line manager asks you to produce an interactive Quiz game for an upcoming event. He doesn’t want anything too fancy, but he wants something neat and user-friendly.
THE MANAGER’S INSTRUCTIONS:
The beginning of your code must: display the author’s name (i.e., code author), clearly state the purpose of the game, outline the rules of the game, display the name of the game (choose an appropriate name) and version of the release, and show the copyright details, i.e., © [author name].

When the user launches the game, he or she must be greeted with a message in the form “Welcome to the world of [Name of game], [Name of player]!!”. Next, what appears on-screen must be the rules of the game and instructions on what he or she must do to play the game pleasantly. From there, the game must ask the user the following question:
Are you happy to play the game with those rules? His or her answer must be a form of “yes” or “no” (your pick). If the user chooses “yes”, then start the game immediately. On the other hand, if the user’s answer is “no”, then end the game and display the message: “Sad to see you go [Name of player]. We hope to see you next time!!” 
If the user enters anything other than yes or no or derivatives thereof, capture that as an error within your code and display it on-screen in the form of a neat message that INSTRUCTS the user they must enter yes or no to proceed. Give them the option to enter yes or no again.

REQUIREMENTS AND RULES:
The game is a series of multiple-choice questions, meaning a question must be posed, and the user must choose the correct answer from a few given choices.
The user needs a way to enter their name before gameplay. Their name can be anything, so there shouldn’t be any restrictions.
The user should have an option to inform the game that they want a certain number of questions to answer in a given round. To that end, limit their options to either a 3 or 5 question round. Catch any errors, make said errors user-friendly, and give the user the option to enter their data correctly again.
You must create 8 to 10 questions in total (up to you what they’re), each with 3 choices: a- [possible answer 1], b- [possible answer 2] and c- [possible answer 3]. One of them must be the correct answer. The user must also have an option to enter their answer which, can only be a, b or c. Catch any potential errors that the user might make, display them into nice, helpful messages, then re-give them the option to enter their answer again.
From your pool of 8 to 10 questions, the computer must display them randomly, one at a time, every time the user answers a question, whether correct or incorrect. However, questions must not be repeated. 
If the question is answered correctly, the user should be awarded 10 points with the following message: “Congrats [name of player]. You got 10 points!!”. If the question is answered incorrectly, the user gets 0 points with the following message: “Aww, what a shame. That wasn’t the correct answer. You got 0 points”.
At the end of the round, sum their points. For a round of 3 questions, they must get two questions correct to win, i.e., 20 points; and, for a 5-question round, four correct answers are needed, i.e., 40 points, to be declared the victor. If the user is the victor, display the message “Bravo, bravo [Name of player]! You’ve won the round!”. On the other hand, if the user loses the game, display the message “Oh dear. So close and yet so far. Good luck next time [Name of player]!”
At the end of a game, regardless of the outcome, the game must ask the user the following question: Would you like to play again? If he or she answers yes, relaunch the game with the last entered round. If their response is no, the message should say: “Thank you for playing [Name of game]. See you next time!”. End the game. And, of course, catch any errors and package them into something digestible for the user. Give the user the option to enter their answer again.    

ADDITIONAL REQUIREMENTS:
Build the program in Python and add your source code file to your Github repository. Do not forget to use comments to show your steps.

---
## [kalyan-hub/Deep_Learning_project@115eb0ca9a...](https://github.com/kalyan-hub/Deep_Learning_project/commit/115eb0ca9a149b34bb61a16e0efbb24ca379f460)
##### 2021-06-16 15:02:07 by kalyan-hub

Create Intro

Deep learning has seen tremendous progress over the past few years. This is largely due to the emergence of deep learning frameworks such as PyTorch and TensorFlow, which have greatly simplified even the most sophisticated research.
With smartphones having become the devices we use the most, the next wave of innovation is going to center on how we can leverage these rapid advances in deep learning to enhance our smartphone experiences. AI-powered mobile applications will become (and already are) smart enough to understand us and help us perform tasks via visual perception, language understanding, and voice recognition, even when not connected to the internet.
To accelerate the deployment of AI models on mobile devices, Facebook has just released PyTorch Mobile, which enables developers to deploy any PyTorch model to both Android and iOS. With PyTorch having become the most used deep learning framework for research, the mobile version will unlock a vast ecosystem of research and development that will ultimately enhance mobile experiences.
In this post, I’ll show how to take a PyTorch model trained on ImageNet and use it to build an Android application that can perform on-device image classification—taking a picture of any object and telling what it is.
Deploying A PyTorch model to Android requires the steps below:
Convert your model to TorchScript format (Python)
Add PyTorch Mobile as a Gradle dependency (Java)
Load your saved model with PyTorch Mobile to perform predictions (Java)
Setup
PyTorch
Visit pytorch.org to install the latest version of PyTorch for your operating system. If you already have a previous version installed, please upgrade to version 1.3.0 (minimum).
Android Studio
Install a recent version of Android Studio.
Model Conversion
To use our PyTorch model on Android, we need to convert it into TorchScript format. Luckily, this is quite an easy process. Below, we’ve loaded a pre-trained MobileNetV2 model, converted it into TorchScript, and saved it for use in our app. We can use any CNN architecture here; however, MobileNetV2 is highly optimized for both high speed and high accuracy on mobile devices.

Run this code and it will save your converted model as mobilenet-v2.pt—you can name your saved model anyway you wish.
It takes a lot of tweaking and fine-tuning to move from V1 of a mobile-ready model to one that’s ready for production. Fritz AI Studio is designed to streamline processes across the project lifecycle.
Building Our Mobile AI Application
Now that we have our model ready in a deployable format, fire up Android Studio, and create a new project named PytorchAndroid . While you can use any name, please stick to this naming for this tutorial to ensure code compatibility with my examples.
Step 1 : Add PyTorch Mobile to your Android project
With your project created in Android Studio, open the app’s build.gradle file and add PyTorch Mobile and TorchVision Mobile, as shown below:
implementation ‘org.pytorch:pytorch_android:1.3.0’
implementation ‘org.pytorch:pytorch_android_torchvision:1.3.0’

Android Studio will prompt you to synchronize your project. Click Sync Now and the dependencies will be downloaded.
Step 2: Add the model to your assets folder
Create an asset folder for your app by right clicking on app and navigating to New -> Folder -> Assets Folder. Now, copy the mobilenet-v2.pt file into the assets folder.
Step 3: Add the ImageNet label
In your main app package, create a file named Constants.java and put the contents of the file linked below into it:
johnolafenwa/PytorchMobile
You can't perform that action at this time. You signed in with another tab or window. You signed out in another tab or…
github.com

Note that I’m not previewing this file here, as a mapping of 1000 indexed names is too long.
Step 4: Add a Classifier class
In your main app package, create a file named Classifier.java and put the following in it.

This class is the most important part of our project—everything else is standard android stuffs. Hence, we shall break it down a bit.
This class defines the module and float arrays for the mean and standard deviation pre-processing. The model is loaded in the constructor.
The preprocess function takes in an image bitmap, resizes it to the specified size, performs mean and std pre-processing, and returns a tensor that can be input into our model.
The argmax function simply takes in scores and returns the index with the maximum score.
The predict function takes in any image bitmap, processes it into a tensor, runs it through the model to obtain a prediction, finds the maximum class using argmax, and finally, using the constants we created in step 3, it obtains the corresponding class name and returns it.
Step 5: Add the Utils class
Create the file Utils.java file containing the code below. It retrieves the absolute path to an asset file. We shall need this to retrieve the path to our model, which we added to assets earlier.

Kickstart your mobile machine learning projects — subscribe to the Fritz AI Newsletter for looks at common mobile ML problems, helpful resources, and much more.
Step 6: Add the Main Activity
Having created all our helpers and processors, add a new Basic Activity. Let the contents of the MainActivity.java file be as follows:

The activity_main.xml file should contain the following:

And content_main.xml:

Our main activity above does one simple thing. On the click of the button, it launches an external camera app. Once an image is captured, a bitmap is obtained from the data returned, and the classifier is used to predict the class of the image. The results are then passed to another activity for display.
Step 7: Add the Result Activity
Create a new Basic Activity named Result and put the following in the Result.java file:

Add this in the activity_result.xml:

Add this in the content_result.xml:

And that’s it! That’s all the files we need.
Building the app
Now, having followed all the steps above, build and run the application on an actual Android phone.
Below are screenshots from my phone:



You should test the app with a variety of items and see the result.
Summary
In this post, we’ve successfully built an Android application that’s able to recognize 1000 categories of items using a model that was originally trained with PyTorch.
This is intended as a reference and a starting point for doing a lot of amazing things with AI on Android, powered by PyTorch Mobile.

---
## [newstools/2021-sahara-reporters@34ba45b52a...](https://github.com/newstools/2021-sahara-reporters/commit/34ba45b52aa397efd07735cc342de9603c7b5664)
##### 2021-06-16 15:52:31 by Billy Einkamerer

Created Text For URL [saharareporters.com/2021/06/16/secondary-school-boy-kill-girl-refusing-be-his-girlfriend]

---
## [pytorch/pytorch@435d945bbd...](https://github.com/pytorch/pytorch/commit/435d945bbdd2bb6e4d7d2c39c7699092ad5c6526)
##### 2021-06-16 18:54:08 by Brian Hirsh

Update base for Update on "add a boxed CPU fallback kernel"

This PR replaces the existing code-generated CPU fallback kernels that XLA uses with a single boxed CPU fallback.

Current state: there are a couple different design ideas that I want to point out, but the logic for the actually kernel is mostly done and passing tests.

### Design

To preface, I'm not 100% tied to the current design and I'm putting the PR up now for opinions and totally open to alternatives, some of which I listed below. Actually after writing this description, I'm leaning toward the following changes:
* Confirm whether or not we can remove all C++ logging info directly in the yaml.


**Current Design**

All of the CPU fallback codegen is deleted. In its place, XLA (and other external backends, later) can choose to opt into a CPU fallback by adding the following code in a C++ file. I have an corresponding [xla-side PR with the xla changes](https://github.com/pytorch/xla/pull/2945/files#diff-1a005c10039f0cb11130a3b740f5de716d2f10acaea121017016025861886798R1).

There's no actual requirement to split up the code into a .h and .cpp file, but that's necessary in the XLA case because they sometimes need to call the fallback directly from their handcrafted kernels.

```
// xla_cpu_fallback.h
#include <ATen/native/CPUFallback.h>
...
void xla_cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack);
...
```
```
// xla_cpu_fallback.cpp
#include "torch_xla/csrc/aten_cpu_fallback.h"
...
void xla_cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
  // Do custom logging here
  ...
  // Call the actual boxed CPU fallback.
  at::native::cpu_fallback(op, stack);
}

TORCH_LIBRARY_IMPL(_, XLA, m) {
  m.fallback(torch::CppFunction::makeFromBoxedFunction<&xla_cpu_fallback>());
}
```

Now that the fallback is exposed in the backend, they can call it directly. Doing so requires converting from an unboxed to a boxed context, which we provide a utility function before. E.g.:
```
#include <ATen/native/CPUFallback.h>

at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::native::call_fallback_fn<&xla_cpu_fallback, decltype(at::addmm)>::call("aten::addmm", self, mat1, mat2, beta, alpha);
  }
  ...
}
```

That `decltype(at::addmm)` logic isn't actually used everywhere in the xla-side PR yet, since you hit issues with overloads. I could use it everywhere once #58092 lands.

**Alternatives: The API for calling the CPU fallback directly is ugly, can we make it nicer?**
We could change the api to use `at::redispatch`, which would make it look something like this:
```
at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::redispatch::addmm(c10::DispatchKeySet(c10::DispatchKey::CPUFallback), self, mat1, mat2, beta, alpha);
  }
  ...
}
```
Which definitely feels cleaner, but also requires adding a new DispatchKey just for this use case. Conditionally calling the CPU fallback doesn't sound like a hugely important use case, so I don't know if giving up one of our 64 dispatch key slots is worth the API improvement. Totally open to other opinions though!


Another more mild improvement that would avoid having to pass operator string names (including overloads) around would be to codegen (yet another) namespaced API. Something like this:
```
at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::fallback::addmm<&xla_cpu_fallback>(self, mat1, mat2, beta, alpha);
  }
  ...
}
```

Writing that out actually I actually like it more (I think it'll let us get rid of `decltype(...)`). Maybe that is nice enough to warrant a new codegen API - I haven't tried adding that yet, but if people like it I'm happy to try it out.

**More alternatives**
The current design also involves the backend manually writing and registering the boxed fallback themselves, but an alternative would be for us to do it in codegen too: they would just need to pass in all of the C++ logging that they want done in the fallback, directly through the yaml. The main downsides:
* Backend code that wants to call the fallback needs to abide by whatever convention our codegen uses to name the generated boxed fallback.
* Passing custom C++ logging through yaml is just more fragile: right now xla uses an `iostream` to log each tensor arg in the operator, so we'd have to either force other backends into the same convention or figure something else out later.

To be fair, we actually already do that: XLA has custom per-tensor-arg logging for all of the generated `out` wrappers in the codegen, which we do by passing their C++ logging info through the yaml. This seems unnecessary though, since `out` wrappers just call into a functional kernel, which is hand written with its own custom logging. So my take is: try to remove custom C++ logging from the yaml, and if it turns out to be really necessary, then we may as well take advantage of that to codegen the fallback.

### Performance impact

While ops that fall back to CPU aren't exactly hot path, we probably don't want to use a boxed fallback if it turns out to be an absolute perf killer.

I ran my benchmarks using callgrind, benchmarking both `at::add` and `at::add_out` run on XLA. My callgrind benchmark for `at::add` can be found here (the add_out benchmark looks basically the same): https://www.internalfb.com/phabricator/paste/view/P415418587. I created the benchmark by hacking the existing xla C++ test build scripts and throwing in a reference to callgrind.

I also attached the full callgrind output for each benchmark; the full output is actually pretty noise and hard to parse, but I focused on everything underneath the `at::add()` call in the output, which was much more stable. My guess is that it's due to some heavyweight async startup processing that xla does.

`at::add`:
before: 88,505,130 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415421001
after: 102,185,654 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415421273
delta: ~15.5% increase

`at::add_out`:
before: 63,897,395 instructions. Full output: https://www.internalfb.com/intern/everpaste/?handle=GBrrKwtAPlix9wUEAOZtrFXpdO5UbsIXAAAz
after: 73,170,346 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415423227
delta: ~14.5% increase

High level takeaway: A framework overhead increase of 10-20% doesn't seem too horrible for the CPU fallback use case.

For structured, functional ops that requires a CPU fallback, we're actually in an unfortunate situation: we're doing even more work than necessary. Our codegen automatically creates a `CompositeExplicitAutograd` kernel which calls into the `out` operator. So the extra work that we end up doing is:
* An extra dispatcher hop: (at::add -> CompositeExplicitAutograd -> CPUFallback -> at::native::add) instead of (at::add -> CPUFallback -> at::native::add)
* An unnecessary tensor allocation (the CompositeExplicitAutograd kernel uses at::empty() to create an output tensor, which is immediately overwritten by the CPU fallback)
* An unnecessary meta() call (the CompositeExplicitAutograd kernel calls it to create the output tensor, but we call it again in the CPU kernel).
* unboxing->boxing->unboxing logic (this is the only strictly required piece)

There are definitely ways to avoid the unnecessary work explained above: one would be to give the boxed fallback higher priority than composite keys (there's [an issue for it here](https://github.com/pytorch/pytorch/issues/55104)), and codegen fallthroughs for all composite ops. It'll require more infra to set up, so I see it as more of a perf knob that we can apply if we need it later.

Unfortunately I couldn't dig much deeper into the differences aside from the aggregate change in instructions, since it looks like callgrind fudged some of the instruction attribution (`at::to_cpu` takes up a ton of instructions, but I don't see any attribution for the `at::native::add` kernel anywhere).


Differential Revision: [D28833085](https://our.internmc.facebook.com/intern/diff/D28833085)

[ghstack-poisoned]

---
## [pytorch/pytorch@23cf255ea4...](https://github.com/pytorch/pytorch/commit/23cf255ea45f4dcbbcec5bc9b71ffcc7505593be)
##### 2021-06-16 18:54:14 by Brian Hirsh

add a boxed CPU fallback kernel

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58065

This PR replaces the existing code-generated CPU fallback kernels that XLA uses with a single boxed CPU fallback.

Current state: there are a couple different design ideas that I want to point out, but the logic for the actually kernel is mostly done and passing tests.

### Design

To preface, I'm not 100% tied to the current design and I'm putting the PR up now for opinions and totally open to alternatives, some of which I listed below. Actually after writing this description, I'm leaning toward the following changes:
* Confirm whether or not we can remove all C++ logging info directly in the yaml.


**Current Design**

All of the CPU fallback codegen is deleted. In its place, XLA (and other external backends, later) can choose to opt into a CPU fallback by adding the following code in a C++ file. I have an corresponding [xla-side PR with the xla changes](https://github.com/pytorch/xla/pull/2945/files#diff-1a005c10039f0cb11130a3b740f5de716d2f10acaea121017016025861886798R1).

There's no actual requirement to split up the code into a .h and .cpp file, but that's necessary in the XLA case because they sometimes need to call the fallback directly from their handcrafted kernels.

```
// xla_cpu_fallback.h
#include <ATen/native/CPUFallback.h>
...
void xla_cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack);
...
```
```
// xla_cpu_fallback.cpp
#include "torch_xla/csrc/aten_cpu_fallback.h"
...
void xla_cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {
  // Do custom logging here
  ...
  // Call the actual boxed CPU fallback.
  at::native::cpu_fallback(op, stack);
}

TORCH_LIBRARY_IMPL(_, XLA, m) {
  m.fallback(torch::CppFunction::makeFromBoxedFunction<&xla_cpu_fallback>());
}
```

Now that the fallback is exposed in the backend, they can call it directly. Doing so requires converting from an unboxed to a boxed context, which we provide a utility function before. E.g.:
```
#include <ATen/native/CPUFallback.h>

at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::native::call_fallback_fn<&xla_cpu_fallback, decltype(at::addmm)>::call("aten::addmm", self, mat1, mat2, beta, alpha);
  }
  ...
}
```

That `decltype(at::addmm)` logic isn't actually used everywhere in the xla-side PR yet, since you hit issues with overloads. I could use it everywhere once #58092 lands.

**Alternatives: The API for calling the CPU fallback directly is ugly, can we make it nicer?**
We could change the api to use `at::redispatch`, which would make it look something like this:
```
at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::redispatch::addmm(c10::DispatchKeySet(c10::DispatchKey::CPUFallback), self, mat1, mat2, beta, alpha);
  }
  ...
}
```
Which definitely feels cleaner, but also requires adding a new DispatchKey just for this use case. Conditionally calling the CPU fallback doesn't sound like a hugely important use case, so I don't know if giving up one of our 64 dispatch key slots is worth the API improvement. Totally open to other opinions though!


Another more mild improvement that would avoid having to pass operator string names (including overloads) around would be to codegen (yet another) namespaced API. Something like this:
```
at::Tensor addmm(const at::Tensor& self,const at::Tensor& mat1,const at::Tensor& mat2,const at::Scalar& beta,const at::Scalar& alpha) {
  ....
  if (...call_fallback...) {
    return at::fallback::addmm<&xla_cpu_fallback>(self, mat1, mat2, beta, alpha);
  }
  ...
}
```

Writing that out actually I actually like it more (I think it'll let us get rid of `decltype(...)`). Maybe that is nice enough to warrant a new codegen API - I haven't tried adding that yet, but if people like it I'm happy to try it out.

**More alternatives**
The current design also involves the backend manually writing and registering the boxed fallback themselves, but an alternative would be for us to do it in codegen too: they would just need to pass in all of the C++ logging that they want done in the fallback, directly through the yaml. The main downsides:
* Backend code that wants to call the fallback needs to abide by whatever convention our codegen uses to name the generated boxed fallback.
* Passing custom C++ logging through yaml is just more fragile: right now xla uses an `iostream` to log each tensor arg in the operator, so we'd have to either force other backends into the same convention or figure something else out later.

To be fair, we actually already do that: XLA has custom per-tensor-arg logging for all of the generated `out` wrappers in the codegen, which we do by passing their C++ logging info through the yaml. This seems unnecessary though, since `out` wrappers just call into a functional kernel, which is hand written with its own custom logging. So my take is: try to remove custom C++ logging from the yaml, and if it turns out to be really necessary, then we may as well take advantage of that to codegen the fallback.

### Performance impact

While ops that fall back to CPU aren't exactly hot path, we probably don't want to use a boxed fallback if it turns out to be an absolute perf killer.

I ran my benchmarks using callgrind, benchmarking both `at::add` and `at::add_out` run on XLA. My callgrind benchmark for `at::add` can be found here (the add_out benchmark looks basically the same): https://www.internalfb.com/phabricator/paste/view/P415418587. I created the benchmark by hacking the existing xla C++ test build scripts and throwing in a reference to callgrind.

I also attached the full callgrind output for each benchmark; the full output is actually pretty noise and hard to parse, but I focused on everything underneath the `at::add()` call in the output, which was much more stable. My guess is that it's due to some heavyweight async startup processing that xla does.

`at::add`:
before: 88,505,130 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415421001
after: 102,185,654 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415421273
delta: ~15.5% increase

`at::add_out`:
before: 63,897,395 instructions. Full output: https://www.internalfb.com/intern/everpaste/?handle=GBrrKwtAPlix9wUEAOZtrFXpdO5UbsIXAAAz
after: 73,170,346 instructions. Full output: https://www.internalfb.com/phabricator/paste/view/P415423227
delta: ~14.5% increase

High level takeaway: A framework overhead increase of 10-20% doesn't seem too horrible for the CPU fallback use case.

For structured, functional ops that requires a CPU fallback, we're actually in an unfortunate situation: we're doing even more work than necessary. Our codegen automatically creates a `CompositeExplicitAutograd` kernel which calls into the `out` operator. So the extra work that we end up doing is:
* An extra dispatcher hop: (at::add -> CompositeExplicitAutograd -> CPUFallback -> at::native::add) instead of (at::add -> CPUFallback -> at::native::add)
* An unnecessary tensor allocation (the CompositeExplicitAutograd kernel uses at::empty() to create an output tensor, which is immediately overwritten by the CPU fallback)
* An unnecessary meta() call (the CompositeExplicitAutograd kernel calls it to create the output tensor, but we call it again in the CPU kernel).
* unboxing->boxing->unboxing logic (this is the only strictly required piece)

There are definitely ways to avoid the unnecessary work explained above: one would be to give the boxed fallback higher priority than composite keys (there's [an issue for it here](https://github.com/pytorch/pytorch/issues/55104)), and codegen fallthroughs for all composite ops. It'll require more infra to set up, so I see it as more of a perf knob that we can apply if we need it later.

Unfortunately I couldn't dig much deeper into the differences aside from the aggregate change in instructions, since it looks like callgrind fudged some of the instruction attribution (`at::to_cpu` takes up a ton of instructions, but I don't see any attribution for the `at::native::add` kernel anywhere).

Differential Revision: [D28833085](https://our.internmc.facebook.com/intern/diff/D28833085/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D28833085/)!
ghstack-source-id: 131615077

---

# [<](2021-06-15.md) 2021-06-16 [>](2021-06-17.md)

