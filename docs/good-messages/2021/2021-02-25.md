# [<](2021-02-24.md) 2021-02-25 [>](2021-02-26.md)

3,033,713 events, 1,538,601 push events, 2,398,025 commit messages, 177,809,542 characters


## [Koi-3088/ForkBot.NET@6b1222b6d4...](https://github.com/Koi-3088/ForkBot.NET/commit/6b1222b6d4ab30321b2f0eb13958ebfafd199e64)
##### 2021-02-25 08:26:46 by Koi-3088

Minor clean.
Revise TradeCord "traded" check, remove potential user path straggler entries because paranoia, some minor fixes.
TradeCord fixes (shocker, I know).
Extract Json serializer.
Minor clean and fixes.
Minor fixes.
Fix Milcery when an Alcremie variant is a parent.
Update to latest Core and ALM dependencies.
Handle non-shiny events in a better way.
Work around a race condition?
Simplify and de-bugify trade completion check.
Fix indexing, improve chance for Melmetal-Gmax because it's nigh impossible to get.
Rework TradeCord internals, add new functionality:
-Migrate user data from ".txt" files to a serialized Json (migration for a large amount of users will take a few minutes, be patient).
-Make TradeCord configurable, add its own settings category.
-Add some template events with an optional end timer (YYYY/MM/DD 8PM as an example, though any local time format should work).
-Add barebones Pokedex (counter, flavor text).
-Can check dex completion by typing `$dex`, check missing entries by typing `$dex missing`.
-Completing the Pokedex will slightly improve shiny rate.
-Can now mass release cherish event Pokemon and shinies ($massrelease shiny/cherish).
-Various tweaks, improvements, and bugfixes.

Slightly change FixOT's behavior:
-If a shown Pokemon is illegal and an event, attempt to find a match within the MGDB first.
-Try to force users to trade away the shown Pokemon, log attempt to change shown Pokemon.
Add consideration for easter eggs being enabled in settings, fix Suicune
Change species rng for TradeCord, some bugfixes (I really need to rewrite this mess)
Add check if we're using ListUtil for Giveaway instead of TradeCord.
Amend commit since I'm squashing and force-pushing while bringing the fork in line with the main branch
Add Giveaway module to Discord bot (#22)

Thanks, rigrassm.
Co-authored-by: Koi-3088 <61223145+Koi-3088@users.noreply.github.com>
Specify USB port instead of adding the first result (can be found via Device Manager).
Re-add boolean check because we don't want to fix everything
FixOT will attempt to regenerate illegal Pokémon.
Apply trash bytes for reasons.
Minor TradeCord fixes and adjustments.
Minor clean for C#9
Use "GetValidPreEvolutions()" instead of "GetPreEvolutions()".
Index forms correctly.
Fix the fixed and re-introduced empty daycare index error.
*an* Ultra Ball.
Add EvoTree breeding for TradeCord.
Remove unnecessary value declarations for pinging on encounter match.
Mildly beautify EncounterBot mark output.
Integrate Anubis' system update prevention into Soft Reset and Regigigas Encounter Modes.
Rename "Regi" Encounter Mode to "Soft Reset".
Speed up "A" clicks for Regigigas and Soft Reset modes.
Add Mark logging output for EncounterBot.
Fix oops (re-order logic, remove unnecessary lines).
Add optional species and form specification for $massrelease
Use an obscure string splitter because people like symbols in their names.
Fix things that broke after rebasing to the latest main repo commit.
Use a less unfortunate field name and value splitter...again.
Fix Marowak-Alola always generating as an NPC trade.
Add filters for "$list <species>" to narrow down results.
Fix Cherish Pichu and Octillery
Stop making dumb mistakes, me (implying the rest of it isn't a dumb mistake).
Can't breed antiques.
Use a less unfortunate embed name and value splitter
Add Melmetal-Gmax to TradeCord.
Add ability to search by caught ball.
Have MassRelease ignore events.
Add specific regional form breeding.
Revise egg rate and egg shiny chance.
Have trade evolutions hold an Everstone.
Add an extra right click when navigating to settings for AutoRoll.
Add reworked encounter/egg/fossil logs.
Minor clean.
Minor clean.
Get rid of EncounterBot, FossilBot, EggFetch text logs until I properly rework them.
Break on an empty page due to aggressive rounding
Add multi-page lists for Tradecord.
More random bugfixes.
Fix some bugs before major clean
Add Language parameter for TradeCord.
Change trainer info input format for TradeCord.
Move focus on Showdown set instead of randomizing a pkm file.
Allow user to enter whatever they want for $list, handle edge cases like Kommo-o
Add "$list all" to show non-duplicate caught species.
Automatically remove from favorites if trading or gifting (small QOL thing).
Change how favorites are removed from user file.
Revert base egg shiny chance nerf.
Fix daycare
Add favorites command to TradeCord.
Slightly nerf eggs.
Fix TradeCord list for shinies
Add TradeCord (my dumbest and messiest project so far, Archit pls don't hate the mess).
Add Showdown output for Star/Square shinies and OTGender.
Add optional link code input for FixOT.
Change how OTName, TID, SID is displayed.
Add Regigigas SR bot.
Add SoJ Camp SR bot.
Ribbons now work with EggTrade (remove ribbons if egg).
Remove EggRoll.
Add another filter for FixOT
Fix.. FixOT
Update offsets for EncounterBot catching.
Slightly change StrongSpawn to work with Regi SR and make it its own mode.
Make SpinTrade only available for USB-Botbase
Update valid eggs for CT
winforms: resize icon.ico to fix crash at startup on unix using mono
Rework Spin, read initial in-game coordinates in order to correct drift
Add TID, SID, Language output for Showdown
Remove obsolete OT and Language parsing
Very minor clean until I have time for a proper one.
Detach controller when stopping USB bot.
Actually set LastUsedBall for EncounterBot (missed when bringing in line with main repo)
Move extra RaidBot timings following the official commit
Remove PKHeX Discord invite from Readme.md

Maybe fewer people will pester devs now about my unofficial fork?
Update for latest main repo EncounterBot commits.
Update README.md
Add back best commit: Red's SpinTrade.
Add egg trades, foreign Dittos and OT for Twitch.
If ItemMule is enabled, also display the item a user is receiving.
Add periodic time sync toggle for all methods of hosting (except for non-soft locked AutoRoll) to (hopefully) prevent den rollover during extended hosts.

Add routine to exit a lobby for SoftLock if no players are ready in time (to preserve soft lock).

Add a routine to recover from disbanded lobbies (when someone disconnects unexpectedly) for SoftLock.

Add a routine to restart game if all else fails and we're stuck in a raid.

Add a routine for adding and deleting friends if we're soft locked and raids go empty.

Slightly reorganize settings, extract methods, minor clean.
Don't use such a generic file name for stream assets.
Check USB port index for running bots. Should fix adding additional USB bots when no config is saved.
Add fixed met date for FixOT.
How do I boolean
Change airplane mode logic, tweak timings and routine for soft lock lobby exit
Rework EggRoll cooldown (static list in favor of a txt file).
Start clean up and refactor
Add setting to increase delay after pressing "Home" after a date skip.
Use USB port index for blocking and sprite pngs if connection type is USB
Add option for airplane host (usb-botbase required)
Add option to softlock on selected species for AutoRoll
Add automatic compatibility for all console languages when date skipping (have to set ConsoleLanguage under ScreenDetection)
Attempt to fix multiple USB device add and connect...again
Minor clean
Fix oops?
Handle add/remove of bots
Distinguish between multiple USB devices, tweak BotRemoteControl for USB, other various fixes
Add SpA modifier for foreign Dittos
Add alpha USB-Botbase support
Fix DateTime parsing for European format for EggRoll
Set fixed EggMetDate and MetDate for EggRoll
More FixOT filters
Remove Beheeyem. Oops.
Split EggRoll into its own routine and trade type, only output "Receiving: Mysterious Egg" if routine is EggRoll, other minor tweaks and fixes
Make FixOT its own queue with roles and counts
Add a couple more OTs to $fix
Parsing for EggRaffle auto-clear and $clearcooldown
Adjust timings and split Watt collecting clicks for AutoRoll
Fix oops with file attachments for Ditto
Further improvements for OT, memes for invalid pokemon (disable EasterEggs)
Add spaces, digits for OT
Randomize memes, cut down bloat
Fix miscellaneous bots after Anubis' recent QOL additions
-Ignore events for OT because headache.
-Add overlooked "$convert <generation>" input for OT.
-Move $clearcooldown to SudoModule
-Clear timer automatically if NoTrainerFound
-More reliable Dittos
-Foreign Dittos for $convert
-Command to clear cooldown for EggRaffle in case trade gets disconnected
-Fix "Trade finished" line to keep result secret
-EggRaffle as a toggle, option to specify channels
-Seed Check output to both DMs and Channel (apparently some want it)
-Randomly generated egg raffle via a "$roll" command with a configurable cooldown
-FixAdOT reworked, has its own command "$fix" and no longer overrides $clone
-Ball: <value> output for Showdown sets
-Fix oversight
-Option to output Seed Check results to Discord channel with a User mention
-Showdown set output for OT name and eggs
-Basic "OT: <name>" option without Showdown set output
-Initial $convert support for EggTrade
-Egg moves for EggTrade test attempt
-Minor update
-EggTrade (by nicknaming a Pokémon "Egg" using $trade)
-Failsafe for memes if enabled but field left blank or incomplete
-Niche breedable Ditto trade mode.
Add minimize button
EggFetch text logs
StrongSpawn mode for EncounterBot
Re-add EncounterBot Master Ball catching
More parsing for FixAdOTs
Park Ball as held item instead of string
Actually remove the offset instead of saying I did
Initial DLC commit
Faster code entry
Removed catching for EncounterBot (need a new offset)
CloneBot mode to fix Nickname and OT if adverts detected

---
## [silont-project/android_kernel_xiaomi_sdm660@bd91932583...](https://github.com/silont-project/android_kernel_xiaomi_sdm660/commit/bd9193258357d62db69c1f2798568f6d9ff5094c)
##### 2021-02-25 09:19:33 by Reinazhard

[SQUASH][TEST]ext4: revert upstream

This is fucked, this is actually fucked, this is so stupid, no. Are you seeing this shit?

Signed-off-by: Reinazhard <muh.alfarozy@gmail.com>

---
## [whisperity/dotfiles@304b0ca1c2...](https://github.com/whisperity/dotfiles/commit/304b0ca1c2cd33f3bf91a7fede17a09f72db594f)
##### 2021-02-25 10:09:05 by Whisperity

[vim] Use <Leader>t(t|b|n) for tab switching

Could you ever image this crap to be so God-forsaken hard?!
Local Linux machine doesn't do fuck all in case of Ctrl-F2.
Through PuTTY, Shift-Fx keybinds are not working.
Setting PuTTY to "TERM=putty" makes Ctrl-Fx and Fx emit the same key
sequence.

Welp. I rarely use the "tabs" feature, so... Let's just use a different
keybind to the whole damn thing and forego this madness with the
function keys.

---
## [mrakgr/The-Spiral-Language@a2ac3f5f32...](https://github.com/mrakgr/The-Spiral-Language/commit/a2ac3f5f32074ed9afed04453766c9dbd6c65035)
##### 2021-02-25 10:37:51 by Marko Grdinić

"9:20am. I am up early today at last. Yesterday I took the time to do the review, so I managed to get a few things off my chest.

9:30am. Yeah, I want to make some time to apply to companies and get this thing out of the way. I thought I would do programming today, but let me focus on the thing I put aside during February.

I am torn between two paths. The first one is the old path of trying to get better at directly ML through various means.

The new one came with my focus on user experience and concurrency. I want to go down this new path. I should forget how much I wanted my own ML library in the past, and do more worthy things.

People focus on all kinds of stupid things. I probably got fooled by the ML wave myself. I thought that the main way of tapping into is to raise my skills at it directly, but sometime the direct approach will not get you where you want to go.

9:40am. Let me pull up a few companies and I will fire off an application to them along with that resume I composed. I'll just send it as text, I do not feel like doing artistry with it. I do not have those skills yet.

Let me first chill a bit and then I will do it.

10:05am. Let me start. Let me get 4 candidates and I'll compose a message for them.

Ok, let me pick Graphcore + 3 others.

> I've fired the following to Graphcore, Fathom Computing, Lightelligence and Lightmatter.

Ah, wait. I contacted Graphcore last time. This time I should do Groq then.

LightOn? Has a [bunch](https://www.youtube.com/channel/UCE6xfSWQaCk1hqu7pCISK1w). [Website](https://lighton.ai/).
Luminous? Has a [video](https://www.youtube.com/watch?v=bpR7qGo1VDk).
Optalysys? Has a bunch.

I guess I'll go with these 3. Where is the link for Optalysys?

https://optalysys.com/

https://static1.squarespace.com/static/59ce1714f14aa109dff40f43/t/5fd601a8034a586a4f393db9/1607860657123/Multiply_and_Fourier_Transform_white_paper_12_12_20.pdf

> The processing of information by means of a coherent optical system has been a proposition for over 50 years. Such systems offer extreme processing speeds for Fourier-transform operations at power levels vastly below those achievable in conventional silicon electronics. Due to constraints such as their physical size and the limitations of optical-electronic interfaces, the use of such systems has so far largely been restricted to niche military and academic interests. Optalysys Ltd has developed a patented chip-scale Fourier-optical system in which the encoding and recovery of digital information is performed in silicon photonics while retaining the powerful free-space optical Fourier transform operation. This allows previously unseen levels of optical processing capabilities to be coupled to host electronics in a form factor suitable for integration into desktop and networking solutions while opening the door for ultra-efficient processing in edge devices. This development comes at a critical time when conventional silicon-based processors are reaching the limits of their capability, heralding the well-publicised end of Moore’s law. In this white paper, we outline the motivations that underpin the optical approach, describe the principles of operation for a micro-scale optical Fourier transform device, present results from our prototype system, and consider some of the possible applications.

I am not sure whether these people are hiring, but let me read the paper.

> A very recent development of further interest is lattice methods for performing what is known as Fully Homomorphic Encryption, or FHE.

Cornami video mentioned this.

> Deep learning commonly makes use of high-level frameworks to streamline the construction of network architectures and efficiently execute computations. Our system is designed for integration into common deep learning languages and systemisations such as TensorFlow, Keras and PyTorch via API calls. The prototype device currently supports the input of array data (Python numpy format) into the system via a single Python import: SiliconPhotonicsDevice and optical Fourier transform function: oft(), with all management of the encoding and optical systems handled by the supporting electronics for the device.

Maybe I really was wasting my time making an ML library.

Ok, I will apply to this. Lot of photonics companies out there it seems. One of the survivors will probably be in that category.

Let me start with Optalysys.

10:45am.

///

I am interested in whether I would be able to improve your tech stack through the power of functional programming. More than just applying for a job, I am also interested in novel ML hardware in general for the sake of doing reinforcement learning, so my personal goals overlap with yours.

I'd like to hear what the pain points in your programming work are. I am confident that I could help.

- Marko Grdinić

///

Attachment: my char sheet.md

Let me go with this. This will be my stock email this time.

...Sent. Now which is next? LightOn and Luminous.

https://luminous.co/

This one seems really small. Just 3 people and no email. It had a contact form so I fired a message there. Well, whatever.

10:50am. LightOn seems promising. Of what I'd consider high tier companies, I tried Graphcore last. Groq will be my target this time.

https://groq.com/careers/?gh_jid=4198717003

This is actually an impressive list of requirements.

> As Sr. Compiler Engineer, you will be responsible for defining and developing compiler optimizations for our state-of-the-art spatial compiler - targeting Groq's revolutionary Tensor Streaming Processor.  You will be the technical lead for Groq's TSP compiler, and be in charge of architecting new passes, developing innovative scheduling techniques, and developing new front-end language dialects to support the rapidly evolving ML space.  You will also be required to benchmark and monitor key performance metrics to ensure that the compiler is producing efficient mappings of neural network graphs to the Groq TSP.  Experience with LLVM and MLIR preferred, and knowledge with functional programming languages an asset. Also, knowledge with ML frameworks such as TensorFlow and PyTorch, and portable graph models such as ONNX desired.

> Design, develop, and maintain optimizing compiler for Groq's TSP
> Expand Groq IR Dialect to reflect ever changing landscape of ML constructs and models
> Benchmark and analyze output produced by optimizing compiler, and drive enhancements to improve its quality-of-results when measured on the Groq TSP hardware.
> Manage large multi-person and multi-geo projects and interface with various leads across the company
> Mentor junior compiler engineers and collaborate with other senior compiler engineers on the team.
> Review and accept code updates to compiler passes and IR definitions.
> Work with HW teams and architects to drive improvements in architecture and SW compiler
> Publish novel compilation techniques to Groq's TSP at top-tier ML, Compiler, and Computer Architecture conferences.

You can really tell what they want from this.

https://groq.com/careers/?gh_jid=4168648003

> Must be familiar with functional programming and persistent data structures
> Excellent programming skills in Haskell, Scala, ML, or C/C++
> Background in compiler design, instruction scheduling, or memory allocators
> Comfortable with bit-level debugging
> Ability to provide excellent technical documentation
> Experience with code reviews, agile development, code repository and CI/CD development and release cycles
> 3 years experience of shipping production level code
> Bonus: Familiarity with optimization problems or operations research

This last one is exactly my skillset. I love that they want FP experience. It seems nice and easy.

Being a senior engineer is something I could do, but it would be too much responsibility. I don't have leadership skills at this point, nor do I want to lead humans anyway. And most importantly, I want to push Spiral at these companies and get them to sponsor it, not lead their division.

11:10am. Now which one do I apply? The senior or the junior position?

The annoying thing is, it does not matter at all. The positions here are just for show. They aren't remove jobs anyway. I just need to get in touch and have a conversation to see where it goes.

Right now I am sending an unembelished resume to these companies, but even by just applying I am already hiding my intentions. Because I got ghosted on my last batch.

If my current approach gives me trouble, I'll apply for a senior position at my next application. I'll give this world a chance for the time being and not overdo the bullshit. Let me go for the junior position then, at the danger of being naive.

11:20am. That is 4 companies down. I'll pause here for at least a week. I'll need some feedback before I adjust my approach for the rest. No point in spamming.

Devices with general computation capabilities like Groq's would be the best fit for me. Though I applied at those companies, photonics chips are likely to be too restricted at this stage.

11:30am. Let me get rid of that last entry. I should hold my thoughts back on what I should accept and not in this public journal. Let things go as they should.

11:35am. I haven't gotten a reply to the Kivy scheduler question. I am going to have to switch UI library as my main target. I'll go with PyQt.

It is just too rough for me to work on schedulers for a completely new platform. If this was .NET I'd be more confident, but right I am still testing the waters and don't want to strain myself. I can always write bindings for Kivy later when I have more exp.

Let me end the morning session here. It is time for a break. When I resume, I will start practicing Python Rx in earnest."

---
## [ipld/go-ipld-prime@7d918468f4...](https://github.com/ipld/go-ipld-prime/commit/7d918468f40d06d8a64a2fb6c162134da497bebb)
##### 2021-02-25 12:08:16 by Eric Myhre

Merge-ignore branch 'schema-dmt-unification'

This commit is a merge commit, but using the "ours" strategy -- meaning
that the contents are functionally ignored and have no impact on the
branch they're being merged into.

Something went off the rails at some point here, and I'm not sure what,
but I'm calling it quits and this work will have to be rebooted in
a fresh set of patches sometime in the future.

The "hackme" file discussing the choices: probably salvagable.
It's not perfect (some sections read harsher than others; should redo
it with a table that applies the same checklist to each option),
and obviously the final conclusion it came to is questionable, but
most of the discussion is probably good.

The "carrier types" sidequest: extremely questionable.
The micro-codegenerator I made for that lives on in its own repo:
see https://github.com/warpfork/go-quickimmut .  But overall,
I don't think this went well.  With these, we got immutability,
and we got it without an import cycle or any direct dependence
on the IPLD Schema codegen output, but... we lost other things,
like the ability to differentiate empty lists from nil lists,
and it drops map ordering again, etc.  All of these are problems
solved by the IPLD Schema codegen, and losing those features again
was just... painful.  And the more places we're stuck flipping
between various semantics for representing "Maybe", the vastly
more likely it becomes to be farming bugs; this approach hit that.
Tracking "Is this nil or empty at this phase of its life" got
very confusing and is one of the main reasons I'm feeling it's
probably wise to put this whole thing down.

The first time I re-wrote a custom order-preservation feature,
I was like "ugh, golang, but okay, fine".  The second and third
and fourth times I ran into it, and realized not doing the work
again would result in randomized error message orders and muck up
my attempts at unit tests for error paths... I was less happy.
I still don't know what the solution to this is, other than trying
to use the IPLD Schema codegen in a cyclic way, because it *does*
solve these problems.

All of the transformation code in the dmt package which flipped
things into the compiler structures: awful.
To be fair, I expected that to be awful.  But it was really awful.

The "carrier types" stuff all being attached to a Compiler type,
for sheer grouping: extremely questionable.
If we were going to have this architecture of types|compiler|dmt
overall, I'd hands down absolutely full-stop definitely no-question
want the compiler to just be its own package.  Unfortunately,
as you already know, golang: if we want the types metadata types to
be immutable, all the code for building them has to be in-package.
Any form of namespacing at all would be an adequate substitute for
a full-on package boundary.  Possibly even just a feature that allowed
some things in a package to be grouped together in the *documentation*
would be satiating!

The "compiler_rules.go" file: really quite good!
Not entirely certain of the "first rule in a set to flunk causes
short-circuit" tactic, because there's at least one case where that
resulted in under-informative error messages that could've reasonably
identified two issues in one pass if not for the short-circuit.
But otherwise, the table-driven approach seemed promising.

Holistically: it seemed like having a "compiler" system that was
separate from the "dmt" data holder types would be a nice separation
of concerns.  When actually writing it: no, it was not.  The compiler
system ended up needing to pass through almost the exact range of
semantics that the dmt expresses (whether good or bad), so that when
the validator system was built on the compiler's data types, it could
provide reasonable responses to any issues in the data that might've
originated from the dmt format (and in turn this ultimately matters
for end-user experience).  With that degree of coupling, the compiler
system ends up forced to have a very limited and sharp-edged API
that's not at all natural, and certainly doesn't add any value.
It's possible this is inevitable (we didn't start this quest in order
to get a nice golang API, we started it to get rid of a dang import
cycle!), but it definitely generated pain.

I'm frankly not sure what the path forward here is.  Having the
validation logic attach to the dmt would solve the coupling pressure;
but the tradeoff would be there's no validation logic on the
constructors which produce the in-memory type info!  Is that okay?
I dunno.  It would definitely make me grit my teeth, but maybe it's one
of the least bad options left, seeing as how many other angles of
attack seem to have turned pretty sour now.
Another angle that deserves more thought is cyclebreaking by removing
self-description from generated types (this gets a mention in the
hackme doc in the diff already, but perhaps isn't studied far enough).

Whatever it is: it's going to start as a new body of work.
This well here is dry.

Also documented and discussed in the web at
https://github.com/ipld/go-ipld-prime/pull/144 .

---
## [ipld/go-ipld-prime@f1859e77a4...](https://github.com/ipld/go-ipld-prime/commit/f1859e77a403c5c2b6ac9ff11803c5e8bfb30f58)
##### 2021-02-25 12:08:16 by Eric Myhre

schema: working to unify interfaces and dmt.  Intermediate checkpoint commit.

This commit does not pass CI or even fully compile, and while I usually
try to avoid those, A) I need a checkpoint!, and B) I think this one is
interestingly illustrative, and I'll probably want to refer to this
diff and the one that will follow it in the future as part of
architecture design records (or even possibly experience reports about
golang syntax).

In this commit: we have three packages:

- schema: full of interfaces (and only interfaces)
- schema/compiler: creates values matching schema interfaces
- schema/dmt: contains codegen'd types that parse schema documents.

The dmt package feeds data to the compiler package, and the compiler
package emits values matching the schema interface.
This all works very nicely and avoids import cycles.

(Avoiding import cycles has been nontrivial, here, unfortunately.
The schema/schema2 package (which is still present in this commit,
but will be removed shortly -- I've scraped most of it over into
this new 'compiler' package already, just not a bunch of the validation
rules stuff, yet) was a dream of making this all work by just having
thin wrapper types around the dmt types.  This didn't fly...
because codegen'd nodes comply with `schema.TypedNode`, and complying
with `schema.TypedNode` means they have a function which references
`schema.Type`... and that means we really must depend on that
interface and the package it's in.  Ooof.)

The big downer with this state, and why things are currently
non-compiling at this checkpoint I've made here, is that we have to
replicate a *lot* of methods into single-use interfaces in the schema
package for this to work.  This belies the meaning of "interface".
The reason we'd do this -- the reason to split 'compiler' into its own
package -- is most because I wanted to keep all the constructor
mechanisms for schema values out of the direct path of the user's eye,
because most users shouldn't be using the compiler directly at all.

But... I'm shifting to thinking this attempt to segregate the compiler
details isn't worth it.  A whole separate package costs too much.
Most concretely, it would make it impossible to make the `schema.Type`
interface "closed" (e.g. by having an unexported method), and I think
at that point we would be straying quite far from desired semantics.

---
## [ye4241/ACNHPoker@f1a5fa7690...](https://github.com/ye4241/ACNHPoker/commit/f1a5fa76902d48828ddbcbbe4ee89a6b3d96326b)
##### 2021-02-25 13:44:45 by Yamino Plays AC

enhancement to map regen icons

Both of these icons are lovely. Celeste is a beautiful character and one of my favs.
Fasil is a sweetheart and it warmed my heart to see her icon here.

These changes include imsge enhancing and resize of fasil.ico and made as a png! If you dont like it its okay! Thanks for all your amazing work in poker 🙌💞

---
## [bcelenza/aws-cdk@bfa1f18c32...](https://github.com/bcelenza/aws-cdk/commit/bfa1f18c32051cd0e90a003bbbe3e806063c2b8c)
##### 2021-02-25 14:04:13 by Robert Djurasaj

chore(rds): add PG12.5 and deprecate PG9.5 and PG9.6 (#12998)

Added PostgreSQL 12.5 version.

Also, I got a friendly email from AWS that some versions of PG will be forcefully updated, and I've decided to mark PG9.5 and 9.6 as deprecated. (Feb 2021 and Nov 2021 EOLs respectfully).

> Amazon RDS is starting the end of life process for PostgreSQL major version 9.5. We are doing this because the PostgreSQL community is planning to discontinue support for PostgreSQL 9.5 on February 11, 2021 [1].
> 
> Amazon RDS for PostgreSQL 9.5 will reach end of life on February 16, 2021 00:00:01 AM UTC. Amazon RDS for PostgreSQL 9.6 will reach end of life in late 2021. Hence, we strongly encourage you to upgrade your databases running PostgreSQL major version 9.5 to 12 or greater at your convenience before February 16, 2021. PostgreSQL 12 contains major innovations including JSON path queries per SQL/JSON specifications and pluggable table storage interface [2]. PostgreSQL 13 introduced parallel processing of indexes with the VACUUM command and improved duplicate data handling by B-tree indexes [3]. Both versions contain numerous fixes to various software defects in earlier versions of the database.
> 
> If you do not upgrade your databases before February 16, 2021, RDS will upgrade your PostgreSQL 9.5 databases to PostgreSQL 12 during a scheduled maintenance window between Feb 16, 2021 00:00:01 UTC and March 16, 2021 00:00:01 UTC. On March 16, 2021 00:00:01 AM UTC, any PostgreSQL 9.5 databases that remain will be upgraded to version 12 regardless of whether the instances are in a maintenance window or not.
> 
> You can initiate an upgrade of your database instance — either immediately or during your next maintenance window — to a newer major version of PostgreSQL using the AWS Management Console or the AWS Command Line Interface. The upgrade process will shut down the database instance, perform the upgrade, and restart the database instance. The database instance may be restarted multiple times during the upgrade process. While major version upgrades typically complete within the standard maintenance window, the duration of the upgrade depends on the number of objects within the database. To avoid any unplanned unavailability outside your maintenance window, we recommend that you first take a snapshot of your database and test the upgrade to get an estimate of the duration. To learn more about upgrading PostgreSQL major versions in RDS, review the Upgrading Database Versions page [4].
> 
> We want to make you aware of the following additional milestones associated with upgrading these databases that are reaching end of life.
> 
> Now through March 16, 2021 00:00:01 AM UTC - You can initiate upgrades of Amazon RDS for PostgreSQL 9.5 instances to PostgreSQL 12 or higher at any time.
> February 16, 2021 00:00:01 AM UTC - After this date and time, you cannot create new RDS instances with PostgreSQL major version 9.5 from either the AWS Console or the CLI. RDS will also automatically upgrade PostgreSQL 9.5 instances to version 12 within the earliest scheduled maintenance window that follows. Restoration of Amazon RDS for PostgreSQL 9.5 database snapshots will result in an automatic upgrade of the restored database to a still supported version at the time.
> March 16, 2021 00:00:01 AM UTC - RDS will automatically upgrade any remaining PostgreSQL 9.5 instances to version 12 whether or not they are in a maintenance window.
> Should you have any questions or concerns, the AWS Support Team is available on the community forums and via Premium Support [5].

----

*By submitting this pull request, I confirm that my contribution is made under the terms of the Apache-2.0 license*

---
## [jhogberg/otp@323bdf3752...](https://github.com/jhogberg/otp/commit/323bdf3752df3af07a25ad43b7db300c8fafc837)
##### 2021-02-25 14:48:38 by John Högberg

re: Document [:ascii:] character class deficiency

The regex library we use can work either in locale-specific mode,
or unicode mode. The locale-specific mode uses a pregenerated
table to tell which characters are printable, numeric, and so on.

For historical reasons, OTP has always used Latin-1 for this table,
so characters like `ö` are considered to be letters. This is fine,
but the library has two quirks that don't play well with each
other:

* The locale-specific table is always consulted for code points
  below 256 regardless of whether we're in unicode mode or not,
  and the `ucp` option only affects code points that aren't
  defined in this table (zeroed).
* The character class `[:ascii:]` matches characters that are
  defined in the above table.

This is fine when the regex library is built with its default ASCII
table: `[:ascii:]` only matches ASCII characters (by definition)
and the library documentation states that `ucp` is required to
match characters beyond that with `\w` and friends.

Unfortunately, we build the library with the Latin-1 table so
`[:ascii:]` matches Latin-1 characters instead, and we can't change
the table since we've documented that `\w` etc work fine with
Latin-1 characters, only requiring `ucp` for characters beyond
that.

At this point you might be thinking that this is a bug in how the
regex library handles `[:ascii:]`. Well, yes, POSIX says it should
match all code points between 0-127, but that's misleading since
it's only true for strict supersets of ASCII: should `[:ascii:]`
match 0x5C if the table is Shift-JIS? It would be just as wrong as
matching `ö`. :-(

Why not try to do the right thing and mark ASCII-compatibility for
each code point, since (for instance) 0x41 is `A` both in ASCII and
Shift-JIS? There's no way to ask a locale whether a code point
refers to the same character in ASCII, so the users would need to
manually go through the tables after generating them. Happy fun
times.

I've settled for documenting this mess since we can't fix this
on our end without breaking people's code, and there's not much
point in reporting this upstream since it'll either be misleading
or far too much work for the user, and PCRE-8.x is nearing the
very end of its life.

---
## [FoxRunTime/FoxRunTime.github.io@b938f314fd...](https://github.com/FoxRunTime/FoxRunTime.github.io/commit/b938f314fde2b92493975c54dc17ca6ca81b06c2)
##### 2021-02-25 21:13:32 by Fox RunTime

trying to fix the fucking redirect

broke the fucking redirect and now im having trouble getting people to give my girlfriend their earth dollars

---
## [FoxRunTime/FoxRunTime.github.io@e9b7719841...](https://github.com/FoxRunTime/FoxRunTime.github.io/commit/e9b7719841732328ee32bb8c254a5350837829f1)
##### 2021-02-25 21:17:57 by Fox RunTime

fuck it, redid the entire-ass thing

i sacrificed my homepage to drive more traffic to my girlfriend because i love her <3 but fucking hell this is torture

---

# [<](2021-02-24.md) 2021-02-25 [>](2021-02-26.md)

