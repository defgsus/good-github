# [<](2021-02-03.md) 2021-02-04 [>](2021-02-05.md)

3,004,223 events, 1,448,822 push events, 2,333,129 commit messages, 186,733,677 characters


## [ajvondrak/remote_ip](https://github.com/ajvondrak/remote_ip)@[36bff8e0e0...](https://github.com/ajvondrak/remote_ip/commit/36bff8e0e0c741565d88f366d26f3128ccdb3b44)
#### Thursday 2021-02-04 01:12:11 by Alex Vondrak

Add an integration test harness

The problem with RemoteIp.Debug doing all its work at compile time is
that it's really hard to test. A standard `mix test` run will compile
the remote_ip app, compile the tests, then run the resulting suite - in
that order.

Because of this, the call sites of RemoteIp.Debug.log/3 throughout the
rest of the remote_ip code will only ever have one static macro
expansion dictated by the configuration values present at the initial
compilation of the app. We won't be able to toggle the full debug logs
on & off from the test suite alone.

Instead, the best we could do is to test ad hoc expansions of the
RemoteIp.Debug.log/3 macro, which is how the tests are currently
written. But even then, we can't do a straightforward call of the macro
like

  test "logging the :headers option" do
    log = capture_log(fn ->
      RemoteIp.Debug.log(:headers, do: ~w[a b c])
    end)
    assert log =~ inspect(~w[a b c])
  end

because it'll be expanded when the tests are compiled! So unless we set
up a config/test.exs with debugging enabled, this test will fail. And if
we actually did configure this test to succeed, then the debug-off tests
would fail. It's an irreconcilable difference at compile time, and we
can't have `mix test` dynamically recompile remote_ip or the suite
(which is arguably a good thing).

One way around this (without using Code.eval_quoted/3 like
debug_test.exs currently does) is to delay the macro expansion by
wrapping its call in a module definition:

  test "logging the :headers option" do
    log = capture_log(fn ->
      defmodule DelayExpansion do
        RemoteIp.Debug.log(:headers, do: ~w[a b c])
      end
    end)
    assert log =~ inspect(~w[a b c])
  end

You can see this technique being used by Elixir's Logger tests for the
compile_time_purge_matching configuration:
https://github.com/elixir-lang/elixir/blob/2153b09/lib/logger/test/logger_test.exs#L471-L523

Using a module like this works because the Kernel.defmodule/2 macro
actually expands into a function call like

  :elixir_module.compile(...)

which is passed in an AST that has *not* been macro-expanded yet. Thus,
the runtime call to :elixir_module.compile will do the work of expanding
the RemoteIp.Debug.log/3 macro (as well as all the rest) once the new
module is being compiled - which happens at the test's runtime.

But overall, this is still quite brittle. The only reason we're even
able to test the ad hoc expansions of RemoteIp.Debug.log/3 is because it
calls Application.get_env/3 at *expansion* time.

It's a subtle point. Currently, the code reads like

  defmodule RemoteIp.Debug do
    defmacro log(...) do
      if Application.get_env(:remote_ip, :debug, false) do
        ...
      else
        ...
      end
    end
  end

which means that Application.get_env/3 will be called whenever we
finally expand the macro. Since things like Code.eval_quoted/3 and the
defmodule trick delay this expansion, we can have the tests do their
Application.put_env/3 calls during their setup, expand ad hoc calls to
RemoteIp.Debug.log/3 at (test) runtime, and make assertions on those
results.

However, if we instead did something like

  defmodule RemoteIp.Debug do
    @enabled Application.get_env(:remote_ip, :debug, false)

    defmacro log(...) do
      if @enabled, do: ..., else: ...
    end
  end

this would break the suite because RemoteIp.Debug will have already been
compiled *before* compiling the tests. With the module attribute, we'd
call Application.get_env/3 at remote_ip's compile time. Then when the
tests get compiled, no matter how much they try to delay the expansion
of RemoteIp.Debug.log/3, the expansion will still be static. E.g., if
debugging were disabled at remote_ip compile time, that effectively
means the RemoteIp.Debug definition would be equivalent to

  defmodule RemoteIp.Debug do
    defmacro log(...) do
      if false, do: ..., else: ...
    end
  end

So by the time we got around to compiling the tests, the
Application.put_env/3 trick would stop working altogether!

As an interesting sidebar, all the above holds true for Elixir's Logger
module: its tests use the defmodule trick, which they can get away with
because they fetch the config at expansion time. And it's actually
important for their use case that this fetch happens at expansion time.
Otherwise, whenever you changed the logger config, you'd have to
recompile the Logger module itself. By pushing the get_env to expansion
time, users never have to worry about recompiling Logger.

This is particularly desirable for Logger, but it actually doesn't
matter for RemoteIp. Unlike Logger.debug/2, Logger.info/2, etc, the
RemoteIp.Debug.log/3 macro is *not* meant to be called by users. The
only "user" of RemoteIp.Debug is internal: the remote_ip code itself. So
no matter what, tweaking the debug config means you'll have to recompile
remote_ip to enable/disable the logs you actually care about (the ones
used internally). Whereas with Logger, after you change the config you
just have to recompile your local uses of Logger.debug/2, Logger.info/2,
etc.

So we'll always need to recompile remote_ip as long as debugging works
as a compile-time option. I think this is a good idea because pushing it
to runtime seems needlessly wasteful for something you probably rarely
want anyway. And if it's always going to be a compile-time option, I'd
like to take the opportunity to write the RemoteIp.Debug code with
Application.compile_env/3 in Elixir v1.10+, which would require a module
attribute (because it actually enforces the compile-time-iness!), which
would break the standard unit tests as described before.

I also don't feel too bad about this recompilation requirement because
it turns out to be not entirely unique to remote_ip. Notable examples
include

* mime, which even uses interesting tricks to warn users to recompile:
  https://github.com/elixir-plug/mime/blob/4a538c5/lib/mime/application.ex#L11-L28

* plug, for the configuration of Plug.Conn.Status:
  https://github.com/elixir-plug/plug/blob/da39326/lib/plug/conn/status.ex#L6
  https://github.com/elixir-plug/plug/blob/da39326/lib/plug/conn.ex#L91-L122

Considering all that, I'd rather not lean on brittle testing
methodologies. Thus, I've wired together this integration test suite. It
just loops through separate Mix projects that depend on the local
remote_ip and runs `mix test` in each app. This forces a recompile of
remote_ip for each separate app, so we get to test remote_ip's behavior
under different configurations. As of yet there's only a single basic
smoke test, but more integrations are soon to come.

---
## [ccodwg/Covid19Canada](https://github.com/ccodwg/Covid19Canada)@[7b677db471...](https://github.com/ccodwg/Covid19Canada/commit/7b677db47139d7c6bddde25fec7ac6bb7b9ce84c)
#### Thursday 2021-02-04 01:52:14 by Jean-Paul R. Soucy

New data: 2021-02-03: SEE ONTARIO DATA NOTES.

Recent changes:

2021-01-27: Due to the limit on file sizes in GitHub, we implemented some changes to the datasets today, mostly impacting individual-level data (cases and mortality). Changes below:

1) Individual-level data (cases.csv and mortality.csv) have been moved to a new directory in the root directory entitled “individual_level”. These files have been split by calendar year and named as follows: cases_2020.csv, cases_2021.csv, mortality_2020.csv, mortality_2021.csv. The directories “other/cases_extra” and “other/mortality_extra” have been moved into the “individual_level” directory.
2) Redundant datasets have been removed from the root directory. These files include: recovered_cumulative.csv, testing_cumulative.csv, vaccine_administration_cumulative.csv, vaccine_distribution_cumulative.csv, vaccine_completion_cumulative.csv. All of these datasets are currently available as time series in the directory “timeseries_prov”.
3) The file codebook.csv has been moved to the directory “other”.

We appreciate your patience and hope these changes cause minimal disruption. We do not anticipate making any other breaking changes to the datasets in the near future. If you have any further questions, please open an issue on GitHub or reach out to us by email at ccodwg [at] gmail [dot] com. Thank you for using the COVID-19 Canada Open Data Working Group datasets.

- 2021-01-24: The columns "additional_info" and "additional_source" in cases.csv and mortality.csv have been abbreviated similar to "case_source" and "death_source". See note in README.md from 2021-11-27 and 2021-01-08.

Vaccine datasets:

- 2021-01-19: Fully vaccinated data have been added (vaccine_completion_cumulative.csv, timeseries_prov/vaccine_completion_timeseries_prov.csv, timeseries_canada/vaccine_completion_timeseries_canada.csv). Note that this value is not currently reported by all provinces (some provinces have all 0s).
- 2021-01-11: Our Ontario vaccine dataset has changed. Previously, we used two datasets: the MoH Daily Situation Report (https://www.oha.com/news/updates-on-the-novel-coronavirus), which is released weekdays in the evenings, and the “COVID-19 Vaccine Data in Ontario” dataset (https://data.ontario.ca/dataset/covid-19-vaccine-data-in-ontario), which is released every day in the mornings. Because the Daily Situation Report is released later in the day, it has more up-to-date numbers. However, since it is not available on weekends, this leads to an artificial “dip” in numbers on Saturday and “jump” on Monday due to the transition between data sources. We will now exclusively use the daily “COVID-19 Vaccine Data in Ontario” dataset. Although our numbers will be slightly less timely, the daily values will be consistent. We have replaced our historical dataset with “COVID-19 Vaccine Data in Ontario” as far back as they are available.
- 2020-12-17: Vaccination data have been added as time series in timeseries_prov and timeseries_hr.
- 2020-12-15: We have added two vaccine datasets to the repository, vaccine_administration_cumulative.csv and vaccine_distribution_cumulative.csv. These data should be considered preliminary and are subject to change and revision. The format of these new datasets may also change at any time as the data situation evolves.

Revise historical data: cases (AB, MB, ON, QC, SK); recovered (NT).

2021-02-02: ONTARIO DATA REPORTING MAY HAVE UNUSUAL NUMBERS FOR THE NEXT SEVERAL DAYS. SEE BELOW. https://www.ontario.ca/page/how-ontario-is-responding-covid-19

“Toronto Public Health has now migrated all of their data to the provincial data system, CCM. This migration has impacted today’s daily counts. Most notably, TPH’s case count is negative following the identification of duplicate cases as well as data corrections to some fields (e.g., long-term care home residents and health care workers), resulting in an underestimation of today's cases. In addition, case counts for other PHUs may have been affected by system outages related to the migration. As a result, we anticipate fluctuations in case numbers over the next few days.”

Note regarding deaths added in QC today: “The data also report 37 new deaths, for a total of 9,899. Among these 37 deaths, 9 have occurred in the last 24 hours, 18 have occurred between January 27 and February 1 and 9 have occurred before January 27 and 1 has occurred at an unknown date.” We report deaths such that our cumulative regional totals match today’s values. This sometimes results in extra deaths with today’s date when older deaths are removed.

https://www.quebec.ca/en/health/health-issues/a-z/2019-coronavirus/situation-coronavirus-in-quebec/#c47900

Note about SK data: As of 2020-12-14, we are providing a daily version of the official SK dataset that is compatible with the rest of our dataset in the folder official_datasets/sk. See below for information about our regular updates.

SK transitioned to reporting according to a new, expanded set of health regions on 2020-09-14. Unfortunately, the new health regions do not correspond exactly to the old health regions. Additionally, the provided case time series using the new boundaries do not exist for dates earlier than August 4, making providing a time series using the new boundaries impossible.

For now, we are adding new cases according to the list of new cases given in the “highlights” section of the SK government website (https://dashboard.saskatchewan.ca/health-wellness/covid-19/cases). These new cases are roughly grouped according to the old boundaries. However, health region totals were redistributed when the new boundaries were instituted on 2020-09-14, so while our daily case numbers match the numbers given in this section, our cumulative totals do not. We have reached out to the SK government to determine how this issue can be resolved. We will rectify our SK health region time series as soon it becomes possible to do so.

---
## [0hq/discordnetworkanalysis](https://github.com/0hq/discordnetworkanalysis)@[3caf2ca80b...](https://github.com/0hq/discordnetworkanalysis/commit/3caf2ca80bfeef602d1115e74d41778105af8ff0)
#### Thursday 2021-02-04 02:32:48 by 0hq

unfucked the code, made it much less stupid, rewarded close friendships and discarded all connections under 3 occurrences

---
## [peff/git](https://github.com/peff/git)@[507ecc73b9...](https://github.com/peff/git/commit/507ecc73b9aafe4783503d7a7ad260562f15b0e2)
#### Thursday 2021-02-04 03:11:12 by Jeff King

commit: give a hint when a commit message has been abandoned

If we launch an editor for the user to create a commit
message, they may put significant work into doing so.
Typically we try to check common mistakes that could cause
the commit to fail early, so that we die before the user
goes to the trouble.

We may still experience some errors afterwards, though; in
this case, the user is given no hint that their commit
message has been saved. Let's tell them where it is.

Signed-off-by: Jeff King <peff@peff.net>

---
## [KangMamles/nbr_kernel](https://github.com/KangMamles/nbr_kernel)@[97924f9743...](https://github.com/KangMamles/nbr_kernel/commit/97924f97430252a51ac2d5d716016b8a49da76b3)
#### Thursday 2021-02-04 03:12:55 by Ahmad Thoriq Najahi

arm64: dts: Add GPU OC support for msm8917

Credits and Thanks to XolosKernel / @Genom48

Cherry-pick always keep Authorship, Fuck you Kangers.

Signed-off-by: Ahmad Thoriq Najahi <tlogitechnjhi@gmail.com>
Signed-off-by: Thagoo <lohitgowda56@gmail.com>

---
## [cybertooth-io/ember-data-autocomplete-js](https://github.com/cybertooth-io/ember-data-autocomplete-js)@[f566c11931...](https://github.com/cybertooth-io/ember-data-autocomplete-js/commit/f566c119318aef910ae2730618edbf5f5dad0c86)
#### Thursday 2021-02-04 03:41:32 by Dan Nelson

⬆️  v3.21.2...v3.24.0 Ember Cli Upgrade (ugh that hurt)

* v1.0.3

* :art: prettier

* :white_check_mark: fucking ember test for addons is so stupid fucking boneheads

* :arrow_up: v3.21.2...v3.24.0 fuck this upgrade

* :arrow_up: rage inducing upgrade FML

---
## [read-0nly/BeanSupreme](https://github.com/read-0nly/BeanSupreme)@[3f3546caa0...](https://github.com/read-0nly/BeanSupreme/commit/3f3546caa0599c30aa54d7692a4674109d4769e1)
#### Thursday 2021-02-04 04:29:16 by obsol

Bugfixing and win handling

[9:11 PM] obsol: 2/3/2021 - The sleepy edition
    - Better Rigidbody handling (should fix cursed guns)
    - Made GenericObject do the thing again even with the new system, wee floaty bumblebeans
    - Things I forgot to note : Fixed hats and painting overall, fixed deathwall, sniper colliders still fucked
[11:27 PM] obsol: 2/3/2021  the nightynight edition
    - The game shows the name of who won, and exits gracefully(?) after the round.
TO FIX: Sometimes the host won't have a hat color the first spawn around. Seems to be a question of if the player manages to spawn in before the host color is set

---
## [Yiyeii1999/Gold-B2-First-exercises-for-Moodle](https://github.com/Yiyeii1999/Gold-B2-First-exercises-for-Moodle)@[0f4634681b...](https://github.com/Yiyeii1999/Gold-B2-First-exercises-for-Moodle/commit/0f4634681b70287c29e1835e553fdc87e8d50802)
#### Thursday 2021-02-04 04:32:25 by Yiyeii1999

Create U9 p90-91 Reading

Six sentences have been removed from the article. Choose from the sentences A-G the one which fits each gap 1-6. There is one extra sentence which you do not need to use.

A. These results probably indicate that the changes that were made were long overdue.
B. This isn't to say that everyone considering a drastic change in life should leap into action.
C. They did not enjoy the experience but the results were superb.
D. However, a random change can also lead us into behaving in a way that we might not have imagined.
E. The reason we haven't made them already is probably because we tend to stick with what is safe and familiar.
F. Nonetheless, it is important to know when to stop.
G. He asked people who were agonizing over a life decision to take part in an experiment he had set up.

BIG DECISION AHEAD? JUST TAKE A CHANCE
The decision-making process is never easy. Emotions can cloud our judgement. The more information you have, the easier a decision should be but more choice frequently results in information overload, which can be overwhelming and have tl1e opposite effect. In fact, at times, relying on chance or an outside intervention can actually help us to make better decisions.
The first reason for this is that by deciding to follow a random instruction, we can end up making decisions that we should have been making all along. 
[1] {1:MCS:%0%A~%0%B~%0%C~%0%D~%100%E~%0%F~%0%G}Tossing a coin and deciding that 'if the coin comes up heads, I'll propose or resign or have a baby' may be the only way that some of us can find the energy to make tough decisions.
A few years ago the economist Stephen Levitt found evidence for this theory. 
[2] {2:MCS:%0%A~%0%B~%0%C~%0%D~%0%E~%0%F~%100%G}They were required to log onto the website he had launched, answer some questions and then hand over responsibility to the flip of a digital coin.
As the months went past, Levitt followed up, asking the volunteers whether they had obeyed the coin or ignored it and how they were now feeling about the decision. 
It's an odd experiment but what makes it powerful is that the toss of the coin randomly divided people into two groups, one pushed towards action and one pushed towards caution, depending on whether the coin came up 'heads' or 'tails'. In fact, people did tend to obey the coin, and the majority of those in the 'action' group reported being happier several months on. 
[3] {3:MCS:%100%A~%0%B~%0%C~%0%D~%0%E~%0%F~%0%G}The research also suggests that, if this is the case, we should probably get on with it, with the help of a coin if necessary. Randomness, then, can prompt us into taking actions that we fear. [4] {4:MCS:%0%A~%0%B~%0%C~%100%D~%0%E~%0%F~%0%G}
A simple example of this unpredictable behavior emerged after an Underground strike in London. Three economists looked at data which identified regular commuters who had to find a different route to work during the strike. What was surprising was that one in 20 of these individuals stuck to their new routes after the strike was over. Even commuters, whom we would imagine had perfected the most efficient way to make their daily journey, can come up with a better route when forced to do so.
Many creative types have used this principle to their advantage. Perhaps the most famous example is the Oblique Strategies, a set of cards assembled by artist Peter Schmidt and musician Brian Eno. The cards are full of complicated instructions which were used while working with David Bowie on some of his celebrated albums, and also with the band Coldplay. Great guitarists would find themselves having to play the drums or jump randomly between chords that Eno was indicating on a blackboard. [5] {5:MCS:%0%A~%0%B~%100%C~%0%D~%0%E~%0%F~%0%G}
One can, of course, take the idea too far. It is true that randomness may well help us to think and behave more creatively.
[6] {6:MCS:%0%A~%0%B~%0%C~%0%D~%0%E~%100%F~%0%G}Once we have found a life we love, we are probably better off staying with the status quo.

---
## [orvit-png/mhapi](https://github.com/orvit-png/mhapi)@[930512a321...](https://github.com/orvit-png/mhapi/commit/930512a32116ae8c2ed606955cf567e6d594b9a6)
#### Thursday 2021-02-04 04:37:45 by orvit

Rename shit.json to items.json

Made the name more like... Minehut - family fucking friendly.

---
## [harmenjanssen/seo-meta-field-nova](https://github.com/harmenjanssen/seo-meta-field-nova)@[722612382a...](https://github.com/harmenjanssen/seo-meta-field-nova/commit/722612382a0f38015fddb0719085497cb6206464)
#### Thursday 2021-02-04 06:57:37 by Harmen Janssen

Always return a default SeoMetaItem

In my opinion, the SEO data acts not so much as "related data", but as attributes of the model.  
That's why I think it's a good idea to always return a `SeoMetaItem` model, to make it safe to assume all SEO properties are there.

I'm motivated by the fact that I'm working on a headless implementation of Nova CMS, creating a JSON API to publish its content to a React application. 
It's a bit more user-friendly on the front-end to have the API contain `page.seo.description = null` instead of `page.seo = null`.

I realize this is opinionated, so let me know if you're willing to merge this, or whether you'd like to see changes. 🙂 
Thanks in advance!

---
## [crawl/crawl](https://github.com/crawl/crawl)@[0178f6a2e2...](https://github.com/crawl/crawl/commit/0178f6a2e2a70d5f050a8e07b579cc62aa968ff9)
#### Thursday 2021-02-04 07:16:22 by Nicholas Feinberg

Remove *Curse

Random curses on equip existed on exactly three unrands (and no randarts).
It was on Scythe of Curses purely for theme, on Obsidian Axe to make it
not a complete no-brainer to swap off the weapon when mesmerized, and on
the Necklace of Bloodlust to discourage swaps when you'd rather not be
randomly berserked.

*Curse wasn't an extremely great mechanism for limiting the latter two.
The randomness of how many scrolls of remove curse were generated meant
that the restriction was sometimes tight and sometimes irrelevant, and
there wasn't any real tactical element to the cost.

So, let's remove it. Amulets are now slow to swap, so the Necklace of
Bloodlust no longer has a *Drain downside at all. The Scythe of Curses
is now {drain, *Drain}, which seems very thematic if not mechanically
necessary, and the obsidian axe likewise gets *Drain. (It could be fun
to have a 'Compel' or 'Sticky' property that makes these weapons slow
to swap, compelling the user like the One Ring... but not right now.)
This should hopefully provide a more interesting set of tactical choices.

Curses!

---
## [NebulaSS13/Nebula](https://github.com/NebulaSS13/Nebula)@[9b2a7dd890...](https://github.com/NebulaSS13/Nebula/commit/9b2a7dd89044d0992382f0fdc966b8a5e1decf81)
#### Thursday 2021-02-04 08:04:24 by thestalker12

Clothing Commits 2 Electric Boogalo (#1210)

* Starting the hardhat descent cause git seems to hate spacesuits and wants them to be untracked for some stupid reason

* Hard Hats

* Fixed Error

* Clothing to the collection

* fixed up bug again

* uPDATED values and Shit(tm)
>

---
## [kaylatheegg/c-game-engine](https://github.com/kaylatheegg/c-game-engine)@[c93047b757...](https://github.com/kaylatheegg/c-game-engine/commit/c93047b75760c886ccba0e2c6704f5f27ada3067)
#### Thursday 2021-02-04 08:13:49 by Kayla

shit. lets be santa

changed object definition and updated all object calls in respect to this
god this was a pain

---
## [TenteEEEE/S3Sampler](https://github.com/TenteEEEE/S3Sampler)@[9b43960c82...](https://github.com/TenteEEEE/S3Sampler/commit/9b43960c8261be969077f14ce53de99919196529)
#### Thursday 2021-02-04 08:56:56 by TenteEEEE

updated: 20210204. New:M1dy - Here We Go by Astrella, DJ Noriken - Quon by +1 Rabbit, Camellia - Syzygia by Jabob, Kobaryo - VANDALIZED AI by Ramen Noodle, TUYU - Compared Child by Rigid & SmileEaglet, Camellia - Shun no shifudo o ikashita Karefumi Paeria 808 shefu no Twerk to Trap shitate by ComplexFrequency & miitchel, uma vs. Morimori Atsushi - GLORY: ROAD by abcbadq & Aimedhades16, Camellia as "fluX Xroise" - Xronier by shrado, Scrappy, & altrewin, Hachi - DONUT HOLE by Kizuflux & Uninstaller, Kazmasa - Phantom's Wonderland by Skeelie, ETAN, AaltopahWi, Dropgun - Dark Sky by ItsVasili, Chito (CV: Minase Inori), Yuuri (CV: Kubo Yurika) - More One Night by That_Narwhal, USAO - Cyaegha by Timbo, UNDEAD CORPORATION - Flowering Night Fever by dibl

---
## [pxseu/pricecheckbot](https://github.com/pxseu/pricecheckbot)@[e7b1627314...](https://github.com/pxseu/pricecheckbot/commit/e7b1627314ea7c8b08eb8adf32d0c771976cc641)
#### Thursday 2021-02-04 13:17:58 by pxseu

💥Remove a dumb scam reason

I see this as a really dumb reason for a ban of a friend of mine. It's really unfair to be excluded for advertising something that clearly was a joke. Your staff should honestly check more often what reasons are provided.
If you can please accept this pull request or at least check him again because I really feel this is unfair.
Thank you for getting through this.
Much appreciation for such a great community but this feels awful.

---
## [LDR-Siren/EmilyC-SamanthaPrater-EruzaArto](https://github.com/LDR-Siren/EmilyC-SamanthaPrater-EruzaArto)@[0ede8dbc96...](https://github.com/LDR-Siren/EmilyC-SamanthaPrater-EruzaArto/commit/0ede8dbc96b5236ed30f0c00f61df21cc90aaa14)
#### Thursday 2021-02-04 13:42:47 by LDR

James has arrived

James has arrived in the discord. After having contacted me about a certain subject and to inform me that Emily has sent him over 150 emails screaming insanely at him, he said he was downloading the discord app. I promptly gave him the group invite and he arrived. We all chattered about the current going ons, but what was interesting was the screen grabs he got of her emails! 

Sorry once more for the mish moshing of items, but like most days, the trollorist is all over the place..

---
## [rswarbrick/opentitan](https://github.com/rswarbrick/opentitan)@[4218402a09...](https://github.com/rswarbrick/opentitan/commit/4218402a09b334578258542849da0d8b37f2b894)
#### Thursday 2021-02-04 15:01:18 by Rupert Swarbrick

[reggen] Define a Register type for use in reggen

The big change in this commit is to define a Register type (in
util/reggen/register.py), to represent registers as parsed from hjson.
Once you do that, it makes sense to define objects to represent things
inside a register like allowed accesses (in access.py), fields (in
field.py), bit ranges (in bits.py).

The plan for this commit was to stop there as an incremental change,
but multireg objects, which wrap a register, are a bit fiddly, so we
also define a MultiRegister class (in multi_register.py).

The idea is that the code in validate.py now actually parses its input
to MultiRegister or Register objects, instead of checking the data is
good and then leaving it as a nested dictionary (following the "parse,
don't validate" mantra).

The most obvious user-visible change is that there should be slightly
better error messages BUT there will be fewer of them. Rather than
collect up error counts, as validate.py does, this code raises a
ValueError. Obviously, that means that we don't get to generate
multiple error messages. Honestly, that seems ok to me: this is a
configuration parser, not a C compiler!

Slightly less obvious is that the parsing code is now more
opinionated. Before this change, there were lots of configuration
errors that you could make that caused the parsing code to emit a
warning (that no-one notices...) and then change the input to look
more sensible. Now, we just raise an error: in my opinion, there's no
real reason not to force a user to Do It Right.

To see how this changes things for code that now uses the Register
types, one relevant part of the diff is the make_intr_alert_reg()
function in validate.py. This has got a little shorter, and no longer
needs to reproduce the logic to generate fields like "genswwraccess"
and similar. Another place to look is check_wen_regs() in the same
file, where multi-register handling has got much simpler.

There are various ways in which this is work in progress. Firstly,
there are other objects that are currently not wrapped (same-address
register lists, windows, skipto) and the top-level "wrapping object"
is still a dictionary. Assuming we're happy with this approach,
wrapping those should be easy enough.

Secondly, there are duplicate "multireg", "register" and "field"
classes. The code in gen_rtl.py constructs actual objects to represent
these. It was doing this by re-parsing the dicts. This commit
simplifies that code a bit, so it's now just re-shaping things like
Register objects. An obvious job for a follow-up commit is to get rid
of the types that are currently in data.py.

Finally, the wrapping of access enums in access.py is a bit ugly. I've
left this similar to the original for now, to avoid yet more changes
in the codebase.

As part of fixing everything up to match again, I also removed
genbasebits support, which seems to have lain unused since the initial
import.

Signed-off-by: Rupert Swarbrick <rswarbrick@lowrisc.org>

---
## [OMGhixD-OG/azerothcore-wotlk](https://github.com/OMGhixD-OG/azerothcore-wotlk)@[a723661602...](https://github.com/OMGhixD-OG/azerothcore-wotlk/commit/a723661602af1d08bf7b2d72d4039d38109bde4d)
#### Thursday 2021-02-04 15:16:39 by Robbie

### TITLE

## fix(DB/Misc): DM-N | Fixes various problems and corrects many extremely noticeable things in Dire Maul North
## |---- Write below the examples with a maximum of 50 characters ----|
## This fixes a series of things.
## 1.) Corrected an issue where two Gordok Brutes was not Pathing. (At the very beginning of the dungeon)
## 2.) Corrected an issue where some packs had too many NPCs spawned.
## 3.) Wandering Eye of Kilrogg will now Spawn one pack of Netherwalker(s) and then proceed to instantly despawn.
## 4.) Corrected Guard Slip'Kiks Movement Speed and gave him a new proper waypoint (thus he no longer collides with the Kodo Skeleton)
## 5.) Resolved an issue where Captain Kromcrush's Guards would despawn after 5 minutes. This was changed to 10 seconds when the fight is either ended or group has wiped.
## 6.) Gordok Spirit's are no longer attackable.

### DESCRIPTION
## Explain why this change is being made, what does it fix etc...
## I'm just gonna leave a  few examples and a link to our Bugreport containing the reports of the findings. (Should explain most of it)
## Bugreport for documentation : https://github.com/OMGhixD-OG/Path-of-Classic-Bugtracker-WoTLK/issues/2

## Provide links to any issue, commit, pull request or other resource
## Showcasement of Fix 1 & 2.) https://cdn.discordapp.com/attachments/806333629663543326/806615678386831431/2021-02-03_21-02-07.mp4 (Fix 1.) & 2.) )
## Showcasement of Fix 3.) https://cdn.discordapp.com/attachments/806333629663543326/806670732749701120/2021-02-04_00-41-03.mp4 (Fix 3.))
## Showcasement of Fix  4.) https://cdn.discordapp.com/attachments/806333629663543326/806643571188301834/2021-02-03_22-51-42.mp4 (Fix 4.) (Circle Marked is the new one, Yes he also draws weapons whilst walking. But didn't do that during the video recording)
## Showcasement of Fix  5.) https://cdn.discordapp.com/attachments/806333629663543326/806636094744952852/2021-02-03_22-22-39.mp4 (Fix 5.)

### How to Reproduce/Test ?
## 1.) https://github.com/OMGhixD-OG/Path-of-Classic-Bugtracker-WoTLK/issues/2 Follow the instructions here and walk through each part.
## 2.) Profit? : P

### How to Test Changes
## Note: I do not really think this requires any further testing (unless someone really wants to), I've documented 5/6 fixes in there and thus that should be more than enough as for testing purposes, However if someone wants too test it then follow these steps)
## 1.) Confirm at the very first part of the dungeon that there are TWO guards (one walking to the right and returning, and one walking on the left and returning)
## 2.) In the first pack on the left side (entrance of the dungeon) there should be a Mage Ogre, Warrior Ogre and a Hyena (Mastiff or something). Confirm the amount of NPCs in that pack. (Reference was taken from Youtube Footage)
## 3.) Run through the first Door that you have to .gobject activate and find npc "Wandering Eye of Kilrogg" He should now when aggroed target you, Cast the netherwalker summoning spell and instantly dissapear after succeeding this cast.
## 4.) Continue through the dungeon and find Guard Slip'kik. Confirm that he no longer runs into the Kodo Skeleton under the stair (and that is really the only problem he had)
## 5.) Engage on boss Captain Kromcrush and bring him to 70% HP or lower (since that is when he spawns his guards) and let them aggro you. Now go ahead and either .gm on or .die on yourself and confirm that the guards despawn after 10-15s (which would be normal behaviour if this was a normal group run and wipe)
## 6.) Go up to the last boss and attempt to attack Gordok Spirit (They are ghost like ogres surrounding the last boss platform in Diremaul North. (Confirm that "You cannot attack that target")

## And that pretty much sums up this entire PR : )

---
## [rswarbrick/opentitan](https://github.com/rswarbrick/opentitan)@[212cf561d1...](https://github.com/rswarbrick/opentitan/commit/212cf561d12a2979afcecf986bffd4aafb7ea16e)
#### Thursday 2021-02-04 15:56:12 by Rupert Swarbrick

[reggen] Define a Register type for use in reggen

The big change in this commit is to define a Register type (in
util/reggen/register.py), to represent registers as parsed from hjson.
Once you do that, it makes sense to define objects to represent things
inside a register like allowed accesses (in access.py), fields (in
field.py), bit ranges (in bits.py) and enum entries (in
enum_entry.py).

The original plan for this commit was to stop there but multireg
objects, which wrap a register, are a bit fiddly so we also define a
MultiRegister class (in multi_register.py).

The motivation for this change is that the code in validate.py now
actually parses its input to MultiRegister or Register objects.
Before, it checked the data was good and left it as a nested
dictionary. Downstream code then has to do silly things like re-parse
integers from strings. This rather violates the "parse, don't
validate" mantra.

**

The most obvious user-visible change from this commit that there
should be slightly better error messages BUT there will be fewer of
them. Rather than collect up error counts, as validate.py does, this
code raises a ValueError. The disadvantage of doing this is that we
don't get to generate multiple error messages. The advantage is that
the code gets much cleaner. This trade-off seems like a good one to
me: this is a configuration parser, not a C compiler!

A slightly less obvious change is that the parsing code is now more
opinionated. Before this commit, there were lots of configuration
errors that you could make that caused the parsing code to emit a
warning (that no-one notices...) and then to change the input to look
more sensible. Now, we just raise an error: in my opinion, there's no
real reason not to force a user to Do It Right.

To see how this commit changes things for code that now uses the
Register types, see the make_intr_alert_reg() function in validate.py.
This has got a little shorter, and no longer needs to reproduce the
logic to generate fields like "genswwraccess" and similar. Another
place to look is check_wen_regs() in the same file, where
multi-register handling has got much simpler.

**

There are various ways in which this is work in progress. Firstly,
there are other objects that are currently not wrapped (same-address
register lists, windows, skipto) and the top-level "wrapping object"
is still a dictionary. Assuming we're happy with this approach,
wrapping those should be easy enough.

Secondly, there are duplicate "multireg", "register" and "field"
classes. The code in gen_rtl.py constructs actual objects to represent
these. It was doing this by re-parsing the dicts. This commit
simplifies that code a bit, so it's now just re-shaping things like
Register objects. An obvious job for a follow-up commit is to get rid
of the types that are currently in data.py.

Finally, the wrapping of access enums in access.py is a bit ugly. I've
left this similar to the original for now, to avoid yet more changes
in the codebase.

**

As part of fixing everything up to match again, I also removed
genbasebits support, which seems to have lain unused since the initial
import.

Signed-off-by: Rupert Swarbrick <rswarbrick@lowrisc.org>

---
## [NotAwesome2/chatsounds](https://github.com/NotAwesome2/chatsounds)@[1dabbc04de...](https://github.com/NotAwesome2/chatsounds/commit/1dabbc04de1f965d012550f3d66a083f9bb153da)
#### Thursday 2021-02-04 16:09:16 by kapppaNeko

Delete why do we tell people to kiss our ass when were mad at them when we could be telling them to kiss me hi im kayla and im looking for love.mp3

---
## [josh-newman/grailbio-base](https://github.com/josh-newman/grailbio-base)@[bb620fe65b...](https://github.com/josh-newman/grailbio-base/commit/bb620fe65b3db8452bc301fe2f9d9bcf3e385b70)
#### Thursday 2021-02-04 17:29:38 by Josh Newman

grail-access: blackbox tests

Summary:
I'd like to make some changes to grail-access. I've wondered for a long time why
it deletes an existing Vanadium principal before it starts an authentication
flow instead of just updating the blessing. I partly think it's inelegant, but
I've also seen multiple problems in #eng-help that were caused or exacerbated
by running grail-access, canceling it, and not realizing that destroys the
credentials.

After a bit of code inspection recently, I think grail-access has quite a bit
of historical cruft in its code (handling of the principal and its blessings is
disorganized and redundant), and I think the patches to work around agent errors
could be fixed in a more principled way.

However, since we use it a lot and I'm learning as I go along, my first step is
to introduce some tests. These all invoke grail-access as if from a shell and
check its behavior and output to ensure user-visible behavior is unchanged.

The connection of this to my actual work is rather distant: I am hoping this
fixes a grail-fuse issue which will make it easier to do non-Go analysis work
and otherwise streamline some dev environment tasks and then I'll be so much
more productive (ha). I think it's worth giving grail-access some attention,
since it is almost in its initial prototype state from three years ago, but it
might just take some time, and I understand if it takes time to review.

Test Plan:
Tests the EC2 blesser. I don't know how to reasonably fake the Google oauth2
server, but that's also the more reasonable flow to test manually, so we may
need to just rely on that.

Tests pass with both Bazel and `go test`.

Reviewers: aeiser

Reviewed By: aeiser

Subscribers: smahadevan, krestivo, afields, sbagaria

Differential Revision: https://phabricator.grailbio.com/D44965

fbshipit-source-id: e2ff93d

---
## [tstelfox/CPP](https://github.com/tstelfox/CPP)@[f3e5d50071...](https://github.com/tstelfox/CPP/commit/f3e5d50071a6c0d02189befa54c671ecfcd50b93)
#### Thursday 2021-02-04 17:59:43 by Codemuncher

What a fucking difficult exercise to wrap my head around but it works not and in the end Thijs was right that it is kind of easier cause you already build everything as the struct and then you just cast that shit to void. Seemed way more complicated. Yolo

---
## [chinu1997/Capstone-Project2-Team2](https://github.com/chinu1997/Capstone-Project2-Team2)@[e21f932ff7...](https://github.com/chinu1997/Capstone-Project2-Team2/commit/e21f932ff7013e5832de57b2ddb89e2772568200)
#### Thursday 2021-02-04 18:24:23 by chinu1997

Update README.md

 Numerous companies from financial indutry often invest considerable resources to improve their predictive models with the aim of having better insights into their customers. Such an interest in model improvement has intensified in recent years mostly because of fast development of machine learning and artificial intelligence. For standard lending institution default predictive model with high performance helps to considerably minimize Credit Loss, resulting in higher revenue and profits. Usually the better predictive model the more efficient is the underwriting policy and collection process. A well-functioning model should distinguish creditworthy customers from those that are credit risks. Often, more-predictive credit-decisioning model can identify a greater number of customers within an institution’s specified risk tolerance, which should expand revenues as well.
In this project the goal is to increase detection of defaulted loans before the loan is issued/offered by P2P lending company - Lending Club. Peer-to-peer lending differs from traditional financial institutions like banks or commercial lending companies.
So, Lending Club is a mediator between investors and borrowers, earning money by charging both. The main Lending Club interest is to attract more clients and maintain protfolio size. The motivation of borrowers is clear, they want to find as cheap capital as possible, so they're seeking for the best offer at the market, which is available for them. In case of investors the motivation is obvious as well. Investors look for high ROI (return of investments), but remembering that returns are proportional to risks, we may formalize saying, that investors look for appropriate returns/risks ratio. If investors experience losses it may cause churn rate growth.
The underwriting process for Lending Club looks like this. Borrower applies for the loan, then if he/she meets all the basic requirements - Lending Club using their scoring model assigns client to respective grade. There are 7 grades and 35 sub-grades. Interest rate is dependent on sub-grade. After that, Lending Club gives access to the loan for investors with information about the loan and the borrower (incl. grade and sub-grade) and investors decide whether or not to invest money in this loan. The lower the grade the higher the interest rate, which means, that investors may take higher risks to gain potentially higher returns.
Seeking for default rate reduction we can end up with too restrictive underwriting policy which does not neccessary correlate with higher ROI for investors, because we'll not let investors choose risky loans, which means lower interests. For Lending Club it probably means the loss of investors with high risk appetite and borrowers with weak credit history, or in case of Lending Club those who need higher loan amount.
Loan default occurs when a borrower fails to pay back a debt according to the initial arrangement. In the case of most consumer loans, this means that successive payments have been missed over the course of weeks or months. Fortunately, lenders and loan servicers usually allow a grace period before penalizing the borrower after missing one payment. The period between missing a loan payment and having the loan default is known as delinquency. The delinquency period gives the debtor time to avoid default by contacting their loan servicer or making up missed payments.
Defaulting on a loan will cause a substantial and lasting drop in the debtor's credit score, as well as extremely high interest rates on any future loan. For loans secured with collateral, defaulting will likely result in the pledged asset being seized by the bank. The most popular types of consumer loans that are backed by collateral are mortgages, auto loans and secured personal loans. For unsecured debts like credit cards and student loans, the consequences of default vary in severity according to the type of loan. In the most extreme cases, debt collection agencies can garnish wages to pay back the outstanding debt.
The loan is one of the most important products of the banking. All the banks are trying to figure out effective business strategies to persuade customers to apply their loans. However, there are some customers behave negatively after their application are approved.

---
## [tych0/puzzlefs](https://github.com/tych0/puzzlefs)@[d64caa8c5c...](https://github.com/tych0/puzzlefs/commit/d64caa8c5c649444855a1dd977d610c867832162)
#### Thursday 2021-02-04 22:21:30 by Tycho Andersen

puzzlefs: a rough draft of serialization

Here's some initial code that explores the ideas in the puzzlefs
specification.

There are essentially two interesting parts: the actual wire format for
puzzlefs and the content defined chunking algorithm. There are a couple
other packages (oci for writing the OCI image, and exe for the actual
binary) which are not very interesting for the purposes of puzzlefs beyond
needing to exist :). The one thing we might want to change here is that we
use the OCI v1.1 "blobs/sha256/" path, without doing the git-style
"blobs/sha256/$first_byte/$second_byte" to keep the directory listings
small. This seems like an opportune time to change that if there ever was
one :)

# format

Mostly we're using serde to serialize things to a cbor format for now.
Perhaps it is reasonable to keep using this format for most objects for the
forseeable future, but one place where we really can't use it is the inode
table, since that requires inodes to be fixed size according to the spec
(so we can do a binary search when resolving inodes on the mount/extract
end of things quickly).

For this POC, I've just implemented fixed sized inodes by bit-banging
things together in a gross way. However, we need to do this better if we
want to generalize it at all, but perhaps it's okay to use serde for
everything that's not the fixed-with inode bit.

There are a few other drawbacks to serde, though:

1. it (reasonably) does not like "extra" data at the end of the stream it
   is de-serializing. This is a problem for techniques like our "blob ref"
   technique, where you just seek() to a particular part of the blob and
   start parsing, because the whole point is to not de-serialize the stream
   all at once. We could work around this by either 1. serde hacks to
   ignore extra data or 2. adding a length to everything. but if it's cbor
   encoded already, it knows its length...

2. it seems non-good to use one serialization primitive for part of a
   format, and a different one for another part.

However, it is "free" and gets us off the ground, so we can keep using it
for now.

# builder

The builder module is the thing that walks the filesystem and actually
builds a puzzlefs image. Right now, it only supports walking a single
filesystem and generating the base image, and does not support adding
deltas, although that's the goal.

There are various content defined chunking algorithms out there. The
obvious prior art open source implementation is the one that is used in
casync, which uses a buzhash. Around the time casync was being developed,
the FastCDC paper was released, which is supposedly much better than most
other things out there according to the paper's results (and various other
googlable results, see the comment in fastcdc_fs.rs for details). So, we
choose this algorithm for now.

The best-looking fastcdc implementation for rust does not exactly implement
an API that is friendly for us. Really, we'd like to get a callback as soon
as a chunk is generated, so that we can take the data stream and end it,
thus only writing it once to the OCI image. However, fastcdc-rs doesn't
offer us that API, so we end up just reading chunks into memory. We only
process these chunks after every file is written to the stream, so may end
up reading single large files into memory. This is fine for now, but
presumably we'll want to fix this at some point.

Finally, we need to choose good parameters for FastCDC. We should do some
study of this to see what works well (and if we want to allow
user-specified sizing, which I think we do not).

For now, I have chosen 10M for the lower bound on chunk size, 40M for the
average size, and 256M for the maximum size, based on the assumption that
the Ubuntu rootfs is 40M. If we choose average chunk sizes any larger, we
will likely not get any sharing across updates of the base rootfs, since
the whole thing will always be one chunk.

Of course there is a tradeoff between how many total chunks there are and
thus how many are needed to represent larger files. Some study about good
parameters would be useful here.

# TODO

The next steps are:

1. implement the "real" wire format (aka use a real parser/unparser) serde
   doesn't have any fixed size emitters, so we'd end up fighting it most of
   the way, I think.
2. implement a reader of the format once the above is mostly reasonable
3. implement a version of FastCDC that doesn't gobble memory like a hungry
   teenager. the primitives from the paper are streaming primitives, so I
   don't think it will be *that* hard.
4. implement a delta generator so we can begin to layer (piece? :D)
   puzzlefs images on top of each other.

Finally, I have given almost no thought to the no_std case for compiling
this code, which is likely what we'll want for compiling this code in the
kernel. But presumably we'll also want to think about that when choosing
libraries, as some offer good no_std support, and some do not.

Signed-off-by: Tycho Andersen <tycho@tycho.pizza>

---
## [Buildstarted/linksfordevs](https://github.com/Buildstarted/linksfordevs)@[f2da371916...](https://github.com/Buildstarted/linksfordevs/commit/f2da371916b01e751fa9eecf737f8ff587c305de)
#### Thursday 2021-02-04 23:09:19 by Ben Dornis

Updating: 2/4/2021 11:00:00 PM

 1. Added: How to Lose Money With 25 Years of Failed Businesses
    (https://joeldare.com/how-to-lose-money-with-25-years-of-failed-businesses)
 2. Added: Fast Software, the Best Software
    (https://craigmod.com/essays/fast_software/)
 3. Added: A short post on Nibby, URL shorteners and the dangers of machine translation
    (https://thecrow.uk/on-link-shorteners-and%20the-dangers-of-google-translate/)
 4. Added: Locating Humans with DNS
    (https://landshark.io/2021/02/04/dns-for-humans.html)
 5. Added: The Problem with Young Internet Entrepreneurs - Clayton Horning
    (https://claytonhorning.com/entrepreneurship/the-problem-with-young-internet-entrepreneurs/)
 6. Added: Enforcing work-life balance with bash scripting, or targeted ways to kill Linux programs – prem.moe
    (https://prem.moe/2021/02/04/Enforcing-work-life-balance-with-bash/)
 7. Added: Art of Starting Things (or How to Start Doing Things That You've Been Delaying Forever) – Unstructed.tech
    (https://unstructed.tech/2021/02/04/art-of-starting-things/)
 8. Added: Working inside a Docker container using Visual Studio Code
    (https://asp.net-hacker.rocks/2021/02/04/remote-docker.html)
 9. Added: Working with large .NET 5 solutions in Visual Studio 2019 16.8 | Visual Studio Blog
    (https://devblogs.microsoft.com/visualstudio/working-with-large-net-5-solutions-in-visual-studio-2019-16-8/)
10. Added: Oh sorry, I was on mute!
    (https://neil.computer/notes/oh-sorry-i-was-on-mute/)
11. Added: Sequencing DNA in our Extra Bedroom « Andrew J. Barry
    (https://abarry.org/dna-sequencing-in-our-extra-bedroom/)
12. Added: Inline caching: quickening
    (https://bernsteinbear.com/blog/inline-caching-quickening/)
13. Added: Can networking be simple? with Tailscale's Avery Pennarun | Hanselminutes with Scott Hanselman
    (https://hanselminutes.simplecast.com/episodes/can-networking-be-simple-with-tailscales-avery-pennarun-ZmoBl_BN)
14. Added: The spectrum of synchronousness - The Engineering Manager
    (https://www.theengineeringmanager.com/remote-working/the-spectrum-of-synchronousness/)
15. Added: I Finally Started Getting Programming
    (https://savo.rocks/posts/i-finally-started-getting-programming/)
16. Added: A Simple and Succinct Zero Knowledge Proof
    (https://decentralizedthoughts.github.io/2020-12-08-a-simple-and-succinct-zero-knowledge-proof/)

Generation took: 00:09:06.5598337

---
## [bbugyi200/htop](https://github.com/bbugyi200/htop)@[9197adf57e...](https://github.com/bbugyi200/htop/commit/9197adf57e04875fe7fd5b768bc5201d5def2548)
#### Thursday 2021-02-04 23:33:49 by Antoine Motet

Fix CPU usage on OpenBSD

The current OpenBSD-specific CPU usage code is broken. The `cpu`
parameter of `Platform_setCPUValues` is an integer in the interval
[0, cpuCount], not [0, cpuCount-1]: Actual CPUs are numbered from
1, the “zero” CPU is a “virtual” one which represents the average
of actual CPUs (I guess it’s inherited from Linux’s `/proc/stats`).
This off-by-one error leads to random crashes.

Moreover, the displayed CPU usage is more detailed with system,
user and nice times.

I made the OpenBSD CPU code more similar to the Linux CPU code,
removing a few old bits from OpenBSD’s top(1). I think it will be
easier to understand, maintain and evolve.

I’d love some feedback from experienced OpenBSD people.

---

# [<](2021-02-03.md) 2021-02-04 [>](2021-02-05.md)

