# [<](2021-04-26.md) 2021-04-27 [>](2021-04-28.md)

3,171,519 events, 1,595,707 push events, 2,543,089 commit messages, 193,595,218 characters


## [wstampley/dr-scripts@2b57965fef...](https://github.com/wstampley/dr-scripts/commit/2b57965fef22ae0f6f7ac9f1e7233c295aadc47b)
##### 2021-04-27 00:47:32 by Katoak

[script] [tendme] Add background monitoring mode; use DRCH


### Background
* `tendme` is duplicating logic for parsing bleeders from your health; should use DRCH instead.
* `tendme` will exit the script after tending a wound (likely a bug in the script), it won't stay perpetually in passive mode monitoring for new bleeders.

### Changes
* Use DRCH common methods for parsing health and identifying wounds
* Add optional argument 'monitor' to put script into passive loop to retend as needed
* Add optional argument 'train' to unwrap and tend wounds to maximize experience
* Encapsulate logic in a class and use `events` module instead of the low-level `scripts.get` 

### Usage
* `;tendme [train] [monitor]`
  - `train` will unwrap and rewrap bandages to maximize experience
  - `monitor` will enter passive loop and retend bleeders when bandages fall off
  - if not in monitor mode (default) then will check health and tend bleeders once then exit

## Tests

### should rewrap wound when bandages fall off
```
> ,tendme monitor

...

The bandages binding your chest become useless and fall apart.

> 
[tendme]>tend my chest

You work carefully at tending your wound.
Doing your best, you only manage to reduce the bleeding a little.
Roundtime: 10 sec.
```

### should train by unwrapping wounds when bandages are ready to be changed
```
> ,tendme train

--- Lich: tendme active.

...

You feel like now might be a good time to change the bandages on your neck.
> 
[tendme]>unwrap my neck

The bandages binding your neck become useless and fall apart.

You unwrap your bandages.
[Roundtime: 20 seconds]

[tendme]>tend my neck

You work carefully at tending your wound.
Doing your best, you are able to stop the bleeding.
Roundtime: 1 sec.
> 
[tendme]>health

```

### should not try to tend clotted, tended, or internal bleeders
```
> ,tendme

--- Lich: tendme active.

[tendme]>health

Your body feels at full strength.
Your spirit feels full of life.
You are strangely full of extra energy.
You have some minor abrasions to the head, a few nearly invisible scars along the head, minor swelling and bruising around the neck compounded by deep cuts across the neck, minor swelling and bruising around the right arm compounded by cuts and bruises about the right arm, an occasional twitching in the right arm, cuts and bruises about the left arm, a severely swollen and deeply bruised right leg compounded by cuts and bruises about the right leg, a constant twitching in the right leg, a severely swollen and deeply bruised left leg compounded by deep slashes across the left leg, some tiny scratches to the right hand, an occasional twitching in the right hand, some tiny scratches to the left hand, a few nearly invisible scars along the left hand, deep slashes across the chest area, minor swelling and bruising in the abdomen compounded by cuts and bruises about the abdomen, a few nearly invisible scars along the abdomen, some tiny scratches to the back, some tiny scars across the back, some minor abrasions to the right eye, some tiny scars across the right eye, some minor abrasions to the left eye, a few nearly invisible scars along the left eye.

Bleeding
            Area       Rate              
-----------------------------------------
            neck       (tended)
        left leg       (tended)
           chest       very profuse
   inside r. leg       very bad
   inside l. leg       slight

> 
[tendme]>tend my chest

You work carefully at tending your wound.
Doing your best, you only manage to reduce the bleeding a little.
Roundtime: 10 sec.

--- Lich: tendme has exited.
```

---
## [applewood524/applewood524@0977b8d848...](https://github.com/applewood524/applewood524/commit/0977b8d8489b3c16329ba0fc45d2fd490c080be8)
##### 2021-04-27 00:56:46 by Galaxy Song

Update readme

It's easy to find people promising everything and delivering almost nothing.
My major strengths are in giving realistic promises, being responsive and responsible, concentrating on business needs and communications. So I am not a pure geek thinking about technology only but, first of all, a making-things-happen person.

About me:
A Software engineer.
Senior Developer with over 15 years of experience in developing web applications.
Expertise is handling end-to-end projects

Programming skills include:

Blockchain: 3 years of experience
Laravel: 7 years of experience
MERN stack: 5 years of experience
WordPress: 5 years of experience
... ... ...

Other skills include API integrations in a system like Blockchain, Social network APIs, Payment Gateway APIs (Stripe,Authorize.NET, CCAvenue, Paypal)
Why choose me over my competitors:
Warranty for applications I’ve developed if they are unmodified.
Creative, thoughtful, reliable, responsive, and responsible.
Always available via freelancer platform. You can expect me answering you ASAP, thinking about your needs, giving suggestions and helping you.
My price is pretty affordable for a Senior Developer.
Ready to introduce you to my clients of previous projects so you can learn more about me from them by asking them how they felt i delivered on their projects.
I have a team of developers around who are ready to work on complex projects. Thus, specialization, quality, and team expansion are not issues at all.

---
## [applewood524/applewood524@883757ca50...](https://github.com/applewood524/applewood524/commit/883757ca503548ef3b4f5d948e63cd3857dc9b5a)
##### 2021-04-27 01:00:01 by Galaxy Song

update

It's easy to find people promising everything and delivering almost nothing.
My major strengths are in giving realistic promises, being responsive and responsible, concentrating on business needs and communications. So I am not a pure geek thinking about technology only but, first of all, a making-things-happen person.

About me:

A Software engineer.

Senior Developer with over 15 years of experience in developing web applications.
Expertise is handling end-to-end projects

Programming skills include:

Blockchain: 3 years of experience

Laravel: 7 years of experience

MERN stack: 5 years of experience

WordPress: 5 years of experience

... ... ...

Other skills include API integrations in a system like Blockchain, Social network APIs, Payment Gateway APIs (Stripe,Authorize.NET, CCAvenue, Paypal)
Why choose me over my competitors:
Warranty for applications I’ve developed if they are unmodified.
Creative, thoughtful, reliable, responsive, and responsible.
Always available via freelancer platform. You can expect me answering you ASAP, thinking about your needs, giving suggestions and helping you.
My price is pretty affordable for a Senior Developer.
Ready to introduce you to my clients of previous projects so you can learn more about me from them by asking them how they felt i delivered on their projects.
I have a team of developers around who are ready to work on complex projects. Thus, specialization, quality, and team expansion are not issues at all.

---
## [applewood524/applewood524@e25ca9775a...](https://github.com/applewood524/applewood524/commit/e25ca9775abc507ebe5cafeb8dc487472a8ed31e)
##### 2021-04-27 01:02:04 by Galaxy Song

update

It's easy to find people promising everything and delivering almost nothing.

My major strengths are in giving realistic promises, being responsive and responsible, concentrating on business needs and communications. 

So I am not a pure geek thinking about technology only but, first of all, a making-things-happen person.

---------------------------------------------------------------------------------------------------------------------------------------------

About me:

A Software engineer.

Senior Developer with over 7 years of experience in developing web applications.

Expertise is handling end-to-end projects

Programming skills include:

Blockchain: 3 years of experience

Laravel: 7 years of experience

MERN stack: 5 years of experience

WordPress: 5 years of experience

... ... ...

---------------------------------------------------------------------------------------------------------------------------------------------

Other skills include API integrations in a system like Blockchain, Social network APIs, Payment Gateway APIs (Stripe,Authorize.NET, CCAvenue, Paypal)
Why choose me over my competitors:
Warranty for applications I’ve developed if they are unmodified.
Creative, thoughtful, reliable, responsive, and responsible.
Always available via freelancer platform. You can expect me answering you ASAP, thinking about your needs, giving suggestions and helping you.
My price is pretty affordable for a Senior Developer.
Ready to introduce you to my clients of previous projects so you can learn more about me from them by asking them how they felt i delivered on their projects.
I have a team of developers around who are ready to work on complex projects. Thus, specialization, quality, and team expansion are not issues at all.

---
## [Sonic121x/Skyrat-tg@530112e09f...](https://github.com/Sonic121x/Skyrat-tg/commit/530112e09f3190ffa84c90f941ee972b44509b53)
##### 2021-04-27 09:10:46 by death and coding

[modular][ready] formal wear for medical and engineering in loadout, gas masks unadded, but coded (#5156)

* for i just threw out the love of my dreams

* Update uniform.dm

* fuck

* biker

* Update glasses.dm

* I don't want any of my sprites in Skyrat. I appreciate you asking though.

* Revert "I don't want any of my sprites in Skyrat. I appreciate you asking though."

This reverts commit d980baf7ada95a04f74870103b2ee09ede67dcda.

* Revert "Revert "I don't want any of my sprites in Skyrat. I appreciate you asking though.""

This reverts commit ad42656117f90747c0aaf7ade3614a16d67fed4b.

* as requested

* casual cargo gear

Co-authored-by: louiseedwardstuart <bonniefluff>

---
## [shwina/cudf@8a666a04e0...](https://github.com/shwina/cudf/commit/8a666a04e0123744eb259d88ac4c04b0b6de4303)
##### 2021-04-27 12:33:03 by Vyas Ramasubramani

Refactor Python and Cython internals for groupby aggregation (#7818)

This PR makes some improvements to the groupby/aggregation code that I identified while working on #7731. The main purpose is to make the code logic easier to follow and reduce some unnecessary complexity; I see minor but measurable performance improvements (2-5% for small datasets) as well, but those are mostly just side effects here. Specifically, it makes the following changes:

1. Inlines the logic for dropping unsupported aggregations. The old function was repetitive and necessitated looping over the aggregations twice, whereas the new approach drops unwanted aggregations on the fly so it only loops once. The new code also makes it so that you only construct a C aggregation object once.
2. Merges the logic from `_AggregationFactory` into `Aggregation`, and removes the constructor for `Aggregation`. The one downside here is that the Cython `Aggregation` object's constructor no longer places it in a valid state; however, in practice the object is always constructed via either the `make_aggregation` function or its various factories, and the object's constructor was only every used in `_drop_unsupported_aggs` anyway. The benefit is we remove the fragmentation between these two classes, making the code much more readable, and the `Aggregation` class actually serves a purpose now beyond just providing a single property `kind` that is only used once: it is now the primary way that other Cython files interact with aggregations. This also means that in most places other Cython modules don't need to work with `unique_ptr[aggregation]` as much anymore (although they do still have to move `Aggregation.c_obj` for performance reasons). `make_aggregation` now returns the Cython class instead of the underlying C++ one.
3. Modified all the "allowed aggregations" sets to use the uppercase names of the aggregations. In addition to simplifying the code a tiny bit, this helps reduce confusion between the aggregation names used in Python for pandas compatibility and the libcudf names (for instance, `idxmin` vs `argmin`, now `ARGMIN`).
4. Explicitly defines all the aggregations on a groupby. I discussed this briefly with @shwina, the change has pros and cons. The benefit is that all of these methods are properly documented now, there's less magic (the binding of methods to a class after its definition can be confusing for less experienced Python developers and has a lot of potential gotchas), and we can use the simpler string-based agg definition wherever possible. The downside is that we now have to define all of these methods. I think the change is definitely an improvement, but I'm happy to change it back if anyone can suggest a better alternative. In the long run we probably need to find a way to share both code and docstrings more effectively between all aggregations (DataFrame, Series, and GroupBy).

Authors:
  - Vyas Ramasubramani (https://github.com/vyasr)
  - GALI PREM SAGAR (https://github.com/galipremsagar)

Approvers:
  - Karthikeyan (https://github.com/karthikeyann)
  - Ashwin Srinath (https://github.com/shwina)
  - GALI PREM SAGAR (https://github.com/galipremsagar)

URL: https://github.com/rapidsai/cudf/pull/7818

---
## [vvchauit/vvchauit@195a744ef7...](https://github.com/vvchauit/vvchauit/commit/195a744ef722a588e57e779e92e8a42ed53cbd3c)
##### 2021-04-27 13:09:01 by VAN CHAU

Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die

Q & A with Author Eric Siegel
Did Nate Silver use predictive analytics to forecast Obama's 2012 election?
No—but Obama did use predictive analytics to help get elected. Nate Silver made election forecasts for each state as a whole: which way would a state trend, overall? In the meantime, the Obama campaign was using predictive analytics to render per-voter predictions. Moving beyond forecasting, true power comes in influencing the future rather than speculating on it—the raison d'être of predictive analytics. Nate Silver publicly competed to win election forecasting, while Obama's analytics team quietly competed to win the election itself. Specifically, team Obama drove per-voter campaign decisions by way of per-vote predictions.

Why does early retirement predict a shorter life expectancy & why do vegetarians miss fewer flights?
These are two more colorful examples of the multitudes of predictive discoveries waiting within data.

University of Zurich discovered that, for a certain working category of males in Austria, each additional year of early retirement decreases life expectancy by 1.8 months. They conjecture that this could be due to unhealthy habits such as smoking and drinking following retirement.

One airline discovered that customers who preorder a vegetarian meal are more likely to make their flight, with the interpretation that knowledge of a personalized or specific meal awaiting the customer provides an incentive, or establishes a sense of commitment.

Predictive analytics seeks out such predictive connections and then works to see how they may combine together for more precise prediction.

What are the hottest trends in predictive analytics?
There have been many exciting improvements in the core technology of predictive analytics. One is "uplift modeling" (a.k.a. "persuasion modeling"), which predicts influence. ..in order to do influence. The Obama campaign used it to influence voters in the 2012 presidential election; marketing uses it to more adeptly persuade customers; and medicine uses it to better select per-patient treatments. This topic is the focus of the final chapter of this book.

Another hot trend is ensemble models. Like the collective intelligence that spawns the wisdom of a crowd of people, we see the same effect with a crowd of predictive models. Each model alone may be fairly primitive such as a few simple rules, so it gets prediction wrong a lot, as an individual person trying to predict also does. But have them come together as a group and there emerges a new level of predictive performance.

Does the NSA use predictive analytics, and how does that impact the amount of data collected on us?
It's a foregone conclusion that the world's largest spy organization employing the world's largest number of Ph.D. mathematicians considers predictive analytics a strategic priority. Predictive analytics realizes a great potential for law enforcement: The automatic discovery of new suspects. The value of this capability multiplies the incentive to collect increasing amounts of data about civilians. The NSA needs data about everyone, including those of us with no connection to crime whatsoever—not to spy on us but to establish a quantitative baseline. This in turn only amplifies the stakes of the contentious security-versus-privacy debate.

What is the coolest thing predictive analytics has done?
One of the most inspirational accomplishments of predictive analytics is IBM's "Jeopardy!"-playing Watson computer, which triumphed against the all-time human champions on the TV quiz show. The questions can be about most any topic, are intended for humans to answer, and can be complex grammatically. It turns out that predictive modeling is the way in which Watson succeeds in determining the answer to a question: it predicts, "Is this candidate answer the correct answer to this question?" It knocks off one correct answer after another—incredible.

What are companies predicting about me as a customer?
Here are just a few examples:

- Facebook predicts which of 1,500 candidate posts (on average) will be most interesting to you in order to personalize your ordered news feed.

- Microsoft helped develop technology that, based on GPS data, accurately predicts one's location up to multiple years beforehand.

- Target predicts customer pregnancy from shopping behavior, thus identifying prospects to contact with offers related to the needs of a newborn's parents.

- Tesco (UK) annually issues 100 million personalized coupons at grocery cash registers across 13 countries. Predictive analytics increased redemption rates by a factor of 3.6.

- Netflix sponsored a $1 million competition to predict which movies you will like in order to improve movie recommendations.

- One top-five U.S. health insurance company predicts the likelihood an elderly insurance policy holder will die within 18 months in order to trigger end-of-life counseling.

---
## [xdy/twodsix-foundryvtt@9a20e66ade...](https://github.com/xdy/twodsix-foundryvtt/commit/9a20e66ade2970958219850d8e3ac79d16a3f09b)
##### 2021-04-27 17:48:25 by Jonas Karlsson

fix: A bit more support for Cepheus Atom (and friends), there is now a system setting to enable showing only END, Lifeblood and (optionally) Contamination instead of the regular characteristics. (Actually uses STR for Lifeblood and PSI for Contamination).

This is a Dreadful Hack that will undoubtedly Break Things Horribly if looked at too closely. If anyone asks, mutant marauders from the wastelands forced me to do it. :)

I.e. I hacked this together without much (or, well, any) concern for how I had originally intended to do this. If/when this gets redesigned it's probably best to just rip out this commit.

---
## [BookStackApp/BookStack@aa6a752e38...](https://github.com/BookStackApp/BookStack/commit/aa6a752e38b73eea201ea8e6a5b994a715e24aec)
##### 2021-04-27 20:36:18 by Dan Brown

Implemented custom select controls because apple hates web developers

They'd rather keep pushing their 2007 era strange form control styles
even though they're horribly outdated, ugly and hard to style. The
only way to override is a full nuking of the default styles, which means
we have to then implement the frigging arrow icon using hacks which would
then conflict with all other sensible browsers so we have to nuke their
styles aswell to ensure some stupid backgroud hack is used everywhere.

I bet apple don't even use their shite default control styles and nuke
them also, Lets see. Yup, First thing I see on the top of their homepage
is a locale select dropdown custom built from about 10 HTML elements. FML

For #2709

---
## [phoenix344/phpbb-mdt-style@8e50eabd32...](https://github.com/phoenix344/phpbb-mdt-style/commit/8e50eabd32205b87beef31f5c665f9c652e65281)
##### 2021-04-27 21:33:14 by phoenix344

Update readme.md

MOTHERFUCKING GOOGLE MATERIAL DESIGN GODDAMNIT!!! CURSE YOU GOOGLE!!!!!!!!! I ONLY SUPPORT IE FOR THE REST OF MY LIFE!!!!!!!

---
## [mrakgr/The-Spiral-Language@e9277ddf54...](https://github.com/mrakgr/The-Spiral-Language/commit/e9277ddf545bc23611b48dd95958222d0863c21d)
##### 2021-04-27 22:47:02 by Marko Grdinić

"9:25am. I figured out a new rule that will make gradient modulation in RNNs a done deal. It should be useful in regular nets as well. But first, let me study the way moving averages are updated a little.

9:40am.

```fs
//let m = 1.05
//let l = List.init 10 (fun i -> (1.1 / m) ** float i)

let t = 100.0
let rec iter n m i =
    if i < n then
        let rec loop (m,s) i =
            if i < 10 then
                let s = (1.1 / m) ** float (i+1)
                let m = (t - 1.0) / t * m + 1.0 / t * s
                loop (m,s) (i+1)
            else m,s

        let m,s = loop (m,1.1) 0
        iter n m (i+1)
    else
        m
let m = iter 100 1.0 0
```

I've been wondering what the average of an exponential series converges to, and the answer here is...

```
val m : float = 1.085496689
```

Even if I set the n to 10000 I still get the same result.

```fs
let t = 100.0
let rec iter n m i =
    if i < n then
        let rec loop (m,s) i =
            if i < 1000 then
                let s = (1.1 / m) ** float (i+1)
                let m = (t - 1.0) / t * m + 1.0 / t * s
                loop (m,s) (i+1)
            else m,s

        let m,s = loop (m,1.1) 0
        iter n m (i+1)
    else
        m
let m = iter 100 1.0 0
```

```
val m : float = 3.664979479
```

Changing the inner loop to a large number causes it to become useless.

And something like this is essentially what I've been using to normalize the RNN gradients. You can tell how shitty standard SGD was if even the KFAC managed to do the job.

```fs
        let rec loop (m,s) i =
            if i < 300 then
```

This is `val m : float = 1.105855851`. As long as i is not too big, it will go in the right direction. And this will be helpful.

But it won't converge to the value I want and do the right thing.

9:50am. A moving average rule will only converge if its inputs and outputs are unrelated.

Here is what I am going to do when it comes to RNNs. I'll constraint the gradient L1 norm of the inputs to be the same as the L1 norm for the outputs. I'll do this in the head of RL agent as well. I meant to constrain it by the norm of the weights, but matching the inputs and the outputs would be better.

10:05am. Yeah, I can trust this. A combination of buffer shaping and gradient modulation tricks will be enough to get me to the level of expertise necessary to ensure stability in RL agents.

10:10am. For now, let me wind down. Yesterday I started the interview by Tegmark. I should also watch the lecture by Deepmind.

https://youtu.be/RL4j4KPwNGM?t=4848
Max Tegmark podcast with Lex Friedman

https://www.youtube.com/watch?v=AIiwuClvH6k&list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&index=8
DeepMind x UCL | Deep Learning Lectures | 8/12 | Attention and Memory in Deep Learning

I really like his optimism about being able to figure out deep learning. Though I am on the opposite sides of the AI safety issue and don't think the brakes are necessary in this race, I still don't mind using the steering wheel. Being able to understand deep learning better could only help my situation.

10:20am. Focus me. Let me go for the lecture by Graves. Today my stress is the smallest it has been in days. I am starting to trust the backprop + buffer shaping + gradient modulation rules.

10:30am. https://youtu.be/AIiwuClvH6k?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&t=516

This is interesting. There are different sensitivity patterns for the data and the action networks.

11:10am. Hmmm, rather than averaging in RNNs, I should consider gradient clipping.

I am working under the theory that everything should be linearized. If so then as long as the paths are separate, I do not need to be too strict on the norms when doing the updates...

Hmmmm, I am not sure. RNNs are tricky that way.

Forget this issue for now. RNNs won't come into play until significantly later anyway.

12:15pm. https://youtu.be/AIiwuClvH6k?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&t=4614

While the attention mechanism is simple, he recommends this blog post, the annotated transformer as the actual architure is complex.

https://youtu.be/AIiwuClvH6k?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&t=5080

Never heard of this before. I'll keep the universal transformers in mind. I'll need something powerful for sequences when I do full poker eventually. Though just embedding the player id and using some cut off point for the betting sequence length is also an option. Sequences will be tough using GPUs.

https://youtu.be/AIiwuClvH6k?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&t=5239

Adaptive computation time is something I've been thinking about myself on and off.

12:45pm. https://www.youtube.com/watch?v=kVU8zTI-Od0&list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&index=5
DeepMind x UCL | Deep Learning Lectures | 5/12 | Optimization for Machine Learning

Let me take a break here. Maybe I'll go for the optimization lecture next.

2:15pm. My plan was to use the weight norm in the last layer, but now that I've gone over it so many times, the rule to just normalize the input gradients in the last layer makes a lot of sense.

Maybe it could make sense in the rest of the layers as well.

The reason for that is the sampling scheme I am using.

In usual supervised learning there is `y - x` which gives a distance, but my plan is to turn everything into a +- 1 move via sampling. So if the goal is to normizalize, in the situation couldn't I use a predictive normalizer that sets all the grads to 1?

It is worth considering. I'd still have to track the input norms for the L1 updates, but otherwise yeah.

I admit something like this had not occured to me. This possibility only emerged because I decided to do the right thing with the replay buffer for the sake of the topmost layer and nonlinearities in general.

To think that a chain of possibilities would emerge pointing in this direction. I am astounded. All of this really could work. I should definitely try it out.

5:10pm. I thought of a toy example, and it broke my mind. I am absolutely shocked, but the backprop rules make sense as a credit assignment method! Also, I now understand what credit assignment even means.

Imagine something like this...

```
X -> X'
Y -> Y'
```

Two independent factor graphs.

Those arrows both have transition weights 1.

And both X and Y have unnormalized probabilities of 1.

Suppose I sample X and Y and propagate them forward. At the end of both nodes I'd get a reward of 1. Propagating that back, X and Y would get assigned credit of 1 in even amounts.

The goal is to maintain that level, namely 2 here.

Now suppose the distribution shifts and the probability of X gets halved to 0.5. That means that the amount of times that X' will be reached will also fall to 0.5 since it depends on X.

The second goal is to maintain that reachability of 2 as well.

Due to the distributional shift, both of those goals will fall to 1.5 and miss their target.

What to do?

To adjust both the credit level and that target node levels we'll shift the weight of `X -> X'`, consider it the transition probability to 2. One can think of it as inverse frequency.

Then it actually makes sense. Both goals get reached.

The model once again reflects reality and what absolutely floored me is that going in the opposite direction when distributing the rewards by multiplying by the weights makes sense.

5:25pm. I must have been crazy to not realize this earlier.

Damn, the tabular graphs all have transition weights of 1 and their normalization factors are the way they are sampled. It was too much for me to step back and see the matrix perspective.

All this time I've been railing about the optimization, but gradients vanishing and gradients exploding is an expected consequence.

5:25pm. I just could not see it before now.

I actually did not think of it in such simple terms at first, my initial starting point was to consider what would credit assignment be like if I got rid of the nonlinearities.

8pm. https://arxiv.org/abs/1711.02257
GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks

This is like a mental disease. I am obsessing and obsessing, desperately trying to get a sliver more of insight from the depths of my own mind. Let me finally watch the Deepmind lecture. Today went much like yesterday. I have no idea when I am going to get over this and start programming.

https://vimeo.com/287812909

Ah, let me watch this first. The lecture by Martens is on stuff that I know.

8:20pm. This is crap.

Let me watch the Deepmind lecture.

9:50pm. https://youtu.be/kVU8zTI-Od0?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&t=4573
> You can actually do this analysis based on kernel theory where you can really see with high probability what a neural network will do if you keep adding layers on top. And what you quickly observe is that the way that the neural network maps the input to its output is a function that degenerates very very fast unless you are extremely careful at how you set the weights at each layer. And the burden of having to do that, of having to set those weights carefully becomes harder and harder and harder as you keep adding more layers.

The rest I mostly knew, but here he is talking about initialization and how in the context of resnets and batch norm, the network starts very linear and then takes advantage of nonlinearity as the training goes along.

I am really wondering how did the batch norm + RNNs story go in the last few years.

https://jaywhang.com/assets/batchnorm_rnn.pdf

I am actually open to using BN at this juncture. My plan is to ditch KFAC and use large batch sizes. I'll have to compute the layerwise statistics anyway, and that includes the gradients, so I might as well go this route. I'll need it for RNNs. I keep going back and forth with the moving average idea in RNNs, but I'll really have to use a moving average for every timestep or use a larger batch size and then take the empirical average.

https://arxiv.org/pdf/1603.09025.pdf
> However, we find that simply averaging statistics over time severely degrades performance.

Yeah, that is what I'd expect.

> This causes the variance of the hidden states to be exactly zero for a long period of time. Normalizing these zerovariance activations involves dividing zero by a small number at many timesteps, which does not affect the forward-propagated activations but causes the back-propagated gradient to explode.

Rather than adding that epsilon, they'd have been better off just making a special case for zero.

https://www.youtube.com/watch?v=87kLfzmYBy8&list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF&index=6
DeepMind x UCL | Deep Learning Lectures | 6/12 | Sequences and Recurrent Networks

I always feel like doing more when the day is over. Let me take a look at this lecture. I really might need plain RNNs for Holdem. Who knows in which direction I will go here.

10:55pm. No forget it. I need to pry myself off from thinking constantly about this. It is like my mind is stuck in an infinite loop. It seems like it is one thing to tell myself that I should accept the rules and quite another to follow the idea.

Let me watch to the interview by Tegmark from where I left off yesterday.

David Silver, Mike Jordan, Ilya Sutskever. Lex Fridman has a lot of stuff.

I have a tab by Michael Litman open.

12:30pm. https://youtu.be/RL4j4KPwNGM?t=9434

Let me stop here. I am too tired. I'll resume tomorrow."

---
## [CaptainJellybones/Rollerbuddy@68cfaac05e...](https://github.com/CaptainJellybones/Rollerbuddy/commit/68cfaac05e131e67bd3ac80bd95e7e81bd5169b2)
##### 2021-04-27 23:55:16 by CaptainJellybones

Minor work on the calculator

For the love of god, remember to change the classes and ID of the shit you copied

---

# [<](2021-04-26.md) 2021-04-27 [>](2021-04-28.md)

