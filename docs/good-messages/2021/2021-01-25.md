# [<](2021-01-24.md) 2021-01-25 [>](2021-01-26.md)

2,957,026 events, 1,433,614 push events, 2,288,761 commit messages, 183,144,021 characters


## [ccodwg/Covid19Canada](https://github.com/ccodwg/Covid19Canada)@[53f3561b18...](https://github.com/ccodwg/Covid19Canada/commit/53f3561b1838abc54ffbea230bbb03311ba858a5)
#### Monday 2021-01-25 01:04:37 by Jean-Paul R. Soucy

New data: 2021-01-24: See data notes for important messages.

Vaccine datasets:

- 2021-01-19: Fully vaccinated data have been added (vaccine_completion_cumulative.csv, timeseries_prov/vaccine_completion_timeseries_prov.csv, timeseries_canada/vaccine_completion_timeseries_canada.csv). Note that this value is not currently reported by all provinces (some provinces have all 0s).
- 2021-01-11: Our Ontario vaccine dataset has changed. Previously, we used two datasets: the MoH Daily Situation Report (https://www.oha.com/news/updates-on-the-novel-coronavirus), which is released weekdays in the evenings, and the “COVID-19 Vaccine Data in Ontario” dataset (https://data.ontario.ca/dataset/covid-19-vaccine-data-in-ontario), which is released every day in the mornings. Because the Daily Situation Report is released later in the day, it has more up-to-date numbers. However, since it is not available on weekends, this leads to an artificial “dip” in numbers on Saturday and “jump” on Monday due to the transition between data sources. We will now exclusively use the daily “COVID-19 Vaccine Data in Ontario” dataset. Although our numbers will be slightly less timely, the daily values will be consistent. We have replaced our historical dataset with “COVID-19 Vaccine Data in Ontario” as far back as they are available.
- 2020-12-17: Vaccination data have been added as time series in timeseries_prov and timeseries_hr.
- 2020-12-15: We have added two vaccine datasets to the repository, vaccine_administration_cumulative.csv and vaccine_distribution_cumulative.csv. These data should be considered preliminary and are subject to change and revision. The format of these new datasets may also change at any time as the data situation evolves.

Upcoming changes:

- The data structure of time series data will change in response to user feedback. This will only consist of adding additional columns to make the data easier to work with. The core columns will remain the same, for now. More details to follow. Initially, the updated dataset will be provided alongside the new dataset. After a time, the new data format will completely replace the old format.

Recent changes:

- 2021-01-24: The columns "additional_info" and "additional_source" in cases.csv and mortality.csv have been abbreviated similar to "case_source" and "death_source". See note in README.md from 2021-11-27 and 2021-01-08.

Revise historical data: cases (MB, NS, SK); mortality (MB).

2 additional deaths removed from MB: one today and one yesterday. Removed the most recent death in each of Northern and Interlake-Eastern to better approximate regional totals on the Manitoba dashboard.

Note regarding deaths added in QC today: “The data also report 41 new deaths, for a total of 9,478 deaths. Among these 41 deaths, 12 have occurred in the last 24 hours, 26 have occurred between January 17 and January 22, 2 have occurred before January 17 and 1 have occurred at an unknown date.” We report deaths such that our cumulative regional totals match today’s values. This sometimes results in extra deaths with today’s date when older deaths are removed.

https://www.quebec.ca/en/health/health-issues/a-z/2019-coronavirus/situation-coronavirus-in-quebec/#c47900

Note about SK data: As of 2020-12-14, we are providing a daily version of the official SK dataset that is compatible with the rest of our dataset in the folder official_datasets/sk. See below for information about our regular updates.

SK transitioned to reporting according to a new, expanded set of health regions on 2020-09-14. Unfortunately, the new health regions do not correspond exactly to the old health regions. Additionally, the provided case time series using the new boundaries do not exist for dates earlier than August 4, making providing a time series using the new boundaries impossible.

For now, we are adding new cases according to the list of new cases given in the “highlights” section of the SK government website (https://dashboard.saskatchewan.ca/health-wellness/covid-19/cases). These new cases are roughly grouped according to the old boundaries. However, health region totals were redistributed when the new boundaries were instituted on 2020-09-14, so while our daily case numbers match the numbers given in this section, our cumulative totals do not. We have reached out to the SK government to determine how this issue can be resolved. We will rectify our SK health region time series as soon it becomes possible to do so.

---
## [im-zach/Zachary-Childers-CPT-168-A01-Lab-8](https://github.com/im-zach/Zachary-Childers-CPT-168-A01-Lab-8)@[cb8fff6bde...](https://github.com/im-zach/Zachary-Childers-CPT-168-A01-Lab-8/commit/cb8fff6bde7e38238a4f81e9fc5f74b32a14fa5f)
#### Monday 2021-01-25 04:32:24 by Zachary Childers

the fucking arrays bothered me holy shit im a stupid asshole for making them go on that long FUCK*

*No i never intend to be this unprofessional in a work environment... maybe?

---
## [taydev/compendium](https://github.com/taydev/compendium)@[152f5d5313...](https://github.com/taydev/compendium/commit/152f5d5313c287d76e5d033db40f0297df4003f1)
#### Monday 2021-01-25 04:44:39 by Taylor Burn

AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA

- Added the Background command, for searching backgrounds.
- Added the Acolyte background from the SRD to the local DB.
- Removed Apache Commons Text from dependencies.
    - Replaced Apache Commons Text with a capitalise function that ignores symbols, capitalising words inside (Parenthesis)
- Fully implemented the Roll command!
    - Thanks again to @BinaryOverload for being a massive help and creating the dice parsing library, much love <3
- Implemented pagination into the Item command.
- Added a source indicator to the search results of the Item command.
- Added support for both Backgrounds and Choices in the MongoDB Codec Registry.
- Added new background searching method: findBackgroundsByName(String)
- Added new magic school searching method: findMagicSchoolsByName(String)
- Slightly tweaked the findItemsByName(String) method to make the alternative name search optional.
    - Might get removed in future, but it was needed for testing pagination.
- Migrated equipment storage in Backgrounds from UUID:Integer to String:Integer, as MongoDB does not know how to serialise UUIDs.
- Added HIDDEN to LanguageType for the purposes of implementing Druidic and Thieves' Cant (soon)
- Moved the abbreviation updating statement from inside Currency.setName(String) to outside Currency.setName(String)
    - Again, MongoDB does not know how to serialise things and it kept trying to update the abbreviation upon retrieval from database.
- Made Choice slightly more database-stable (annotations!)
- Added Choice.parse(Document) for converting documents into choices (because, again, MongoDB does not know how to serialise things)
- Added in ElementUtils.eval(String) for parsing math inside RollCommand
    - Credit to the StackOverflow post I yoinked this from, link is in comment above method
- Replaced the old ElementUtils.createBackgroundSummary(Background) with a new one that works with the Background command.
- Gave the same abbreviation-statement-movement treatment to Source
- Cleaned up CTF

---
## [Beinsezii/bsz-gimp-plugins](https://github.com/Beinsezii/bsz-gimp-plugins)@[fcc5d465f8...](https://github.com/Beinsezii/bsz-gimp-plugins/commit/fcc5d465f831eabb0573d6b5edb4dae61cca7223)
#### Monday 2021-01-25 06:42:58 by Beinsezii

Pixelbuster many many new ops.

Now has most of the obvious operations from
https://doc.rust-lang.org/std/primitive.f64.html#implementations
Slightly changed the layout so that it's easier to read. All 'standard'
ops that have assigned symbols are at the top sorted by reason.
Everything after is just f64 impls sorted by name.
god damn do I love [n]vim shortcuts. :normal for dayyyys

---
## [LPD-VRChat/LPD-Officer-Monitor](https://github.com/LPD-VRChat/LPD-Officer-Monitor)@[38db80bef6...](https://github.com/LPD-VRChat/LPD-Officer-Monitor/commit/38db80bef60870d27e1d2c07fad3d314f6f8647e)
#### Monday 2021-01-25 06:43:05 by CaptainDestructo

I hate programming so much, but I can't stop. It's an addiction. It keeps me up at night. Why won't this stupid code just work... I am about ready to put my head through my monitor. JUST WORK

---
## [slooooth/MusicPlayer](https://github.com/slooooth/MusicPlayer)@[64c3e56787...](https://github.com/slooooth/MusicPlayer/commit/64c3e56787b249c4eda8dce35e12eb0dd73c11bd)
#### Monday 2021-01-25 07:32:13 by slooooth

Dev (#2)

* rapid developemnt branch

made so that I dont have to push tiny-ass changes to master all the time

* fuck

writing dir every debug session is lame

* shit

fix program crash on improperly formatted xml file

* dumbass

if a dir exists but no file, will do nothing
fixed

* fasd;lfkj

* gae

* fuc

using statement rm

Co-authored-by: Genr8or <technospace119@gmail.com>

---
## [marigold-ui/marigold](https://github.com/marigold-ui/marigold)@[928486b7f3...](https://github.com/marigold-ui/marigold/commit/928486b7f33e296c8dd8f06efa675149dc4ab1ab)
#### Monday 2021-01-25 07:37:31 by Sebastian Sebald

chore: Increase time before issues goes stale (#710)

We are not that fast and 30 days is kinda annoying :D

---
## [ilammy/product-docs](https://github.com/ilammy/product-docs)@[f967ba8407...](https://github.com/ilammy/product-docs/commit/f967ba84074f9d9f434b2aa3c6d947430ef53669)
#### Monday 2021-01-25 10:51:04 by Alexei Lozovsky

Reference implementation of Soter container

Okay, here's my stupid idea. In addition to specs with prose, provide
users with a reference implementation of whatever is described. Some
people are better at understanding concepts through code. Plus, this
provides an "executable specification" and tests. I have found this
invaluable when writing Secure Cell spec: that caught quite a few
mistakes and misconceptions that I had about its implementation.

(History-savvy people here could have found parallels between this and
the concept of "literate programming". Indeed, if only Knuth's ideas
caught up, this would have been a perfect opportunity for that approach.
Unfortunately, it's not really popular to have good tooling.)

The reference implementation is written in Go. Aside from an obvious
reasons having to do with Cossack Labs being a Go shop, this choice has
a number of other benefits:

  - very good standard library, with a lot of cryptographic primitives
    available which will come very handy later

  - readable to most programmers who are familiar with C syntax

  - almost immediately executable if you copy-paste the sample code,
    thanks to "push to GitHub to publish a package" approach

  - reference implementation can actually reuse its own code for
    different cryptosystems

Unfortunately, you can't run this on Go Playground without much hassle,
but that would have been amazing.

Now, note that "reference implementation" is different from the
"official GoThemis source code". The overriding objective here is to
illustrate key points in Themis algorithms. Readability and correctness
are the main focus of the reference implementation. Performance,
usability, good error reporting, feature completeness -- all of this can
be sacrificed for the sake of brevity and clarity of the code.

Themis Core's C code is anything but easy to understand cryptography.
Since even its maintainers make mistakes when deciphering code paths
leading to arcane OpenSSL invocations, any cryptography audit would be
pretty expensive for it. Hopefully, this reference implementation can
serve as a better way to understand the cryptography behind Themis.

Now... Here's an implementation of Soter container for starters.

---
## [thwaitesproject/thwaites](https://github.com/thwaitesproject/thwaites)@[5e473b41e6...](https://github.com/thwaitesproject/thwaites/commit/5e473b41e65546e0eb6524ab9426f6bc4655288e)
#### Monday 2021-01-25 11:09:19 by Will Scott

Simulation still crashes only applying dirichlet in div term

This was last (?) attempt to try and get vertical ice front to work.

With the momentum advection change in the last commit the simulation
was more stable at higher Kv (1e-2m^2/s) actually getting to 100 days
and surviving the first density front leaving the cavity. However, it
still crashed with Kv =1e-3. If you added the Kv switch in (increase
Kv to 0.1m^2/s if unstable density gradient) then the simulation was
stable. Good news was that this only kicked after the density front
was past the ice shelf. Before with with old limiter the vertical density
gradient field went positive (i.e unstable) during the spinup, when it
probably should not have done.

All that was good, but then if you increase the horizontal resolution
the simulation crashed much earlier at 7 days, long before the density front
had got to the ice front. The blow up still started at the ice front
though.

Obviously something wrong with the bcs setup.

I had a long meeting on Friday evening with Stephan talking about bcs.
He reckoned that viscosity/diffusivity terms were fine. Last thing to
try was if we should only apply the dirichlet u.n =0 condition on the
divergence term. In momentum advection technically the dirichlet condition
sets u = 0 so you are affecting the tangential flow too. A key part
of this is that you are trying to preserve the symmetry of the the divergence
term with the grad pressure term. That is why appyling dirichlet condition
on the velocity enforces a neumann condition on the pressure.

I added a label key 'closed' as part of the dictionary so that it would
only apply u.n if 'closed' was not in th dictionary. However the simulation
still blew up at the same time.

I think we are going to move on from vertical ice front now. We may have to
come back if the squeezed triangle limiter doesnt work in 3d...

---
## [pxseu/powercord-weather](https://github.com/pxseu/powercord-weather)@[ef4520d8a4...](https://github.com/pxseu/powercord-weather/commit/ef4520d8a4f2c18b2b9edc1f5971a0871c0509da)
#### Monday 2021-01-25 11:28:50 by pxseu

📋README changed a bit

im really angry. i read somewhere in a psychology article that people dont approach really attractive people because they are too intimidated. but wtf no girls are even approaching me, it's not my fault that my biceps are so large. this is becoming really problematic to me. didnt really look for a gf, because i'm above that but like nothings happening and im frustrated (no advice needed, i dont think i can change my face to become less attractive) ONE OF THEM EVEN SAID IM UGLY. MANIPULATIVE BITCH IS JUST TRYING TO LOWER MY SELF ESTEEM SO I GET WITH HER. FUCK, WOMEN CAN BE MANIPULATIVE SOMETIMES.

---
## [newstools/2021-iol-cape-times](https://github.com/newstools/2021-iol-cape-times)@[9e148fa404...](https://github.com/newstools/2021-iol-cape-times/commit/9e148fa4041b105fc4f29a863a75363e5ff30576)
#### Monday 2021-01-25 12:51:09 by NewsTools

Created Text For URL [www.iol.co.za/capetimes/news/alvon-collisons-partner-in-moving-tribute-after-losing-love-of-his-life-3760fae9-0078-4e5f-9a7c-08fa836df83e]

---
## [FraTeG/A5_kernel](https://github.com/FraTeG/A5_kernel)@[df4b7d9365...](https://github.com/FraTeG/A5_kernel/commit/df4b7d9365bf4628a667ca88dfedc931ea93c5f9)
#### Monday 2021-01-25 13:41:11 by googyanas

fs/sync: Make sync() satisfy many requests with one invocation

Dave Jones reported RCU stalls, overly long hrtimer interrupts, and
amazingly long NMI handlers from a trinity-induced workload involving
lots of concurrent sync() calls (https://lkml.org/lkml/2013/7/23/369).
There are any number of things that one might do to make sync() behave
better under high levels of contention, but it is also the case that
multiple concurrent sync() system calls can be satisfied by a single
sys_sync() invocation.

Given that this situation is reminiscent of rcu_barrier(), this commit
applies the rcu_barrier() approach to sys_sync().  This approach uses
a global mutex and a sequence counter.  The mutex is held across the
sync() operation, which eliminates contention between concurrent sync()
operations.  The counter is incremented at the beginning and end of
each sync() operation, so that it is odd while a sync() operation is in
progress and even otherwise, just like sequence locks.

The code that used to be in sys_sync() is now in do_sync(), and
sys_sync()
now handles the concurrency.  The sys_sync() function first takes a
snapshot of the counter, then acquires the mutex, and then takes another
snapshot of the counter.  If the values of the two snapshots indicate
that
a full do_sync() executed during the mutex acquisition, the sys_sync()
function releases the mutex and returns ("Our work is done!").
Otherwise,
sys_sync() increments the counter, invokes do_sync(), and increments
the counter again.

This approach allows a single call to do_sync() to satisfy an
arbitrarily
large number of sync() system calls, which should eliminate issues due
to large numbers of concurrent invocations of the sync() system call.

Changes since v1 (https://lkml.org/lkml/2013/7/24/683):

o	Add a pair of memory barriers to keep the increments from
	bleeding into the do_sync() code.  (The failure probability
	is insanely low, but when you have several hundred million
	devices running Linux, you can expect several hundred instances
	of one-in-a-million failures.)

o	Actually CC some people who have experience in this area.

Reported-by: Dave Jones <davej@redhat.com>
Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Cc: Alexander Viro <viro@zeniv.linux.org.uk>
Cc: Christoph Hellwig <hch@lst.de>
Cc: Jan Kara <jack@suse.cz>
Cc: Curt Wohlgemuth <curtw@google.com>
Cc: Jens Axboe <jaxboe@fusionio.com>
Cc: linux-fsdevel@vger.kernel.org

Signed-off-by: Paul Reioux <reioux@gmail.com>

---
## [LDR-Siren/EmilyC-SamanthaPrater-EruzaArto](https://github.com/LDR-Siren/EmilyC-SamanthaPrater-EruzaArto)@[c6d4603d9f...](https://github.com/LDR-Siren/EmilyC-SamanthaPrater-EruzaArto/commit/c6d4603d9f7770f2ad17e574286fd8431c3d16a3)
#### Monday 2021-01-25 14:05:25 by LDR

James, Anon, and more Deleting

Oh how I had wished. Yesterday was fairly quiet till later. We almost had 24 full hours of complete silence from Samily. But alas all great moments end fairly quickly and usually with a steaming pile of shit. As this day did.

She is back to screaming about James. Of course.

Anon is apparently watching her computer. I am sure they have much better things to do than to watch some insane chick that screams at Facebook and twitter. She is far beneath their radar. 

And the deleting. Thanks to one of our own, who went through and found out she is back to deleting things, again. Which tends to happen when she is hiding things, has been called out, or someone has told her she has gone too far. 

Personally all her rattlings are back to just that, rattles. Same old shit, over and over again.

---
## [reorezz/Fps-Game-Repo](https://github.com/reorezz/Fps-Game-Repo)@[36e331d8af...](https://github.com/reorezz/Fps-Game-Repo/commit/36e331d8af0eed81bb5e64f22f9cfa0d9514b456)
#### Monday 2021-01-25 14:22:42 by reorezz

fps controller complete

there are still isssues regarding the fov handler ,lets see if i can improve on it or not ,
the mainproblem is icannot test for my self how the feel seems to be of the game ,so i cannot decide as my shit ass bitch ass laptop lags
na i am fine

---
## [LGLTeam/Android-Mod-Menu](https://github.com/LGLTeam/Android-Mod-Menu)@[3595e6dfaa...](https://github.com/LGLTeam/Android-Mod-Menu/commit/3595e6dfaa0cd42f96d6cb081965b56571684ebe)
#### Monday 2021-01-25 16:47:24 by LGLTeam

Updated 2.0

DO NOT UPDATE if you are not prepared. You will need to read README again when you use this update

Changelog:
- Updated gradle and NDK target
- Can now force loading menu while waiting for the lib to be loaded
- Add saving logcat to file. It can be useful to diagnose the issues within the mod
- A rework of feature list. Now you must assign feature numbers manually. The benefit is you can easly remember the numbers and you don't need to re-order your Changes anymore when you remove/add/re-order your features. Feature numbers can be like 1,3,200,10... instead in order 0,1,2,3,4,5...
- Settings list has been moved to cpp. The numbers are assigned as negative
- Additional fixes for AIDE CMODs
- Updated README.md and README-MOBILE.md. Be sure to read it again
- Cleaned up codes again to stop the small kinderforum haters from hating us. You may notice some codes have been moved or removed
- Some minor fixes

Maybe there is more changes I forgot to list

To haters, stop using this template immediately and stop spreading hate please. There is no reason to hate. 95% of modders love this template already. To lovers, show us love by making videos of your cool mods with mod menu ❤️❤️❤️.

---
## [tebb/naming-cheatsheet](https://github.com/tebb/naming-cheatsheet)@[b7242ee7d2...](https://github.com/tebb/naming-cheatsheet/commit/b7242ee7d2cc25032fff551d52aa91c0ab7f46b6)
#### Monday 2021-01-25 17:08:11 by tebb

Clarify Note on A/HC/LC Pattern

Thank you.  This cheatsheet is very useful.

If I understood the logic, my changes to README.md may be helpful.  If not, sorry :)

Also ... Could you add shouldUpdateComponent and shouldComponentUpdate into the table (or an extension to the table below Note:).  It isn't obvious to me which columns 'should', 'Update' and 'Component' are in because 'Update' is the verb (action?).

Thanks.

---
## [gnosis/gp-swap-ui](https://github.com/gnosis/gp-swap-ui)@[a25a951739...](https://github.com/gnosis/gp-swap-ui/commit/a25a95173927f30ee79e349886f3c49f330c8c18)
#### Monday 2021-01-25 17:23:47 by Anxo Rodriguez

[xDai] Xdai custom NPM (#102)

Brings xDAI to the swap UI:
- Allows to connect
- Shows xDAI instead of ETH
- Allows to Wrap xDAI into wxDAI
- Allows to Sell xDAI (it wraps and then sells wxDAI)

![image](https://user-images.githubusercontent.com/2352112/105481766-cb989380-5ca7-11eb-84c6-e9e10da879d3.png)

![image](https://user-images.githubusercontent.com/2352112/105482349-8a54b380-5ca8-11eb-876a-2017443062c7.png)


Also includes Blockscout link:
![image](https://user-images.githubusercontent.com/2352112/105482276-71e49900-5ca8-11eb-829f-afa2757019c7.png)



## Why another xDAI PR
So here is a different approach than #94 

I was getting to some deadends with the former approaches, some criticisms 
- Some depended in a forked version of the SDK we don't control. Moreover, they are not even pushed to somewhere, they leave as a gigantic PR in Honeyswap repository that will probably never be merged. On top of this, they forked a SKD that is now sligly obsolete.
- The hacks bring a lot of typescript redefinitions to the project, and force us to change their imports, and/or do uggly hacks

## Current approach
This approach is not perfect either, it's basically a combination of:
- I forked their repo
- I PR them 2 PRS (one just adds xDAI, other adds any network). I was hopping they merge, but
- We cannot wait until Uniswap merges, so I did also a fork, and published my own package with the PR I sent to them
- In this PR I use my NPM package

Lot's of other things are happening in this PR. Sorry for making such a big PR, but I'm happy to walk through if it helps. 
I tried to at least break the commits in smaller pieces with a comment. 

## Help for the review
If it helps, let me know I can do a quick zoom session and clarify the parts. 
- My NPM package has the code from https://github.com/Uniswap/uniswap-sdk/pull/53 . Plan is to replace it once is merged.
- I tried to use our pattern of not overriding SRC, and managed in most cases, but still there was some exceptions. The exceptions where mostly because of the type, they required some objects (used as maps) to have all networks as keys. When I added xDAI they need now a value for that. I can change the type, but that would require the same change in the source file plus in every place where it's used (cause now you change the type and can return undefined)
- Most of the changes are just change on the path of the import, mostly `useContract` and `utils` because they are used a lot 
-  The most weird thing here is the updater. It was extracted from Dima's solution. He came up with a smart way to modify ETH label. Is hacky but it saves a lot of headaches (and it's contained). U'll see this in the PR in `utils/xdai/hack.ts`
- The rest, I think is easy to follow

## Not in this PR
Some labels need to be reviewed. It sometimes show ETH or WETH when is xDAI or wxDAI

Here I was wrapping xDAI:
![image](https://user-images.githubusercontent.com/2352112/105482553-d273d600-5ca8-11eb-8312-30a84f19df3e.png)

![image](https://user-images.githubusercontent.com/2352112/105482600-e3bce280-5ca8-11eb-9939-3c7320fe5eed.png)


## How to test
* Go to:
http://localhost:3000/#/swap?inputCurrency=0xb7D311E2Eb55F2f68a9440da38e7989210b9A05e&outputCurrency=0xe91D153E0b41518A2Ce8Dd3D7944Fa863463a97d&exactAmount=0.1
* Trade

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[2f15792aa1...](https://github.com/mrakgr/The-Spiral-Language/commit/2f15792aa1baa609779a4c99d5f390b8b0b4d47a)
#### Monday 2021-01-25 19:37:27 by Marko Grdinić

"2pm. Done with breakfast and chores, but I was so busy yesterday I did not even do my morning reading. Let me just fire that bank email and I'll take a proper break.

2:15pm. Time for a break.

3:15pm. Time to resume.

Focus me.

```
(base) PS C:\Users\Marko\Source\Repos\The Spiral Language\VS Code Plugin> cythonize
Usage: cythonize [options] [sources and packages]+

cythonize: error: no source files provided
```

I saw this in an Nvidia article yesterday.

https://developer.nvidia.com/blog/accelerating-python-on-gpus-with-nvc-and-cython/

`cythonize -i cppsort.pyx`

Instead of doing that `setup.py` I could have just used this.

```
cdef int range(int fron, int nearTo):
    cdef int loop(int s, int i):
        if i < nearTo: return loop(s+i,i+1)
        else: return s
    loop(0,fron)

range(0,100000)
```

It is telling me that the nested function is not allowed there.

```
(base) PS C:\Users\Marko\Source\Repos\The Spiral Language\Spiral Compilation Tests\cython_experiments\test2> python tail_rec.cp38-win_amd64.pyd
  File "tail_rec.cp38-win_amd64.pyd", line 1
SyntaxError: Non-UTF-8 code starting with '\x90' in file tail_rec.cp38-win_amd64.pyd on line 1, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details
```

How annoying.

3:35pm.

```fs
cdef int loop(int s, int i, int nearTo):
    if i < nearTo: return loop(s+i,i+1,nearTo)
    else: return s
cpdef int sequence(int fron, int nearTo):
    return loop(0,fron,nearTo)
```

Yes, this does get TCO'd.

```fs
cdef int loop(int s, int i, int nearTo):
    if i < nearTo: return loop(s+i,i+1,nearTo)
    else: return s
cpdef int sequence_tailrec(int fron, int nearTo):
    return loop(0,fron,nearTo)

cdef int loop(int i, int nearTo):
    if i < nearTo: return i + loop(i+1,nearTo)
    else: return s
cpdef int sequence(int fron, int nearTo):
    return loop(0,fron,nearTo)
```

Let me try this. Though for all I know the C compiler might be good enough to transform this as well.

Yeah, it is.

What if I just do a def?

Actually, no I fucked up the above.

```
cdef int loop(int i, int nearTo):
    if i < nearTo: return i + loop(i+1,nearTo)
    else: return 0
```

Let me do it like this.

```
import tail_rec
print(tail_rec.sequence(0,2_000_000_000))
```

Ah, wait, nothing is getting printed when I do this.

```
print(tail_rec.sequence(0,2_000_0))
```

If I add another zero to the end, it stack overflows silently in the background.

```
import tail_rec
print(tail_rec.sequence_tailrec(0,2_000_000_000))
```

The tail recursive version works just fine. Ok.

Let me do that automatic Cython import. It was in the docs.

> Use Pyximport, importing Cython .pyx files as if they were .py files (using setuptools to compile and build in the background). This method is easier than writing a setup.py, but is not very flexible. So you’ll need to write a setup.py if, for example, you need certain compilations options.

3:50pm.

```
import pyximport
pyximport.install()
import tail_rec
print(tail_rec.sequence_tailrec(0,2_000_000_000))
```

This works, but it does not produce a residual that I could use. Because of that, `import tail_rec` is an import error in the editor.

Ok...

What happens if I try passing a string into the Cython function that expects an int?

```
(base) PS C:\Users\Marko\Source\Repos\The Spiral Language\Spiral Compilation Tests\cython_experiments\test2> python run.py
Traceback (most recent call last):
  File "run.py", line 4, in <module>
    print(tail_rec.sequence_tailrec(0,"2_000_000_000"))
  File "tail_rec.pyx", line 4, in tail_rec.sequence_tailrec
    cpdef int sequence_tailrec(int fron, int nearTo):
TypeError: an integer is required
```

Not bad. Ok.

```
import pyximport
pyximport.install(language_level=3)
import tailrec
# print(tail_rec.sequence(0,2_000_000_000)) # Diverges
print(tailrec.sequence_tailrec(0,2_000_000_000))
```

Figured out how to set the language level as well.

Ok...

4:05pm. I am thinking.

At this point there is nothing really stoping me from starting work on the backend. If I start now, it won't take long at all.

Yes...

Before I start let me just figure out what the type of python objects is.

https://cython.readthedocs.io/en/latest/src/userguide/language_basics.html

Also before I start making a compiler backend, I should at least study the basics of the language I am targetting.

https://cython.readthedocs.io/en/latest/src/userguide/language_basics.html#c-variable-and-type-definitions

Oh, it has C unions!

This will make it a lot easier to implement union types.

>  Arrays use the normal C array syntax, e.g. int[10], and the size must be known at compile time for stack allocated arrays. Cython doesn’t support variable length arrays from C99.

Interesting. I wonder why this is so?

But I won't by using C arrays, but Python ones instead as the default.

> declares a parameter called int which is a Python object. You can also use object as the explicit return type of a function, e.g.:

Ah, I see.

```
cpdef object test_obj((object,object) x):
    a,b = x
    return a + b

cpdef object qwe(object a, object b):
    cdef (object, object) x = a,b
    return test_obj(x)
```

> tuple_test.pyx:6:9: Tuple types can't (yet) contain Python objects.

Ah crap.

4:45pm. Let me not give up here. What about structs?

4:50pm.

```
cdef struct QWE:
    object a
    object b
```

This is so useless of it.

4:55pm. Let me not get too pessimistic here. I'll try installing the prerelease and seeing how that goes.

https://pypi.org/project/Cython/#history

How do I install the prerelease using conda?

https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html#managing-packages

It is not on the conda package list. What do I do here?

https://pypi.org/search/?q=cython

I don't think it is possible to install the prerelease through pip either.

5:30pm. https://github.com/cython/cython/issues/

Opened issue 3985 here requesting to support Python objects in cdef structs, unions and tuples.

5:35pm. This was actually quite a shock to me. It wrecked my compilation plan. This is definitely a point in favor of doing a direct C/Python backend.

5:40pm. The thing I had in mind would have been extremely performant, but I'll have to lower my expectations here.

Ah, forget it. I'll give it an extra week. I need to study the way Python works before I make a decision.

6:10pm.

https://cython.readthedocs.io/en/latest/src/tutorial/cdef_classes.html

Let me try out if structs work in `cdef classes`.

```
cdef class Qwe:
    cdef object x
    def __init__(self, q):
        self.x = q

cpdef object test_obj(object x):
    q,w = x
    return q+w

cpdef object qwe(object a, object b):
    x = a,b
    cdef Qwe z = Qwe(x)
    return z #test_obj(x)
```

Oh, it does work in cdef classes.

6:20pm. No this is not what I need. Obviously I won't start work on the backend today. Let me take a look at embedding Python section of its documentation.

I think I am going to put in some overtime today. I really need more time in the day. Today I literally only started at 3pm for some reason. It is all the winter's fault making me stay in bed until the fire gets started.

https://docs.python.org/3/extending/embedding.html

6:20pm.

```cpp
#define PY_SSIZE_T_CLEAN
#include <Python.h>

int
main(int argc, char *argv[])
{
    wchar_t *program = Py_DecodeLocale(argv[0], NULL);
    if (program == NULL) {
        fprintf(stderr, "Fatal error: cannot decode argv[0]\n");
        exit(1);
    }
    Py_SetProgramName(program);  /* optional but recommended */
    Py_Initialize();
    PyRun_SimpleString("from time import time,ctime\n"
                       "print('Today is', ctime(time()))\n");
    if (Py_FinalizeEx() < 0) {
        exit(120);
    }
    PyMem_RawFree(program);
    return 0;
}
```

I can't tell at all how much of these are macros and how much are actual function calls. If these are not macros, then is the Python interpreter state global? That is shit. I guess I now know why the GIL is necessary. I was expecting the interpreter to be passed as a pointer, but it is nowhere to be found.

https://docs.python.org/3/extending/index.html

Let me go through this all in turn. I need to focus on this. It is not a given that I am going to use Cython. I want the best possible option.

...If I am just doing the Python backend in C, assuming I can accept it being global, then I can see how it would make things easier.

7:15pm. https://docs.python.org/3/c-api/refcounting.html#c.Py_DECREF

Having to do reference counting manually in a direct backend would be pain itself.

8:05pm. Let me stop here for the day.

I've decided. Despite the recent blow, I can't deal with reference counting. Neither can I deal calling Python library functions manually. This C style code is absolutely attrocious. Even if I could deal with ref counting, I cannot possibly subject the user to this.

If something like Cython did not exist, I'd compile to straight Python instead. I should be glad that I at least have it.

8:20pm. Got a reply. I do not understand why structs and tuples would get treated differently. Nevermind that for now.

Let me close here. I need to unwind.

Tomorrow I will start work on the Cython backend. It won't be hard. I've preped my mind for the task, and I just need to execute it. In fact, Cython not having some of the vital stuff that I'd need for Spiral's compilation will actually make implementing those features easier. I am thinking of unions here. Both the recursive and the non-recursive unions would need to be heap allocated in Cython. I have no choice, but to do this here.

I am guessing that tuples will land in Cython at some point, until then I'll just compile the compound types containing objects to objects. I'll deal with it that way.

It won't be hard.

8:30pm. I am going to make a dent in that backend tomorrow. I'll have something to show for it next month."

---
## [Empow-PAT/fall2020game](https://github.com/Empow-PAT/fall2020game)@[829fa68516...](https://github.com/Empow-PAT/fall2020game/commit/829fa685162a9285b982f192cbfa67804e84c116)
#### Monday 2021-01-25 21:48:52 by kijavnash

To be, or not to be, that is the question:
Whether 'tis nobler in the mind to suffer
The slings and arrows of outrageous fortune,
Or to take arms against a sea of troubles
And by opposing end them. To die—to sleep,
No more; and by a sleep to say we end
The heart-ache and the thousand natural shocks
That flesh is heir to: 'tis a consummation
Devoutly to be wish'd. To die, to sleep;
To sleep, perchance to dream—ay, there's the rub:
For in that sleep of death what dreams may come,
When we have shuffled off this mortal coil,
Must give us pause—there's the respect
That makes calamity of so long life.
For who would bear the whips and scorns of time,
Th'oppressor's wrong, the proud man's contumely,
The pangs of dispriz'd love, the law's delay,
The insolence of office, and the spurns
That patient merit of th'unworthy takes,
When he himself might his quietus make
With a bare bodkin? Who would fardels bear,
To grunt and sweat under a weary life,
But that the dread of something after death,
The undiscovere'd country, from whose bourn
No traveller returns, puzzles the will,
And makes us rather bear those ills we have
Than fly to others that we know not of?
Thus conscience does make cowards of us all,
And thus the native hue of resolution
Is sicklied o'er with the pale cast of thought,
And enterprises of great pitch and moment
With this regard their currents turn awry
And lose the name of action.

---
## [ttaylorr/git](https://github.com/ttaylorr/git)@[b28c7eaab4...](https://github.com/ttaylorr/git/commit/b28c7eaab4391a9e87d7a9abb3fbd9113af7c930)
#### Monday 2021-01-25 21:51:51 by Taylor Blau

packfile: prepare for the existence of '*.rev' files

Specify the format of the on-disk reverse index 'pack-*.rev' file, as
well as prepare the code for the existence of such files.

The reverse index maps from pack relative positions (i.e., an index into
the array of object which is sorted by their offsets within the
packfile) to their position within the 'pack-*.idx' file. Today, this is
done by building up a list of (off_t, uint32_t) tuples for each object
(the off_t corresponding to that object's offset, and the uint32_t
corresponding to its position in the index). To convert between pack and
index position quickly, this array of tuples is radix sorted based on
its offset.

This has two major drawbacks:

First, the in-memory cost scales linearly with the number of objects in
a pack.  Each 'struct revindex_entry' is sizeof(off_t) +
sizeof(uint32_t) + padding bytes for a total of 16.

To observe this, force Git to load the reverse index by, for e.g.,
running 'git cat-file --batch-check="%(objectsize:disk)"'. When asking
for a single object in a fresh clone of the kernel, Git needs to
allocate 120+ MB of memory in order to hold the reverse index in memory.

Second, the cost to sort also scales with the size of the pack.
Luckily, this is a linear function since 'load_pack_revindex()' uses a
radix sort, but this cost still must be paid once per pack per process.

As an example, it takes ~60x longer to print the _size_ of an object as
it does to print that entire object's _contents_:

  Benchmark #1: git.compile cat-file --batch <obj
    Time (mean ± σ):       3.4 ms ±   0.1 ms    [User: 3.3 ms, System: 2.1 ms]
    Range (min … max):     3.2 ms …   3.7 ms    726 runs

  Benchmark #2: git.compile cat-file --batch-check="%(objectsize:disk)" <obj
    Time (mean ± σ):     210.3 ms ±   8.9 ms    [User: 188.2 ms, System: 23.2 ms]
    Range (min … max):   193.7 ms … 224.4 ms    13 runs

Instead, avoid computing and sorting the revindex once per process by
writing it to a file when the pack itself is generated.

The format is relatively straightforward. It contains an array of
uint32_t's, the length of which is equal to the number of objects in the
pack.  The ith entry in this table contains the index position of the
ith object in the pack, where "ith object in the pack" is determined by
pack offset.

One thing that the on-disk format does _not_ contain is the full (up to)
eight-byte offset corresponding to each object. This is something that
the in-memory revindex contains (it stores an off_t in 'struct
revindex_entry' along with the same uint32_t that the on-disk format
has). Omit it in the on-disk format, since knowing the index position
for some object is sufficient to get a constant-time lookup in the
pack-*.idx file to ask for an object's offset within the pack.

This trades off between the on-disk size of the 'pack-*.rev' file for
runtime to chase down the offset for some object. Even though the lookup
is constant time, the constant is heavier, since it can potentially
involve two pointer walks in v2 indexes (one to access the 4-byte offset
table, and potentially a second to access the double wide offset table).

Consider trying to map an object's pack offset to a relative position
within that pack. In a cold-cache scenario, more page faults occur while
switching between binary searching through the reverse index and
searching through the *.idx file for an object's offset. Sure enough,
with a cold cache (writing '3' into '/proc/sys/vm/drop_caches' after
'sync'ing), printing out the entire object's contents is still
marginally faster than printing its size:

  Benchmark #1: git.compile cat-file --batch-check="%(objectsize:disk)" <obj >/dev/null
    Time (mean ± σ):      22.6 ms ±   0.5 ms    [User: 2.4 ms, System: 7.9 ms]
    Range (min … max):    21.4 ms …  23.5 ms    41 runs

  Benchmark #2: git.compile cat-file --batch <obj >/dev/null
    Time (mean ± σ):      17.2 ms ±   0.7 ms    [User: 2.8 ms, System: 5.5 ms]
    Range (min … max):    15.6 ms …  18.2 ms    45 runs

(Numbers taken in the kernel after cheating and using the next patch to
generate a reverse index). There are a couple of approaches to improve
cold cache performance not pursued here:

  - We could include the object offsets in the reverse index format.
    Predictably, this does result in fewer page faults, but it triples
    the size of the file, while simultaneously duplicating a ton of data
    already available in the .idx file. (This was the original way I
    implemented the format, and it did show
    `--batch-check='%(objectsize:disk)'` winning out against `--batch`.)

    On the other hand, this increase in size also results in a large
    block-cache footprint, which could potentially hurt other workloads.

  - We could store the mapping from pack to index position in more
    cache-friendly way, like constructing a binary search tree from the
    table and writing the values in breadth-first order. This would
    result in much better locality, but the price you pay is trading
    O(1) lookup in 'pack_pos_to_index()' for an O(log n) one (since you
    can no longer directly index the table).

So, neither of these approaches are taken here. (Thankfully, the format
is versioned, so we are free to pursue these in the future.) But, cold
cache performance likely isn't interesting outside of one-off cases like
asking for the size of an object directly. In real-world usage, Git is
often performing many operations in the revindex (i.e., asking about
many objects rather than a single one).

The trade-off is worth it, since we will avoid the vast majority of the
cost of generating the revindex that the extra pointer chase will look
like noise in the following patch's benchmarks.

This patch describes the format and prepares callers (like in
pack-revindex.c) to be able to read *.rev files once they exist. An
implementation of the writer will appear in the next patch, and callers
will gradually begin to start using the writer in the patches that
follow after that.

Signed-off-by: Taylor Blau <me@ttaylorr.com>

---

# [<](2021-01-24.md) 2021-01-25 [>](2021-01-26.md)

