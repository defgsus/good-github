# [<](2021-05-12.md) 2021-05-13 [>](2021-05-14.md)

3,008,816 events, 1,474,912 push events, 2,343,491 commit messages, 180,772,003 characters


## [GregariousJB/Enigmatica6@e4d60dd7de...](https://github.com/GregariousJB/Enigmatica6/commit/e4d60dd7ded72b02b7fb4b864ffd998240a9b17e)
##### 2021-05-13 03:50:16 by MuteTiefling

Preliminary Blood Magic and Occultism Progression

Reworked ID removals to use ID replacements where valid. This requires an update to KubeJS.

Eidolon will open up the Blood Altar, Alchemy Table, and Low Grade rituals in Occultism, opening up some extra crafting mechanics. Blood Altar Tier 2 and up will require parallel progress in Occultism.

Petal Apothecary can make Sprouted Fungus, making a simpler way to create potions. I'm thinking enchanting should be pretty late game, considering the power level. We have a lot of really fancy potions and a cool new system with Ars Nouveau for making some very powerful brews, but enchants almost entirely invalidate potions. By making them easier and enchanting later, I'd hope to get people to use them a bit more.

Alchemy Table recipe for Nocturnal Powder and Illumination powder added. Nice reagents for other crafting, and very handy when searching for mobs early on.

Cutting Fluid made a bit easier, but more varied. Now requires snake rattles (farmable), apatite dust (goes a long way), charcoal dust, and plant oil. This should hopefully give magic players a decently automatable ore doubling system.

Considering boosting output for ores, considering Create's Milling is also fairly early game.

Otherstone now crafted from Arcane Stone

Leather Stripe now a cutting board recipe

Spirit Attuned gem now made from Shadow Gem

New Recipe for Divination Rod

Impure White chalk moved to Alchemy Table

White Chalk moved to Blood Altar

Occultism Bowl recipes moved to Enchanting Apparatus

---
## [mrakgr/The-Spiral-Language@5d8ffd803b...](https://github.com/mrakgr/The-Spiral-Language/commit/5d8ffd803b59cb7c436d470aca9df68d9421d586)
##### 2021-05-13 10:44:43 by Marko Grdinić

"10:05am. I had some inspiration last night on long term credit assignment. I said that the answer must be to predict them, but I started to imagine, what if the input to the net is a one hot vector? Then I could just accumulate the gradients and pass them on when the neuron is active.

This is it. Without a doubt this is how long term credit assignment should be done.

https://arxiv.org/abs/1809.03702
Sparse Attentive Backtracking - Temporal Credit Assignment Through Reminding

I saw this paper mentioned in the `Optimizing Agent Behavior over Long Time Scales by Transporting Value` paper, and just its title was enough to catch my attention. I read it a bit, and this paragraph caught my attention.

> Humans have a remarkable ability to remember events from the distant past which are associated with the current mental state (Ciaramelli et al., 2008). Most experimental and theoretical analyses of memory have focused on understanding the deliberate route to memory formation and recall. But automatic reminding—when memories pop into one’s head—can have a potent influence on cognition. Reminding is normally triggered by contextual features present at the moment of retrieval which match distinctive features of the memory being recalled (Berntsen et al., 2013; Wharton et al., 1996), and can occur more often following unexpected events (Read & Cesa, 1991). Thus, an individual’s current state of understanding can trigger reminding of a past state. Reminding can provide distracting sources of irrelevant information (Forbus et al., 1995; Novick, 1988), but it can also serve a useful computational role in ongoing cognition by providing information essential to decision making (Benjamin & Ross, 2010).

> In this paper, we identify another possible role of reminding: to perform credit assignment across long time spans. Consider the following scenario. As you drive down the highway, you hear an unusual popping sound. You think nothing of it until you stop for gas and realize that one of your tires has deflated, at which point you are suddenly reminded of the pop. The reminding event helps determine the cause of your flat tire, and probably leads to synaptic changes by which a future pop sound while driving would be processed differently. Credit assignment is critical in machine learning. Back-propagation is fundamentally performing credit assignment. Although some progress has beenmade toward credit-assignment mechanisms that are functionally equivalent to back-propagation (Lee et al., 2014; Scellier & Bengio, 2016; Whittington & Bogacz, 2017), it remains very unclear how the equivalent of back-propagation through time, used to train recurrent neural networks (RNNs), could be implemented by brains. Here we explore the hypothesis that an associative reminding process could play an important role in propagating credit across long time spans, also known as the problem of learning long-term dependencies in RNNs, i.e., of learning to exploit statistical dependencies between events and variables which occur temporally far from each other.

I completely forgot that long term memories are strengthened by reminders in the human brain!

Without a doubt, the simple idea I had is correct. Rather than using a separate net to predict the gradients given the inputs, it would be far better to just accumulate and take advantage of sparsity.

I only read the paper for a bit as it was 1am when I got this idea, I went to bed soon afterwards. Let me do it right now.

http://proceedings.mlr.press/v130/lamb21a.html
Neural Function Modules with Sparse Arguments - A Dynamic Approach to Integrating Information across Layers

Also, this caught my attention.

> The key contribution of our work is to combine attention, sparsity, top-down and bottom-up feedback, in a flexible algorithm which, as we show, improves the results in standard classification, out-of-domain generalization, generative modeling, and learning representations in the context of reinforcement learning.

In the context of RL. That seems up my alley.

At any rate, long term credit assignment seemed like a huge deal, but looking at it now, it is pretty much solved as far as I am concerned.

10:45am. The SAB paper is too complicated. No, it is not as simple as my own idea. Let me read the other paper.

6/15. These papers are so underwhelming. I am not interested in minor improvements such as these.

11:10am. Forget it, let me take a break here. I don't know, maybe the idea I had is bad. It would reinforce wrong memories in many cases. Just forget it. It is programming time for me. The community can work things out on its own.

12:20pm. Done with chores. Breakfast is next.

I had some inspiration and I just figured out how to do exact gradient propagation in temporal hierarchies when the timescales of different modules are arbitrary.

Let me go back to the one shot idea of input vectors. This is merely the starting point.

1) In such an where inputs only have a single active element at any given time, only the active element would have its gradient propagated.

2) But even if I accumulate the gradients for the next time the element gets activated, the gradient path on the lower layer might not be the same. So the credit assignment would have variance. This scheme is a dead end.

What I started thinking about is going to my earlier idea of predicting the gradients. In order to improve the accuracy of such prediction, I thought about - what if I passed extra information just for the sake of the backward pass. At first I thought these should be ordinary features, but then I realized something interesting.

The input at each feedforward layer is just input * grad. Inputs I can keep around in memory without large impact. It would not be more expensive than regular backprop.

But grad multipliers could be precalculated.

I can just pretend that the gradient coming from above is 1, and then calculate the grad coeficient.

12:35pm. Actually, no wait. Isn't this the same as keeping around the trace?

Actually it is.

Forget that. An even better idea would in fact be to keep around the trace, but use sparsity to filter out the irrelevant timesteps for the upper layers.

Yeah, backprop is not wrong. It is just that you can't expect to keep everything around when attending to multiple timescales. What is needed is a filtering mechanism.

I do not really know what it would look like.

12:40pm. Ok, it is not important at this jucture. This level of insight might not be enough to invent the thing outright and resolve long term credit assignment issues, but it is enough to allow me to predict where the innovations will come from.

Let me have breakfast here."

---
## [facundomartinezabeldano/TPOrga@e004a53969...](https://github.com/facundomartinezabeldano/TPOrga/commit/e004a53969c4516684e2bb310ec36afb4771d328)
##### 2021-05-13 11:57:06 by Facundo

Taller 3 archivos subidos

Hello darkness, my old friend
I've come to talk with you again
Because a vision softly creeping
Left its seeds while I was sleeping
And the vision that was planted in my brain
Still remains
Within the sound of silence

---
## [brandon-b-miller/cudf@8a666a04e0...](https://github.com/brandon-b-miller/cudf/commit/8a666a04e0123744eb259d88ac4c04b0b6de4303)
##### 2021-05-13 13:29:13 by Vyas Ramasubramani

Refactor Python and Cython internals for groupby aggregation (#7818)

This PR makes some improvements to the groupby/aggregation code that I identified while working on #7731. The main purpose is to make the code logic easier to follow and reduce some unnecessary complexity; I see minor but measurable performance improvements (2-5% for small datasets) as well, but those are mostly just side effects here. Specifically, it makes the following changes:

1. Inlines the logic for dropping unsupported aggregations. The old function was repetitive and necessitated looping over the aggregations twice, whereas the new approach drops unwanted aggregations on the fly so it only loops once. The new code also makes it so that you only construct a C aggregation object once.
2. Merges the logic from `_AggregationFactory` into `Aggregation`, and removes the constructor for `Aggregation`. The one downside here is that the Cython `Aggregation` object's constructor no longer places it in a valid state; however, in practice the object is always constructed via either the `make_aggregation` function or its various factories, and the object's constructor was only every used in `_drop_unsupported_aggs` anyway. The benefit is we remove the fragmentation between these two classes, making the code much more readable, and the `Aggregation` class actually serves a purpose now beyond just providing a single property `kind` that is only used once: it is now the primary way that other Cython files interact with aggregations. This also means that in most places other Cython modules don't need to work with `unique_ptr[aggregation]` as much anymore (although they do still have to move `Aggregation.c_obj` for performance reasons). `make_aggregation` now returns the Cython class instead of the underlying C++ one.
3. Modified all the "allowed aggregations" sets to use the uppercase names of the aggregations. In addition to simplifying the code a tiny bit, this helps reduce confusion between the aggregation names used in Python for pandas compatibility and the libcudf names (for instance, `idxmin` vs `argmin`, now `ARGMIN`).
4. Explicitly defines all the aggregations on a groupby. I discussed this briefly with @shwina, the change has pros and cons. The benefit is that all of these methods are properly documented now, there's less magic (the binding of methods to a class after its definition can be confusing for less experienced Python developers and has a lot of potential gotchas), and we can use the simpler string-based agg definition wherever possible. The downside is that we now have to define all of these methods. I think the change is definitely an improvement, but I'm happy to change it back if anyone can suggest a better alternative. In the long run we probably need to find a way to share both code and docstrings more effectively between all aggregations (DataFrame, Series, and GroupBy).

Authors:
  - Vyas Ramasubramani (https://github.com/vyasr)
  - GALI PREM SAGAR (https://github.com/galipremsagar)

Approvers:
  - Karthikeyan (https://github.com/karthikeyann)
  - Ashwin Srinath (https://github.com/shwina)
  - GALI PREM SAGAR (https://github.com/galipremsagar)

URL: https://github.com/rapidsai/cudf/pull/7818

---
## [jdbener/Project-Delta-Playtesting-Files@4c76e86c78...](https://github.com/jdbener/Project-Delta-Playtesting-Files/commit/4c76e86c78811cf463780879bd720589b4f1918d)
##### 2021-05-13 18:16:32 by jdbener

Added New Faction (Harmonizing Musicians)

• Added New Faction (Harmonizing Musicians): "...Realize I'm merely an imitation of humans.", "Gonna Jump Now and Be Free", "I'm Addicted to the Madness", "Living in the Shadows", Amp up the Volume, Backstage Meltdown, Band Equipment, Cocaine Induced Inspiration, ?, Digital Diva, Ecdysiast Singer, Inspiring Message, ?, Label Manager, Lust, Musician's Laptop, Pitch Correction Implant, Record Sales, Street Performer, Sway of the Music, Tour Deadlines, Vocaloid Voice Actress, and Vocal Training.
• Added Anger, Bouncer, Devoted Mother, Double or Nothing, Farmer, Fear, Happiness, Information Sniffer, Internet Search, Office Worker, Reconnaissance Satellite, Sadness, School Boy, School Girl, Scry, Secretary, Security Guard, and Traffic Cop.
• Hana has been renamed to Blossom (except on her origin card where she has been renamed to Hana Shinutsu.)
• Field Generator is now a Generator Asset.
• Changed Tradeable to only be activateable once each turn (but it is no longer restricted to deployment phases).

---
## [baggy-beagle/git-practice1-final@09a804d082...](https://github.com/baggy-beagle/git-practice1-final/commit/09a804d08297d2a541a50471254b5c9e9d4a5cf1)
##### 2021-05-13 18:42:11 by Sergey Avdeichik

Revert "add first hajf of my favorite song"

This reverts commit fd83e9eee20098cb9c3f5b61e0a921ce9d081005.

I was born in 1970, outskirts of Moscow1
Went wild since I was in diapers
I swore too much at the age of
Saw girl's c*nt for the first time in the kindergarten
And later on in the school with a crappy uniform2, fights and glue3
The things that made me stronger
I used to steal cash from the dressing rooms4
I took up smoking at the age of 8
And got laid for the first time at 11
Then I bailed on parents
Began to hang out at the dump
Boots and a military jacket
Brezhnev died5, I began lifting at the gym,
Bought boxing gloves, which
Were stolen at school. Didn't work too well for Vitjok and Kolja
Floor, the blood stains, got filed a case
Had burgled a makeshift barrack at night, got props, time flows...
Dodged military due to kidney disease6, got drunk and rough up a cop
Then moved to the chick's place in Kuryanovo
With my fellows we began to rob the cooperators
On Rizhskaya9, killed a gypsy woman, that's how it ended up.
I found myself on the federal wanted list, bought a car
Got hooked on meth
Just Butyrka helped, thanks to Kerya.
The court a year later, the sentence, 15 years
The north, midge, prison settlement11, fierce faces
I'm a loose cannon, escaped the courtroom with some goof
We fled to the forest, I'm eating his meat, the freight, hitchhiking...
Moscow, lay down low
New passport, new job, new car
Addressing issues, lots of cocaine
I got married, backing the banks, settled down in the mansion on Taganka
My son was born, my mother died, I went on Cuba on vacation for a half a year...

---
## [mrakgr/The-Spiral-Language@144b3e0b7f...](https://github.com/mrakgr/The-Spiral-Language/commit/144b3e0b7f604397d8c1263fc78894256d47a259)
##### 2021-05-13 18:48:24 by Marko Grdinić

"1:20pm. Done with breakfast. Let me chill a little.

Attentional mechanisms are definitely the secret behind long term credit assignment. It would be just too much work to scrap all the backprop rules and come up with completely new things. It is much more likely the case that keeping traces around selectively is the secret behind the brain capability in this domain.

1:35pm. Let me catch up with Hagure Idol and I will get some programming done. It is not my job to worry about attention mechanisms and the exact way of making long term credit assignment work.

So what if the hints are there? Making a new algorithm from such sparse info is a lot harder than solving a murder mystery. I am just wasting my time. Eventually, it will get figured out what the brain does. Until then, I should just focus on my own niche which is programming.

1:40pm. I've caught up to the Lord of Mysteries manhua, now let me do the same for Hagure Idol. I am a few chapters behind. After that the Kumo thread. After that I'll try to get some programming done today. Half a month is nearly over and I barely did a thing. I can qualify the last two weeks as a vacation.

2:55pm. Pfft...how did I waste this much time?

Let me start. Unless I want to become a researcher and dedicate myself to studying math and doing experiments in order to write papers, I should focus on the task at hand.

I am at a huge disadvantage not having powerful enough hardware and not being able to use the brain's internals as a reference. It bothers me. All of this does bother me.

For the past 3 weeks, this has been the biggest hurdle for me to cross which is coping with my inadequacy. No matter how powerful my imagination is, I can't imagine opening the skull and peering at the brain inside. I can't reason out the correctness of float using programs in advance.

Right now I am obsessing about my gradient ideas propagation ideas. I need to leave that out of mind.

Eventually the mysteries will be solved and I will be able to chase the superhuman with the tools that I need.

3:05pm. The ML researchers probably could not program as well that I do, but that won't help me unless I actually do it.

I hate ML down to my core. I read papers, but it only opens up more questions, rarely does it give me answers! I am so prideful, but it wants me to show humility. It points a gun at me, threatens to kill me, and asks me to keep calm. I hate it.

3:10pm. Right now I should start work on the uniform player, and then the linear one, but my mind is too much in disarray. I'd rather think about the papers that I've read than do programming work.

Sigh...and I had such good pace a few days ago. I gave into the urge to read papers, and this is the result. But it is fine. I won't read the same thing again. In the past few weeks my focus keeps switching targets, eventually I'll lock on to the original goal.

Let me take a break just to focus on the task at hand. Maybe I should step away from the screen for a bit. Let me do that.

4:50pm. I need to look up if any citations for Generative Minimization Networks came out. In the ML thread, one person said that no image samples is a red flag. Nope, nothing.

Generative Adversarial Networks are Special Cases of Artificial Curiosity (1990) and also Closely Related to Predictability Minimization (1991)

Right now I am reading this paper by Schmidhubber instead.

> The NIPS 2014 GAN paper [20] states that PM differs from GANs in the sense that PM is NOT based on a minimax game with a value function that one agent seeks to maximize and the other seeks to minimise. It states that for GANs "the competition between the networks is the sole training criterion, and is sufficient on its own to train the network," while PM "is only a regularizer that encourages the hidden units of a neural network to be statistically independent while they accomplish some other task; it is not a primary training criterion" [20]. But this claim is incorrect, since PM is indeed a pure minimax game, too, e.g., [82], Equation 2. There is no "other task." In particular, PM was also trained [64, 67, 68, 82, 86, 72] (also on images [82, 86]) such that "the competition between the networks is the sole training criterion, and is sufficient on its own to train the network."

https://sites.google.com/view/creditassignmentindlanddrl/home#h.p_R-9af1GhoCjr

https://beyondbackprop.github.io/

I wish I could find the Beyond Backprop talk by Schmidhuber. Instead I only found this site.

https://arxiv.org/abs/2012.14905
Meta Learning Backpropagation And Improving It
> Many concepts have been proposed for meta learning with neural networks (NNs), e.g., NNs that learn to control fast weights, hyper networks, learned learning rules, and meta recurrent NNs. Our Variable Shared Meta Learning (VS-ML) unifies the above and demonstrates that simple weight-sharing and sparsity in an NN is sufficient to express powerful learning algorithms (LAs) in a reusable fashion. A simple implementation of VS-ML called VS-ML RNN allows for implementing the backpropagation LA solely by running an RNN in forward-mode. It can even meta-learn new LAs that improve upon backpropagation and generalize to datasets outside of the meta training distribution without explicit gradient calculation. Introspection reveals that our meta-learned LAs learn qualitatively different from gradient descent through fast association.

This paper sounds super interesting, I'll admit.

https://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html

I do not feel like watching the BB vids. Let me read the paper and article by Jurgen.

5:25pm. The intro to the paper is amazing.

> The shift from standard machine learning to meta learning involves learning the learning algorithm (LA) itself, reducing the burden on the human designer to craft appropriate learning algorithms (Schmidhuber, 1987). Recent meta learning has primarily focused on similar tasks such as few-shot learning (Finn et al., 2017) or adaptation to related environments or goals (Houthooft et al., 2018). This is in stark contrast to human-engineered LAs that generalize across a wide range of datasets or environments. Very recently, it was demonstrated that meta learning can also successfully generate LAs that generalize across wide spectra of environments (Kirsch et al., 2020; Alet et al., 2020; Oh et al., 2020), e.g., from toy environments to Mujoco and Atari. Unfortunately, however, the large number of human-designed and unmodifiable inner-loop components (e.g., backpropagation) in many recent algorithms remains a limitation.

> Is it possible to implement modifiable versions of backpropagation or related algorithms as part of the end-to-end differentiable activation dynamics of a neural net (NN), instead of inserting them as separate fixed routines? Here we propose the Variable Shared Meta Learning (VS-ML) principle for this purpose. It introduces a novel way of using sparsity and weight-sharing in NNs for meta learning. We build on the arguably simplest neural meta learner, the meta recurrent neural network (Meta RNN) (Hochreiter et al., 2001; Duan et al., 2016; Wang et al., 2016). We then show that VS-ML can implement certain end-to-end differentiable fast weights (Schmidhuber, 1992; 1993a; Ba et al., 2016; Schlag & Schmidhuber, 2017), hyper networks (Ha et al., 2016)), learned learning rules (Bengio et al., 1992; Gregor, 2020; Randazzo et al., 2020), or hebbian-like synaptic plasticity (Schmidhuber, 1991; 1993a; Miconi et al., 2018; 2019; Najarro & Risi, 2020) as special cases. Our mechanism, VS-ML, can implement backpropagation solely in the forward-dynamics of an RNN. Consequently, it enables meta-optimization of backprop-like algorithms. In fact, our system meta learns LAs from scratch that learn faster than online backpropagation and generalize to datasets outside of the meta training distribution. Our VS-ML RNN is the first neural meta learner without hard-coded backpropagationthat shows such strong generalization. Our results may also be of interest with regard to biologically plausible versions of backpropagation.

> VS-ML blurs the semantic distinction between activations, weights, and LAs. We show how these concepts are emergent in ‘neural’ networks with ‘weight’ sharing. VS-ML learns the function each neuron performs and its LA.

I am making a strange face as I read this.

I collated my experiences and made that training scheme, but now that I've reached my limit I still find myself rattling in a slightly bigger box than a few years ago. These guys want to go far beyond.

If Jurgen or anybody else can help me exceed my limits, I do not mind heaping praise upon praise on them and considering them my superior. I know better than anybody that my performance has not been much. I cannot say that I did not put in the effort either. After six years, what I have now is exactly how good I am. I cannot say to myself that if I tried, I could do it.

5:55pm. There isn't anything I can use in the paper. They use backprop to train the metalearner too. Let me read the article.

6:05pm. Done. There were more references than actual text.

https://arxiv.org/abs/1608.05343
Decoupled Neural Interfaces using Synthetic Gradients

I won't bother reading this. This stuff is 5 years old. Let me take a look at the Beyond Backprop vids.

https://slideslive.com/38938066/bioconstrained-intelligence-and-the-great-filter

Let me give this a shot.

> “Modern” backpropagation (Linnainmaa, 1970) is now widely used. Many tasks, however, cannot be solved by backprop. I give examples where credit assignment can be achieved or greatly improved through other methods such as artificial evolution, compressed network search, universal search, the Optimal Ordered Problem Solver, meta-learning.

I really wish I could find this talk by Schmidhuber.

6:20pm. https://slideslive.com/38938066/bioconstrained-intelligence-and-the-great-filter

The first talk is not that bad. Surprisingly greedy infomax (GIM) outperforms supervised end to end learning. Maybe in the future backprop's use will be to just do local optimization.

I did a great deal to infer all those tricks to make it work better, but large and non interlaping timescales across modules are hell to deal with for me.

https://slideslive.com/38938068/gradientfree-learning-with-nevergrad

Let me check out these non-gradient optimization things.

7pm. Done with lunch. Let me continue for longer.

https://slideslive.com/38942370/randomized-automatic-differentiation

Let me watch this.

7:15pm. https://slideslive.com/38942061/zorb-a-derivativefree-backpropagation-algorithm-for-neural-networks

Let me go for this. The previous video is a snoozefest.

Actually, this is interesting. It is similar to the reverse learning idea that I had.

https://arxiv.org/abs/2011.08895
ZORB - A Derivative-Free Backpropagation Algorithm for Neural Networks

I am actually pretty happy when I see somebody doing what I thought of in the past. No way can I take the time to do these experiments.

7:45pm. https://slideslive.com/38938065/hypernets-and-the-inverse-function-theorem

Let me watch this. ZORB is actually quite different what I had in mind for reversible learning in terms of implementation. I do not understand the scale and shift operations. But I guess it is fine.

I am not going to be diving into this. Let me watch this talk as well.

8:10pm. Had to take a break. Let me resume.

ZORB really caught my attention. I'll link it along with GMN paper in the next PL update. It is remarkable how faster the training is.

But though it caught my attention, I am not interested in it. Right now I am looking for an answer to the question of long term credit assignment. So ZORB is besides the point here, it has the same weaknesses as backprop in that regard.

8:30pm. https://slideslive.com/38938070/new-old-tricks-for-online-metalearning

This is the last one. Let me just go for 10m and then I am finally done for the day.

8:40pm. Nothing interesting there. For a workshop titled Beyond Backprop, there wasn't much on going beyond backprop there. ZORB and GIM lived up to the title, but the rest did not.

https://arxiv.org/abs/2008.01342
LoCo: Local Contrastive Representation Learning

https://arxiv.org/abs/2101.03419
Training Deep Architectures Without End-to-End Backpropagation: A Brief Survey

https://scholar.google.com/scholar?as_ylo=2021&hl=hr&as_sdt=2005&sciodt=0,5&cites=8826125980550854976&scipsc=

I am going to die if I try going for this right now so let me stop it here.

8:45pm. There is no point to doing this research. I am just to anxius about this issue to give it a rest properly. I am going to look into synthetic gradient methods, GIM and what else is related to them.

Once I get this out of my system, maybe at that point I will be able to start doing some real programming. I am almost at the threeshold, but need to make that final step."

---
## [education/GitHubGraduation-2021@f2a380f819...](https://github.com/education/GitHubGraduation-2021/commit/f2a380f819a759548453857809b72371a1f24a7e)
##### 2021-05-13 20:42:02 by krishna kakade

 added myself krishnadevz.md

This kid is graduating and also doing cool things finally my mother is happy and god bless you all and teachers thank you github for this awesome platform because of github dev community I got my first developer job :).
Thank you 
take care 
kind regards 
krishna

---

# [<](2021-05-12.md) 2021-05-13 [>](2021-05-14.md)

