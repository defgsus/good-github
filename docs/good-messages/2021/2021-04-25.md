# [<](2021-04-24.md) 2021-04-25 [>](2021-04-26.md)

2,209,477 events, 1,244,577 push events, 1,809,629 commit messages, 99,369,259 characters


## [TeamDraco/teamdraco@c9fc2857ab...](https://github.com/TeamDraco/teamdraco/commit/c9fc2857abe9ceda1df66554e585a2f8d93cc486)
##### 2021-04-25 02:05:14 by Coda

im sorry andante im fucking up ur shit oh god oh fuck

---
## [AntonYudintsev/HT_case_study@680ef6f555...](https://github.com/AntonYudintsev/HT_case_study/commit/680ef6f555ad1b40c94a977232d51691118c2b10)
##### 2021-04-25 04:35:20 by Aliot

step3: fix algorithm and data structure.

* first, simplest: let's use FNV1 (it is not THE fastest hash, but it is simple, portable, and better distributed)
  -3% speed up, less probes

* second, we need only one incorrect hash value - 0. gives us -31% on miss, -0.5% on hit.
  That is very good change, for several reasons: a) it is faster to compare with zero, b) we have better hashes (as we don't waste one bit)

* third, let's use separate hashes, keys, and values to different arrays -21% on hit and -23% on miss.
  Keep in mind, we have absolutely same amount of probes, which is like 2.5 times higher than in required in SC,
  and yet our performance is almost same in OA (we still loose 12%)!

  Why this helps performance, is obvious. It is just much more cache friendly to iterate on smaller array.
  Which is not that obvious, is that it is actually way better structure, and it allows to save more memory.
  Keys, values, and hashes all have different alignment (in real life scenario).

  Original implementation was wasting 4 bytes on aligning int to alignment of string (which is >=8 on 64 bit platforms).
  so we waster our mem bandwidth on downloading 4 bytes of bullshit data, AND wasted memory.
  that alone saves us 128kb of memory (on ~16384 capacity).
  We still loose ~2.82 times on MISS.
  One can argue that keys should be in same array as value (as when we finally make a hit, we usually should return the data of value and we don't here).
  Well, I didn't made that test and data set, and the whole point of this analysis is that in any real scenario OA is (probably) a better solution.
  If we would need to return the value which is different from key and we get many hits, then fine (and in this particular test that wouldn't matter much).

* Fourth, Load Factor IS _NOT_ an actual production/platform requirement!

  It _is_ just part of a specific algorithm. The actual requirement (which it represents) is memory usage.
  So, if we want compare apples to apples, we should compare performance of HT on same MEMORY footprint, not on the same LF (which, again, doesn't matter).
  With that change we would loose by 4% on HIT and loose 4% on MISS.

---
## [KesunyianAyam/rufus@252759eb91...](https://github.com/KesunyianAyam/rufus/commit/252759eb917018392913245f58ab321a7ed5b42b)
##### 2021-04-25 04:46:40 by Pete Batard

[grub] add yet another frigging patch to GRUB "2.04"

* GRUB 2.0 maintainer think they're doing a fine job, even when there are
  CRITICAL SECURITY FIXES that should warrant an immediate out of bound
  release, and instead consider that waiting MONTHS or YEARS to release
  anything is not a big deal at all.
* Ergo, distros, such as Ubuntu, start to pick whatever security patches
  they see fit, since they can simply not RELY on the upstream project to
  produce security releases in a timely manner. One such patch is:
  https://lists.gnu.org/archive/html/grub-devel/2021-03/msg00012.html
* But since there is no new GRUB release per se, they still call their GRUB
  version, onto which they applied patches that have come into existence
  more than 2 years after the actual 2.04 release, "GRUB 2.04".
* Obviously, since GRUB 2.04 + literally hundreds of cherry picked patches
  does deviate a lot from the last release, THINGS BREAK IN SPECTACULAR
  FASHION, such as the recently released Ubuntu 21.04 failing to boot with
  the error: grub_register_command_lockdown not found.
* Oh, and of course, regardless of all the above, if you ask anyone, they'll
  tell you that there's nothing fundamentally wrong with the GRUB release
  process (even if they should long have released 2.05, 2.05-1 and 2.05-2,
  were their maintainer ready to acknowledge that delaying releases DOES
  CREATES MAJOR ISSUES DOWSTREAM, as many people REPEATEDLY pointed to them
  on the GRUB mailing list) or with the Ubuntu GRUB versioning process (that
  really shouldn't be calling their version of GRUB "grub-2.04" but instead
  something like "grub-2.04_ubuntu"). Oh no siree! Instead, the problem must
  all be with Rufus and its maintainer, who should either spend their lives
  pre-emptively figuring which breaking patch every other distro applied out
  there, or limit media creation to DD mode, like any "sensible" person
  would do, since DD mode is the ultimate panacea (Narrator: "It wasn't").
* So, once again, a massive thanks to all the people who have been involved
  in the current GRUB 2.0 shit show, whose DIRECT result is to make end
  users' lives miserable, while GRUB maintainers are hell bent on continuing
  to pretend that everything's just peachy and are busy patting themselves
  on the back on account that "Fedora recently dropped more than 100 of the
  custom patches they had to apply to their GRUB fork" (sic). Nothing to see
  here, it's just GRUB maintainer's Jedi business as usual. Besides, who the
  hell cares about Windows users trying to transition to Linux in a friendly
  manner anyway. I mean, as long as something doesn't affect existing Linux
  users, it isn't a REAL problem, right?...

---
## [AmayaWah/goonstation@24f6cd7530...](https://github.com/AmayaWah/goonstation/commit/24f6cd75306650a172e69ebe2a7a79c3a1824150)
##### 2021-04-25 07:38:55 by BatElite

Resprites building materials & standardises their code a bunch (#4086)

* Sprites & functions & oh god this needs unfucking
* Do rods, mostly. We have procs for this shit y'all
* Tile stacking/taking cleanup
* Sheet consumption stuff
* Addresses review, enlargens and changes sprites

Co-authored-by: ZeWaka <zewakagamer@gmail.com>

---
## [Skyrat-SS13/Skyrat-tg@530112e09f...](https://github.com/Skyrat-SS13/Skyrat-tg/commit/530112e09f3190ffa84c90f941ee972b44509b53)
##### 2021-04-25 12:26:08 by death and coding

[modular][ready] formal wear for medical and engineering in loadout, gas masks unadded, but coded (#5156)

* for i just threw out the love of my dreams

* Update uniform.dm

* fuck

* biker

* Update glasses.dm

* I don't want any of my sprites in Skyrat. I appreciate you asking though.

* Revert "I don't want any of my sprites in Skyrat. I appreciate you asking though."

This reverts commit d980baf7ada95a04f74870103b2ee09ede67dcda.

* Revert "Revert "I don't want any of my sprites in Skyrat. I appreciate you asking though.""

This reverts commit ad42656117f90747c0aaf7ade3614a16d67fed4b.

* as requested

* casual cargo gear

Co-authored-by: louiseedwardstuart <bonniefluff>

---
## [rizalgowandy/cockroach@f2c2f2e3db...](https://github.com/rizalgowandy/cockroach/commit/f2c2f2e3dbbb52ff08fd30d4c533695c491103d8)
##### 2021-04-25 13:02:29 by craig[bot]

Merge #64060

64060: kvserver: fix delaying of splits with uninitialized followers r=erikgrinaker a=tbg

Bursts of splits (i.e. a sequence of splits for which each split splits
the right-hand side of a prior split) can cause issues. This is because
splitting a range in which a replica needs a snapshot results in two
ranges in which a replica needs a snapshot where additionally there
needs to be a sequencing between the two snapshots (one cannot apply
a snapshot for the post-split replica until the pre-split replica has
moved out of the way). The result of a long burst of splits such as
occurring in RESTORE and IMPORT operations is then an overload of the
snapshot queue with lots of wasted work, unavailable followers with
operations hanging on them, and general mayhem in the logs. Since
bulk operations also write a lot of data to the raft logs, log
truncations then create an additional snapshot burden; in short,
everything will be unhappy for a few hours and the cluster may
effectively be unavailable.

This isn't news to us and in fact was a big problem "back in 2018".
When we first started to understand the issue, we introduced a mechanism
that would delay splits (#32594) with the desired effect of ensuring
that, all followers had caught up to ~all of the previous splits.
This helped, but didn't explain why we were seeing snapshots in the
first place.

Investigating more, we realized that snapshots were sometimes spuriously
requested by an uninitialized replica on the right-hand side which was
contacted by another member of the right-hand side that had already been
initialized by the split executing on the left-hand side; this snapshot
was almost always unnecessary since the local left-hand side would
usually initialize the right-hand side moments later.  To address this,
in #32594 we started unconditionally dropping the first ~seconds worth
of requests to an uninitialized range, and the mechanism was improved in
 #32782 and will now only do this if a local neighboring replica is
expected to perform the split soon.

With all this in place, you would've expected us to have all bases
covered but it turns out that we are still running into issues prior
to this PR.

Concretely, whenever the aforementioned mechanism drops a message from
the leader (a MsgApp), the leader will only contact the replica every
second until it responds. It responds when it has been initialized via
its left neighbor's splits and the leader reaches out again, i.e.  an
average of ~500ms after being initialized. However, by that time, it is
itself already at the bottom of a long chain of splits, and the 500ms
delay is delaying how long it takes for the rest of the chain to get
initialized.  Since the delay compounds on each link of the chain, the
depth of the chain effectively determines the total delay experienced at
the end. This would eventually exceed the patience of the mechanism that
would suppress the snapshots, and snapshots would be requested. We would
descend into madness similar to that experienced in the absence of the
mechanism in the first place.

The mechanism in #32594 could have helped here, but unfortunately it
did not, as it routinely missed the fact that followers were not
initialized yet. This is because during a split burst, the replica
orchestrating the split was typically only created an instant before,
and its raft group hadn't properly transitioned to leader status yet.
This meant that in effect it wasn't delaying the splits at all.

This commit adjusts the logic to delay splits to avoid this problem.
While clamoring for leadership, the delay is upheld. Once collapsed
into a definite state, the existing logic pretty much did the right
thing, as it waited for the right-hand side to be in initialized.

Closes #61396.

cc @cockroachdb/kv

Release note (bug fix): Fixed a scenario in which a rapid sequence
of splits could trigger a storm of Raft snapshots. This would be
accompanied by log messages of the form "would have dropped incoming
MsgApp, but allowing due to ..." and tended to occur as part of
RESTORE/IMPORT operations.


Co-authored-by: Tobias Grieger <tobias.b.grieger@gmail.com>

---
## [getkey/Commit-Logs-From-Last-Night@5d297016f1...](https://github.com/getkey/Commit-Logs-From-Last-Night/commit/5d297016f15fe65074acb8fffb4f05bf8f262399)
##### 2021-04-25 13:47:12 by Parag Bhatnagar

catching all 7 dirty words

Modified word_list.py to catch all of George Carlin's 7 dirty words you can't say on television
- the words being shit, piss, fuck, cunt, cocksucker, motherfucker and tits.

---
## [RajaKunalPandit1/Practice_questions@e47eb83bf0...](https://github.com/RajaKunalPandit1/Practice_questions/commit/e47eb83bf0e2e169b73f509942d846455b09e83e)
##### 2021-04-25 14:29:53 by Raja Kunal Pandit

Create Magical Number.cpp

Your friend loves magic and he has coined a new term - "Magical number". To perform his magic, he needs that Magic number. There are N number of people in the magic show, seated according to their ages in an ascending order. Magical number is that seat no. where the person has the same age as that of the given seat number.
Help your friend in finding out that "Magical number"
Output Status:
Correct Answer.Correct Answer
Execution Time:0.01

---
## [benk0913/RLSC@ee7e38efba...](https://github.com/benk0913/RLSC/commit/ee7e38efbafe65b10a6412fff95fbbeac7deabcf)
##### 2021-04-25 17:46:07 by NativKalo

sounds for da slime and da ghosties

What the fuck did you just fucking say about me, you little bitch? I'll have you know I graduated top of my class in the Navy Seals, and I've been involved in numerous secret raids on Al-Quaeda, and I have over 300 confirmed kills. I am trained in gorilla warfare and I'm the top sniper in the entire US armed forces. You are nothing to me but just another target. I will wipe you the fuck out with precision the likes of which has never been seen before on this Earth, mark my fucking words. You think you can get away with saying that shit to me over the Internet? Think again, fucker. As we speak I am contacting my secret network of spies across the USA and your IP is being traced right now so you better prepare for the storm, maggot. The storm that wipes out the pathetic little thing you call your life. You're fucking dead, kid. I can be anywhere, anytime, and I can kill you in over seven hundred ways, and that's just with my bare hands. Not only am I extensively trained in unarmed combat, but I have access to the entire arsenal of the United States Marine Corps and I will use it to its full extent to wipe your miserable ass off the face of the continent, you little shit. If only you could have known what unholy retribution your little "clever" comment was about to bring down upon you, maybe you would have held your fucking tongue. But you couldn't, you didn't, and now you're paying the price, you goddamn idiot. I will shit fury all over you and you will drown in it. You're fucking dead, kiddo.

---
## [mrakgr/The-Spiral-Language@97c491aaaf...](https://github.com/mrakgr/The-Spiral-Language/commit/97c491aaaf30d21f7cb769ad6cdd98d66ef0617a)
##### 2021-04-25 19:57:42 by Marko Grdinić

"10:55am. I am having inspiration again. I guess I am not going to program again. Instead, let me bring these thoughts to a conclusion.

The backprop issues I've talked about in the reversiblity paper are not wrong.

Optimizing `1000 - y` and `10 - 10 * 10 * y` should be the same thing. The gradient would certainly be the same when `y = 0`. Yet there is something missing.

That something are optimization preferences.

11:05am. Seen from the perspective of NNs, rather than just the gradients, I should in fact take account of the weight norms in the upper layers. In addition to propagating the gradients, I should propagate the optimization preference.

11:10am. I've been thinking how I would design my own learning rules and...I don't think that the whole idea of differentiable manifolds needs to be swept aside. In fact imposing a manifold is the first thing you would do for efficiency. After that, there is nothing wrong with the backprop local rules either.

They do in fact give you the gradient.

But the algorithm is blind to the structure in some sense. Getting a gradient of 64 at `y` when optimizing `y` and when optimizing `a*b*y` might be completely different thing. The gradient does in fact tell you how much the target will change when you move `y` around. There is no doubt about that here. But it does not tell you by how much the y should be moved.

In the first case, if you are optimizing something like `64 - y`, then it makes complexte sense to move in the step of 64. But if you are optimizing `1 - a * b * y`, then it does not make any sense at all. The thing has to slow down by necessity.

On these toy examples, the degree it should slow down is exactly obvious. Counter the multiplication and turn it into a division.

11:25am. Densenets and resnets were a big innovation allowing deep hierarchies to be trained, but I do not think they are right. Constantly baraging the net with gradients won't allow invariants to form even though it might deal with the vanishing gradient problem.

Seriously considering my local optimization ideas might be the way to go.

Yesterday I made my determination to make my goal studying these deep nets. That was right.

But I've also been thinking about differentiable manifolds and comparing them to the bayesian idea, and manifolds are not wrong.

There is something I've figured out about the Gassian priors. Rather than having the weights decay, which struck me as intrinsically wrong, why don't I in fact put a manifold on the weights?

https://en.wikipedia.org/wiki/Gaussian_function

I mean, look at the bell curve here. This could serve as the local error function for the weight updates. And it would have the behavior of inhibiting movement when the weight gets too far from its mean. This should be the way to do it. I can't imagine neurons in the brain just decaying.

I wanted to find a way to put the priors at the buffer shaping level, but that is not right. Instead why don't I substitute probability priors with manifold shaping. In fact shaping the manifold by changing the replay buffer probabilities is the purpose of that as well.

11:50am. I am just lying here thinking - it is absolutely insane to use the backprop rules as they are.

Yeah, I need to modify them. Given the structure of the network, there should be better rules out there. Backprop is a good starting point when it comes to looking for them though. I already have to fiddle with init and the learning rates. Why not go the whole length?

Let me give that statistics book a try. I guess I'll explore today as well.

But when I strike, I will hit it right.

It is all about the rules. If I can grasp the right rules, I can attain everything. I should have courage and mix it up, much like nature did.

I won't throw out the idea of manifolds, but I should throw out the idea of blindly following the rules that backprop gives me.

12:30pm.

> Necessity is obvious. To show the sufficiency, let E be a collection of subsets of E that is both a p-system and a d-system. First, E is closed under complements: A ∈ E ⇒ E \ A ∈ E, since E ∈ E and A ⊂ E and E is a d-system. Second, it is closed under unions: A, B ∈ E ⇒ A ∪ B ∈ E, because A∪B = (Ac ∩Bc)c and E is closed under complements (as shown) and under intersections by the hypothesis that it is a p-system. Finally, this closure extends to countable unions: if (An) ⊂ E, then B1 = A1 and B2 = A1 ∪ A2 and so on belong to E by the preceding step, and Bn    n An, which together imply that  n An ∈ E since E is a d-system by hypothesis

Ugh, forget it. I am not good at visualizing these proofs in my mind unless there is a need for it.

1:40pm. Done with breakfast.

I am still thinking about it. Terror, trepidation, fear, loneliness, I've been going through those feelings in the past few days, but today it seems they are abating and in the depths of my heart I am starting to feel determination.

To get to this point, wasn't that the purpose of making Spiral? Wasn't that the purpose of the last six years of effort?

I've lived my life by my own rules. So it only goes to reason, that I'll do ML according my own rules as well.

I know quite a lot. I know most of the important things in deep learning. I've studied the subject to death. But it is not like I am good at it.

But I could be.

My own rules might turn out to be good like I hope, or they might turn out to be bad.

What is important is that I keep watch and think. If I make my own neurolab, and keep watch of what I am doing, then the second half of my journey will begin.

Without much fanfare, the first part of it ended when I finished the tabular player.

The only question I need to ask myself is whether I want to take responsiblity for my own vision, or do I want to run away into the false safety of society?

I've failed before, and I will fail in the future. That does not matter.

1:50pm. If I continue going on this road, I think that the majority of heavy lifting in programming is behind me. I won't have to get my hands so extremely dirty in the future. I've been a hard labourer, programming wise in the past.

Well, it is not like the effort will be light, but *pats the box next to me* increasingly the workload will be done by my little guy.

1:55pm. I can do it. I can't figure out learning, but everything points that I should be able to take my current insights and significantly improve on it.

Though I won't use it right now, I already understand that while backprop's local rules might not be strictly composable, only weakly so, locally optimizing processes are strongly composable. If I keep going in this direction at some point I will find out the secret of why the brain has such strong generative capabilities.

There is something about the idea that the upper layers should generate targets instead of propagating raw gradients. The idea has potential.

I thought that before, but then I got scared off without really investigating the idea.

2pm. It does not have to be like this. I can throw in the towel here, or I can start my ascent to transcendence.

Maybe current methods are no good and cannot be improved with just hacking and zero theory. Maybe the Singularity will not happen in my lifetime. Maybe I'd be better off getting a job instead and living a normal life.

2:05pm. But wouldn't it be wonderful if I lived believing in it? Wouldn't it be wonderful if my ideas succeed and I made a superhuman poker agent? Wouldn't it be wonderful (for me) if I took it on a rampage and made my millions that way?

Wouldn't it be wonderful if in the future my vision of Spiral bore fruit and I got access to those AI chips?

Wouldn't it be wonderful if I managed to take advantage of those increased capabilities to develop generally intelligent agents? Wouldn't it be wonderful to exchange my will and their power?

2:15pm. This won't happen unless I work on it.

But I can't treat it as a chore. I can't treat it as some drudgery that I have to do.

What I will get doing here is stacking real benefits together to get something great.

Trying out my various ideas and seeing them work would be an enormous benefit. It wouldn't be money by itself, but it would in fact be something better.

I need to believe that is better. I need to believe that the programming skills I've attained, and the ML skills that I will attain are the real value of my journey.

If I did, I'd never have to endure loneliness again.

2:20pm. I should pursue my goal with zeal. Make the agent, get the money, resolve the health issue with mom, go into the beyond right into the maw of tomorrow.

It does not need to be fancy or complicated.

All I need is the right rules, and the right hardware.

And the belief in myself and the determination to follow my own path.

2:50pm. Let me not push myself. Why don't I take a break for a few days? How about I just chill without restrictions and beating myself up over not programming every hour of the day?

The surest way to get sick of work is to constantly keep doing it. Normal people take breaks two days a week for a reason. Only I myself am plugged into it 24/7.

2:55pm. My ideas are all significant improvements. After much effort, I figured out completely how to make the networks reward invariant. And now my ideas have been extended to normalizing the weight gradients in the hidden layers.

All this will amount in benefits. But even though I might have found burried in the ground, I own own it until I actually dig it out and take it for myself.

In 2018 I was particularly hard for KFAC, but though KFAC can do reward normalization, it is very much a reactive thing. It needed an identity matrix of about 0.1. It made thing a little better, but the keyword is little. It is very likely that the net could blow up before the covariance matrices get estimated. It could blow up because those 0.1 raw gradients as well.

In contrast to that, my ideas are both much simpler, are predictive rather than reactive, and perfectly resolve the issues involved in reward variance.

The reasoning is impecable, but I need to actually go and try it out.

3:10pm. Yeah, let me have a vacation. Reading novels when I am not dead tired from work, watching anime, playing games. Just today, and maybe tomorrow. And then I will do the monthly review and jump into this with all my zeal.

https://www.youtube.com/watch?v=S27pHKBEp30
LSTM is dead. Long Live Transformers!

Let me do some random things like watching this transformer video. I keep looking into what transformers are and promptly forgetting it afterwards.

3:25pm. https://youtu.be/S27pHKBEp30?t=515
> Transfer learning never really worked on these LSTM models.

Huh, this is new to me. I never knew about that.

https://youtu.be/S27pHKBEp30?t=1053

Hnnnmmm, now I remember. I should consider using this for poker instead of regular RNNs. But regular RNNs should be more straightforward to deal with.

Obviously, feedforward nets would be the easiest, so I should go with that first. If I use a binary representation for stack sizes, I'll be able to do very large sequences.

3:50pm. https://www.youtube.com/watch?v=wFsI2WqUfdA
DeepMind x UCL | Deep Learning Lectures | 9/12 | Generative Adversarial Networks

It has been a while since I watched any deep learning lectures. I used to do this a lot.

https://www.youtube.com/channel/UCP7jMXSY2xbc3KCAE0MHQ-A

Oh, Deepmind has a lot here. There is even a RL course.

...Hmmm, maybe I'll take a look at the advanced topics. Vanilla single agent RL is useless. They should be teaching CFR.

https://youtu.be/wFsI2WqUfdA?t=1352

I only picked this video up on a whim, but it is pretty good. I am starting to understand why GANs are unstable.

4:40pm. https://youtu.be/wFsI2WqUfdA?t=2088

I guess it was not because of that.

7pm. https://youtu.be/7Pcvdo4EJeo?t=458

I am still watching these vids.

This non GAN stuff is better than I thought. The previous lecture was getting boring so I skipped it. I am actually pretty tired of these lectures, but they are quite good.

https://youtu.be/7Pcvdo4EJeo?t=4091

Amortized variational inference is interesting. I saw this once before in the context of probabilistic programming. It was by that Wood guy, I forgot what his first name was. I think it was Frank. He published that paper with Baydin.

https://youtu.be/7Pcvdo4EJeo?t=4224

I wish he talked about ensembling, but I guess that won't happen here.

8:55pm. https://youtu.be/7Pcvdo4EJeo?t=5145

Ah wait, didn't Wood say that amortized inference was exact inference? This lecture did not mention that.

9:05pm. I should watch the unsupervised learning lecture, but I am too tired right now and my stress is sky high.

I am thinking, and it is not necessarily a given that local optimization will give incorrect targets for GAN training. I mean, if it can give exact gradient, who is to say that local training will degenerate to autoencoder training. Maybe it will give unrealistic input targets that average out to the right thing rather than the average pixel like in regular autoencoders.

9:10pm. Hack the backpropagation rules, replace global with local training, shatter the dogma and the dominance of the current gradient based training! Make my own manifold! Shape the replay buffer!

I can see it. And I can feel it. I just need the courage to grasp it.

Right now, my fear is pushing me into information seeking mode. I should not be watching these lectures, but I can't help it. Eventually I will get tired of being in this state and just go for it. But until then, it is not too horrible of an idea to spend my time studying.

Ultimately, what I want to do will not be any of the papers as it is too heretical. I am going to have to do it on my own."

---
## [muhopensores/dmc4_hook@142fa95ae4...](https://github.com/muhopensores/dmc4_hook/commit/142fa95ae440d9c0b7f8c7d5af7b45e12dc8759f)
##### 2021-04-25 20:51:36 by Mstislav Kapustka

fixed background rendering

with shittiest workaround possible, late night programming. damn i gotta work tomorrow fuckk

---
## [Offlical/horrible-ldjam-48@8d40b7d9d5...](https://github.com/Offlical/horrible-ldjam-48/commit/8d40b7d9d5829e46755130d754bcd1aba03168c3)
##### 2021-04-25 21:08:58 by Offlical

"Finished" game, added ending classes and just whatever god awful game is this

this game fucking sucks

---
## [aircrftdev/Aircrft-Dev-Staging-Landkit-2.0.0-beta1@73c82c9212...](https://github.com/aircrftdev/Aircrft-Dev-Staging-Landkit-2.0.0-beta1/commit/73c82c921213589218df01a06f2d1a88124cbb39)
##### 2021-04-25 22:59:22 by aircrftdev

Commit

This platform is an act of :heart:. I hope you share this with your friends
& family. I thank you for taking the time to do this practice with me.
I am so grateful to journey alongside you. Sending blessings of
prosperity, abundance, joy, and deep personal fulfillment. - Chai
Forest | Captain/Curator Moonshots

---
## [aircrftdev/Aircrft-Dev-Staging-Landkit-2.0.0-beta1@abed9cf623...](https://github.com/aircrftdev/Aircrft-Dev-Staging-Landkit-2.0.0-beta1/commit/abed9cf623364cacc49169f127741e751e7b4484)
##### 2021-04-25 23:10:40 by aircrftdev

Commit

This platform is an act of :heart:. I hope you share this with your friends
& family. I thank you for taking the time to explore the technology
solutions with having thoughtfully curated to help uplift, empower,
and make your daily life a little bit more wondeful. I am so grateful
to journey alongside you. Sending blessings of prosperity, abundance,
joy, and deep personal fulfillment. - Chai Forest | Captain/Curator
Moonshots

---
## [vawser/Cinders-DS3@0c5d9437a7...](https://github.com/vawser/Cinders-DS3/commit/0c5d9437a785f1f80b58f16f089a2b4ed4d5c6ed)
##### 2021-04-25 23:31:28 by Vawser

Update

# Map
- Changed the Church of Sin bonfire into the Swamp of Sin and moved it onto the wooden platform above the swamp in the Profaned Capital.
- Removed enemies near the Secluded Cloister bonfire.

# Bugfixes
- Fixed Rosaria/McDonnell relationship dialogue.
- Fixed Master Benjin's dialogue in m40.

# Bosses
- Added a new boss in the Profaned Capital swamp: the Fallen Protector and the Fair Maiden
 - Drops Soul of the Maiden, which can be transposed into:
  - Blueblood Sword
  - Quicksilver Shield
  - Life Ring
 - The Quicksilver Set will appear in the swamp cave after they have been defeated.

---

# [<](2021-04-24.md) 2021-04-25 [>](2021-04-26.md)

