# [<](2021-07-12.md) 2021-07-13 [>](2021-07-14.md)

3,599,188 events, 1,429,858 push events, 2,381,909 commit messages, 206,985,111 characters


## [didierrevelo/holbertonschool-web_front_end](https://github.com/didierrevelo/holbertonschool-web_front_end)@[00d018f533...](https://github.com/didierrevelo/holbertonschool-web_front_end/commit/00d018f533a865f852289ea890a598b107895eec)
#### Tuesday 2021-07-13 02:03:44 by didierrevelo

12. Paragraphs
The 12-index.html file is created based on the 10-index.html file and the following tags and changes are added:

* About Us paragraphs:
  + in the About Us section
      - after the first h3 (who are we) create a paragraph with the text: Lorem ipsum dolor sit amet, consectetur adipisicing elit. Ipsum, omnis expedited! Eum, praesentium cumque accusantium rem, sit quaerat est nisi ratione, deserunt ducimus quidem iste dicta quibusdam atque maxime cum!
      - after the second h3 create a paragraph with the text: Lorem ipsum dolor sit amet, consectetur adipisicing elit. Ipsum, omnis expedited! Eum, praesentium cumque accusantium rem, sit quaerat est nisi ratione, deserunt ducimus quidem iste dicta quibusdam atque maxime cum!
      - after the third h3 create a paragraph with the text: Lorem ipsum pain sit amet, consectetur adipisicing elit. Ipsum, omnis expedited! Eum, praesentium cumque accusantium rem, sit quaerat est nisi ratione, deserunt ducimus quidem iste dicta quibusdam atque maxime cum!

* Latest news paragraphs:
  + in the Latest news section
      - in the first article
         \ create a paragraph with text Career before the heading
         \ create a paragraph with text Lorem ipsum pain sit amet, consectetur adipiscing elit. Id Sextilius factum negabat. Quo tandem mode? At eum nihili facit; Quae contraria sunt his, malane? after the heading
      - in the second article
         \ create a paragraph with text Digital Life before the heading
         \ create a paragraph with text Lorem ipsum pain sit amet, consectetur adipiscing elit. Tum mihi Floor: Quid ergo? Tum ille: Ain tandem? Non autem hoc: igitur ne illud quidem. Sed quod proximum fuit non vidit. We commodius agimus. An nisi populari fame? after the heading
      - in the third article
         \ create a paragraph with text Social before the heading
         \ create a paragraph with text Lorem ipsum pain sit amet, consectetur adipiscing elit. Non igitur bene. Quid enim est a Chrysippo praetermissum in Stoicis? Pugnant Stoici cum Peripateticis. Prioris generis est docilitas, memory; Apparet statim, quaesent officia, quae actiones. after the heading

* Contact paragraph:
  + in the Contact section after the heading
      - create a paragraph with the text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Id Sextilius factum negabat. Quo tandem mode? At eum nihili facit; Quae contraria sunt his of him, malane?

* Additional paragraphs:
  + below the level 2 Services heading add a paragraph with text We work with you
  + below the level 2 Works heading add a paragraph with text Take a look in our portfolio
  + below the level 2 About Us heading add a paragraph with text Everything about us
  + below the level 2 Testimonials heading add a paragraph with text We are more than a digital company
  + below the level 2 Contact heading add a paragraph with text We like to know new people
this solves task 12 project 0x00-html_advanced

---
## [Prathyusha-L/100DaysofCode---Day-18](https://github.com/Prathyusha-L/100DaysofCode---Day-18)@[96d99821ac...](https://github.com/Prathyusha-L/100DaysofCode---Day-18/commit/96d99821ac2a9b0ebff38e3545321855fc0bc73a)
#### Tuesday 2021-07-13 04:23:42 by L Prathyusha

NLP

All that we express (either verbally or in composed) conveys immense measures of data. The theme we pick, our tone, our determination of words, all that adds some kind of data that can be deciphered and esteem separated from it. In principle, we can comprehend and even foresee human conduct utilizing that data. 



Yet, there is an issue: one individual may create hundreds or thousands of words in a revelation, each sentence with its comparing intricacy. In the event that you need to scale and examine a few hundreds, thousands or millions of individuals or assertions in a given topography, then, at that point the circumstance is unmanageable. 



Information created from discussions, assertions or even tweets are instances of unstructured information. Unstructured information doesn't fit conveniently into the conventional line and segment construction of social data sets, and address by far most of information accessible in the real world. It is muddled and difficult to control. All things considered, on account of the advances in disciplines like AI a major unrest is continuing in regards to this point. These days it is not, at this point about attempting to decipher a book or discourse dependent on its watchwords (the antiquated mechanical way), yet about understanding the significance behind those words (the psychological way). This way it is feasible to identify sayings like incongruity, or even perform assumption examination. 



Normal Language Processing or NLP is a field of Artificial Intelligence that enables the machines to peruse, comprehend and get significance from human dialects. 




It's anything but a control that spotlights on the connection between information science and human language, and is scaling to loads of ventures. Today NLP is blasting because of the immense enhancements in the admittance to information and the increment in computational force, which are permitting experts to accomplish significant outcomes in regions like medical care, media, money and HR, among others. 



Use Cases of NLP 



In basic terms, NLP addresses the programmed treatment of regular human language like discourse or text, and albeit the actual idea is interesting, the genuine worth behind this innovation comes from the utilization cases. 



NLP can assist you with loads of undertakings and the fields of use simply appear to increment consistently. We should make reference to certain models: 



NLP empowers the acknowledgment and expectation of infections dependent on electronic wellbeing records and patient's own discourse. This ability is being investigated in medical issue that go from cardiovascular infections to sadness and even schizophrenia. For instance, Amazon Comprehend Medical is an assistance that utilizes NLP to remove infection conditions, meds and therapy results from patient notes, clinical preliminary reports and other electronic wellbeing records. 



Associations can figure out the thing clients are saying about a help or item by distinguishing and separating data in sources like web-based media. This estimation investigation can give a great deal of data about clients decisions and their choice drivers. 



An innovator at IBM fostered a psychological aide that works like a customized internet searcher by learning about you and afterward help you to remember a name, a tune, or anything you can't recollect the second you need it to. 



Organizations like Yahoo and Google channel and arrange your messages with NLP by breaking down text in messages that move through their workers and halting spam before they even enter your inbox. 



To help recognizing counterfeit news, the NLP Group at MIT fostered another framework to decide whether a source is exact or politically one-sided, identifying if a news source can be trusted or not. 



Amazon's Alexa and Apple's Siri are instances of insightful voice driven interfaces that utilization NLP to react to vocal prompts and do all that like track down a specific shop, disclose to us the climate estimate, propose the best course to the workplace or turn on the lights at home. 



Having an understanding into what's going on and what individuals are discussing can be truly important to monetary merchants. NLP is being utilized to follow news, reports, remarks about potential consolidations between organizations, everything can be then fused into an exchanging calculation to create monstrous benefits. Keep in mind: purchase the gossip, sell the news. 



NLP is additionally being utilized in both the pursuit and choice periods of ability enlistment, distinguishing the abilities of likely recruits and furthermore spotting possibilities before they become dynamic hands on market. 



Fueled by IBM Watson NLP innovation, LegalMation fostered a stage to computerize routine case errands and help legitimate groups save time, drive down expenses and shift key core interest. 



NLP is especially blasting in the medical services industry. This innovation is improving consideration conveyance, illness finding and cutting expenses down while medical care associations are going through a developing selection of electronic wellbeing records. The way that clinical documentation can be improved implies that patients can be better perceived and profited through better medical services. The objective ought to be to streamline their experience, and a few associations are as of now dealing with this. 



Number of distributions containing the sentence "regular language preparing" in PubMed in the period 1978â€“2018. Starting at 2018, PubMed included in excess of 29 million references for biomedical writing 



Organizations like Winterlight Labs are creating colossal upgrades in the treatment of Alzheimer's sickness by observing psychological debilitation through discourse and they can likewise uphold clinical preliminaries and studies for a wide scope of focal sensory system issues. Following a comparable methodology, Stanford University created Woebot, a chatbot advisor determined to assist individuals with uneasiness and different problems. 



In any case, genuine debate is around the subject. Several years prior Microsoft showed that by dissecting enormous examples of web search tool inquiries, they could recognize web clients who were experiencing pancreatic malignant growth even before they have gotten a finding of the infection. How might clients respond to such finding? Furthermore, what might occur in the event that you were tried as a bogus positive? (implying that you can be determined to have the sickness despite the fact that you don't have it). This reviews the instance of Google Flu Trends which in 2009 was declared as having the option to foresee flu however later on evaporated because of its low exactness and failure to meet its projected rates. 



NLP might be the way in to a viable clinical help later on, however there are as yet numerous difficulties to look for the time being. 



Fundamental NLP to intrigue your non-NLP companions 



The principle disadvantages we face these days with NLP identify with the way that language is precarious. The way toward comprehension and controlling language is amazingly perplexing, and consequently it isn't unexpected to utilize various methods to deal with various difficulties prior to restricting everything together. Programming dialects like Python or R are profoundly used to play out these procedures, yet prior to jumping into code lines (that will be the subject of an alternate article), comprehend the ideas underneath them. We should sum up and clarify probably the most oftentimes utilized calculations in NLP when characterizing the jargon of terms: 



Pack of Words 



Is an ordinarily utilized model that permits you to include all words in a piece of text. Essentially it's anything but an event grid for the sentence or archive, dismissing punctuation and word request. These word frequencies or events are then utilized as highlights for preparing a classifier. 



To bring a short model I took the main sentence of the tune "Across the Universe" from The Beatles: 



Words are streaming out like unending precipitation into a paper cup, 



They crawl while they pass, they get away across the universe 



Presently how about we tally the words: 



This methodology may mirror a few drawbacks like the shortfall of semantic significance and setting, and the realities that stop words (like "the" or "a") add clamor to the investigation and a few words are not weighted in like manner ("universe" loads not exactly "they"). 



To take care of this issue, one methodology is to rescale the recurrence of words by how regularly they show up in all writings (not simply the one we are investigating) so the scores for successive words like "the", that are likewise continuous across different writings, get punished. This way to deal with scoring is designated "Term Frequency â€” Inverse Document Frequency" (TFIDF), and improves the sack of words by loads. Through TFIDF incessant terms in the content are "remunerated" (like "they" in our model), yet they likewise get "rebuffed" if those terms are successive in different writings we remember for the calculation as well. Despite what might be expected, this technique features and "rewards" remarkable or uncommon terms thinking about all writings. By and by, this methodology actually has no setting nor semantics. 



Tokenization 



Is the way toward fragmenting running content into sentences and words. Fundamentally, it's the errand of cutting a content into pieces called tokens, and simultaneously discarding certain characters, like accentuation. Following our model, the aftereffect of tokenization would be: 



Pretty straightforward, isn't that so? All things considered, in spite of the fact that it might appear to be very essential for this situation and furthermore in dialects like English that different words by a clear space (called portioned dialects) not all dialects act something similar, and all things being equal, clear spaces alone are not adequate enough in any event, for English to perform legitimate tokenizations. Parting on clear spaces may separate what ought to be considered as one token, as on account of specific names (for example San Francisco or New York) or acquired unfamiliar expressions (for example free enterprise). 



Tokenization can eliminate accentuation as well, facilitating the way to a legitimate word division yet in addition setting off potential inconveniences. On account of periods that follow contraction (for example dr.), the period following that shortened form ought to be considered as a component of a similar token and not be taken out. 

Code: -

text = "In Brazil they drive on the right-hand side of the road. Brazil has a large coastli"

from nltk.tokenize import word_tokenize

# Passing the string text into word tokenize for bre

token = word_tokenize(text)

token






Stop Words Removal 



Incorporates disposing of normal language articles, pronouns and relational words, for example, "and", "the" or "to" in English. In this interaction some normal words that seem to offer next to zero benefit to the NLP objective are separated and avoided from the content to be prepared, consequently eliminating broad and incessant terms that are not instructive about the comparing text. 



Stop words can be securely overlooked via doing a query in a pre-characterized rundown of catchphrases, opening up information base space and improving handling time. 



There is no widespread rundown of stop words. These can be pre-chosen or worked without any preparation. A potential methodology is to start by receiving pre-characterized stop words and add words to the rundown later on. In any case it appears to be that the overall pattern throughout the past time has been to go from the utilization of enormous standard stop word records to the utilization of no rundowns by any means. 



The thing is stop words expulsion can clear out significant data and adjust the setting in a given sentence. For instance, in the event that we are playing out an opinion investigation we may lose our calculation track in the event that we eliminate a stop word like "not". Under these conditions, you may choose a negligible stop word rundown and add extra terms relying upon your particular goal. 



Stemming 



Alludes to the way toward cutting the end or the start of words fully intent on eliminating joins (lexical increases to the base of the word). 



Joins that are appended toward the start of the word are called prefixes (for example "astro" in "astrobiology") and the ones joined toward the finish of the word are called additions (for example "ful" in "accommodating"). 



The issue is that joins can make or grow new types of a similar word (called inflectional attaches), or even make new words themselves (called derivational fastens). In English, prefixes are consistently derivational (the attach makes another word as in the case of the prefix "eco" in "biological system"), yet additions can be derivational (the append makes another word as in the case of the postfix "ist" in "guitarist") or inflectional (the join makes another type of word as in the case of the addition "er" in "quicker"). 



Alright, so how might we differentiate and cleave the right piece? 



A potential methodology is to consider a rundown of normal joins and rules (Python and R dialects have various libraries containing attaches and strategies) and perform stemming dependent on them, obviously this methodology presents restrictions. Since stemmers use algorithmics approaches, the aftereffect of the stemming interaction may not be a genuine word or even change the word (and sentence) which means. To balance this impact you can alter those predefined strategies by adding or eliminating appends and runs, yet you should consider that you may be improving the presentation in one region while delivering a debasement in another. Continuously take a gander at the entire picture and test your model's exhibition. 



So if stemming has genuine restrictions, for what reason do we utilize it? As a matter of first importance, it very well may be utilized to address spelling mistakes from the tokens. Stemmers are easy to utilize and run quick (they perform straightforward procedure on a string), and in the event that speed and execution are significant in the NLP model, stemming is positively the best approach. Keep in mind, we use it with the goal of improving our exhibition, not as a sentence structure work out. 



Lemmatization 



Has the target of decreasing a word to its base structure and gathering various types of a similar word. For instance, action words in past tense are changed into present (for example "went" is changed to "go") and equivalents are bound together (for example "best" is changed to "acceptable"), consequently normalizing words with comparative importance to their root. Despite the fact that it appears to be firmly identified with the stemming cycle, lemmatization utilizes an alternate way to deal with arrive at the root types of words. 



Lemmatization settle words to their word reference structure (known as lemma) for which it requires definite word references in which the calculation can investigate and connect words to their relating lemmas. 



For instance, the words "running", "runs" and "ran" are on the whole types of "run", so "run" is the lemma of the relative multitude of past words. 



Lemmatization additionally thinks about the setting of the word to tackle different issues like disambiguation, which implies it can segregate between indistinguishable words that have various implications relying upon the particular setting. Consider words like "bat" (which can compare to the creature or to the metal/wooden club utilized in baseball) or "bank" (relating to the monetary organization or to the land close by a waterway). By giving a grammatical feature boundary to a word ( regardless of whether it's anything but a thing, an action word, etc) it's feasible to characterize a part for that word in the sentence and eliminate disambiguation. 



As you would effectively envisioned, lemmatization is a considerably more asset serious assignment than playing out a stemming interaction. Simultaneously, since it requires more information about the language structure than a stemming approach, it requests more computational force than setting up or adjusting a stemming calculation. 



Theme Modeling 



Is as a technique for uncovering covered up structures in sets of writings or reports. Basically it bunches writings to find inactive points dependent on their substance, preparing singular words and doling out them esteems dependent on their appropriation. This strategy depends on the suspicions that each report comprises of a combination of subjects and that every point comprises of a bunch of words, which implies that on the off chance that we can recognize these secret themes we can open the significance of our writings. 



From the universe of subject demonstrating procedures, Latent Dirichlet Allocation (LDA) is likely the most normally utilized. This moderately new calculation (imagined under 20 years prior) fills in as a solo learning technique that finds various themes fundamental an assortment of reports. In unaided learning strategies like this one, there is no yield variable to direct the learning cycle and information is investigated by calculations to discover designs. To be more explicit, LDA discovers gatherings of related words by: 



Allotting each word to an arbitrary theme, where the client characterizes the quantity of points it wishes to uncover. You don't characterize the actual points (you characterize only the quantity of subjects) and the calculation will plan all reports to the themes such that words in each record are for the most part caught by those fanciful subjects. 



The calculation goes through each word iteratively and reassigns the word to a theme taking into contemplations the likelihood that the word has a place with a point, and the likelihood that the report will be produced by a subject. These probabilities are determined on numerous occasions, until the intermingling of the calculation. 



Dissimilar to other bunching calculations like K-implies that perform hard grouping (where themes are disconnected), LDA appoints each record to a combination of subjects, which implies that each archive can be portrayed by at least one points (for example Archive 1 is depicted by 70% of point A, 20% of theme B and 10% of subject C) and reflect more sensible outcomes.

---
## [prachi-agrawal1179/Hackerearth-Problems](https://github.com/prachi-agrawal1179/Hackerearth-Problems)@[6363c1f48a...](https://github.com/prachi-agrawal1179/Hackerearth-Problems/commit/6363c1f48a9c53520c61898eb135226c74e75171)
#### Tuesday 2021-07-13 05:56:35 by Prachi Agrawal

Akash's Girlfriend

Akash singh is a student of Mathematics at Geekland University. These days he is busy with his girlfriend Jassi. On the other hand, Jassi don't like mathematics that much. One day, Jassi decided to find all the strings of length N (comprising only of characters from '0' to '9') having odd number of 0's. For Example: 103,012,000 are all strings of length 3 having Odd number of 0's. She asked Akash to find number of such strings of for a given length of string modulus 1000000009 (10^9 + 9). Akash being busy in organizing college fest asks for your help. Help Akash impressing his girlfriend.

---
## [Aeneon/TDK](https://github.com/Aeneon/TDK)@[009bf3d067...](https://github.com/Aeneon/TDK/commit/009bf3d067edb11f188ef1dd8519f2a518637eda)
#### Tuesday 2021-07-13 06:14:17 by Avalon Studios

Icon for ActionMenu â€¦

Later this Year I'll work on my iPad 2 - just for nostalgic Reasons, because I love this Skeuomorphic UI, which existed back in the old iOS Days. Thank you Scott â€¦

---
## [Hashitha0602/STAY-FIT](https://github.com/Hashitha0602/STAY-FIT)@[23e0e213d1...](https://github.com/Hashitha0602/STAY-FIT/commit/23e0e213d151d9808af9fe1dbc657ffb58016f28)
#### Tuesday 2021-07-13 09:51:12 by Sai Hashitha

Add files via upload

Introduction:
StayFit is a web application for analyzing and logging strength training and body building data. StayFit has two different portals i.e exercise portal and recommendations portal. Apart from this, it also includes an admin portal. Exercise portal aims to present workout data in a way that highlights and encourages progressive overload and long term commitment. There are many workout tracking apps already available, what sets this one apart is the focus on data. If you want a workout application that guides you through a routine this isn't the tool for you. Exercise portal hasno server and works completely offline using IndexedDB. On the other hand, if you manage the routine yourself and track your workouts in a spreadsheet with a bunch of formulas and charts, you're probably the kind of person who would like this application. Recommendations portal provides 3 different subscription plans (Basic, Standard, Premium) where the user can choose their plan according to their requirement. These plans provide basic workout tutorials, workout recommendations and food recommendations according to the userâ€™s weight. These plans recommend a particular type of workout according to his/her weight and the same follows with food recommendations i.e., for Breakfast, Lunch and Dinner based on the calorie intake.

Features:

Simple workout data entry supporting reps, sets, weight, duration, warmup and failure sets
Musculature visualizations
Intensity and volume calculations
Import from CSV
Export to JSON
Track and visualize personal records
Exercise search
Punchcard graph
Supports multiple workouts per day
Proper time zone support (important for people who travel)
Workouts portal works offline, no network connection required
Lots of charts
Provides different subscription plans for users (Basic, Standard, Premium), paid through payment gateway (PayPal).
Basic plan provides workout tutorials
Standard plan provides workout recommendations including features of basic plan.
Premium plan includes food recommendations including features of standard
A chatbot for conducting an online chat conversation via text.
Admin portal for providing recommendations on daily basis.
Lightbox extension for larger view of pictures and gifs.
Ngrok for temporarily hosting a webpage(https://ngrok.com/docs).
3 modules:

Exercise portal
Recommendation's portal
Admin portal
Software Requirements:
-> Frontend :- HTML,CSS,JS,JQuery
-> Backend :- PHP
-> Styling :- Bootstrap
-> Database :- MySql

Hardware Requirements: -> Operating System :- Windows/Mac/Linux
-> Browser which supports javascript and Indexed DB (Browser Database)

Step-wise instructions:

Download the zip file of this repository
Unzip the folder
Place the folder into htdocs of xampp folder (C:\xampp\htdocs)
Open database/admin.sql file
Import it to the wamp/xampp folder (Note: Username for phpmyadmin is â€˜rootâ€™; password is â€˜â€™)
Opening file for the project is index.php
Exercise portal and recommendations portal can be accessed from the same page.
To login as admin, use initial credentials (Note: Username is admin, password is admin)
Logout will directly redirect the user as well as admin to home page
dbconfig.php contains the data for the connection of the localhost to the database of phpmyadmin. Login credentials for phpmyadmin can be restructured there as well.

---
## [bass-assembler/bass-history](https://github.com/bass-assembler/bass-history)@[d9fc9163ff...](https://github.com/bass-assembler/bass-history/commit/d9fc9163ffad1b138bd5dcf70573ddbb04ec5e4c)
#### Tuesday 2021-07-13 10:50:18 by Near

Update to bass v02r11 release.

Adds updated size detection code to snes-cpu and snes-smp.
I also gave up on the x86 assembler. The ISA is just too insanely bloated and complex. Made it to opcode 0xa4 at least (~35% sans FPU/MMX/SSE*.)
Gave up after hitting AccumulatorOffset vs RegisterEffective. How can you tell the difference between these two?
mov eax,[eax] or mov eax,[bx+si]
mov eax,[label]
You have to basically try decoding both completely, and on error go to the next one.
No simple wildcards. Not even regular expressions would practically do the job, they'd be 4,000+ bytes long per regex.
And even then, there are dozens of cases where the exact same instruction can be encoded two different ways.
There are lots of these with the accumulator, eg add eax,eax can be one byte, or it can be multi-byte ala add eax,ebx.
The whole design of having a table of opcodes to run through just isn't practical on this ISA.
It seems the only way to do it would be to have a list along the lines of:
string name;
vector<vector<type>> operand;
Run through the opcode, and every , counts as a separate operand.
Each operand would get a list of "possible" modes that it could represent:
* can be accumulator
* can be register
* can be offset
* can be effective
* can be ptr:offset
For many of these, you'd need additional information. Eg lea eax,[eax] is fine, ala RegisterEffective encoding. But lea eax,eax is not, even though that's a valid encoding for it, so we subdivide more: can be effectiveAddress ( [eax], [eax+ebp*8+disp8] ), can be effectiveRegister ( eax ), etc. Then there are the types for the ModR/M encoding: is the register from the 32-bit set? (eax, ebx, ecx ...) Or from the 16-bit set? (ax, bx, cx, ...) Or from the MMX set? (xmm0, xmm1, ...) How about effective addressing? Is it 32-bit? ([eax]) 16-bit? ([bx+si])
Once done, you'd basically need a huge function that qualifies every possibility, one by one. And this would have to be manually sorted to pick the shorter encodings when possible, rather than in the structure of a table. I don't even think a resorted table could pull off some of the edge cases.
Next, we have the same problem on every arch with not being able to pick minimal sizes of operands, but with x86 it's really, really bad to mess this up.
jmp byte is 2 bytes vs jmp dword being 5.
nasm and yasm are infinite-pass assemblers that keep recoding to pick the minimal forms. bass is not, so you have to manually declare this information.
This makes them more desirable to use, and the complexity would bog down bass' simplicity.
Then there's the point that bass is targeted as a binary patcher. But x86 binaries are always in fucked up complicated containers for OS loaders. Windows has PE, Linux has ELF, etc. Without parsing those, you have to do your own address translation via org/base, and have to manually target sections. And assembling your own binary from nothing wouldn't be practical at all (good luck building a PE header by hand.)
And then there's amd64 extensions. My math parser is 32-bit, as using 64-bit integers for math is a lot slower. I'd have to extend that, and convert every unsigned type to uint64_t or more. Then there's dealing with all the added complexity: REX extensions to the SIB extensions to the ModR/M extensions, etc.
And lastly the fact that nobody's probably ever going to use it ...
Well, it was fun to learn how the encoding structure works. I think I can use this knowledge to refine nall/detour, but my initial plan (detour = disassemble + reassemble with bass) was not really practical, as the amount of code necessary to assemble and disassemble x86 is an order of magnitude larger than to do the same for SNES CPU/SMP.
But honestly even detour.hpp doesn't seem all that useful to work on. I only really ever hook Windows kernel32/user32 functions, and those all have standard WINAPI function prologues already, so the minimalist implementation I have is enough.
And then there's the matter of how often would I even use it? I rarely hack other applications anymore. It's still a fun idea to eventually do one day, but I'm thinking it doesn't make a lot of sense to do that today. So ... fork this off as part of header-magic's code rather than a part of nall, and call it a day for now.
That frees me up to post bass v03, and get back to work on bsnes/phoenix stuff.

---
## [bass-assembler/bass-history](https://github.com/bass-assembler/bass-history)@[d4ae8e605a...](https://github.com/bass-assembler/bass-history/commit/d4ae8e605afd5c0fe393135fabb5d84146a48f6f)
#### Tuesday 2021-07-13 10:50:18 by Near

Update to bass v00 release.

bass (as in treble) is essentially xkas v15+. It's basically a clean rewrite (took seven hours to write), meant to correct design mistakes made from the previous version, clean things up some more, and change some design patterns.
The syntax is 99% identical to xkas v10+, with the exceptions of missing fillto (complicates things by requiring bidirectional address translation, use fill instead), and loadpc/savepc (may re-add them, but they are redundant thanks to mutable defines.) And of course, to be consistent with bsnes, the "header" option has been removed. Add it yourself if you want it.
The main internal change is that it now uses C++0x throughout, uses an upgraded RDP that allows custom handling for unrecognized tokens (cuts core size in half), and it no longer tries to merge all supported architectures into a single source file. While it's a nice idea in theory, it makes the arch modules a pain in the ass to write. The new design is based on archs inheriting the Bass class and extending it. This gives more flexibility and results in cleaner code.
And of course, the compelling reason to upgrade ... it has macro support.
Given that it's brand new, there's bound to be some evil things you can do to exploit the grammar. I don't care too much about that right now. I'm mainly interested in bugs with what should be legal syntax. Testing help would of course be appreciated :)
Example:
Code:
	mapper lorom
	org $008000; fill $8000

	define pushall php; rep #$30; pha; phb; phd; phx; phy
	define pullall rep #$30; ply; plx; pld; plb; pla; plp

	macro square(n)
	  {n}*{n}
	endmacro

	macro fill(addr, length, byte)
	  //you will need to use +/- labels or hard-coded branches for relativity
	  lda.b #{byte}
	  ldx.w #{addr}
	  ldy.w #{length}
	  -; sta $0000,x; dey; bne -
	  rts
	endmacro

	org $008000
	  dw {square(8)}
	  {fill($4000, $2000, $ff)}
	  incsrc "test/include.asm"
	  incbin "test/include.bin"
	  {pushall}
	  {pullall}
You'll notice that defines no longer require quotes for multi-block values. This is because both macros and defines use a line-based parser. It means you can't declare multiple new defines on the same line (why would you?), but it gives a much nicer syntax in return.
Macros are basically super defines. When you call fill($4000, $2000, $ff), it will actually set the defines for {addr}, {length} and {byte} prior to inlining the macro into your source code. The reason for doing this is unified syntax. Both defines and macro parameters only need one specialized syntax: {name}
If you are worried about conflicts (eg you use really generic global define names), I'd suggest prefixing your macro arguments yourself, eg:
Code:
	macro test(.x, .y)
	  lda {.x}+{.y}
	endmacro
Now, {square 8} looks nicer than {square(8)}, but it leads to an ambiguity when you have a no-argument macro and a define by the same name. Consider the syntax temporary for now.
Suggestions are welcome.

---
## [bass-assembler/bass-history](https://github.com/bass-assembler/bass-history)@[1e12d6fc63...](https://github.com/bass-assembler/bass-history/commit/1e12d6fc63b5e9a44f285a48d5a3e73b14e60c62)
#### Tuesday 2021-07-13 10:50:18 by Near

Update to bass v02r08 release.

Updated the documentation. Defines are now argumentless macros instead of redefinable labels.
The reason for doing that is because labels are meant to be used before being defined, and thus cannot be redefined later.
By trying to shoehorn defines onto them for the nicer {}-less syntax, it allows the same undeclared define to go beyond the first pass.
Anyway, as a bonus for having to use {}, the evaluation happens post-expansion, allowing for the "lda #$00" -> byte-size detection to work, whereas lda #offset's size cannot be determined, as offset has already been evaluated to a raw integer.
Code:
	mapper lorom
	org $8000; fill $8000; org $8000

	macro add n; clc; adc {n}; endmacro
	macro sub n; sec; sbc {n}; endmacro
	macro addsub x,y
	  {add {x}}
	  {sub {y}}
	endmacro

	define offset $7efff8 + 8
	define length $20
	define 'A' 0x61
	define 'B' 0x62

	main:
	  {addsub #$20,#$40}
	  lda {offset}
	  {add #{length}}
	  db "ABCD"
	  jml main

	if {pc} >= 0x8040
	  error "Out of space! PC = {pc}"
	elseif {pc} >= 0x8020
	  warning "Almost out of space! PC = {pc}"
	endif

	macro m1 x
	  macro m2 y
	    {x} {y}
	  endmacro
	endmacro

	{m1 lda}; {m2 #$ff}

	macro test; nop #2; endmacro; {test}
The last idea I was considering was allowing more than one seek with +/- labels. Before with xas, I used to have +/-/++/--/+++/---. The syntax looked really ugly in code usage, however.
What I was thinking was that we define them with + and - only, but when evaluating them, allow ++ to mean (go to the second + forward) and -- to mean (go to the second - backward). Stupid example:
Code:
	-;
	  lda {bitpos}; tax
	  lda $2000,y
	  -; beq +; lsr; dex; bra -; +
	  sta $2000,y
	  dey; bne --
Agreed? How many should we reasonably allow? I was thinking anything more than three (+++/---) would be insane. I liked the +1 and -2 syntax, but those are also valid integer expressions for the math parser. I'd have to use a unique markup for them (like $ for hex and % for decimal) if we were to evaluate these inside of the math parser's tokenizer. But I don't see a lot of value in that. Only really obvious one would be something like:
Code:
	dw ++ - +; +; incbin data.bin; +
Which is probably better expressed anyway as:
Code:
	dw data.end - data; data: incbin data.bin; .end:
If we wanted the added flexibility, we'd have to use {+} and {-}.
Code:
	dw {++} - {+}; +; incbin data.bin; +
	dw {+2} - {+1}; +; incbin data.bin; +  //this could work too now
	-; lsr; dex; bne {-}
Is that worth it? Labels don't really use {} otherwise, so it's a bit unorthodox.

---
## [bass-assembler/bass-history](https://github.com/bass-assembler/bass-history)@[4bf9795fd5...](https://github.com/bass-assembler/bass-history/commit/4bf9795fd55d75b1d7f73ad759bb4ffce2f36f16)
#### Tuesday 2021-07-13 10:50:18 by Near

Update to bass v02r06 release.

Testing would be appreciated this time.
So, defines now declare values rather than expressions. Macros are now used instead.
Macros are also parsed after conditionals now, so you can modify macros based on conditions; which to me, is more useful than creating (not using) conditionals inside of macros.
They lack the specialized single-line form (unless you add endmacro to the end), but are now parsed at the block level.
As such, macros can be nested. God help us.
Also added warning and error. print was updated to match their syntax. They only take a single string argument now, and can expand {pc} to the program counter (origin+base). Conditionals can use {pc} as well.
Lastly, change enqueue pc and dequeue pc to pushpc and pullpc. If we ever need to save/restore something else, we'll update the syntax to be more expressive. Made this change because print pc is no longer valid, so it didn't match syntactically.
Example file macro.asm:
Code:
	mapper lorom
	org $8000; fill $8000; org $8000

	macro add n; clc; adc {n}; endmacro
	macro sub n; sec; sbc {n}; endmacro
	macro addsub x,y
	  {add {x}}
	  {sub {y}}
	endmacro

	define offset $7efff8 + 8
	define length 32
	define 'A' 0x61
	define 'B' 0x62

	main:
	  {addsub #$20,#$40}
	  lda^ offset
	  {add #length}
	  db "ABCD"

	if {pc} >= 0x8040
	  error "Out of space! PC = {pc}"
	elseif {pc} >= 0x8020
	  warning "Almost out of space! PC = {pc}"
	endif

	macro m1 x
	  macro m2 y
	    {x} {y}
	    nop #2
	  endmacro
	endmacro

	{m1 lda}
	{m2 #$ff}
Oh, you'll get a warning if you don't close a macro, or close one that is not opened. Same as with if/endif.
Now, one major catch here, not sure how to avoid it.
The way my parser works is that macros can be expanded from one block into one or more blocks.
Example: {add #$20} is one block, but becomes clc; adc #$20, which is two blocks.
So what I do is evaluate all macros at the start of each line. So what happens when we try: {m1 lda}; {m2 #$ff} ?
It becomes: macro m2 y; lda {y}; endmacro; {m2 #$ff}. m2 has not been expanded on this line. So even though we end up declaring m2 before hitting {m2} ... it hasn't been expanded because the expansion process didn't know about it yet.
I don't have any easy way around this. The only real solution would be something like ... evaluate at the block-level, and further sub-divide each block. This would mean something like a recursive call to assembleBlock. Meaning, we'd be crushing the stack. With a 1MB stack, it wouldn't be too difficult to crash the assembler by nesting a couple hundred of these expansions onto a single line. And there's no way I can stop that.
It would be far safer for the rule to be: macro declarations must appear on their own lines. But yeah, that's still a restriction that nothing else has.

[No archive available]

---
## [bass-assembler/bass-history](https://github.com/bass-assembler/bass-history)@[2944d42f92...](https://github.com/bass-assembler/bass-history/commit/2944d42f929e7af846f3a47e8b8e2d1d7244bb74)
#### Tuesday 2021-07-13 10:50:18 by Near

Update to bass v02r04 release.

All opcodes from 0x00-0x81 implemented. I don't know what the difference is between 0x80 and 0x82, ndisasm thinks 0x82 is an invalid opcode in fact.
imul is now the most fucked opcode of all:
Code:
	lock imul eax,[es:ebx*8+ebp+0x01234567],0x89abcdef  //and you thought lda ($addr,s),y was insane
This is why CISC is bad. I still don't even know what the hell segment selectors do in 32-bit protected mode ...
Also simplified the addressing modes as much as I could. The ExactSize values can only really apply to immediates, and really only benefit those 8-bit sign-extended values.
Updated the effective address decoding to validate the operand size when not using indirect addressing.
Most move instructions in, those were the big ones for hooking. And now I get to figure out the Sreg variants of mov and lea.

---
## [martin506/swordigo-2](https://github.com/martin506/swordigo-2)@[e90f8e2d6c...](https://github.com/martin506/swordigo-2/commit/e90f8e2d6cc29ce0d1395817a3be7071fedaa3a9)
#### Tuesday 2021-07-13 15:47:57 by Martynas Simanavicius

fuking stupid ass shit kodas tai susiktai save sistemai yra

well well well the code for save and load system has been written, now I am off to drwing all that shit

---
## [boyney123/evb-cli](https://github.com/boyney123/evb-cli)@[fa72d0ad10...](https://github.com/boyney123/evb-cli/commit/fa72d0ad1084bab0217d9b2538ac69888b0b9c92)
#### Tuesday 2021-07-13 16:57:28 by David Boyne

Landing page built with Gitpages

Hey,

I created this project a landing page using Gitpages, which you can watch here: https://twitter.com/boyney123/status/1414991347040374790

Here is a demo of the page in action: https://www.gitpages.app/examples/env-cli.html

Just wondered if you would like the page on the project or not?

Up to you.

You can find out how to use GitHub Pages to host the file if you wanted too: https://docs.gitpages.app/docs/guides/hosting#step-2---setup-github-pages

Would love to know your thoughts or feedback!

Cheers,
Dave

Cheers,
Dave

---
## [coornio/ALUI](https://github.com/coornio/ALUI)@[279035a9ff...](https://github.com/coornio/ALUI/commit/279035a9ff54200e9f191de5fd536180924d89de)
#### Tuesday 2021-07-13 17:29:24 by Î£Ï„Î­Ï†Î±Î½Î¿Ï‚ "Coornio/8924th" Î’Î»Î±ÏƒÏ„ÏŒÏ‚

THERE IS SO MUCH SHIT DONE I LOST TRACK

Don't be like me, messing around with everything and not making commits in a timely manner. Good god. Let's start with media-related changes.

For the fonts, fixed the outline thickness of "mono" and added an outline variant, as well as a "webdings" font, also with outline variant. 

For the UI icons, I just dumped some stuff that wasn't actually in use to clean up a bit. 

Now back to actual code changes! 

postfx.cfg : Just an addition of an XY variant of blur3 and blur5 shaders.
stdedit.cfg: Moved `vdirdelta` here instead. Name change to blend paint mode.
stdlib.cfg: A whole lot of cleaning up vars. A bunch of them were removed, some were relocated/renamed. Blur postFX is now toggleable, as it does eat into frame times quite considerably. I'm no shader newbie even, pls halp

helpers.cfg: Just comment fixes.
common.cfg: Added `--` and `++` convenience functions, with built-in clamping too. Added `subdivlist` to break up a big list into smaller lists, as the name implies. Minor comment adjustments and cleanups.
colorconvert.cfg: Added `|A` convenience function to add a transparency value to an RGB24. Makes for cleaner code. Fixed value flooring in the RGB to HSV converter.

_scoreboard_old.cfg, controls.cfg, scoreboard.cfg, editstats.cfg: Minor color value updates.
editvars.cfg: Removed redundant Y definition.
/hud/recorder.cfg: this was duplicated by accident and thus removed.
dialogs.cfg: Fixed the var name for map loading. Again.

main.cfg: Adjustment of KBhook UI to include stack debugging. Not actually important for end users. Also adjusted to reopen itself if it happens to close.
options.cfg: Minor cleanups, added checkbox for postFX blurring.
player_setup.cfg: Cleanups to var usage, nothing much. 
/menus/recorder.cfg: Now adjusted to use the new advanced fields. Added indicator of when a recording is already happening. Cleaned up useless code, no longer resets resolution multiplier if recording. Fixes for the name of the recording.
server_browser.cfg: Massive code cleanups, overhaul of the server entry code, cleaner visuals.

mapmodel_browser.cfg, map_browser.cfg: Massive overhaul to code, doing away with the old scroll box and opting for a paged approach instead. Due to the way the UI works, this is massively more efficient as there's less elements to render at once. Scrolling is hardwired to work like it did before.  Massive variable cleanups as well.
Going forward, this overhaul will come to the texture browser and prefab browser as well.

style.cfg: Added custom postFX shader for blurring. More efficient for the intended purpose overall. Cleaned up color var usage across the entire codebase, introduced some new ones, and renamed a few others. Things are more consistent now. Cleaned up logic for pulsating outlines.
lib.cfg: Tidied up the legend and comments all around, some parts still need adjusting. Cleaned up code for titlebar and its buttons.  Direct use of style options where possible instead of calling `UIbox` for them for efficiency. Cleaned up `UIfastimg`,`UItriangle`, `UIcheckbox`, `UIradio`, `UIkeybox`. `UIhoversound` was broken up, the main function turned into `UIhoveronce`, with `UIhoversound` now calling upon it. Added `UIscrollsound`. Comments added for `UIsinwave`. Removed `UImillisreset` since `UIhoveronce` basically does what it was meant to do.  `UIsetbgblur` now checks its master switch. and calls the new custom blur. Color var updates across the board. `UIcaret` and its new `UInumcaret` sibling moved to a new category at the bottom.  New `UIadvfieldRAint` variation of the field series added. Mostly complete, but direct number input and BACKSPACE/DELETE do not work as intended. Introduced new vars to better handle field requirements, preparing for future float field. Adjusted `.UI_KBinput` to be more efficient overall. Added new code to handle inputs for the INT field.

---
## [chapel-lang/chapel](https://github.com/chapel-lang/chapel)@[a6a8ed23e8...](https://github.com/chapel-lang/chapel/commit/a6a8ed23e80abc934aac3fadc202e27c50e8824e)
#### Tuesday 2021-07-13 17:45:55 by Engin Kayraklioglu

Merge pull request #18046 from e-kayrakli/add-foreach3

Implement the `foreach` loop

This PR implements the `foreach` loop as proposed in
https://github.com/chapel-lang/chapel/issues/16404.

Co-developed with @mppf
Supersedes his https://github.com/chapel-lang/chapel/pull/17014

#### Background
`foreach` loops are the user facing way of implementing sequential yet
vectorizable loops. Before this PR, the vectorization support relied on non-user
facing pragmas to be attached to the iterator symbols. With this PR, those
pragmas are no longer necessary if the iterator uses `foreach` loops.

#### Implementation
The implementation is rather straightforward. `foreach` loops are just
`ForLoop`s that are made order independent from the time they are created.
`foreach`-ness of the loop is not stored in the `ForLoop`'s interface.

Similar to what we have today, a `ForLoop` can be made order independent during
resolution if it consists of only a yield and iterates over something that is
also order independent. If that happens to a `for`, after that point it is
indistinguishable from a `foreach` for the rest of the compilation.

Note that we still need the pragma `order independent yielding loops` (and it's
inverse, just for completeness (?) ) to mark iterators that uses non-`for` loops
as order independent. However, with this PR, flags corresponding to those
pragmas are no longer added by the compiler. Instead, the compiler choses
between `for`/`foreach` when writing iterators for `forall` expressions, for
example.

#### Future work
- We need to implement `foreach` intents. (It appears we'll also add `for`
  intents https://github.com/chapel-lang/chapel/issues/17857)

- @bradcray has made a good point that intent implementations for
  `for`/`foreach` and `forall` may end up to be sufficiently different.
  Depending on what we see then, we could think of creating a `ForeachLoop` AST
  node.

- Do we want `foreach` expressions? Currently they fail with a generic syntax
  error.

- We should probably beef up testing, especially for the standard/internal
  module iterators that now use `foreach` to make sure that they are vectorized
  as appropriate. (I can add some more testing in this PR, but maybe not
  exhaustively)

[Reviewed by @mppf]

#### Testing

- [x] release/examples with `--fast`
- [x] standard
- [x] gasnet

---
## [mamedev/mame](https://github.com/mamedev/mame)@[ca6df411d9...](https://github.com/mamedev/mame/commit/ca6df411d99a0c7e816c375cbe19e92cb10ef10a)
#### Tuesday 2021-07-13 18:51:19 by Firehawke

Apple softlist updates for July 2021 (#8293)

* New working software list additions (apple2gs_flop_orig.xml)
------------------------------------------------------------

Pipe Dream [4am, Firehawke]
ShowOff (Version 1.1) [4am, Firehawke]

* New working software list additions (apple2gs_flop_orig.xml)
------------------------------------------------------------

Calendar Crafter (Version 1.2) [4am, Firehawke]
Designer Prints (Version 1.0) [4am, Firehawke]
Designer Puzzles (Version 1.0) [4am, Firehawke]
Mercury (Version 1.0) [4am, Firehawke]
Storybook Weaver [4am, Firehawke]
Storybook Weaver: World of Adventure (Version 1.0) [4am, Firehawke]
Storybook Weaver: World of Make-Believe (Version 1.0) [4am, Firehawke]

Title correction on "Storybook Weaver" from apple2gs_flop_misc software list.

* New working software list additions (apple2_flop_orig.xml)
----------------------------------------------------------

Prince of Persia (800K 3.5") [4am, Firehawke]
Woolly's Birthday (Version 1.0) (800K 3.5") [4am, Firehawke]
Troll Sports Math (800K 3.5") [4am, Firehawke]

New working software list additions (apple2_flop_clcracked.xml)
---------------------------------------------------------------

Astro Attack (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Spell Master (Version 1.3.1) (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Dinner on a Disk (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Micro Barmate (Version 1.0) (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Math Skills (Version 1.3) (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Fractions (Version 3.0) (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Perception (Version 3.1) (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
LogoMotion (Version 1.2) (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Cross Country Rallye (Softsmith) (cleanly cracked) [4am, san inc., Firehawke]
Algebra I (Version 1.3) (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Bugbyter (Version 1.11) (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Midnight Malady (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Mummy's Curse (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Lazer Maze (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Zenith (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Supermap (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Guardian (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Game of the U.S. (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
L.A. Land Monopoly (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Cosmic Combat (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
The Game Show (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Creature Venture (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]
Lazer Silk (SoftSmith) (cleanly cracked) [4am, san inc., Firehawke]

* Software list items promoted to working
---------------------------------------
apple2_flop_orig.xml: Aliens [4am, Firehawke]
apple2_flop_clcracked.xml: Aliens [4am, Firehawke]

* New working software list additions (apple2_flop_clcracked.xml)
---------------------------------------------------------------

Disk Director (SoftSmith) (cleanly cracked) [4am, san inc, Firehawke]

New working software list additions (apple2gs_flop_orig.xml)
------------------------------------------------------------

Paperboy [4am, Firehawke]
Columns GS [4am, Firehawke]
Snoopy's Reading Machine [4am, Firehawke]

* Software list items promoted to working
---------------------------------------
apple2_flop_orig.xml: 2400 A.D. [4am, Firehawke]

* Correct Fantavision (Apple IIgs) and Print Shop IIgs compatibility information.

* New working software list additions (apple2_flop_clcracked.xml)
---------------------------------------------------------------

Pro Football Pointspread Prediction System (cleanly cracked) [4am, Firehawke]
Stickybear Drawing (cleanly cracked) [4am, san inc, Firehawke]
Bumble Games (Version 1.4) (cleanly cracked) [4am, san inc, Firehawke]
Beach Landing (cleanly cracked) [4am, san inc, Firehawke]
Temple of Apshai rev. 4 (cleanly cracked) [4am, san inc, Firehawke]
Adventure Double Feature Volume II (cleanly cracked) [4am, san inc, Firehawke]
Moptown Parade (1981 Version) (cleanly cracked) [4am, san inc, Firehawke]
Crush, Crumble and Chomp! (1981-06-22 Version) (cleanly cracked) [4am, san inc, Firehawke]
Con-Putation (cleanly cracked) [4am, san inc, Firehawke]
Drinks on a Disk (SoftSmith) (cleanly cracked) [4am, san inc, Firehawke]
Friends or Lovers (SoftSmith) (cleanly cracked) [4am, san inc, Firehawke]
Little Speller (Version 2.0) (SoftSmith) (cleanly cracked) [4am, san inc, Firehawke]
The Final Frontier (Version 1.0) (SoftSmith) (cleanly cracked) [4am, san inc, Firehawke]
Farm Ledger Pro (cleanly cracked) [4am, Firehawke]
Print Your Own BINGO Plus (Version 01.23.88) (cleanly cracked) [4am, Firehawke]

New working software list additions (apple2gs_flop_orig.xml)
------------------------------------------------------------

Charlie Brown's ABC's [4am, Firehawke]

* New working software list additions (apple2_flop_orig.xml)
----------------------------------------------------------

Where in Europe is Carmen Sandiego? (800K 3.5") [4am, Firehawke]

* New working software list additions (apple2_flop_orig.xml)
----------------------------------------------------------

The Playroom (Version 1.0) (800K 3.5") [4am, Firehawke]
SuperPrint! (Version 1.4) (800K 3.5") [4am, Firehawke]
Rampage (800K 3.5") [4am, Firehawke]
Hegira: Tale of a Galactic Exile [4am, Firehawke]
Stickybear Math (800K 3.5") [4am, Firehawke]

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[50340a22ce...](https://github.com/mrakgr/The-Spiral-Language/commit/50340a22cec0a81921cb5353e6eb8c3d94ba59aa)
#### Tuesday 2021-07-13 18:54:08 by Marko GrdiniÄ‡

"10:55am. I slept poorly. Let me get over my groginess a little first and then I will start.

11:40am. I am almost ready to start now. But at this point in the day...well, I'll do a little.

12:05pm. I am gathering my thoughts.

If I exclude fusion optimizations and multiple heads, then the transformer would be easy to implement.

And that is what I should do.

12:10pm. A part of me is thinking how to do it all at once, but the strategically minded part of me is telling to avoid the difficulties and just get the Leduc to run first. That is the most important thing of all. Once deal with that hardship, I can look into making the module wide and fast.

12:20pm. Let me take a break here.

1:35pm. Done with breakfast. Let me chill a bit more. I want to read Rebuild World. On the outside I am just lazing about, but internally my tension is high and I want to run away from it all.

I am going to muster the urge to do the simplest kind of possible transformer and get it to work on Leduc. Once I've cleared that goal, then I can start thinking about what comes next.

2:05pm. Let me try to push a bunch of lines out. I just have to do the transformer, and maybe after that I'll try removing the actor and training a value net directly using noise keys. That is as far as I will go.

2:20pm. I still haven't started, let me do it. Today if necessary I'll do it till 9pm or longer. And I'll go to bed earlier.

Implementing the transforer is not difficult at all. I am near the end of the first part of my journey. The regular transformer is easy, and the multiheaded one is something I will be able to manage. I just need to visualize it vividly and it will flow out of me and into the code.

But before that, let me ask a question on the RL sub.

2:40pm. https://www.reddit.com/r/reinforcementlearning/comments/ojeqz6/would_it_be_viable_to_replace_the_actor_with/

Let me do a little research on my own as well.

https://www.google.com/search?q=noisy+q+learning&oq=noisy+q+learning

Oh there is stuff on Q learning in the presence of noisy rewards. That is exactly the problem with poker.

https://www.auai.org/uai2016/proceedings/papers/219.pdf
Taming the Noise in Reinforcement Learning via Soft Updates

https://core.ac.uk/download/pdf/227521246.pdf
Enhancing Performance of Reinforcement Learning Models in the Presence of Noisy Rewards

https://pythonhealthcare.org/2020/07/15/noisy-duelling-double-deep-q-learning-controlling-a-simple-hospital-bed-system/

3:25pm. https://arxiv.org/abs/2102.04376
Adversarially Guided Actor-Critic
> Despite definite success in deep reinforcement learning problems, actor-critic algorithms are still confronted with sample inefficiency in complex environments, particularly in tasks where efficient exploration is a bottleneck. These methods consider a policy (the actor) and a value function (the critic) whose respective losses are built using different motivations and approaches. This paper introduces a third protagonist: the adversary. While the adversary mimics the actor by minimizing the KL-divergence between their respective action distributions, the actor, in addition to learning to solve the task, tries to differentiate itself from the adversary predictions. This novel objective stimulates the actor to follow strategies that could not have been correctly predicted from previous trajectories, making its behavior innovative in tasks where the reward is extremely rare. Our experimental analysis shows that the resulting Adversarially Guided Actor-Critic (AGAC) algorithm leads to more exhaustive exploration. Notably, AGAC outperforms current state-of-the-art methods on a set of various hard-exploration and procedurally-generated tasks.

I am looking up references to the noisy nets paper.

3:25pm. Let me take a nap here.

https://arxiv.org/abs/1703.07608
Deep Exploration via Randomized Value Functions

I'll read this paper by Osband later as well.

I have a certain understanding of how the problem in front of me should be handled, I just need some time to digest it properly.

3:40pm. The AGAC paper is impressive. But my problem is not so much exploration, but working through the variance to just get to the point where it can do hand reading.

3:45pm. https://arxiv.org/abs/2105.04683
Deep Bandits Show-Off: Simple and Efficient Exploration with Deep Networks

https://arxiv.org/abs/2105.10699
Denoising Noisy Neural Networks: A Bayesian Approach with Compensation

https://arxiv.org/abs/2104.07495
Self-Supervised Exploration via Latent Bayesian Surprise

https://arxiv.org/abs/2101.02649
CoachNet: An Adversarial Sampling Approach for Reinforcement Learning

https://arxiv.org/abs/2106.03228
Distributional Reinforcement Learning with Unconstrained Monotonic Neural Networks

Let me take a nap for a while and then I will dive into these papers. Forget about games, manga and novels. If necessary I am going to leave all of that aside for the next few weeks until I implement the transformer and my noise-keyed value function ideas.

5:35pm. Let me do this reading. There is no way I am stopping at the usual time today. From here, and until I am finished with the transformers and then the Holdem agent, I am going to keep pushing.

But in return when the agent is playing and training, I am going to take a proper break. The first one in the past 6.5 years.

Some days ago, I had a lot of fun just reading during the day when the agent was training. That is the best way to live.

Let me go through these papers.

https://arxiv.org/abs/2105.04683
Deep Bandits Show-Off: Simple and Efficient Exploration with Deep Networks

2 pages in and this paper seems promising.

3/22. Oh this is something new. I might be able to make use of this.

6:10pm.

> In this scheme Thompson Sampling (TS) is implemented as in Algorithm 3, which underlines where TS promotes exploration by sampling from a distribution over model parameters Pn(Î¸). In principle this provides an elegant Bayesian approach to tackle the exploration-exploitation dilemma embodied by contextual bandits. Unfortunately, representing and updating a posterior distribution over model parameters Pn(Î¸) exactly becomes intractable for complex models such as deep neural networks.

> To obviate this problem, several techniques that heuristically approximate posterior sampling have emerged, such as randomly perturbing network parameters [20â€“22], or bootstrapped sampling [23]. Within the scheme of Algorithm 2 the role of random perturbation and bootstrapped sampling are to heuristically emulate the model sampling procedure promoting exploration in the PREDICT subroutine (see TS Algorithm 3). However, systematic empirical comparisons recently demonstrated that simple strategies such as epsilon-greedy [17, 24] and Bayesian linear regression [25] remain very competitive compared to these approximate posterior sampling methods in deep contextual bandit. In particular, [18] showed that linear models where the posterior can be computed exactly, and epsilon-greedy action selection overwhelmingly outrank deep methods with approximate posterior sampling in a suite of contextual bandit benchmarks based on real-world data.

This paper is wonderful. It tells me quite a bit. In truth, I thought of tracking variance, but until now I had no idea how to use that for exploration. Instead of the noisy key idea that I had, I could just use the USB algorithm here to get a one-hot sampling distribution. That would be great!

The main gist of the idea is to take the variance and divide it by the weight again before taking the square. There is also a log factor over n.

6:10pm. I am starting to hear thunder outside. If it gets worse, I'll have to run.

6:45pm. Done with lunch. And I am also done with the SAU paper. It is great. I am in fact going to implement it instead of the noisy key idea. It is not that this could not work, but I feel like it will be a nightmare trying to tune it.

I am not an AI that I can peer inside and estimate how the scale and centering of the key affects the overall optimization. In the ML Reddit thread on GANs where the user found that adding Gaussian noise stabilizes optimization, he later chimed in to reply that the scale matters a lot.

SAU won't be a perfect solution, but it is a viable way to get exploration while having the sampling distribution be one hot.

6:50pm. I want to skip the NoisyNN paper, but let me just take a peek.

7:15pm. I spent way too long on that paper. Let me keep going.

https://arxiv.org/abs/2104.07495
Self-Supervised Exploration via Latent Bayesian Surprise

This one is next.

8pm. It does not seem there is an easy way to calculate the probability of an argmax of a vector of univariate gaussians.

https://arxiv.org/abs/2106.03228
Distributional Reinforcement Learning with Unconstrained Monotonic Neural Networks

I've gone through the other papers. I am barely even looking at them. The SAU paper caught my imagination completely.

https://arxiv.org/abs/1908.05164
Unconstrained Monotonic Neural Networks

Let me take a look at this and then I'll post the SAU paper on the RL sub. Maybe I got some replies too.

8:45pm. https://www.youtube.com/watch?v=FgmMK6RPU1c
Best Multi-Armed Bandit Strategy? (feat: UCB Method)

I'll watch this tomorrow, For over an hour now my eyes were just glazing over the papers. Yeah, just trying to plow on is not an effective studying strategy. I need to spread my effort out.

I might not have done any programming today, but finding the SAU paper was a very lucky find. Let me post it.

8:50pm. Let me turn off here as the thunder is finally here. I'll go to bed even if it is 5 hours earlier than usual."

---
## [UltraCakeBakery/dot-dollar-database](https://github.com/UltraCakeBakery/dot-dollar-database)@[0b32e39917...](https://github.com/UltraCakeBakery/dot-dollar-database/commit/0b32e399176505261554d72d98060a4728fcebb4)
#### Tuesday 2021-07-13 21:03:21 by Jack van der Bilt

test: shitty benchmark code to see how bad proxies perform

I am blown away by the performance difference. Up to 5x better performance when bypassing the proxies. This isn't an issue with my code however. Proxies in general have much worse performance, even with just a `return Reflect.get()` in the handler.get(). We definitely need to offer ways one day to either cache query builds or have some magical alternative be good enough for our needs.

---

# [<](2021-07-12.md) 2021-07-13 [>](2021-07-14.md)

