# [<](2021-04-27.md) 2021-04-28 [>](2021-04-29.md)

3,135,951 events, 1,534,165 push events, 2,481,058 commit messages, 196,278,482 characters


## [vlaci/nix-doom-emacs@d6d7b90e90...](https://github.com/vlaci/nix-doom-emacs/commit/d6d7b90e90afa8e51f4433f81a8d8b6f88a774e8)
##### 2021-04-28 07:55:20 by github-actions[bot]

test/doom.d/init.el: Updating from hlissner/doom-emacs - 193382e2

### Changes for test/doom.d/init.el

```diff
--- 
+++ 
@@ -111,7 +111,8 @@
 
        :lang
        ;;agda              ; types of types of types of types...
-       ;;cc                ; C/C++/Obj-C madness
+       ;;beancount         ; mind the GAAP
+       ;;cc                ; C > C++ == 1
        ;;clojure           ; java with a lisp
        ;;common-lisp       ; if you've seen one lisp, you've seen them all
        ;;coq               ; proofs-as-programs
@@ -124,6 +125,7 @@
        emacs-lisp        ; drown in parentheses
        ;;erlang            ; an elegant language for a more civilized age
        ;;ess               ; emacs speaks statistics
+       ;;factor
        ;;faust             ; dsp, but you get to keep your soul
        ;;fsharp            ; ML stands for Microsoft's Language
        ;;fstar             ; (dependent) types and (monadic) effects and Z3
@@ -138,9 +140,8 @@
        ;;julia             ; a better, faster MATLAB
        ;;kotlin            ; a better, slicker Java(Script)
        ;;latex             ; writing papers in Emacs has never been so fun
-       ;;lean
-       ;;factor
-       ;;ledger            ; an accounting system in Emacs
+       ;;lean              ; for folks with too much to prove
+       ;;ledger            ; be audit you can be
        ;;lua               ; one-based indices? one-based indices
        markdown          ; writing docs for people to ignore
        ;;nim               ; python + lisp at the speed of c
@@ -159,7 +160,7 @@
        ;;(ruby +rails)     ; 1.step {|i| p "Ruby is #{i.even? ? 'love' : 'life'}"}
        ;;rust              ; Fe2O3.unwrap().unwrap().unwrap().unwrap()
        ;;scala             ; java, but good
-       ;;scheme            ; a fully conniving family of lisps
+       ;;(scheme +guile)   ; a fully conniving family of lisps
        sh                ; she sells {ba,z,fi}sh shells on the C xor
        ;;sml
        ;;solidity          ; do you need a blockchain? No.
@@ -167,6 +168,7 @@
        ;;terra             ; Earth and Moon in alignment for performance.
        ;;web               ; the tubes
        ;;yaml              ; JSON, but readable
+       ;;zig               ; C, but simpler
 
        :email
        ;;(mu4e +gmail)

```

---
## [TheGrammarJew/Botto@3e40ab2912...](https://github.com/TheGrammarJew/Botto/commit/3e40ab29120d247baaa72c34fbdfbc92b11560f7)
##### 2021-04-28 12:14:07 by Evan

Really starting to see the evils of weak typing.
psutil passes a Process object to cpu_percent() if you call it from a Process object. Y'know, how that kind of thing works under-the-hood in OOP.
But it's (probably) static, and doesn't have a 'self' var. And it gives ZERO shits. no errors, no warnings, nothing.

What the fuck. I don't even.

---
## [datalad/datalad@7eaf389a8c...](https://github.com/datalad/datalad/commit/7eaf389a8c505f41e57db252d0442e6b16bbf9b8)
##### 2021-04-28 12:46:17 by Yaroslav Halchenko

Merge remote-tracking branch 'origin/master' into rf-eval-results

* origin/master: (45 commits)
  Updating mailmap
  TST: annexrepo: Adjust test for TestRepo change
  BF: annexjson2resulti() could not handle file=null results
  TST: no_annex: Avoid racy failure on adjusted branches
  BF: Pre-population of testrepos already needs HTTP test server setup
  RF: Move AutomagicIO functionality to -deprecated
  very basic test for completion helper
  RF: hello java my old friend, I've come to love your kludge again
  RF: Benchmark run needs -deprecated
  TST: Make test robust to moving `ls` command
  DOC: Remove `ls` from docs
  BF: Consolidate dry(-)run arguments and issue deprecation warning
  TST: Adjust tests of github helpers
  TST: Adjust for new reporting behavior
  BF: Let docs match actual behavior
  RF: Deprecate --dryrun in favor of --dry-run
  RF: Github helpers now report records to be able to continue in error
  ENH: remove a shadow from an attempt to have secrets in travis CI
  MNT: Post-release dance
  RF: Move otherwise unused safe_print() helper fpr ls() to -deprecated
  ...

Conflicts:
	datalad/distribution/create_sibling_github.py - took this
	datalad/interface/base.py - took this
	datalad/interface/ls.py - removed as well, not sure if ever would be tuned up in -deprecated

---
## [philjak/BookStack@aa6a752e38...](https://github.com/philjak/BookStack/commit/aa6a752e38b73eea201ea8e6a5b994a715e24aec)
##### 2021-04-28 13:45:41 by Dan Brown

Implemented custom select controls because apple hates web developers

They'd rather keep pushing their 2007 era strange form control styles
even though they're horribly outdated, ugly and hard to style. The
only way to override is a full nuking of the default styles, which means
we have to then implement the frigging arrow icon using hacks which would
then conflict with all other sensible browsers so we have to nuke their
styles aswell to ensure some stupid backgroud hack is used everywhere.

I bet apple don't even use their shite default control styles and nuke
them also, Lets see. Yup, First thing I see on the top of their homepage
is a locale select dropdown custom built from about 10 HTML elements. FML

For #2709

---
## [arangodb-helper/official-images@94a9070056...](https://github.com/arangodb-helper/official-images/commit/94a90700567fbb73dd4d52b4174feb234df53891)
##### 2021-04-28 16:05:41 by Guillaume Quintard

[varnish] overload the binaries with scripts

This is a follow-up of #9914. The now current approach works well EXCEPT
when people exec into the container with `bash` or whatever since the
entrypoint can't work it magic.

So, the blunt approach is to create a series of trampoline scripts that
will just add `-n /var/lib/varnish` before all arguments. It's not super
elegant, and I'm not a fan of overloading `PATH` but:
- it works transparently
- you can easily bypass the trampoline by using the binary's absolute
  path

I decided against generating the scripts from the `Dockerfile`s to avoid
escape hell, but I can be convinced otherwise if it makes reviews
easier.

---
## [GoldenAlpharex/Skyrat-tg@530112e09f...](https://github.com/GoldenAlpharex/Skyrat-tg/commit/530112e09f3190ffa84c90f941ee972b44509b53)
##### 2021-04-28 17:10:07 by death and coding

[modular][ready] formal wear for medical and engineering in loadout, gas masks unadded, but coded (#5156)

* for i just threw out the love of my dreams

* Update uniform.dm

* fuck

* biker

* Update glasses.dm

* I don't want any of my sprites in Skyrat. I appreciate you asking though.

* Revert "I don't want any of my sprites in Skyrat. I appreciate you asking though."

This reverts commit d980baf7ada95a04f74870103b2ee09ede67dcda.

* Revert "Revert "I don't want any of my sprites in Skyrat. I appreciate you asking though.""

This reverts commit ad42656117f90747c0aaf7ade3614a16d67fed4b.

* as requested

* casual cargo gear

Co-authored-by: louiseedwardstuart <bonniefluff>

---
## [ttnghia/cudf@8a666a04e0...](https://github.com/ttnghia/cudf/commit/8a666a04e0123744eb259d88ac4c04b0b6de4303)
##### 2021-04-28 19:17:52 by Vyas Ramasubramani

Refactor Python and Cython internals for groupby aggregation (#7818)

This PR makes some improvements to the groupby/aggregation code that I identified while working on #7731. The main purpose is to make the code logic easier to follow and reduce some unnecessary complexity; I see minor but measurable performance improvements (2-5% for small datasets) as well, but those are mostly just side effects here. Specifically, it makes the following changes:

1. Inlines the logic for dropping unsupported aggregations. The old function was repetitive and necessitated looping over the aggregations twice, whereas the new approach drops unwanted aggregations on the fly so it only loops once. The new code also makes it so that you only construct a C aggregation object once.
2. Merges the logic from `_AggregationFactory` into `Aggregation`, and removes the constructor for `Aggregation`. The one downside here is that the Cython `Aggregation` object's constructor no longer places it in a valid state; however, in practice the object is always constructed via either the `make_aggregation` function or its various factories, and the object's constructor was only every used in `_drop_unsupported_aggs` anyway. The benefit is we remove the fragmentation between these two classes, making the code much more readable, and the `Aggregation` class actually serves a purpose now beyond just providing a single property `kind` that is only used once: it is now the primary way that other Cython files interact with aggregations. This also means that in most places other Cython modules don't need to work with `unique_ptr[aggregation]` as much anymore (although they do still have to move `Aggregation.c_obj` for performance reasons). `make_aggregation` now returns the Cython class instead of the underlying C++ one.
3. Modified all the "allowed aggregations" sets to use the uppercase names of the aggregations. In addition to simplifying the code a tiny bit, this helps reduce confusion between the aggregation names used in Python for pandas compatibility and the libcudf names (for instance, `idxmin` vs `argmin`, now `ARGMIN`).
4. Explicitly defines all the aggregations on a groupby. I discussed this briefly with @shwina, the change has pros and cons. The benefit is that all of these methods are properly documented now, there's less magic (the binding of methods to a class after its definition can be confusing for less experienced Python developers and has a lot of potential gotchas), and we can use the simpler string-based agg definition wherever possible. The downside is that we now have to define all of these methods. I think the change is definitely an improvement, but I'm happy to change it back if anyone can suggest a better alternative. In the long run we probably need to find a way to share both code and docstrings more effectively between all aggregations (DataFrame, Series, and GroupBy).

Authors:
  - Vyas Ramasubramani (https://github.com/vyasr)
  - GALI PREM SAGAR (https://github.com/galipremsagar)

Approvers:
  - Karthikeyan (https://github.com/karthikeyann)
  - Ashwin Srinath (https://github.com/shwina)
  - GALI PREM SAGAR (https://github.com/galipremsagar)

URL: https://github.com/rapidsai/cudf/pull/7818

---
## [Fargowilta/FargowiltasSouls@e9f734d292...](https://github.com/Fargowilta/FargowiltasSouls/commit/e9f734d292de46053310992dbd4c59bc6b2dd92e)
##### 2021-04-28 19:26:45 by terrynmuse

fixed jungle ench having some sort of ghost jump even with toggle off?
FIXED EOW FUCKY WUCKY FLYING I THOUGHT I FIXED THIS AGES AGO FUCK
deviantt teleports at you whenever she's starting a strong attack
stardust cell shots are only minion type when emode ml is alive (mitigate funny crit swarming other bosses to death)
cultist instantly begins a pillar ritual the moment he goes below 50% life
cultist fragment duration buffed a LOT, persists for way longer

---
## [Fargowilta/FargowiltasSouls@10f20c6b09...](https://github.com/Fargowilta/FargowiltasSouls/commit/10f20c6b09d2a5bb02850f3cd247341a779b072f)
##### 2021-04-28 19:26:45 by terrynmuse

removed minimal presets option from mod config (since its mainly for soul toggles)
toggles minimal presets now enables:
 mining ench buffs
 trawler lures
 higher run speed
 tim's concoction
further decreased spawn rates of lunar pillar enemies post-1 mech (regular rates post golem)
soul of terraria sets your minion crit equal to your highest crit between melee/ranged/magic
skeletron v-spread covers much more space and has staggered accel, but front gap widened
destroyer temporarily has max speed when exiting coil (this is so he uncoils and isnt in your way as much)
The Lightning Rod nerfed to 28 damage (was 35, -20%)
nerfed gutted heart creeper base life to 40 (was 50)
boc p2 clone buffed, immune to knockback unless within a much closer range of you
moon lord p2 solar chain blasts no longer shorten during true eye deathray, but always much shorter (should be nerf overall)
champion of nature x1.5 life (huge buff)

---
## [mrakgr/The-Spiral-Language@0a70978cbb...](https://github.com/mrakgr/The-Spiral-Language/commit/0a70978cbbbed762a7e10b8bada8807bd600312c)
##### 2021-04-28 19:52:22 by Marko Grdinić

"9am. I am still thinking about it. I guess something must have clicked during Martens' lecture because I've realized that if I used the entire data set, I'd have no need to keep track of moving averages to get the correct grad modifier.

I've been thinking a lot of ideas on how to send rescale gradients to the other layers, but it never occured to me to think about what if I simply used a large enough batch size and then took the L1 norm of the gradients based on that. If I did that, I'd get invariance to the scale of the rewards automatically!

9:15am. This kind of modulation is extremely important. I would not even have to split the dataset and sample from it one at a time. The net would do the right thing on its own.

I would not have to hack the head to rescale the gradients it sends to the input. I can just take the most meaningful part of the second order methods just so I could make RL work.

Even with something like a single sample, I could keep a ratio at the top level. Or I could just track them intra layer.

I am thinking how to deal with RNNs and it is giving me a headache.

9:25am. RNNs are trouble. I really will need to estimate statistics separately for each of the layers should I decide to go down that route. But it is worth it.

I seriously have no idea why the method I have in mind is not used considering all the trouble with gradient propagation. It is not like rescaling the gradient updates using the local norm is hard. It makes a lot more sense than getting the variance after all the values have been added together like in Adam and RMSprop.

https://arxiv.org/pdf/2003.07845.pdf
PowerNorm - Rethinking Batch Normalization in Transformers

I really should just use a larger batch size. Let me read this paper. I should be able to learn something from it.

> The work of (Ioffe, 2017) proposed batch renormalization to remove/reduce the dependence of batch statistics to batch size. It was shown that this approach leads to improved performance for small batch training as well as cases with non-i.i.d. data. Along this direction, the work of (Singh & Shrivastava, 2019) proposed “EvalNorm,” which uses corrected normalization statistics. Furthermore, the recent work of (Yan et al., 2020) proposed “Moving Average Batch Normalization (MABN)” for small batch BN by replacing batch statistics with moving averages.

I should get familiar with this.

10:05am. This paper is really interesting. How did they derive the intermediate gradient update for PN? I do not understand where the equation comes from. In fact, something like this was something I've been wondering a while now - how to approximate the gradient through a moving average.

> Furthermore, the recent work of (Yan et al., 2020) proposed “Moving Average Batch Normalization (MABN)” for small batch BN by replacing batch statistics with moving averages.

I'd bet I'd find something on this subject in this paper.

10:35am. https://arxiv.org/abs/2001.06838
Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization

I can't figure out the approximate moving average update in power norm. I'd have to play around with it in order to understand it. Let me read this paper next.

https://arxiv.org/abs/1711.03953
Breaking the Softmax Bottleneck: A High-Rank RNN Language Model

This also caught my eye in the appendix. I am interested in what could replace a softmax.

10:40am. Wow MABN works even with a batch size of 1. Ok, I'll admit that there have been a few good things done since I was last active in ML. I should take the time to study batch norm properly.

https://www.reddit.com/r/MachineLearning/comments/jc1fp2/r_neurips_2020_spotlight_adabelief_optimizer/

Found this while Googling the sub for 'moving average'.

https://arxiv.org/abs/2010.07468
AdaBelief Optimizer - Adapting Stepsizes by the Belief in Observed Gradients

It got highly upvoted. Let me read it.

10:50am. This is actually pretty great. Rather than obsessing about my own thing, I feel like at a buffet table again. This was the feeling I had back in 2018 where everything was a meal.

> To solve the problems above, we propose “AdaBelief”, which can be easily modified from Adam. Denote the observed gradient at step t as gt and its exponential moving average (EMA) as mt. Denote the EMA of g 2 t and (gt − mt) 2 as vt and st, respectively. mt is divided by √ vt in Adam, while it is divided by √ st in AdaBelief. Intuitively, √ 1 st is the “belief” in the observation: viewing mt as the prediction of the gradient, if gt deviates much from mt, we have weak belief in gt, and take a small step; if gt is close to the prediction mt, we have a strong belief in gt, and take a large step. We validate the performance of AdaBelief with extensive experiments. Our contributions can be summarized as:

Ah this is it. I was wondering how to use the gradient centering information for ages. To think it would be like this!

I thought along the lines of centering the gradients and gave it up due to bias issues, but yes, using it to set the learning rate is an option!

I am still going to go with my idea of tracking the L1 norm of the grads before adding them to the weight matrix.

I need that for reward invariance, but I'll be using this optimizer on top.

11:10am. Hmmm, I never got the bias correction in the original Adam.

But since they are constants, I can imagine ignoring the e term and folding that sort of thing into the learning rate. It is not a big deal.

Let me take a look at the MABN paper next.

12:15pm. I don't get the two batch norm papers. The both involve hacking the backward pass, and I am not even sure where the update is in the MABN paper as everything is so crammed together interspersed with equations and theorems.

The Adabelief paper is exceptional. Adam did not quite sell me on it, but Adabelief will do the trick.

At the very least, thanks to the Powernorm paper, I understand the problems involved in batch norm better.

https://www.youtube.com/watch?v=oGH7dmwvuaY
AdaBelief Optimizer: Theory and Practical Guidelines

I do not feel like watching this now.

Let me have breakfast here.

12:25pm. I'll watch some of the Lex interviews. I still haven't finished the one by Tegmark.

1:05pm. Done with breakfast and the Tegmark interview. I guess I'll go for the one by Litman next. Forget batch norm.

Right now I am looking into data dependent initialization papers.

1:10pm. Enough of this. Just discovering Adabelief was a benefit enough in itself in the past week. Deciding to normalize the gradients via getting the norm from a large batch directly or a moving average is also great. I'll also keep track of the norm of the inputs as well. I'll combine those two optimization tricks.

Forget thinking about the cases like RNNs with small batch sizes or convolutional nets. I do not need a perfect technique here.

I've decided that I won't go for the replay buffer normalization trick after all. What I've realized is that if I have perfect reward scaling, and my trick will in fact give that to me, I won't need to collect a whole replay buffer and turn everything into signals. I can just collect 1-2k samples and eval them all in place.

This is something even KFAC can't give me because the covariance matrix cannot be perfectly inverted. There are advantages to doing less instead of more.

Let me watch the interview by Littman.

He is the guy who made that Udacity RL course with Isbell. The course was well made, but I have it a scathing review years ago because it barely covered deep learning and was mostly a philosophy course in the end. It did not even cover CFR.

Nonetheless I am curious as to what the recent events are by a RL researcher.

2:05pm. Ah, right. I said I would not sample, but how am I going to reweight by the reward probabilities in that case?

Yeah, I forgot about that. I'll need the buffer after all. Regardless, everything else will hold.

I am going to change the way Adabelief updates the mean variance. That thing has those epsilons everywhere. Instead of that, I'll pick a lower and upper bound and stick to it.

Regardless of the data, it is not wise to make moves more that 100x for example. So I'll clip the variance updates to between [1/100^2,100^2]. Forget messing with that crappy epsilon.

On top of the weight rescaling I'll be doing using the L1 norm, this will be enough for everything. The actual bounds should be related to the length of the moving average window, but is is not wise to go beyond a certain point regardless of what the data says.

2:15pm. https://youtu.be/c9AbECvRt20?t=350
> And then I had kids and I stopped listening to music and I've started to realize that my musical taste has sort of frozen out. And so I decided in 2018 I think to start listening to the top 10 billboard songs each week. So I'd be on the treadmill and I listen to that week's top 10 songs so I could find out what was popular now. And what I've discovered that I had no musical taste whatsoever. I like what I am familiar with. So the first time I'd hear a song, the first week it was on the chart, I'd be like 'Ugh'. And the second week I was into it a little bit. And the third week, I was loving it. And the fourth week it was just part of me.

A NPC doing his NPC signaling. I suppose my life would be easier if I was like that, but conformism is advantage in pursuing one's own path.

https://www.reddit.com/r/MachineLearning/comments/hciw10/r_wolfenstein_and_doom_guy_upscaled_into/

I am checking out the top rated posts in the last year and stumbled upon this.

https://arxiv.org/abs/2011.02150
EAdam Optimizer - How ε Impact Adam

Let me take a look at this paper.

...Trivial paper. Nevermind it. I was wondering why Adabelief adds the e parameter to e and then added that epsilon again, but this paper is a point that they are doing the right thing.

Now Google is confusing me. It says that Adabelief has 15 citations, but in the past I could look at those links. Where is Google Schoolar?

Some adversarial generation. Nevermind.

https://www.reddit.com/r/MachineLearning/comments/h940xb/what_is_the_best_way_to_learn_about_reinforcement/

I did not see the handle that it is Rich Sutton endorsing this course.

https://www.reddit.com/r/MachineLearning/comments/hgufqx/r_geoff_hinton_i_thought_i_had_a_very_good_idea/

There are a bunch of links, but nothing good like Adabelief.

https://www.reddit.com/r/MachineLearning/comments/i4ko0u/r_hopfield_networks_is_all_you_need/
Hopfield Networks is All You Need

Here a paper that I should save. The crap on the ML sub is just gossip like that Hinton thread. I am just wasting my time here.

95 pages. Wow.

3:05pm. It really was a complete accident that I found about Adabelief.

https://arxiv.org/abs/1910.06764
Stabilizing Transformers for Reinforcement Learning

Another interesting paper. Let me save it for later reading. At this point I am below 200 upvotes and had enough of digging through trash. This one above is a Deepmind paper so even if I had missed it now, I'd have found it at some point. Let me watch the Adabelief talk and then I'll get back to the interviews.

https://www.youtube.com/watch?v=oGH7dmwvuaY
AdaBelief Optimizer: Theory and Practical Guidelines

It is only 23m.

https://youtu.be/oGH7dmwvuaY?t=20

Hmmm, SGD is not good for training GANs? I recall it being said in one of the Deepmind lectures that KFAC did well on that.

https://youtu.be/oGH7dmwvuaY?t=58

This is different than in the paper. Do I maybe have an out of date version?

https://arxiv.org/pdf/2010.07468v1.pdf

No, it is the most up to date, but it seems the update went through some changes regarding that epsilon.

https://youtu.be/oGH7dmwvuaY?t=666

He talks about epsilons here.

https://youtu.be/oGH7dmwvuaY?t=784

What is this `True = False` here?

3:45pm. https://youtu.be/oGH7dmwvuaY?t=1084

If it was the old me, I'd completely ignore the role of epsilon, but they are constantly switching it around in these papers.

This just goes to show that I should be combining my own and this idea.

The reason why these large eps are necessary is because of bad conditioning, but if I had a bit extra modulation to squeeze the gradient signals towards where they should be I'll be able to fin this.

Everything is set.

3:55pm. I am going to modify Adabelief so it uses the L1 norm instead of the variance. Variance is not appropriate for this kind of task of estimating the centered mean. Right now it resembles an AC update and those don't need to square their accumulation.

I am thinking what would happen if you increase the gradients by a large amount and have something like 2 - 1 vs that 5 times.

Right now Adadelta would...no wait. I am on the wrong track. I think Adadelta is right since it penalizes outliers...

...No forget this line of thought, let me finish the talk.

Done. Now let me go for the talk by Litman.

I am going to finish this soon and then take a proper break for one day. After that, I'll make working on ML my freedom again.

https://www.youtube.com/watch?v=LAyZ8IYfGxQ

Here is a podcast by Isbell as well. I must have missed it yesterday.

Thankfully, they'll finish the AI risk section soon because it is putting me to sleep.

5pm. https://youtu.be/c9AbECvRt20?t=3261
> Mike: So I think as humans often do, as in recent past as well, people extrapolate. It is like, oh if you can do that which is obviously very hard. Then obviously you could do all these other problems that we wanna solve, that we know are also really hard. And it turned out very few of them ended up being practical. Partly because I think...neural nets, certainly at the time were struggling to be consistent and reliable. And so training them in a reinforcement learning setting was a bit of a mess. I had generation after generation of masters students who wanted to do value function approximation. Basically, reinforcement learning with neural nets. And over and over and over again, we were failing. We couldn't get the good results that Jerry Tessauro got.

> Mike: I now believe that Jerry is a neural net whisperer. He has a particular ability to get neural networks to do things that other people would find impossible. And it is not the technology, it is technology and Jerry together.

> Lex: Yes, which speaks to the role of the human expert in the process of machine learning.

> Mike: Right, it is so easy, we are so drawn to the idea that's the techology that is where the power is coming from that we lose sight of the fact that we need a really good...that's just, no one would think, hey there is this really great piece of software. Here is like GNU Emacs or whatever. (Lex laughs) Um, doesn't that prove that computers are super powerful and basically gonna take over the world.

> Mike: No, Stallman is great of a hacker. He was able to make the code do these amazing things. He couldn't have done it without the computer. But the computer couldn't have done it without him.

> Mike: And so people discount the role of people like Jerry who have a, **who have a particular set of skills.**.

5:55pm. https://youtu.be/c9AbECvRt20?t=4460

I am surprised to see ML saying that he read about people teaming up with computers to beat computers. I looked into that and computers + human being better than just computer was only true at the the start.

But I suppose it is plausible logically.

https://youtu.be/c9AbECvRt20?t=4569

Lex mentions that pairs are not better than individual AI players in chess as well.

7:15pm. https://youtu.be/c9AbECvRt20?t=6513

Now it comes out, I hadn't read the book but he recommended Program or Be Programmed. I can imagine what it is. Yes, everyone will need to program and much more in the future.

7:20pm. https://www.youtube.com/watch?v=uPUEq8d73JI
David Silver: AlphaGo, AlphaZero, and Deep Reinforcement Learning | Lex Fridman Podcast #86

I'd like to go through this interview next, but I am too tired.

Now that I've found AdaBelief, I realize that measuring L1 norms of the gradients and inputs would be covered by the optimizer itself.

But one thing that I can see is that the optimizer because it measures only individual values would be a lot slower in adapting.

Basically, if the other layers are poorly initialized, their scale will be uniformly messed up. Just by look at a small part it will be possible to tell what the scale is. Whereas the optimizer like AdaBelief or Adam would consider each weight in isolation.

7:30pm. Hrmmmm...now that I've wrote this, I am not sure.

I mean, couldn't AdaBelief do the same thing if I ramped up the other beta factor?

No it is not the same thing, what I wrote is right. Suddenly a part of the other net changing should cause the entirety of the present net to adjust even on parts it has not seen.

That is what my own update gives me.

There are these modulation dynamics at different scales...it will work. But this is quite fascinating. My drive to do ML has definitely been rekindled.

https://paperswithcode.com/method/radam

I am just checking out some of those other methods AdaBelief compared itself against.

> Thus, to reduce such variance, it is better to use smaller learning rates in the first few epochs of training - which justifies the warmup heuristic.

Such a complicated thing. AdaBelief will have the same problem as well, but my new modulation rules will take care of it. They will definitely update much faster than the AdaBelief centered variance.

https://arxiv.org/abs/2006.00719
AdaHessian - An Adaptive Second Order Optimizer for Machine Learning

Let me take a look at this. This is quite interesting.

https://www.youtube.com/results?search_query=adahessian

There is a video that compares all these. Pss, just pick Ada ... The part that says the name is cut off by the time length! Is it AdaBelief? Or AdaHessian?

https://arxiv.org/abs/2007.01547
Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers

This paper really has everything.

> A key insight of our comparison is that a practitioner with a new deep learning task can expect to do about equally well by taking almost any method from our benchmark and tuning it, as they would by investing the same computational resources into running a set of optimizers with their default settings and picking the winner.

8pm. The AdaHessian paper confuses me. Isn't the diagonal of the Hessian the gradient squared?

> The Hessian diagonal D can be efficiently computed using the Hutchinson’s method.

What is this method?

http://blog.shakirm.com/2015/09/machine-learning-trick-of-the-day-3-hutchinsons-trick/

Ok, I see. So in the Hessian free method are they estimating D just by multiplying the gradient over time with random vectors?

8:30pm. Actually it seems the method requires a Hessian vector product. I'll give it a pass. Let me read the Crowded Valley paper.

8:45pm. 10/34. Holy crap, there is a ton of these.

9pm. AdaBelief caught my eye, but in these tests it performs slightly worse than Adam. Ehhh, who knows.

I'll go with it anyway. Since it does uses gradient centering information in its design, I am absolutely going to use it over anything else. It is what I'd expect a first order method should be doing. It does seem to be good at optimizing autoencoders. Though I might be falling for the author's story.

I'll have to take risks and go with what makes sense.

9:10pm. It is just so difficult to use my usual reasoning method to get to anything. In programming, the obsession would converge to a proof of correctness of a particular program. Here I am just spinning my mind, trying to get the smallest of advantages. And I can never find solid ground. And because of that I can never trully build my expertise.

The post human AIs will have their work cut out of them. I am not going to be able to figure this out with my meager abilities. But in some respects I already know a decent amount.

https://scholar.google.com/scholar?cites=794903835077311857&as_sdt=2005&sciodt=0,5&hl=hr

Oh, here are the actual citations. I was wonder why there were only a few before. I was looking at the wrong page.

https://openreview.net/pdf/b3d064c86ebe3e60b1df68fff70ee335af15d5af.pdf
ADAMP: SLOWING DOWN THE SLOWDOWN FOR MOMENTUM OPTIMIZERS ON SCALE-INVARIANT WEIGHTS

9:40am. Another hacky update. But it does mention that LN does not make the weights scale invariant. The fix they propose is only there for BN.

I'll ignore BN in my nets since anything I am interested in is not really applicable to it. And it has not been found to work better for RL problems. ...Actually, I am not at all sure if these last two sentences are true, but things are complex enough without BN backing the backwards pass and requiring moving averages to be computed. I have my local modulation idea to go with, so I'll go with that.

AdaBelief is exactly what I needed here. With Adam, the local modulation I thought of would be somewhat superflous, but with AdaBelief the rules compose nicely.

Enough of this. Let me catch up with Frieren. Tomorrow, I'll for the interview with David Silver and the rest. I should also get started on the monthly review.

As for programming, I should be able to start once my inner tension lets up a little.

Though it seems like I've been slacking for the past week, my work day does not seem to have a start and an end point, and my nerves have been honed to razor sharpness. At some point my brain will get tired of constantly being in a state of full battle readiness, but until then I'll continue thinking and seeking out info."

---
## [Skyrat-SS13/Skyrat-tg@fbbf3a21a9...](https://github.com/Skyrat-SS13/Skyrat-tg/commit/fbbf3a21a98377f25e48d3ec30e1f78760b8e2f3)
##### 2021-04-28 22:29:53 by linnpap

[NON-MODULAR] More drugs. (Part 2) (#5170)

* whole bunch of shit

-gives quaaludes and opium more distinct effects
-adds pcp
-adds thc
-replaces space drugs in weed plants with thc
-adds hash
-adds dabs
-rebalances amount of drugs you can produce

* Update opium.dm

* newlines

* runtime fix

* runtime fix attempt 2

* amazing

* i dont recall the variable names being this fucked

* I FUCKING HATE GIT

* please so help me god

---
## [OctaForge/libcubescript@da7548664c...](https://github.com/OctaForge/libcubescript/commit/da7548664c81b9243ced65dc75ab79efe77b95dc)
##### 2021-04-28 23:44:38 by Daniel Kolesa

remove b and F arg types + renames + set default args to none

setting default args to none rather than whatever default
value allows for easily checking whether the arg was set,
without losing anything (since e.g. calling get_integer on
a none value still returns a 0)

'b' and 'F' were kinda ugly and handled special niches, which
are no longer a thing now that we're defaulting to none

---

# [<](2021-04-27.md) 2021-04-28 [>](2021-04-29.md)

