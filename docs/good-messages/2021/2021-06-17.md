# [<](2021-06-16.md) 2021-06-17 [>](2021-06-18.md)

3,421,935 events, 1,571,526 push events, 2,539,841 commit messages, 195,189,700 characters


## [Frethren/FireManGame](https://github.com/Frethren/FireManGame)@[d1e5dbbe53...](https://github.com/Frethren/FireManGame/commit/d1e5dbbe535e4a2e99ea98425d9c91bb70551077)
#### Thursday 2021-06-17 02:55:41 by Miles Ghiggioli

I WANT TO FUKING DIE

Yeah it kinda works i dont know how but it does

---
## [CB-Mdk/CB-Mdk.github.io](https://github.com/CB-Mdk/CB-Mdk.github.io)@[3fa21478d6...](https://github.com/CB-Mdk/CB-Mdk.github.io/commit/3fa21478d6a03975f4042e0d9eb71de8d7c0d409)
#### Thursday 2021-06-17 03:26:56 by unknown

FUCK YOU MR GUZMAN, YOUR FUCKING WIFE, AND YOUR FUCKING SON

---
## [ya-sach1/izumi](https://github.com/ya-sach1/izumi)@[440260c1ed...](https://github.com/ya-sach1/izumi/commit/440260c1eda98571a847799e5e4ccab718b0fa1e)
#### Thursday 2021-06-17 03:30:57 by ya-sach1

Fuck you I'm too tired to write something decent I'll amend tomorrow AAAAAAA IT'S 5:30 AMMM WHAT THE FUCKKKKKk

---
## [jws85/Dotfiles](https://github.com/jws85/Dotfiles)@[efe9e8c042...](https://github.com/jws85/Dotfiles/commit/efe9e8c04284a3ae3d7cac87f6b54771aa17b5f9)
#### Thursday 2021-06-17 04:18:57 by J. W. Smith

Remove i3blocks-todo

Never really used, eating up CPU cycles.

I've had really poor luck with productivity hacks.  For short-term,
timed, and pressing todo items, I'm probably going to try bullet
journaling again, but this time with an easier to work with notebook
and pen.

I have an idea of something that would be my personal Grand Unifying
Theory of personal organization... but thus far it's a total pipe
dream...

---
## [clarencelol/kernel_xiaomi-sdm660-4.19](https://github.com/clarencelol/kernel_xiaomi-sdm660-4.19)@[65bce5498d...](https://github.com/clarencelol/kernel_xiaomi-sdm660-4.19/commit/65bce5498dfa4c00cb54bdeb39502d872f325452)
#### Thursday 2021-06-17 11:36:20 by Peter Zijlstra

sched/core: Fix ttwu() race

Paul reported rcutorture occasionally hitting a NULL deref:

  sched_ttwu_pending()
    ttwu_do_wakeup()
      check_preempt_curr() := check_preempt_wakeup()
        find_matching_se()
          is_same_group()
            if (se->cfs_rq == pse->cfs_rq) <-- *BOOM*

Debugging showed that this only appears to happen when we take the new
code-path from commit:

  2ebb17717550 ("sched/core: Offload wakee task activation if it the wakee is descheduling")

and only when @cpu == smp_processor_id(). Something which should not
be possible, because p->on_cpu can only be true for remote tasks.
Similarly, without the new code-path from commit:

  c6e7bd7afaeb ("sched/core: Optimize ttwu() spinning on p->on_cpu")

this would've unconditionally hit:

  smp_cond_load_acquire(&p->on_cpu, !VAL);

and if: 'cpu == smp_processor_id() && p->on_cpu' is possible, this
would result in an instant live-lock (with IRQs disabled), something
that hasn't been reported.

The NULL deref can be explained however if the task_cpu(p) load at the
beginning of try_to_wake_up() returns an old value, and this old value
happens to be smp_processor_id(). Further assume that the p->on_cpu
load accurately returns 1, it really is still running, just not here.

Then, when we enqueue the task locally, we can crash in exactly the
observed manner because p->se.cfs_rq != rq->cfs_rq, because p's cfs_rq
is from the wrong CPU, therefore we'll iterate into the non-existant
parents and NULL deref.

The closest semi-plausible scenario I've managed to contrive is
somewhat elaborate (then again, actual reproduction takes many CPU
hours of rcutorture, so it can't be anything obvious):

					X->cpu = 1
					rq(1)->curr = X

	CPU0				CPU1				CPU2

					// switch away from X
					LOCK rq(1)->lock
					smp_mb__after_spinlock
					dequeue_task(X)
					  X->on_rq = 9
					switch_to(Z)
					  X->on_cpu = 0
					UNLOCK rq(1)->lock

									// migrate X to cpu 0
									LOCK rq(1)->lock
									dequeue_task(X)
									set_task_cpu(X, 0)
									  X->cpu = 0
									UNLOCK rq(1)->lock

									LOCK rq(0)->lock
									enqueue_task(X)
									  X->on_rq = 1
									UNLOCK rq(0)->lock

	// switch to X
	LOCK rq(0)->lock
	smp_mb__after_spinlock
	switch_to(X)
	  X->on_cpu = 1
	UNLOCK rq(0)->lock

	// X goes sleep
	X->state = TASK_UNINTERRUPTIBLE
	smp_mb();			// wake X
					ttwu()
					  LOCK X->pi_lock
					  smp_mb__after_spinlock

					  if (p->state)

					  cpu = X->cpu; // =? 1

					  smp_rmb()

	// X calls schedule()
	LOCK rq(0)->lock
	smp_mb__after_spinlock
	dequeue_task(X)
	  X->on_rq = 0

					  if (p->on_rq)

					  smp_rmb();

					  if (p->on_cpu && ttwu_queue_wakelist(..)) [*]

					  smp_cond_load_acquire(&p->on_cpu, !VAL)

					  cpu = select_task_rq(X, X->wake_cpu, ...)
					  if (X->cpu != cpu)
	switch_to(Y)
	  X->on_cpu = 0
	UNLOCK rq(0)->lock

However I'm having trouble convincing myself that's actually possible
on x86_64 -- after all, every LOCK implies an smp_mb() there, so if ttwu
observes ->state != RUNNING, it must also observe ->cpu != 1.

(Most of the previous ttwu() races were found on very large PowerPC)

Nevertheless, this fully explains the observed failure case.

Fix it by ordering the task_cpu(p) load after the p->on_cpu load,
which is easy since nothing actually uses @cpu before this.

Fixes: c6e7bd7afaeb ("sched/core: Optimize ttwu() spinning on p->on_cpu")
Reported-by: Paul E. McKenney <paulmck@kernel.org>
Tested-by: Paul E. McKenney <paulmck@kernel.org>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lkml.kernel.org/r/20200622125649.GC576871@hirez.programming.kicks-ass.net
Signed-off-by: Ratoriku <a1063021545@gmail.com>
Signed-off-by: clarencelol <clarencekuiek@icloud.com>

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[71aed5633e...](https://github.com/mrakgr/The-Spiral-Language/commit/71aed5633ef0fcacde5737a88adfe5ba4185ded7)
#### Thursday 2021-06-17 14:39:16 by Marko GrdiniÄ‡

"2:30pm. Done with breakfast. Let me do some reading and then I'll go for the second half of the session.

3pm. Done with chores. I do not seem to be reading much, instead I seem to be spending most of my time in thoughts, but still let me finish the chapter and then I will resume.

3:20pm. Let me resume. I'll leave the new Kumo chapter for later.

Ah yeah. I forgot to fix the raise to 0 bug that I realized yesterday.

```
raise_to = if raise_min = 0 then $"False" else fun raiseTo => next (Log_prob_one,(RaiseTo:)) |> loop
```

Let me just do this.

3:35pm. Hmmm, it seems that sometimes raising causes the opponent to act instead of my own player. Where is this bug coming from?

3:40pm. The pattern is Call Raise Call after which I'll get a chance to act as the opponent. I need to figure this out.

```
inl vs_human game human_pid p =
    let rec loop = function
        | Terminal: _ as g => g
        | Action: player_state,game_state,pid,actions,next as g =>
            if pid = human_pid then g
            else
                inl cs,_ = p (am.singleton (player_state,game_state,pid,actions))
                next (index cs 0)
    loop game
```

Is the problem here?

```
        | Action: player_state, (p1,p2,(community_card,_)), pid, actions, next =>
            inl trace = show_trace (pl2_observations player_state human_pid)
```

Or maybe here somewhere. Ah maybe...

```
inl vs_human game human_pid p =
    let rec loop = function
        | Terminal: _ as g => g
        | Action: player_state,game_state,pid,actions,next as g =>
            if pid = human_pid then g
            else
                inl cs,_ = p (am.singleton (player_state,game_state,pid,actions))
                loop (next (index cs 0))
    loop game
```

Maybe I just need to try looping again. This should be the solution.

3:55pm. Playing against the random player is pretty boring, but it is revealing a bunch of bugs. Just now I had a flush, but it thinks it is a pair.

Let me hack the deck again.

```
    inl swap a b =
        inl q = index deck a
        inl w = index deck b
        set deck a w
        set deck b q

    swap 0 (4+26)
    swap 1 (3+26)
    swap 2 0
    swap 3 (10+39)

    swap 4 (3+13)
    swap 5 (9+26)
    swap 6 (7+26)
    swap 7 8
    swap 8 (5+26)
```

I think this should be the correct hand.

```
    swap 0 0
    swap 1 (10+39)
    swap 2 (4+26)
    swap 3 (3+26)

    swap 4 (3+13)
    swap 5 (9+26)
    swap 6 (7+26)
    swap 7 8
    swap 8 (5+26)
```

No actually it is this. Now I'll be able to investigate this bug.

```
                inl state =
                    if has_card' hand (suit:0 rank:) then q0+1, q1, q2, q3
                    elif has_card' hand (suit:1 rank:) then q0, q1+1, q2, q3
                    elif has_card' hand (suit:2 rank:) then q0, q1, q2+1, q3
                    elif has_card' hand (suit:3 rank:) then q0, q1, q2, q3+1
                    else state
```

Wait, this might be wrong. I might not have just a single suit of a particular rank.

```
    inl flush() =
        inl find_hand ~suit =
            let rec loop rank (h,c as state) =
                inl (h,c as state) =
                    if has_card' hand (suit:rank:) then update5 h c (full (suit:rank:)),c+1
                    else state
                if c = 5 then hand_score {score=Score.flush; hand=h}
                else loop (rank-1) state
            loop (NumRanks-1) ((-1, -1, -1, -1, -1), 0)
        let rec find_suit rank (q0,q1,q2,q3 as state) =
            if 0 <= rank then
                inl f suit = has_card hand (suit:rank:)
                inl q0,q1,q2,q3 = q0 + f 0, q1 + f 1, q2 + f 2, q3 + f 3
                match state with
                | 5u8,_,_,_ => find_hand 0 | _,5u8,_,_ => find_hand 1
                | _,_,5u8,_ => find_hand 2 | _,_,_,5u8 => find_hand 3
                | _ => find_suit (rank-1) state
            else straight()
        find_suit (NumRanks-1) (0,0,0,0)
```

Let me do it like this.

```
        let rec find_suit rank (q0,q1,q2,q3 as state) =
            if 0 <= rank then
                inl f suit = has_card hand (suit:rank:)
                match q0 + f 0, q1 + f 1, q2 + f 2, q3 + f 3 with
                | 5u8,_,_,_ => find_hand 0 | _,5u8,_,_ => find_hand 1
                | _,_,5u8,_ => find_hand 2 | _,_,_,5u8 => find_hand 3
                | state => find_suit (rank-1) state
            else straight()
```

Whops, I was too hasty. Let me do this.

4:10pm. Great, now flushes work. Let me get rid of that deck hack.

```
    inl swap a b =
        inl q = index deck a
        inl w = index deck b
        set deck a w
        set deck b q

    swap 0 0
    swap 1 (10+39)
    swap 2 (4+26)
    swap 3 (3+26)

    swap 4 (3+13)
    swap 5 (9+26)
    swap 6 (7+26)
    swap 7 8
    swap 8 (5+26)
```

I'll back it up here.

All this just underscores how necessary testing is. And having an UI makes things a lot easier. Imagine if I tried playing from the command line.

It would have been horrible if I just skipped straight to training without testing it out for a while.

4:30pm. This is quite boring. Let me do 5 more hands and then I am done. I'll start the review after that.

...Ok, it works. If it does not, I'll get more feedback once I train a proper agent and play against it. The uniform player is just an aggrodonk. I do not feel like playing anymore.

4:35pm. Compared to when I started testing, I found a large number of bugs and dealt with them. Now the game is ready to be used for agent training.

It will piece of cake to make use of it for its intended purpose.

Let me commit here."

---
## [blindchimp/dwyco](https://github.com/blindchimp/dwyco)@[a4649c4ee2...](https://github.com/blindchimp/dwyco/commit/a4649c4ee27f422a6d38b3f88bf7e733caf4a381)
#### Thursday 2021-06-17 15:40:27 by blindchimp

upgrades to some of the firebase libs

putting everything to versions on google's website doesn't compile. what a surprise. randomly tried stuff until it kinda worked. i have no idea if this will fix any of the stuff the play console is complaining about. the "compile options" were recommended by gradle. i have no idea why. but it seems to work as long as you don't specify the latest version of gradle-crashlytics. why? i have no idea. crashlytics is becoming a pain in my ass. gradle is a comedy. just try something until it works. wonderful.

---
## [Livie123/emerge_project_fixed](https://github.com/Livie123/emerge_project_fixed)@[e249e45b6c...](https://github.com/Livie123/emerge_project_fixed/commit/e249e45b6c32e7a7c95e20a0fd819a51533619cb)
#### Thursday 2021-06-17 17:36:21 by Livie123

Update gallery2.html

 Add 
<section class="project-section">


        <div class="project-wrapper">
          <img src="./assets/Images/Luo_image_02.magic_pose.jpg">
          <div class="project-description">
    
            <h2>Magic Pose</h2>
            <p>Magic Pose is part of my Shoujo Instagram series. Check it out by pressing the link.</p>
            <a href="https://www.instagram.com/p/B-aSv3LhnsL/"></a>
          </div>
        </div>
        
       <div class="project-wrapper">
          <img src="./assets/Images/IMG_3073.heic">
          <div class="project-description">

            <h3>Katalog Of Dream Morpheus (Ink Drawings), 2021</h3>
            <p>Inspired by KATALOG OF DREAM MORPHEUS which is a hand drawn animation that explores dreams through lines, scribbles, and figures drawn in black ink.
              The irrational and illogical nature of dreams is represented through a series of seemingly random cuts and transitions.
            </p>
          </div>
        </div> 
        
        
       <div class="project-wrapper">
          <img src="./assets/video/katalog_dream_morpheus.mp4">
          <div class="project-description">
   
            <h4>KATALOG OF DREAM MORPHEUS, 2021, animated video, 1 minute</h4>
            <p>KATALOG OF DREAM MORPHEUS is a hand drawn animation that explores dreams through lines, scribbles, and figures drawn in black ink.
              The irrational and illogical nature of dreams is represented through a series of seemingly random cuts and transitions.</p>
          </div>
        </div> 
        
        <div class="project-wrapper">
          <img src="./assets/video/student_ani.mp4">
          <div class="project-description">
   
            <h5>A Student, 2020, animated video, 1:36 minute</h5>
            <p>This animation is about anxieties and worries that many students are struggling with. These anxieties caused by many factors creates 
              this physical manifestation of anxieties which is the black scribbles.</p>
          </div>
        </div> 
        
      <section>

---
## [jolaf/mypy](https://github.com/jolaf/mypy)@[47bafd63d8...](https://github.com/jolaf/mypy/commit/47bafd63d8087687d717e41202d15b569188c7f9)
#### Thursday 2021-06-17 17:49:22 by Ivan Levkivskyi

Start using TypeAliasType in the semantic analyzer (#7923)

This PR starts using the new `TypeAliasType` in the semantic analyzer. This PR doesn't yet pulls the trigger to enable the recursive types, but it is now essentially one line of code away. This PR:
* Makes type analyzer return a `TypeAliasType` instead of eagerly expanding the alias target.
* Refactors `TypeAliasExpr` to refer to `TypeAlias` (sorry for the noise).
* Makes few minor fixes to make all existing tests pass.
* Adds few logistic changes around `get_proper_type()` I found necessary while playing with actual recursive types over the weekend.

Here are some strategical comments:
* Taking into account how easy it was to make all existing tests pass, I don't think it is necessary to introduce a hidden option flag that would eagerly expand all type aliases after semantic analyzis.
It would probably make sense to test this well locally before a public release.
* There is a special case for no arguments generic aliases. Currently one is allowed to write `L = List; x: L[int]`, I preserve this by using eager expansion in this special case, otherwise it would complicate the whole logic significantly. This is also mostly a legacy thing because we have built-in aliases like `List = list` magically added by semantic analyzer.
* I have found that just carelessly sprinkling `get_proper_type()` is not a best strategy. It saves all the existing special-casing but also introduces a risk for infinite recursion. In particular, "type ops tangle" should ideally always pass on the original alias type. Unfortunately, there is no way to fix/enforce this (without having some severe performance impact). Note it is mostly fine to "carelessly" use `get_proper_type()` in the "front end" (like `checker.py`, `checkexpr.py`, `checkmember.py` etc).

Here is my plan for the next five PRs:
1. I am going to try merging `SubtypeVisitor` and `ProperSubtypeVisitor`, there is very large amount of code duplication (there is already an issue for this).
2. I am going to try to get rid of `sametypes.py` (I am going to open a new issue, see my arguments there).
3. I am going to enable the recursive aliases and add sufficiently many tests to be sure we are safe about infinite recursion in type ops.
4. I am going to change how named tuples and typed dicts are represented internally, currently they are stored as `TypeInfo`s, but will be stored as `TypeAlias`. Essentially there will be almost no difference between `A = Tuple[int, int]` and `A = NamedTuple('A', [('x', int), ('y', int)])`. This will allow typed dicts and named tuple participate in recursive types.
5. I am going to switch from using unbound type variables to bound type variables for generic type aliases, since now they are almost identical to `TypeInfo`s so it IMO it really makes sense to make them uniform (and avoid confusions and code duplication in future).
5a. Potentially as a follow-up I may add support for generic named tuples and typed dicts, since steps 4 plus 5 will make this almost trivial.

There is another important thing to call out, previously unions never contained another unions as items (because constructor flattened them), and some code might implicitly rely on this. IMO we should probably update these places, since maintaining this guarantee may be painful.

Yet another important thing is that this may break many plugins, so we need to announce this in #6617 when will merge this.

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[c966fe8a3f...](https://github.com/mrakgr/The-Spiral-Language/commit/c966fe8a3f62ec5200d90223503f0584a4006e50)
#### Thursday 2021-06-17 18:09:49 by Marko GrdiniÄ‡

"4:40pm. Now let me make the half yearly review for my special place.

///

At the time of the previous report [Spiral v0.2](https://github.com/mrakgr/The-Spiral-Language) was very raw, and it was not until about mid March when I did the editor support redesign that it could be considered stable. Much like in 2018 I've been programming in it and fixing bugs as I go along. Right now, it is in very good shape. Whatever complains I could make, they would be about Cython not being good enough to support a functional language like Spiral, but overall the situation is quite good. And whatever flaws Python has, PyTorch makes it worth using it. I did not opt to do a ML library like last time since I know from experience that it would eat up all my time.

It is not really worth it to make one for GPUs anymore. It would have been different in early and maybe even mid 10s, but right now the established frameworks have too much of a lead and I see no need to sacrifice my lifespan for this again.

If I could get my hands on neurochips I'd seriously consider it.

Months ago, I did take stock of [the landscape](https://www.reddit.com/r/MachineLearning/comments/kzsokz/d_list_of_novel_ml_hardware_companies_january_2021/) and send out a bunch of emails to those that I saw as most promising, but I barely got a reply and the whole experience soured me on these companies. In my mind, I see that these companies would benefit greatly by ditching the usual C + Python combo in favor of a language that would suit their hardware. I imagined what kind of back and forth I could have to convince them to sponsor Spiral, but once I actually tried I realized that I do not know how to get them to even engage with me.

It is really a pity. To think I'd need to have fame and reputation just to put my foot through the door.

An ideal situation for me and the ML community at large would be to have a bunch of sponsors paying me 3k a month just to do the backends for their hardware and giving me samples free of charge, but I've made my resolve to just buy what I need on the open market. Without Spiral these chips will need more time to develop, but it is not like it would slow me down personally. I am not going to go out of my way to do these people favors by screaming outside their window in order to get their attention. If they know what is good for them they can come to me. I promise I'll at least be willing to talk.

Right now I am finally working on my old plan. In fact, if I made my start 4 months ago and said I wanted to conquer online poker through reinforcement learning, my actions since then would have actually made sense. I did not do something crazy like work on a ML library and a language for years instead of my stated goal.

This time around I have some good results. Compared to 2018, I've changed my thinking about backprop and the new perspective gave me a steady stream of inspiration. The [monthly PL sub reviews](https://www.reddit.com/r/ProgrammingLanguages/comments/nqjmnv/june_2021_monthly_what_are_you_working_on_thread/h0exwqh?utm_source=share&utm_medium=web2x&context=3) showcase the evolution of my thinking.

My grasp on both deep learning and tabular RL is finally strong enough that I've managed to infer a [reward-scale invariant RL algorithm](https://github.com/mrakgr/The-Spiral-Language/blob/34834344b90de3ca3ec684580eb2d36b5c777e2b/Spiral%20Compilation%20Tests/cython_experiments/ui_leduc18%20(signSGD%20%2B%20infNorm%20%2B%20EM%20value%20nets)/control.py#L33) that combines all the strengths of tabular RL and deep learning. I've bencharked it against a tabular CFR player it gets close to its performance in roughly the same amount of training cycles. In the [May monthly review](https://www.reddit.com/r/ProgrammingLanguages/comments/n2orrx/may_2021_monthly_what_are_you_working_on_thread/gwn0wge/?context=3) I had a bunch of complicated ideas for how to do policy averaging using dropout with key sharing. At that time I had not heard about stochastic weight averaging and now I that I've tried it out I can confirm personally that for NNs averaging in weight space is enough to average in policy space.

When self play training is done, unlike in supervised learning the policy tends to have large oscilations. This happens even in the tabular case. And the way to dampen that is to do averaging of policies. It is a relief that for NNs it is enough to average the weights since I won't have to pay the huge expense of keeping old policy nets around.

The algorithm I've linked needs to be used together with signSGD, rescaling by the infinity norm of the layer, or a variant of Adam that tracks the infinity norm instead of the variance for the reward scale invariance property to hold. Personally I'll just stick to signSGD and have the batch size be a hyperparameter that needs tuning. In all cases I expect setting the actor learning rate to something sensible like 2 ** -8 or 2 ** -9 to work.

I've tested it thoroughly on Leduc and have confidence in using on Holdem next. In fact, the [HU Holdem game](https://github.com/mrakgr/The-Spiral-Language/blob/34834344b90de3ca3ec684580eb2d36b5c777e2b/Spiral%20Compilation%20Tests/cython_experiments/ui_holdem1/ui_holdem.py#L196) is done with testing, and I just have to start training agents on it using the algorithm I've showcased.

I've been seeking something like this for years. Even before 2018 when I touched deep RL for the first time, I remember thinking that value function learning using NNs makes no sense. And now I've finally resolved this inner conflict. Since my 2019 attempt at learning formal proofs did not take me in the direction I wanted, I am not going to write a paper, but it should be worth a blog post at some point at least. This is my reward for spending so much time on tabular RL instead of skipping straight to deep architectures.

At this point, I am sure that I am only a few weeks of steady training away from having a superhuman HU Holdem agent. I'll start out by training a relatively small and shallow one, and iterate towards deeper and wider architectures. In theory I could go straight to training the deepest and widest player I can afford, but using the old masters as baselines will allow me to tell how good the players are. HU Holdem is complex enough that I do not actually have a of gauging how good the players being trained are so I am going to have to do this.

It won't be too hard to beat even the best humans using such an agent, but who knows where the absolute limits of the game lie and how the training would have to be to reach it. Given the complexity of Holdem, most likely it will continue to get better for as long as I keep training it.

It is time to find out the truth of whether there is any point to cultivating my ML skills. Lately I've been getting wary of the social aspects of living in this world. Not long after DeepBlue beat Kasparov the machines became dominant at the game. But who are those deriving benefits from chess? It is not the AI researchers making new versions of StockFish who are lauded, but human players like Magnus Carlsen.

This is similar to poker. If you want to actual get the kind of money the best players in the world make, trying to do that via AI would quickly get one barred as a cheater from online sites. The cultural force normalizes reality so that humans are the top even when they shouldn't be.

On the surface today, AI works is being done in the open, but if you look at it more closely, the incetives are only there for the academics to publish papers in the open. I've finally made a significant algorithmic innovation, but consider the obstacles to publishing:

* I'd need to spent at least a month or two doing thorough testing. Leduc is enough to convince me, but other people would want to see more.
* All the ML papers I've seen have those useless proofs taking up space and I'd need to do them to be taken seriously, but as trite as they are, I cannot do them. I do not know how to prove literally anything of interest in deep learning.
* Even if I somehow succeed and gain academic credit, just what use is that to me? It is not like I am getting paid for that. Trading the academic credit for a job in ML might net me money, but all jobs would have me be a wage slave no matter how highly paid they are.
* Success in academic circles means eroding my own edge in the real world. I'd be spending valuable time harming myself.

I am being quite generous by putting the algorithm out in the open, but it is fairly simple and other people would have figured out the ingredients for it eventually. And I want the field to go faster. The sooner the Singularity gets here, the better.

I have some thoughts on how to do deep exploration properly. In the past my main ideas were to somehow make ensembles work and I haven't actually tried that, but I do have better ideas now. Instead of ensembles it might be possible to make use of [upside down RL](https://arxiv.org/abs/1912.02875), in particular [the decision transformers](https://arxiv.org/abs/2106.01345) and combine it with [the duality gap GAN training](https://arxiv.org/abs/2103.12685). The the GAN generator itself could then be used to derive the exploration policy by generating sequences that serve as a stand in for it. This makes some sense as living beings should not do their exploration in the real world where their actions might get them killed. Instead they'd do them through their dreams and internal simulations.

I have an inkling how to do long term credit assignment too. I can now sense that training a NN using Hebbian learning would give it the property of being reversible. Rather than keeping traces which backprop requires, it makes a lot more sense to reconstruct the inputs based on the outputs and do credit assignment that way. I do not know how to make this work, and I won't even try until I get access to neurochips. Then it will be a matter of doing enough experimentation to find the right recipe to make this work.

All in all, my position as far as ML understanding is concerned is solid and will only get better as more interesting algorithms get discovered. The poker agents are not far from being summoned into existence. I've also made my peace with backprop.

But I am wary. I've once read that a great agent without an interface is worth a fortune, but an interface without an agent is worthless, but I am not sure how seriously to take that. Given how much difficulty proving the value of Spiral to anybody, but myself is I am starting to get concerned that the Turing-test passing, 'social' aspects of AI might be my undoing. Training the agents will be so easy, that I can't help but think that the obstacles to their use will be huge. My luck when it comes to making money has been quite bad. Will it really turn around here?

I am completely sure that ML won't be the problem, so the universe will have to find some other way to make me fall flat.

This sense of danger and trepidation is somehow invigorating and motivating. It seems unimaginable for a loser such as myself to finally win at something, but if I can connect this next part, the benefits will be unimaginable. I'll wash away my dull and mediocre past and replace it with a shining, golden future. Perhaps I am just going there to die, but the safety of the present is always illusory.

I do not know whether it was worth it to take the time to make a language and cultivate my programming skills. But the lead up was all for this. If I can make superhuman agents at the foremost gambling games in the world, and yet cannot find a way to turn a profit on that, then the difficulty level of this game is really too high. I'll accept that normies are really hard to beat and will stop looking down on them. I'll give up on this madness that has been guiding me.

But if I can win I swear that I will tear up this reality and replace it with my own. I will go through my rite of passage, and return to cultivating the arts. And rather than just for playing games, I will have the agents of post-humanity assist me in making them.

///

5:15pm. `def state_probs_grad(): # Prediction errors modulate the state probabilities. The cool part is the centering.`

I am not doing the centering at all. Stale comments suck. Let me remove that last sentence.

8:05pm. Done with the review. Let me finally close for the day here. I'll put this through a spell checking tool later. Let me have some fun here."

---
## [furokku/twitter-cli](https://github.com/furokku/twitter-cli)@[cf0bc96302...](https://github.com/furokku/twitter-cli/commit/cf0bc9630245e3c8ab2479337e460538e9a757c1)
#### Thursday 2021-06-17 18:33:06 by furokku

holy shit how is this the fifth commit for a god damn readme file

---
## [T1NK0/Lugagesorting](https://github.com/T1NK0/Lugagesorting)@[1d54f90f97...](https://github.com/T1NK0/Lugagesorting/commit/1d54f90f97039b6765148fa1e99172272b7737a7)
#### Thursday 2021-06-17 19:41:20 by Tinko

Working on the god damn lugage creation... SHIT AINT WORKING!

---
## [danott/www.danott.co](https://github.com/danott/www.danott.co)@[bf410f278b...](https://github.com/danott/www.danott.co/commit/bf410f278b894e174db49677e05f6b1e299f2b74)
#### Thursday 2021-06-17 20:09:50 by Dan Ott

ðŸ“ˆ Build that writing habit

Some type-as-you-think poetry emerged today. Most of it is garbage. "The
times they are a changelog" feels like a good note to end on. That
intersection of colloquialisms and developer nomenclature always makes
me happy. I know it's a narrow audience who will get it. The specific of
that mystery invite some in further, and the nonsense is lost on others.

Writing daily in git is interesting. You have first, the writing. Then a
commit message is the reflection on the writing. Wild.

---
## [pixaxeofpixie/prometheus-tech](https://github.com/pixaxeofpixie/prometheus-tech)@[f7f0afc014...](https://github.com/pixaxeofpixie/prometheus-tech/commit/f7f0afc014773c17274cef470413498450f9d9fe)
#### Thursday 2021-06-17 20:13:53 by stormtrooper145

da da da da daaaa
it's tha motha fuckin' D oh double G (Snoop dogg)
da da da da daaaa
You know im mobbin' with the D.R.E
yeah yeah yeah
you know who's back up in this mothafucker *echoes*
What what what what
so blaze the weed out there
Blaze it up
Blaze that shit up nigga
Yeah waz up snoop
Top dogg buy them all nigga burn this shit up
D-P-G-C my nigga turn that shit up
CPT, LBC yeah we hookin' back up
N' when they bang this in the club baby you gotta get up
thug niggas, drug dealers yeah they givin' it up
low life, your life boy we livin' it up
take a chance thats why we dancin'
in the party fo' sho'
slip my ho a fourty four n' she crept in it back do'
bitches lookin' at me strange but you know i don't care
step up in this mothafucker just to swingin' my hair
Bitch quit talkin' Crip walk
If you down with the set
Take a Bullet with some dick
take this dope from this jet
outta town put it down for father of rap
n' if your ass get crack bitch shut your trap
come back get back thats the part of success
n' if you believe the X then you'll relievin' your stress
music in between
da da da da daaaa
it's the mothafuckin' D-R-E
Dr. Dre mothafucker [Snoop] what what what what
da da da da daaaa
you know im mobbing with the D oh double G
Straight off the fuckin' street's of CPT
King of the beats n' you ride to em' in your fleet (Fleetwood) wood
or Coupe DeVille rollin on dubs
How you feel?
Whoopty whoop nigga what?
Dre n' snoop chronic'ed out
In the 'llac with doc in the back
Sippin' 'gnac, clip in the strap
Dippin' through hoods
What hoods? Compton, longbeach, ingelwood
South central out to the westside (westside)
It's california love this california bud
Got a nigga gang of pub
I'm on one, I might bail up in the Century Club
With my jeans on n' my team's strong
Get my drink on n' my smoke on
Then go home with somethin' to poke on (waz up bitch)
Loc' it's on for the two tripple oh
Comin' real it's the next episode *Echoes*
Music in between
hold up.. heeeeey
For my niggas who be thinkin' we soft
We don't.. plaaaay
We gonna rockin' til the weels fall of
Hold up.. heeeeey
For my niggas who be acting to bold
Take a.. seeeeat
Hope you ready for the next episode heeeeeey

---
## [FHIR/GoFSH](https://github.com/FHIR/GoFSH)@[fa27c21ea2...](https://github.com/FHIR/GoFSH/commit/fa27c21ea21d6ade6b9b95ec33656f3928ee99d1)
#### Thursday 2021-06-17 20:14:32 by Mint Thompson

Remove sea creature utility methods

With this commit, we bid a fond farewell to our longtime supporters and
accomplices, the sea creatures listed with the results when running
goFSH. Through resources compliant and noncompliant, the sea creatures
were there for us, ready to appear and report their current moods. For
this service to us, we remain gratitudinous, now and always.

For those concerned with the welfare of these sea creatures no longer in
employ, allow me to assure you, that you may put your hearts at ease: no
sea creatures have been harmed, and all have found gainful employment at
positions matching their skills and experience gained. We hope that
their time working as part of the command line interface will be fondly
remembered, and of benefit to them, as they continue to sojourn as we
all do, through the rolling waves and fathomless depths of life.

And should it arise that we once again require the services of sea
creatures, they will return! Oh, with such fanfare and triumph we should
welcome them back, should we need them once more, to make consistent and
delightful the command line interface output formatting. They remain in
good health, and are living their best lives, as best they are able. But
for now, we say to them good-bye, and wipe the tears from our eyes as we
do.

---

# [<](2021-06-16.md) 2021-06-17 [>](2021-06-18.md)

