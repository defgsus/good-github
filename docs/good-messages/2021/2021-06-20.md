# [<](2021-06-19.md) 2021-06-20 [>](2021-06-21.md)

1,990,416 events, 1,182,211 push events, 1,737,863 commit messages, 95,505,687 characters


## [NotNotes/RadioSite](https://github.com/NotNotes/RadioSite)@[f7dcc47e85...](https://github.com/NotNotes/RadioSite/commit/f7dcc47e856dd0268386ab11be4e7159bcea8ffe)
#### Sunday 2021-06-20 00:15:59 by w0zzah

20/6 "The cocaine did the trick, completed a few glitches in the css but I ended up getting lost on my way to school, drove an extra 2 hours on accident. Wasted $40 worth of petrol and 2 hours of my life. It's ok though because no one noticed I was missing. Signing off to go and preserve my sanity, Suck my cock god"

---
## [metabrainz/artwork-indexer](https://github.com/metabrainz/artwork-indexer)@[29678db6aa...](https://github.com/metabrainz/artwork-indexer/commit/29678db6aa62aaa46a0ff01c140c669732574d35)
#### Sunday 2021-06-20 01:31:11 by Michael Wiencek

Major refactoring/fixes

 * Add a script to generate functions/triggers/handlers

   This should help keep the CAA and EAA code in sync, which is
   currently a pain to do manually.

   Updates to the SQL especially are more straightforward. Rather than
   manually editing plpgsql source code, we'll add any necessary
   information to the PROJECTS structure and let the SQL generation do
   its magic.

   The new generated SQL isn't identical to the old; bugs and
   inconsistencies have been fixed (including, I believe, CAA-126), and
   other improvements mentioned below have been made. The
   "indexed_metadata" section for events has been left sparse until
   we're certain what sort of information the IA actually uses.

 * Split move_image into copy_image + delete_image

   If a move_image event succeeds in copying the image but then fails in
   deleting it, we'd want to retry the deletion but not the copy. It's
   more logical then to push two discrete events (copy + delete).

   However, we need a way to indicate that one event depends on another
   (e.g. the deletion should not run until the copy completes). To do
   that, I've added a `depends_on` column to the event_queue table
   indicating a set of parent event to wait on.

   If an event with children fails, the child events become "stuck"
   (i.e. remain queued) until the failed parent is dealt with manually.

 * Add/update comments in sql/create.sql

 * Add tests

---
## [onesketchyguy/MasterOfPuppets](https://github.com/onesketchyguy/MasterOfPuppets)@[2e111c0ed9...](https://github.com/onesketchyguy/MasterOfPuppets/commit/2e111c0ed9027c15549cdac30095149f2ab667f6)
#### Sunday 2021-06-20 06:48:13 by onesketchyguy

Added lots of tools

I added a few new tools. Biggest among them being a tool that creates assets for us so that all we need to do is add the item icons. The item icons aren't associated with the items themselves other than through the scene, not the editor in any way, otherwise they would be automatic aswell.

Another big thing is that I've  added is the character creator itself. It's not functional yet, but it's very close. I think a couple more days of work and it'll be operational.

Along the way I've created a CopyComponentUtil which copies component parameters and pastes them onto a component which is not of the same type.

I've also added a new CustomToggle object which has in my opinion a much better visualization than just a sprite turning on or off.

A panel manager was added for menus and such.

And a ForceImmediateLayoutUpdate tool was added to make sure the LayoutGroup component would layout properly on start. There is an option to make this component periodically update, which I recommend as in my experience swapping between panels can cause it to need to update again.

I'm too tired to explain the rest of the changes, many are self explanatory like the CCItemButton or CharacterItem.

These additions have all been in order to move the old character creation demo over to a proper system which we can use to start generating character assets.

---
## [ilammy/lisp](https://github.com/ilammy/lisp)@[bfc67db705...](https://github.com/ilammy/lisp/commit/bfc67db70587222e61ed6bbb78f00f26570ef8b7)
#### Sunday 2021-06-20 06:53:21 by ilammy

hacks: Use constants where you mean constants

I was not that well versed with Pollen syntax back when I started
writing and using those. Make sure to use actual constants where
a constant is needed. This enables better syntax highlighting.

There are a bunch of cases where vertical bars are necessary, such as
when the constant is used right before a period (as Racket syntax for
identifiers allows periods). I mean, this *is* solvable (by duplicating
constants), but fuck that.

---
## [mosra/magnum-plugins](https://github.com/mosra/magnum-plugins)@[e8ebb746ac...](https://github.com/mosra/magnum-plugins/commit/e8ebb746ac159233d4101c604dd76462eb029f08)
#### Sunday 2021-06-20 08:42:05 by Vladimír Vondruš

modules: Glslang, you're the reason PEOPLE HATE BUILDSYSTEMS.

Confuckinggratulations, you managed to split the project into even more
tiny useless badly named libraries! Yes, it's amazing that such a
generic name like /usr/lib/libMachineIndependent.a is actually glslang,
who would have thought! All the developers who ever got their hands on
glslang and didn't cancel the whole project immediately upon seeing this
horror should get retroactively fired. FFS.

---
## [BekaBadzagua/TSU-TopElectro](https://github.com/BekaBadzagua/TSU-TopElectro)@[28d11cbc40...](https://github.com/BekaBadzagua/TSU-TopElectro/commit/28d11cbc4067c8643d5c32620d9db01ea53429ec)
#### Sunday 2021-06-20 10:01:36 by BekaBadzagua

Damn: this files were modified too long ago

* Long long time ago, there was a boy who didn't write clean commits
* some times he didnt write commits at all --___--

[!] So, I dont remember what the heck was modified in this files

* by the way, I added some translation for "kabeli" category

---
## [tdauth/wowtsr](https://github.com/tdauth/wowtsr)@[6bbe8e8c80...](https://github.com/tdauth/wowtsr/commit/6bbe8e8c8000f9b6de3e9c47c8125936b7649467)
#### Sunday 2021-06-20 11:09:35 by barade

Improvements

- Add Evolution upgrade to AI.
- Nether Drakes for Demon and Draenei require any tier 2 hall and have a Demon Fire icon now and Demon Fire requires the upgrade first.
- Evolution requires hero level 30 not 40 now.
- Replace Dispell Magic from Succubus with Incite Unholy Frenzy.
- Add all spells to tooltips from Succubus and Eredar Sorcerer.
- Add Draenei quest 1 to the "-ping" chat command.
- Add a second Outland portal to Northrend East.
- Add boss quest markers to all bosses.
- Replace reward item for Draenei quest 2 (Legion Doom-Horn) with a Claws +12 since the horn is available in the Black Market.
- Increase hero level of Magtheridon and give him Demon Master.
- Add Forsaken quest 4 Legendary Items.
- Tranquility restores trees.
- Add Attribute Bonus to Magtheridon.
- Murloc Sorcerer has Crushing Wave now.
- Bonus hero Illidan causes splash damage in alternate form and has Attribute Bonus and unit abilities.
- Rokhan's Voodoo Spirits has 9 levels now.
- Increase cooldown of Monsoon.
- Use standard hero abilities for Chen and Rexxar.
- Automatically haunt all goldmines in range when recreating the town hall for the Undead AI.
- Change the hotkey of Breath of Fire to G for Anasterian Sunstrider.

---
## [spxctreofficial/LandOfHeroesGame](https://github.com/spxctreofficial/LandOfHeroesGame)@[991395660e...](https://github.com/spxctreofficial/LandOfHeroesGame/commit/991395660e5fc828360eae6958fb14ce254c04ef)
#### Sunday 2021-06-20 16:08:55 by spxctreofficial

Game Overhaul, PostFX, and Bug Fixes

This commit adds full multi-bot functionality to the game, as well as the long-awaited Competitive 2v2 game mode.

In multi-bot scenarios, no bot should have the same champion anymore. If there are more players than champions available to pick, then they will automatically default to the Regime Soldier. The ability of the bots to choose the best champions for themselves is also lowered when lowering the difficulty. Other minor improvements to the bot champion selection have been added.
Gamemode Selection has been added, and there are currently two game modes at the moment to choose from, with more to come later.
Bots have also been slightly slowed down, and certain things have been added a delay to feel more natural.
Competitive 2v2 is a new game mode consisting of the player and an ally bot versus two opponent bots. Currently, with the obvious changes, Competitive 2v2 functions similarly to FFA. However, more bot logic and ally coherence will be added in the future.
A new nemesis system has been added to the game. When a champion takes tons of damage from an enemy champion, the enemy champion may become their nemesis. When a champion is another champion's nemesis, they will be prioritized when attacking and dealing damage (although the latter is still not finished.) If the nemesis attacks their dominating champion, there is a chance for the nemesis to become a nemesis for that champion's teammates as well. As for minions of a champion, they will always set any nemesis that attacks their "owner" as their nemesis. There is a small chance for a bot to stop considering a champion as their nemesis after dealing damage to them.
Particle effects have been added to indicate current target and blood effects. Other effects such as camera-shake have been dialed down since it was quite jarring. There was also a camera-shake bug that would leave the champions slightly away from their slot, but this has been fixed and ShakeImage has been replaced by ShakeOccupant in the ChampionSlot script.
Cards have also been overhauled. No longer are there 52 prefabs for dealing with cards. Now, there is only one prefab: the Card Template. The Card Template randomly loads a card from a list of CardScriptableObjects, which now contain the information instead of the prefabs. This is more efficient, easier to manage, and requires less code and effort.
Hand authority has been a pain in the ass for the last two days, so I have just completely disabled it for my mentality's sake.
Other bugs and fixes as well as previous weird effects with attempting to play cards out of turn have been fixed. Bugs that have to do with incorrect abilities have also been fixed, such as the problem with Arya's Smite. The project itself has also been organized and structured neatly, with assembly definitions added for cleaner code and faster compile times. It's also worth mentioning that the formatting on confirm dialogs has been greatly improved. Other notable changes include a noticeably less aggressive motion blur effect, optimized blur, bloom, glow effects, and more refined tooltips throughout the game for a better overall experience.
This should be considered the first commit to having the game fully playable and stable.

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[63268d75e3...](https://github.com/mrakgr/The-Spiral-Language/commit/63268d75e3f2b29f3406496efa19d08bb51aa490)
#### Sunday 2021-06-20 17:09:52 by Marko Grdinić

"2:45pm. One more chapter while I finish this pear and then I will start.

3:05pm. Ugh, just one more...

3:15pm. Let me start. I can't keep slacking for longer.

Think me, what do I have to do next?

...The game still runs...

Ok, let me give it a try.

```
inl holdem_schema () =
    inl stack_size = 50
    inl field_size = 6 // Can represent 2 ** field_size - 1 binary values in the serializer.
    inl trace_length = stack_size + 8 // RaiseTo (3..stack_size) - RaiseTo (0,1,2) + 3 OStreetOver + 4 Check + 4 Call.
    inl policy,value =
        open serialization.dense.array
        inl stack : pu st = bin_int field_size
        inl card = wrap (in:suit_rank out:full) (int 4 ** int 13)
        inl policy =
            inl oaction = alt {oStreetOver=Unit; oFold=Unit; oCall=Unit; oRaiseTo=stack} : pu oaction
            (stack ** stack ** stack ** array 7 card) ** array trace_length oaction
        inl value = policy ** (card ** card)
        policy,value
    inl action =
        open serialization.sparse.int
        inl raiseTo = wrap (in:(+) -3 out:(+) 3) (int (stack_size-3)) // RaiseTo (3..stack_size) - RaiseTo (0,1,2)
        alt {fold=Unit; call=Unit; raiseTo=int 100} : pu action
    schema {policy value action}
```

Let me put a few words first about the schema. I meant to truncate the inputs, but I've decided that this way would be easier after all. It is no big deal. It will slow down the training, but will allow me to judge the network capacity accurately.

Plus I originally meant to train with a stack size of 100. 50 should be a manageable level. To start things off, I'll use a shallow, narrow architecture. Similar to before, but with an extra layer. I'll train it on the CPU for the time being. I'll leave it on for a few hours...

Agh...

I should leave it overnight. Hopefully stressing my computer like that won't break it.

I always turn off the computer before I go to bed, but it seems I will have to make a lifestyle change. I'll have to get used to the noise. Well, it should not be much. I'll get used to it quickly.

3:35pm. I am thinking about various things. 100m hands played is a level I'd consider training a novice player. I am guessing that would be in the range of 24h of training.

With Leduc it took me half an hour for 1m hands.

So it would take me a full day just to get 50m hands on Leduc. With the much bigger inputs on Holdem, I'd be lucky to get 10m hands a day. Maybe it would drop all the way down to 1m.

...I'll ramp up the batch size to 10k if needed to saturate the GPU.

3:40pm. I think that 1 week of training is my limit. I simply can't afford to wait more than that before putting the agent to use. Sure I can leave them training to the side while the main one is raiding, but that is the limit.

3:55pm. Let me take a nap just for a bit.

4:20pm. I took some time to plan a little.

I said I would iterate things, but now that I've come to an understanding of just how long the training would be, I see that I can't afford it.

I train a shallow baseline for a day. Then I should make a bigger feedforward net, and a transformer with the same number of parameters and train each of the for a bunch of 2h runs.

Then I should tally up the results and scale up the winner.

Though I say that, what I really mean to do is work on the transformer until I figure out how to make it better than a FF net on the poker game.

Thankfully this testing I will do against a static baseline player so variance will play less of a role in their training. I should be able to take the canned PyTorch transformer layers, figure out how to make sure that the activations and LN are in the right place, figure out how to make skip connections work on them and then scale them up. This will take some effort. I still do not have a good feel for how various architectures work. That one paper says that vanilla works best, but I have no idea what they've been benchmarked against.

4:30pm. It is time to concern myself with the transformers. I can easily set up the training and let it run. Transformers I need to study and learn about.

But first, I should set things up. I'll set up the baseline and run it for a single iteration just to make sure it works. Then I will set up the game so I can play against it.

After that I can just let it run for 24h while I study transformers on the side.

4:35pm. Let me take a short break here first.

5pm. Time to set up the baseline. It should not be too hard.

```
    inl vs_self : u64 * (ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2)) -> a u64 r2 =
        train.vs_self leduc.game()
    inl vs_one : u64 * (ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2)) * (ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2)) -> a u64 r2 =
        train.vs_one leduc.game()
    inl neural =
        inl schema = agent.neural.leduc_schema()
        inl handler : _ -> ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2) =
            train.neural_handler agent.neural.leduc_extractor schema
        inl size =
            inl policy = serialization.dense.array.size schema.policy
            inl value = serialization.dense.array.size schema.value
            inl action = serialization.sparse.int.size schema.action
            namedtuple "Size" {action policy value}
        namedtuple "Neural" {handler size}
```

I need to do the equivalent of this.

```
    inl uniform : _-> a _ _ * (a u64 r2 -> _) = agent.uniform.policy
    let rec loop game =
        match train.vs_human game human_pid uniform with
```

Hmmm, this is how I set it up in the game. Forget `vs_self` and `vs_one` here. This is just for the UI.

```
    inl neural =
        inl schema = agent.neural.leduc_schema()
        inl handler : _ -> ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2) =
            train.neural_handler agent.neural.leduc_extractor schema
        inl size =
            inl policy = serialization.dense.array.size schema.policy
            inl value = serialization.dense.array.size schema.value
            inl action = serialization.sparse.int.size schema.action
            namedtuple "Size" {action policy value}
        namedtuple "Neural" {handler size}
```

Actually, let me factor this thing out and put it in neural.

```
Unification failure.
Got:      schema (i16 * i16 * i16 * a i16 i8) (a i16 oaction * i8 * i8) action
Expected: schema ((i16 * i16 * i16 * a i16 i8) * a i16 oaction) (i8 * i8) action
```

What is wrong with the handler?

```
schema ((i16 * i16 * i16 * a i16 i8) * a i16 oaction) (i8 * i8) action
```

This is the type of the schema. This should be right.

```
Unification failure.
Got:      schema ((i16 * i16 * i16 * a i16 i8) * a i16 oaction) (i8 * i8) action
Expected: schema ((i16 * i16 * i16 * a i16 i8) * a i16 oaction) (i8 * i8) action
```

Huh, what is the problem now?

```
    inl schema : schema _ _ _ = failwith ""
    inl handler =
        train.neural_handler holdem_extractor schema
```

```
Unification failure.
Got:      schema ((i16 * i16 * i16 * a i16 i8) * a i16 oaction) (i8 * i8) ?
Expected: schema ((i16 * i16 * i16 * a i16 i8) * a i16 oaction) (i8 * i8) ?
```

This feels like a compiler bug.

```
nominal schema present future actions = {
    policy : serialization.dense.array.pu present
    value : serialization.dense.array.pu (present * future)
    action : serialization.sparse.int.pu actions
    }
```

How is it possible that I put an entirely different schema into `train`? I sure got myself here.

```
inl holdem_cython_schema () =
    inl schema = holdem_schema()
    inl handler = train.neural_handler holdem_extractor schema
    inl size =
        inl policy = serialization.dense.array.size schema.policy
        inl value = serialization.dense.array.size schema.value
        inl action = serialization.sparse.int.size schema.action
        namedtuple "Size" {action policy value}
    namedtuple "Neural" {handler size}
```

Now this typechecks.

5:30pm. Ok, good, but I can just put this into uniform.

```
inl main () (stack,human_pid,set : st * u8 * _) : () =
    open hand_scorer
```

What do I do for this main function. Hmmm, let me factor it out.

```
inl ui_fun (stack,human_pid,set : st * u8 * _) : () =
```

Now I can also factor out the AI agent from this.

```
inl main () =
    ui_fun agent.uniform.policy
```

Now I can just partially apply it main. Let me bring in the neural player.

```
inl game stack = // Stack size should be at least 2.
    inl deck : a u64 card = $"numpy.arange(0,52,dtype=numpy.int8)"
    $"numpy.random.shuffle(!deck)"
```

Mhhhh, I need to make sure that a fresh deck is generated each time.

```
                inl (p1_cs : a _ _),p1_update = p1 p1_data
                inl (p2_cs : a _ _),p2_update = p2 p2_data
                inl p1_results = am.generic.map2 (<|) p1_nexts p1_cs
                inl p2_results = am.generic.map2 (<|) p2_nexts p2_cs
                inl rs = loop (am.append p1_results p2_results)
                inl p1_rs : a u64 _ = am.slice (from:0 nearTo:length p1_cs) rs |> p1_update
                inl p2_rs : a u64 _ = am.sliceFrom (length p1_cs) rs |> p2_update
                am.mapFold (fun (p1_i,p2_i) i => if i = 0 then index p1_rs p1_i,p1_i+1,p2_i else index p2_rs p2_i,p1_i,p2_i+1)
                    (0,0) pids |> fst
```

Decided to put some type annotations in the train functions.

```
inl main () =
    inl vs_self = train.vs_self (hu_holdem.game 50)
    inl vs_one = train.vs_one (hu_holdem.game 50)
    inl uniform = agent.uniform.policy
    inl neural = agent.neural_ff.holdem_cython_schema()
    ui_fun uniform
```

Now I can make these closures like so.

Let me also do the same on the leduc side. I want to clean things up.

6:15pm. No forget it. Who cares if people looking at this have to deal with the type annotations. Forget this.

I am done with lunch and want to close for the day. I'll do it properly on the holdem side.

6:25pm.

```
| Action: player_state,game_state,pid,actions,next =>
```

I am really getting bogged down in the irrelevant here. Why is the actions array here a u64 instead of i16?

```
| Action: pl2 obs act * state * u8 * a u64 act * (log_prob * act -> node state obs act)
```

Ah, because of this.

6:30pm. https://www.youtube.com/watch?v=XpFKPVquSeA
NieR Replicant & Automata Music Mix Study/Work (Most Beautiful and Emotional NieR Compilation) 2020

Hmmm, there a bunch of songs I haven't heard yet. Neir has a standout soundtrack.

At any rate, I am reviewing and found a few places where I am not using 16-bit arrays.

6:35pm.

```
inl main () =
    inl vs_self = train.vs_self (hu_holdem.game 50)
    inl vs_one = train.vs_one (hu_holdem.game 50)
    inl uniform_player = agent.uniform.policy
    inl neural = agent.neural_ff.holdem_cython_schema()
    inl ui = ui_fun uniform_player
    record {vs_self vs_one uniform_player neural ui}
```

Let me close here. I do not feel like bringing in `control.py` today. Training the baseline will have to wait for tomorrow.

3:40pm. Let me just put an eye on it.

```py
def optimize(moduleList : list,learning_rate : float = 2 ** -7,signSGDfactor : float = 1):
    """
    Interpolates between signSGD and infinity norm normalization.

    signSGDfactor - The interpolation factor for signSGD. 0 is pure infinity norm normalization, while 1 is pure signSGD.
    """
    assert (0 <= signSGDfactor <= 1)
    assert (0 <= learning_rate)
    with torch.no_grad():
        for module in moduleList:
            infNorm = torch.scalar_tensor(0)
            paramGroup = [x for x in module.parameters() if x.grad is not None]
            for x in paramGroup: infNorm = torch.max(infNorm,torch.linalg.norm(x.grad.flatten(),ord=float('inf')))
            for x in paramGroup: # The scalars operations are grouped for efficiency.
                if torch.is_nonzero(infNorm):
                    if 0 < signSGDfactor: x -= learning_rate * signSGDfactor * torch.sign(x.grad)
                    if signSGDfactor < 1: x -= learning_rate * (1 - signSGDfactor) / infNorm * x.grad
                    x.grad = None
```

Yeah, I'll get rid of infinity norm this time.

```py
prediction_errors = torch.abs(at_action_value - prediction_values_for_state) # [batch_dim,state_dim]
```

I should have abs be inplace here.

```py
def belief_tabulate(state_probs : Tensor,head : Tensor,action_indices : Tensor,at_action_value : Tensor,at_action_weight : Tensor):
    # state_probs[batch_dim,state_dim]
    # head[action_dim*2,state_dim]
    # action_indices[batch_dim] : map (action_dim -> batch_dim)
    # at_action_value[batch_dim,1] : map (action_dim -> batch_dim)
    # at_action_weight[batch_dim,1] : map (action_dim -> batch_dim)
    num_actions = head.shape[0]//2
    head_weighted_values = head[:num_actions,:] # [action_dim,state_dim]
    head_value_weights = head[num_actions:,:] # [action_dim,state_dim]

    def update_head(): # Weighted moving average update. Works well with probabilistic vectors and weighted updates that CFR requires.
        state_weights = at_action_weight * state_probs # [batch_dim,state_dim]
        head_weighted_values.index_add_(0,action_indices,at_action_value * state_weights)
        head_value_weights.index_add_(0,action_indices,state_weights)

    def calculate():
        values = torch.nan_to_num_(head_weighted_values / head_value_weights) # [action_dim,state_dim]
        def state_probs_grad(): # Prediction errors modulate the state probabilities.
            prediction_values_for_state = values[action_indices,:] # [batch_dim,state_dim]
            prediction_errors = torch.abs(at_action_value - prediction_values_for_state) # [batch_dim,state_dim]
            return at_action_weight * prediction_errors # [batch_dim,state_dim]

        def action_fun(action_probs : Tensor, sample_probs : Tensor): # Implements the VR MC-CFR update. Could be easily adapted to train an ensemble of actors.
            # policy_probs[batch_dim,action_dim]
            # sample_probs[batch_dim,action_dim]
            prediction_values_for_action = state_probs.mm(values.t()) # [batch_dim,action_dim]
            at_action_sample_probs = torch.gather(sample_probs,-1,action_indices.unsqueeze(-1)) # [batch_dim,1]
            at_action_prediction_value = torch.gather(prediction_values_for_action,-1,action_indices.unsqueeze(-1)) # [batch_dim,1]
            at_action_prediction_adjustment = (at_action_value - at_action_prediction_value) / at_action_sample_probs # [batch_dim,1]
            prediction_values_for_action.scatter_add_(-1,action_indices.unsqueeze(-1),at_action_prediction_adjustment)
            reward = (action_probs * prediction_values_for_action).sum(-1,keepdim=True) # [batch_dim,1]
            # No need to center the gradients being passed into a probability vector's backward pass. Softmax for example, centers them on its own.
            def probs_grad(): return -at_action_weight * prediction_values_for_action # [batch_dim,action_dim]
            return reward, probs_grad
        return state_probs_grad, action_fun
    return update_head, calculate
```

This whole segment needs optimization, but in not sure if torch script will be able to deal with all this closure passing. Given the amount of time I am going to be training the poker agents, I am going to have to optimize various parts.

```
    action_mask_inv = torch.logical_not(action_mask)
    # Interpolates action probs with an uniform noise distribution for actions that aren't masked out.
    sample_probs = action_probs.detach() * (1 - epsilon)
    if 0 < epsilon: sample_probs += action_mask_inv / (action_mask_inv.sum(-1,keepdim=True) * (1 / epsilon))
```

I should put the calculation of the inv mask here.

```
        if is_update_head: update_head()
        if is_update_value: state_probs.backward(state_probs_grad())
        if is_update_policy: action_probs.backward(action_probs_grad())
```

Let me get rid of those tracker comments.

```py
def run(i_tabular,i_nn,vs_self,vs_one,neural,uniform_player,tabular): # old NN vs old tabular
    batch_size = 2 ** 10
    with open(f'dump/agent_{i_tabular}_avg.obj','rb') as f: tabular_agent_old = pickle.load(f)

    def r(name,value,policy,head):
        def tabular_player(is_update_head=False,is_update_policy=False,epsilon=0,tabular_agent=tabular_agent_old):
            return tabular.create_policy(tabular_agent,is_update_head,is_update_policy,epsilon)

        def neural_player(is_update_head=False,is_update_value=False,is_update_policy=False,epsilon=0.0):
            return neural.handler(partial(model_evaluate,value,head,policy,is_update_head,is_update_value,is_update_policy,epsilon))

        r : np.ndarray = vs_one(batch_size * 2 ** 8,neural_player(),tabular_player())
        print(f'The mean is {r.mean()} for the {name} player.')
    with open(f'dump/nn_agent_{i_nn}.obj','rb') as f: r('regular',*pickle.load(f))
    with open(f'dump/nn_agent_{i_nn}_avg.obj','rb') as f: r('average',*pickle.load(f))
```

This I am going to have to change.

```py
def create_tabular_agent(n,m,vs_self,vs_one,neural,uniform_player,tabular):
    batch_size = 2 ** 10
    head_decay = 2 ** -2

    tabular_agent = tabular.create_agent()
    tabular_avg_agent = tabular.create_agent()
    def tabular_player(is_update_head=False,is_update_policy=False,epsilon=0.0,agent=tabular_agent):
        return tabular.create_policy(agent,is_update_head,is_update_policy,epsilon)

    def run(is_avg : bool = False):
        tabular.head_multiply_(tabular_agent,head_decay)
        for a in range(3):
            vs_self(batch_size,tabular_player(True,False,2 ** -2,agent=tabular_agent))
        vs_self(batch_size,tabular_player(False,True,2 ** -2,agent=tabular_agent))
        tabular.optimize(tabular_agent)
        if is_avg: tabular.average(tabular_avg_agent,tabular_agent)

    def train():
        print('Training the tabular agent.')
        for a in range(n):
            run()
            if (a + 1) % 30 == 0: print(a+1)

    def avg():
        print('Averaging the tabular agent.')
        for a in range(m):
            run(True)
            if (a + 1) % 30 == 0:
                print(a+1)
                r : np.ndarray = vs_self(batch_size * 2 ** 4,tabular_player(agent=tabular_agent))
                print(f'The mean reward is {r.mean()} for the regular agent.')
                r : np.ndarray = vs_self(batch_size * 2 ** 4,tabular_player(agent=tabular_avg_agent))
                print(f'The mean reward is {r.mean()} for the average agent.')
    train()
    avg()
    with open(f"dump/agent_{n + m}.obj",'wb') as f: pickle.dump(tabular_agent,f)
    with open(f"dump/agent_{n + m}_avg.obj",'wb') as f: pickle.dump(tabular_avg_agent,f)
```

This I can just erase. Not going to need it.

7pm. Let me close here. It is not as easy as just snapping my fingers. I'll run it and connect it to the game UI tomorrow.

My long awaited dream is not that far off. If I could get my hands on neurochips, I bet I could speed up the training by a factor of 1000 for these shallow models. I could implement the game directly on chip and avoid both the data transfer impact and the game being all on a single thread.

Training 1b hands per day in the future is not a far fetched dream. If I could make some money at low stakes, I'll have the resources to actual order one of the things from the companies that make them. Alternatively, I could try applying for a job seriously this time. Groq was the only one whose HR drone was even interested in setting up an interview, and I ended that on a rude note, so I am not sure whether that is an option.

Hmmmm, inquiring about purchasing these devices might be a good way to get around HR.

7:05pm. Nevermind that for now. If the GPU is not enough to beat the small stakes, I might as well curse my bad luck and give up. Not everything has to go my way. Not all lives have to be successful.

But as long as there is a chance I can win, I should go forward."

---
## [deliciousmods/1956_beta](https://github.com/deliciousmods/1956_beta)@[611c9cbfc5...](https://github.com/deliciousmods/1956_beta/commit/611c9cbfc5432d2a651e9b281ea39c5035afb38e)
#### Sunday 2021-06-20 18:16:51 by Fantom

Fixes number odin (#1799)

* Fixes number odin

Dynamic wargoals for Hungary (thanks Waffle for inspiration)
Generic: industry build up in core states (not spamming factories in new colonial states with low compliance)
FRA: industry build up in core states, if puppets are there, they are getting autonomy lowering, few fixes for Indochina
USA: NATO branch is no longer bound to LaR focus -> shall be accessible to everyone with or without required DLC

* Few tweaks

EST: industry requirements slightly redone
UK:  Tension requirements adjusted
BEL: fixed a bug

* PRC revamp + few suggestions implemented

Revamped most of the PRC focuses, shifted and merged few branches, tried to remove some excessive lines of code. Also changed few events from "immediate effect" to "effect" so that the modifier is not applied instantly
Added decision to move capital - shall no longer rely on event only

GER: should invite Turkey with BfTB if goes Ottoman path
IRQ: get Hatay from France if it wasn't ceded to Turkey prior
RAJ: added description, @Brit

* Few Tweaks

Tweaks to Chinese AI path

Chinese + warlords manufacturers shall now properly appear

Tweaked few descriptions of Albanian events

* Welp

Few more fixes for PRC:
2 bugs fixed
Ideas replacement made better (hopefully)
Replaced one disgusting line in focus tree I created by mistake
Higher chance to get a pact with Japan if player wants it
Hopefully no more factories deletion after defeating warlords (crippled economy even more beforehand)

* Few PRC-related and random tweaks

PRC:
Nerfed few modifiers regarding PRC-CHI interaction
Few changes to conscription laws to make them look more realistic and appealing
Few search filters here and there, removed effect tooltips

SAU:
Few search filters
Changed focus GFX to match the focus better

Events:
tried to improve the localization of few events

* Fixing localization issue + Icelandic nat.spirit bugs

Saved localization file in wrong format (UTF-8 and not UTF-8 with BOM), should be correct now. Sorry for that

Few fixes for bugs in Icelandic focus tree

* USA fixes

Too fast made tree, too many unresolved bugs..

Spawning trained troops, not veterans on islands if released
Hawaii will no longer receive free troops because the event was bugged
Marshall Plan is now timed for two years
Korea shall receive civil war only in case it controls all of Korea (issue mentioned in bugreport-suggestions, thanks for that)

---
## [Lenkly/festivent](https://github.com/Lenkly/festivent)@[052f76a031...](https://github.com/Lenkly/festivent/commit/052f76a0312fdf7b3b7b0eeb15aa98f52eb5a0c6)
#### Sunday 2021-06-20 19:39:07 by Lydia

fix global styles (#57)

* refactoring globalstyles

* fix proptypes according to warnings

* god damn typechecker, shut the fuck up

* fix line height problems with cut off text

---
## [JessicaTheGunLady/SM-TF2-ProjectileDrop](https://github.com/JessicaTheGunLady/SM-TF2-ProjectileDrop)@[5e5ed188ce...](https://github.com/JessicaTheGunLady/SM-TF2-ProjectileDrop/commit/5e5ed188ceff527b7ba1efae5c827daa828f2750)
#### Sunday 2021-06-20 21:28:27 by JessicaTheGunLady

Improved optimisation

Managed to improve the performance of this plugin by using DataPacks, thanks to NoSoop for reminding me this existed and thanks to my brain for not being so goddamn useless this time, unfortunately it wasn't working well enough for me to create a fixed projectile deviation angle based on the clip size of a weapon, so no fair & balanced Beggar's.

P.S: Jesus Christ I DESPISE Vector values, thank god I only needed to BARELY touch them to make this plugin work because otherwise I would've lost my SANITY trying to put up with them, if someone else wants to attempt the plugin I mentioned then go ahead, 'cause I'm sure as hell not doing it with the mathematical capability I currently have.

---
## [SunburntRock89/botstion](https://github.com/SunburntRock89/botstion)@[8d663217f8...](https://github.com/SunburntRock89/botstion/commit/8d663217f8fa18d691a3bbe495c073798cd42f5f)
#### Sunday 2021-06-20 22:15:44 by Leo MG Nesfield (LMGN)

:art: fuck you macos (since this is probably going on gitlost @theLMGN says hi!)

---
## [StKelsewhere/Data-Persistence-Project](https://github.com/StKelsewhere/Data-Persistence-Project)@[e7f3ffec3a...](https://github.com/StKelsewhere/Data-Persistence-Project/commit/e7f3ffec3afc3bae83eeb316b8924c943c1f7883)
#### Sunday 2021-06-20 22:37:16 by StKelsewhere

broken save-load persistence

having trouble implementing saving and loading high scores with this ridiculous "press space to restart" thing that was in here when i started. i'm learning to absolutely despise adding code to other people's code when i've never seen it before. when i create code from scratch, i have a constant mental map of it that makes it easy to add or change things. walking into someone else's code and attempting to add things feels bizarre and makes it hard to think. it's not like i can do anything about it, but i wish Unity's curriculum designers could accommodate people like me by doing the thing they tell us to do in every tutorial: comment your damn code.

---
## [StackExchange/StackExchange.Redis](https://github.com/StackExchange/StackExchange.Redis)@[e635b072bc...](https://github.com/StackExchange/StackExchange.Redis/commit/e635b072bc30624ef07a9c1e70824ebca66d1ad0)
#### Sunday 2021-06-20 23:23:02 by Nick Craver

Omg, I'm a fucking idiot

It was the subscription bridge's physical connection bubbling up through the result processor from its (far faster) tracer PING/ECHO response, triggering the overall ServerEndPoint too early.

In retrospect, I'm a moron. This whole changeset should fix a list of connection issues now though, woooooo!

---

# [<](2021-06-19.md) 2021-06-20 [>](2021-06-21.md)

