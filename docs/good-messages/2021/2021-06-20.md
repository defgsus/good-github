# [<](2021-06-19.md) 2021-06-20 [>](2021-06-21.md)

1,990,416 events, 1,182,211 push events, 1,737,863 commit messages, 95,505,687 characters


## [NotNotes/RadioSite@f7dcc47e85...](https://github.com/NotNotes/RadioSite/commit/f7dcc47e856dd0268386ab11be4e7159bcea8ffe)
##### 2021-06-20 00:15:59 by w0zzah

20/6 "The cocaine did the trick, completed a few glitches in the css but I ended up getting lost on my way to school, drove an extra 2 hours on accident. Wasted $40 worth of petrol and 2 hours of my life. It's ok though because no one noticed I was missing. Signing off to go and preserve my sanity, Suck my cock god"

---
## [EdwardNashton/mojave-sun@67bfc8853e...](https://github.com/EdwardNashton/mojave-sun/commit/67bfc8853efe27cf414fbd395aa6a130f79d5eb6)
##### 2021-06-20 00:53:24 by EdwardNashton

Bad Boys Den: Update of Raider's Base. (#325)

* Bad Boys Den: Update of Raider's Base.

Remade one shop to Butcher Shop. Add a small Warehouse with graveyard. Slightly remade a Garage, improved living house and the Bar.

* Bad Boys Den: Small Fixes

* Bad Boys Den: Hatred of Low Walls

Fixed god damn low walls. Also added a few things like carcass. pickaxes etc.

* Bad Boys Den: Extra Fixes fo free!

Spotted a some few small errors. Fixed it.

* Bad Boys Den: End My Suffering

* Bad Boys Den: Area Errors...Area Errors Everywhere!

Fixed an areas near Administration.

Co-authored-by: Edward Nashton <eddienigma48@gmail.com>

---
## [mosra/magnum-plugins@e8ebb746ac...](https://github.com/mosra/magnum-plugins/commit/e8ebb746ac159233d4101c604dd76462eb029f08)
##### 2021-06-20 08:42:05 by Vladimír Vondruš

modules: Glslang, you're the reason PEOPLE HATE BUILDSYSTEMS.

Confuckinggratulations, you managed to split the project into even more
tiny useless badly named libraries! Yes, it's amazing that such a
generic name like /usr/lib/libMachineIndependent.a is actually glslang,
who would have thought! All the developers who ever got their hands on
glslang and didn't cancel the whole project immediately upon seeing this
horror should get retroactively fired. FFS.

---
## [mrakgr/The-Spiral-Language@63268d75e3...](https://github.com/mrakgr/The-Spiral-Language/commit/63268d75e3f2b29f3406496efa19d08bb51aa490)
##### 2021-06-20 17:09:52 by Marko Grdinić

"2:45pm. One more chapter while I finish this pear and then I will start.

3:05pm. Ugh, just one more...

3:15pm. Let me start. I can't keep slacking for longer.

Think me, what do I have to do next?

...The game still runs...

Ok, let me give it a try.

```
inl holdem_schema () =
    inl stack_size = 50
    inl field_size = 6 // Can represent 2 ** field_size - 1 binary values in the serializer.
    inl trace_length = stack_size + 8 // RaiseTo (3..stack_size) - RaiseTo (0,1,2) + 3 OStreetOver + 4 Check + 4 Call.
    inl policy,value =
        open serialization.dense.array
        inl stack : pu st = bin_int field_size
        inl card = wrap (in:suit_rank out:full) (int 4 ** int 13)
        inl policy =
            inl oaction = alt {oStreetOver=Unit; oFold=Unit; oCall=Unit; oRaiseTo=stack} : pu oaction
            (stack ** stack ** stack ** array 7 card) ** array trace_length oaction
        inl value = policy ** (card ** card)
        policy,value
    inl action =
        open serialization.sparse.int
        inl raiseTo = wrap (in:(+) -3 out:(+) 3) (int (stack_size-3)) // RaiseTo (3..stack_size) - RaiseTo (0,1,2)
        alt {fold=Unit; call=Unit; raiseTo=int 100} : pu action
    schema {policy value action}
```

Let me put a few words first about the schema. I meant to truncate the inputs, but I've decided that this way would be easier after all. It is no big deal. It will slow down the training, but will allow me to judge the network capacity accurately.

Plus I originally meant to train with a stack size of 100. 50 should be a manageable level. To start things off, I'll use a shallow, narrow architecture. Similar to before, but with an extra layer. I'll train it on the CPU for the time being. I'll leave it on for a few hours...

Agh...

I should leave it overnight. Hopefully stressing my computer like that won't break it.

I always turn off the computer before I go to bed, but it seems I will have to make a lifestyle change. I'll have to get used to the noise. Well, it should not be much. I'll get used to it quickly.

3:35pm. I am thinking about various things. 100m hands played is a level I'd consider training a novice player. I am guessing that would be in the range of 24h of training.

With Leduc it took me half an hour for 1m hands.

So it would take me a full day just to get 50m hands on Leduc. With the much bigger inputs on Holdem, I'd be lucky to get 10m hands a day. Maybe it would drop all the way down to 1m.

...I'll ramp up the batch size to 10k if needed to saturate the GPU.

3:40pm. I think that 1 week of training is my limit. I simply can't afford to wait more than that before putting the agent to use. Sure I can leave them training to the side while the main one is raiding, but that is the limit.

3:55pm. Let me take a nap just for a bit.

4:20pm. I took some time to plan a little.

I said I would iterate things, but now that I've come to an understanding of just how long the training would be, I see that I can't afford it.

I train a shallow baseline for a day. Then I should make a bigger feedforward net, and a transformer with the same number of parameters and train each of the for a bunch of 2h runs.

Then I should tally up the results and scale up the winner.

Though I say that, what I really mean to do is work on the transformer until I figure out how to make it better than a FF net on the poker game.

Thankfully this testing I will do against a static baseline player so variance will play less of a role in their training. I should be able to take the canned PyTorch transformer layers, figure out how to make sure that the activations and LN are in the right place, figure out how to make skip connections work on them and then scale them up. This will take some effort. I still do not have a good feel for how various architectures work. That one paper says that vanilla works best, but I have no idea what they've been benchmarked against.

4:30pm. It is time to concern myself with the transformers. I can easily set up the training and let it run. Transformers I need to study and learn about.

But first, I should set things up. I'll set up the baseline and run it for a single iteration just to make sure it works. Then I will set up the game so I can play against it.

After that I can just let it run for 24h while I study transformers on the side.

4:35pm. Let me take a short break here first.

5pm. Time to set up the baseline. It should not be too hard.

```
    inl vs_self : u64 * (ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2)) -> a u64 r2 =
        train.vs_self leduc.game()
    inl vs_one : u64 * (ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2)) * (ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2)) -> a u64 r2 =
        train.vs_one leduc.game()
    inl neural =
        inl schema = agent.neural.leduc_schema()
        inl handler : _ -> ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2) =
            train.neural_handler agent.neural.leduc_extractor schema
        inl size =
            inl policy = serialization.dense.array.size schema.policy
            inl value = serialization.dense.array.size schema.value
            inl action = serialization.sparse.int.size schema.action
            namedtuple "Size" {action policy value}
        namedtuple "Neural" {handler size}
```

I need to do the equivalent of this.

```
    inl uniform : _-> a _ _ * (a u64 r2 -> _) = agent.uniform.policy
    let rec loop game =
        match train.vs_human game human_pid uniform with
```

Hmmm, this is how I set it up in the game. Forget `vs_self` and `vs_one` here. This is just for the UI.

```
    inl neural =
        inl schema = agent.neural.leduc_schema()
        inl handler : _ -> ra u64 (pl2 card action * leduc_state * u8 * a u64 action) -> a u64 (log_prob * action) * (a u64 r2 -> a u64 r2) =
            train.neural_handler agent.neural.leduc_extractor schema
        inl size =
            inl policy = serialization.dense.array.size schema.policy
            inl value = serialization.dense.array.size schema.value
            inl action = serialization.sparse.int.size schema.action
            namedtuple "Size" {action policy value}
        namedtuple "Neural" {handler size}
```

Actually, let me factor this thing out and put it in neural.

```
Unification failure.
Got:      schema (i16 * i16 * i16 * a i16 i8) (a i16 oaction * i8 * i8) action
Expected: schema ((i16 * i16 * i16 * a i16 i8) * a i16 oaction) (i8 * i8) action
```

What is wrong with the handler?

```
schema ((i16 * i16 * i16 * a i16 i8) * a i16 oaction) (i8 * i8) action
```

This is the type of the schema. This should be right.

```
Unification failure.
Got:      schema ((i16 * i16 * i16 * a i16 i8) * a i16 oaction) (i8 * i8) action
Expected: schema ((i16 * i16 * i16 * a i16 i8) * a i16 oaction) (i8 * i8) action
```

Huh, what is the problem now?

```
    inl schema : schema _ _ _ = failwith ""
    inl handler =
        train.neural_handler holdem_extractor schema
```

```
Unification failure.
Got:      schema ((i16 * i16 * i16 * a i16 i8) * a i16 oaction) (i8 * i8) ?
Expected: schema ((i16 * i16 * i16 * a i16 i8) * a i16 oaction) (i8 * i8) ?
```

This feels like a compiler bug.

```
nominal schema present future actions = {
    policy : serialization.dense.array.pu present
    value : serialization.dense.array.pu (present * future)
    action : serialization.sparse.int.pu actions
    }
```

How is it possible that I put an entirely different schema into `train`? I sure got myself here.

```
inl holdem_cython_schema () =
    inl schema = holdem_schema()
    inl handler = train.neural_handler holdem_extractor schema
    inl size =
        inl policy = serialization.dense.array.size schema.policy
        inl value = serialization.dense.array.size schema.value
        inl action = serialization.sparse.int.size schema.action
        namedtuple "Size" {action policy value}
    namedtuple "Neural" {handler size}
```

Now this typechecks.

5:30pm. Ok, good, but I can just put this into uniform.

```
inl main () (stack,human_pid,set : st * u8 * _) : () =
    open hand_scorer
```

What do I do for this main function. Hmmm, let me factor it out.

```
inl ui_fun (stack,human_pid,set : st * u8 * _) : () =
```

Now I can also factor out the AI agent from this.

```
inl main () =
    ui_fun agent.uniform.policy
```

Now I can just partially apply it main. Let me bring in the neural player.

```
inl game stack = // Stack size should be at least 2.
    inl deck : a u64 card = $"numpy.arange(0,52,dtype=numpy.int8)"
    $"numpy.random.shuffle(!deck)"
```

Mhhhh, I need to make sure that a fresh deck is generated each time.

```
                inl (p1_cs : a _ _),p1_update = p1 p1_data
                inl (p2_cs : a _ _),p2_update = p2 p2_data
                inl p1_results = am.generic.map2 (<|) p1_nexts p1_cs
                inl p2_results = am.generic.map2 (<|) p2_nexts p2_cs
                inl rs = loop (am.append p1_results p2_results)
                inl p1_rs : a u64 _ = am.slice (from:0 nearTo:length p1_cs) rs |> p1_update
                inl p2_rs : a u64 _ = am.sliceFrom (length p1_cs) rs |> p2_update
                am.mapFold (fun (p1_i,p2_i) i => if i = 0 then index p1_rs p1_i,p1_i+1,p2_i else index p2_rs p2_i,p1_i,p2_i+1)
                    (0,0) pids |> fst
```

Decided to put some type annotations in the train functions.

```
inl main () =
    inl vs_self = train.vs_self (hu_holdem.game 50)
    inl vs_one = train.vs_one (hu_holdem.game 50)
    inl uniform = agent.uniform.policy
    inl neural = agent.neural_ff.holdem_cython_schema()
    ui_fun uniform
```

Now I can make these closures like so.

Let me also do the same on the leduc side. I want to clean things up.

6:15pm. No forget it. Who cares if people looking at this have to deal with the type annotations. Forget this.

I am done with lunch and want to close for the day. I'll do it properly on the holdem side.

6:25pm.

```
| Action: player_state,game_state,pid,actions,next =>
```

I am really getting bogged down in the irrelevant here. Why is the actions array here a u64 instead of i16?

```
| Action: pl2 obs act * state * u8 * a u64 act * (log_prob * act -> node state obs act)
```

Ah, because of this.

6:30pm. https://www.youtube.com/watch?v=XpFKPVquSeA
NieR Replicant & Automata Music Mix Study/Work (Most Beautiful and Emotional NieR Compilation) 2020

Hmmm, there a bunch of songs I haven't heard yet. Neir has a standout soundtrack.

At any rate, I am reviewing and found a few places where I am not using 16-bit arrays.

6:35pm.

```
inl main () =
    inl vs_self = train.vs_self (hu_holdem.game 50)
    inl vs_one = train.vs_one (hu_holdem.game 50)
    inl uniform_player = agent.uniform.policy
    inl neural = agent.neural_ff.holdem_cython_schema()
    inl ui = ui_fun uniform_player
    record {vs_self vs_one uniform_player neural ui}
```

Let me close here. I do not feel like bringing in `control.py` today. Training the baseline will have to wait for tomorrow.

3:40pm. Let me just put an eye on it.

```py
def optimize(moduleList : list,learning_rate : float = 2 ** -7,signSGDfactor : float = 1):
    """
    Interpolates between signSGD and infinity norm normalization.

    signSGDfactor - The interpolation factor for signSGD. 0 is pure infinity norm normalization, while 1 is pure signSGD.
    """
    assert (0 <= signSGDfactor <= 1)
    assert (0 <= learning_rate)
    with torch.no_grad():
        for module in moduleList:
            infNorm = torch.scalar_tensor(0)
            paramGroup = [x for x in module.parameters() if x.grad is not None]
            for x in paramGroup: infNorm = torch.max(infNorm,torch.linalg.norm(x.grad.flatten(),ord=float('inf')))
            for x in paramGroup: # The scalars operations are grouped for efficiency.
                if torch.is_nonzero(infNorm):
                    if 0 < signSGDfactor: x -= learning_rate * signSGDfactor * torch.sign(x.grad)
                    if signSGDfactor < 1: x -= learning_rate * (1 - signSGDfactor) / infNorm * x.grad
                    x.grad = None
```

Yeah, I'll get rid of infinity norm this time.

```py
prediction_errors = torch.abs(at_action_value - prediction_values_for_state) # [batch_dim,state_dim]
```

I should have abs be inplace here.

```py
def belief_tabulate(state_probs : Tensor,head : Tensor,action_indices : Tensor,at_action_value : Tensor,at_action_weight : Tensor):
    # state_probs[batch_dim,state_dim]
    # head[action_dim*2,state_dim]
    # action_indices[batch_dim] : map (action_dim -> batch_dim)
    # at_action_value[batch_dim,1] : map (action_dim -> batch_dim)
    # at_action_weight[batch_dim,1] : map (action_dim -> batch_dim)
    num_actions = head.shape[0]//2
    head_weighted_values = head[:num_actions,:] # [action_dim,state_dim]
    head_value_weights = head[num_actions:,:] # [action_dim,state_dim]

    def update_head(): # Weighted moving average update. Works well with probabilistic vectors and weighted updates that CFR requires.
        state_weights = at_action_weight * state_probs # [batch_dim,state_dim]
        head_weighted_values.index_add_(0,action_indices,at_action_value * state_weights)
        head_value_weights.index_add_(0,action_indices,state_weights)

    def calculate():
        values = torch.nan_to_num_(head_weighted_values / head_value_weights) # [action_dim,state_dim]
        def state_probs_grad(): # Prediction errors modulate the state probabilities.
            prediction_values_for_state = values[action_indices,:] # [batch_dim,state_dim]
            prediction_errors = torch.abs(at_action_value - prediction_values_for_state) # [batch_dim,state_dim]
            return at_action_weight * prediction_errors # [batch_dim,state_dim]

        def action_fun(action_probs : Tensor, sample_probs : Tensor): # Implements the VR MC-CFR update. Could be easily adapted to train an ensemble of actors.
            # policy_probs[batch_dim,action_dim]
            # sample_probs[batch_dim,action_dim]
            prediction_values_for_action = state_probs.mm(values.t()) # [batch_dim,action_dim]
            at_action_sample_probs = torch.gather(sample_probs,-1,action_indices.unsqueeze(-1)) # [batch_dim,1]
            at_action_prediction_value = torch.gather(prediction_values_for_action,-1,action_indices.unsqueeze(-1)) # [batch_dim,1]
            at_action_prediction_adjustment = (at_action_value - at_action_prediction_value) / at_action_sample_probs # [batch_dim,1]
            prediction_values_for_action.scatter_add_(-1,action_indices.unsqueeze(-1),at_action_prediction_adjustment)
            reward = (action_probs * prediction_values_for_action).sum(-1,keepdim=True) # [batch_dim,1]
            # No need to center the gradients being passed into a probability vector's backward pass. Softmax for example, centers them on its own.
            def probs_grad(): return -at_action_weight * prediction_values_for_action # [batch_dim,action_dim]
            return reward, probs_grad
        return state_probs_grad, action_fun
    return update_head, calculate
```

This whole segment needs optimization, but in not sure if torch script will be able to deal with all this closure passing. Given the amount of time I am going to be training the poker agents, I am going to have to optimize various parts.

```
    action_mask_inv = torch.logical_not(action_mask)
    # Interpolates action probs with an uniform noise distribution for actions that aren't masked out.
    sample_probs = action_probs.detach() * (1 - epsilon)
    if 0 < epsilon: sample_probs += action_mask_inv / (action_mask_inv.sum(-1,keepdim=True) * (1 / epsilon))
```

I should put the calculation of the inv mask here.

```
        if is_update_head: update_head()
        if is_update_value: state_probs.backward(state_probs_grad())
        if is_update_policy: action_probs.backward(action_probs_grad())
```

Let me get rid of those tracker comments.

```py
def run(i_tabular,i_nn,vs_self,vs_one,neural,uniform_player,tabular): # old NN vs old tabular
    batch_size = 2 ** 10
    with open(f'dump/agent_{i_tabular}_avg.obj','rb') as f: tabular_agent_old = pickle.load(f)

    def r(name,value,policy,head):
        def tabular_player(is_update_head=False,is_update_policy=False,epsilon=0,tabular_agent=tabular_agent_old):
            return tabular.create_policy(tabular_agent,is_update_head,is_update_policy,epsilon)

        def neural_player(is_update_head=False,is_update_value=False,is_update_policy=False,epsilon=0.0):
            return neural.handler(partial(model_evaluate,value,head,policy,is_update_head,is_update_value,is_update_policy,epsilon))

        r : np.ndarray = vs_one(batch_size * 2 ** 8,neural_player(),tabular_player())
        print(f'The mean is {r.mean()} for the {name} player.')
    with open(f'dump/nn_agent_{i_nn}.obj','rb') as f: r('regular',*pickle.load(f))
    with open(f'dump/nn_agent_{i_nn}_avg.obj','rb') as f: r('average',*pickle.load(f))
```

This I am going to have to change.

```py
def create_tabular_agent(n,m,vs_self,vs_one,neural,uniform_player,tabular):
    batch_size = 2 ** 10
    head_decay = 2 ** -2

    tabular_agent = tabular.create_agent()
    tabular_avg_agent = tabular.create_agent()
    def tabular_player(is_update_head=False,is_update_policy=False,epsilon=0.0,agent=tabular_agent):
        return tabular.create_policy(agent,is_update_head,is_update_policy,epsilon)

    def run(is_avg : bool = False):
        tabular.head_multiply_(tabular_agent,head_decay)
        for a in range(3):
            vs_self(batch_size,tabular_player(True,False,2 ** -2,agent=tabular_agent))
        vs_self(batch_size,tabular_player(False,True,2 ** -2,agent=tabular_agent))
        tabular.optimize(tabular_agent)
        if is_avg: tabular.average(tabular_avg_agent,tabular_agent)

    def train():
        print('Training the tabular agent.')
        for a in range(n):
            run()
            if (a + 1) % 30 == 0: print(a+1)

    def avg():
        print('Averaging the tabular agent.')
        for a in range(m):
            run(True)
            if (a + 1) % 30 == 0:
                print(a+1)
                r : np.ndarray = vs_self(batch_size * 2 ** 4,tabular_player(agent=tabular_agent))
                print(f'The mean reward is {r.mean()} for the regular agent.')
                r : np.ndarray = vs_self(batch_size * 2 ** 4,tabular_player(agent=tabular_avg_agent))
                print(f'The mean reward is {r.mean()} for the average agent.')
    train()
    avg()
    with open(f"dump/agent_{n + m}.obj",'wb') as f: pickle.dump(tabular_agent,f)
    with open(f"dump/agent_{n + m}_avg.obj",'wb') as f: pickle.dump(tabular_avg_agent,f)
```

This I can just erase. Not going to need it.

7pm. Let me close here. It is not as easy as just snapping my fingers. I'll run it and connect it to the game UI tomorrow.

My long awaited dream is not that far off. If I could get my hands on neurochips, I bet I could speed up the training by a factor of 1000 for these shallow models. I could implement the game directly on chip and avoid both the data transfer impact and the game being all on a single thread.

Training 1b hands per day in the future is not a far fetched dream. If I could make some money at low stakes, I'll have the resources to actual order one of the things from the companies that make them. Alternatively, I could try applying for a job seriously this time. Groq was the only one whose HR drone was even interested in setting up an interview, and I ended that on a rude note, so I am not sure whether that is an option.

Hmmmm, inquiring about purchasing these devices might be a good way to get around HR.

7:05pm. Nevermind that for now. If the GPU is not enough to beat the small stakes, I might as well curse my bad luck and give up. Not everything has to go my way. Not all lives have to be successful.

But as long as there is a chance I can win, I should go forward."

---
## [Smileysworld/16-Dreamz@a0c7cf9322...](https://github.com/Smileysworld/16-Dreamz/commit/a0c7cf93228cb30ac4adb3ecbc2960e93b143f3e)
##### 2021-06-20 17:28:20 by Micha kinchen

Create Big Shout Out To Scarlet Pereal Casino in biloxi MS Best Place Ever I Know I'm Not Being loyal but If u only Knew I Would Like To Dedacate Every thing To God And My Best Friend Todd Barrel They I Want U know You Made Me Know What's Real Can't Wait to see U Adoptr pearl And big Shout out 16Dreamz Ya Inspired me more than I know

---
## [JessicaTheGunLady/SM-TF2-ProjectileDrop@5e5ed188ce...](https://github.com/JessicaTheGunLady/SM-TF2-ProjectileDrop/commit/5e5ed188ceff527b7ba1efae5c827daa828f2750)
##### 2021-06-20 21:28:27 by JessicaTheGunLady

Improved optimisation

Managed to improve the performance of this plugin by using DataPacks, thanks to NoSoop for reminding me this existed and thanks to my brain for not being so goddamn useless this time, unfortunately it wasn't working well enough for me to create a fixed projectile deviation angle based on the clip size of a weapon, so no fair & balanced Beggar's.

P.S: Jesus Christ I DESPISE Vector values, thank god I only needed to BARELY touch them to make this plugin work because otherwise I would've lost my SANITY trying to put up with them, if someone else wants to attempt the plugin I mentioned then go ahead, 'cause I'm sure as hell not doing it with the mathematical capability I currently have.

---
## [BatoolMM/rweekly.org@0056d543be...](https://github.com/BatoolMM/rweekly.org/commit/0056d543be92d1b735ba79add84b6b27acb927ab)
##### 2021-06-20 22:23:16 by Eric Fletcher

Add R Discord Server to Resources Section

The R discoRd server is a friendly and dedicated community for R enthusiasts, programmers, statisticians, data scientists, and students. Whether you are looking to connect with fellow useRs, have awesome data viz to share, or just needed help with your stats assignment, you are at the right place. The various channels that make up the discoRd server are provided below. Currently, there are 739 members.

Link: https://discord.gg/xKC9zEYsZP

R-MAIN channels:

 #👋🏻general: General chat
#💬r-chat: R-related chat. Please stay on topic or go to #👋🏻general or #😸random 
#📰r-news: Get up-to-date on the latest R-related news.
#💼r-jobs: Discuss R-related careers and job openings 
#😸random: Random chat, doesn't have to be R-related
#💩shitposting: Even more random than #😸random

R-SHARE channels:

#⭐starboard: Featured posts from other R-SHARE channels
#📊r-data-viz: Showcase your best data visualizations here!

R CHALLENGES channels:
#🧩challenge-topics: This is where members can suggest topics for the upcoming challenge.
#🧩challenge-discuss: Discuss solutions or general approach. 
#🧩challenge-starboard: Check out past challenge topics and the best solutions!

GENERAL R HELP channels:

#📜r-help-read-me: Learn about question asking. 

R-RESOURCES:

#📝resources : Post R resources or see what's already posted
#📂r-datasets: Share any R dataset resources here!
#📂reddit-datasets: Daily r/datasets/new subreddit feeds posted here
#📝resources : Post R resources or see what's already posted
#📚library : R books and PDFs 

TOPICAL HELP/DISCUSSION channels:

#📊statistics: General statistics
#💾dbi: Database, data engineering, data workflows, etc.
#📈tidymodels: Modeling, machine learning, statistical modeling, etc.
#✨shiny: Interactive UI, dashboards, RShiny apps, plotly, etc.
#📦package-dev:  Discuss topics about package development
#🌊natural-science: Physics, chemistry, biology, etc.
#🧠social-science: Econometrics, political science, sociology
#🧮bayesians: Bayesian statistics
#🌍gis: Geospatial analysis, maps, shape files, etc.
#💹finance: Finance

---

# [<](2021-06-19.md) 2021-06-20 [>](2021-06-21.md)

