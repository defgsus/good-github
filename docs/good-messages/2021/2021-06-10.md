# [<](2021-06-09.md) 2021-06-10 [>](2021-06-11.md)

3,687,229 events, 1,494,143 push events, 2,453,707 commit messages, 194,896,567 characters


## [gnoem/garden@367c31030f...](https://github.com/gnoem/garden/commit/367c31030f4872af1993330753955eda5cb94f26)
##### 2021-06-10 06:06:51 by Gnome

absolutely ridiculous workaround for safari ignoring bottom padding for some reason

i wasted 2 hours of my life on this shit

---
## [gitster/git@afffc20b47...](https://github.com/gitster/git/commit/afffc20b4780ac1404c01e9fc642668c8e3929ef)
##### 2021-06-10 06:18:56 by Derrick Stolee

CodingGuidelines: recommend singular they

Technical writing seeks to convey information with minimal friction. One
way that a reader can experience friction is if they encounter a
description of "a user" that is later simplified using a gendered
pronoun. If the reader does not consider that pronoun to apply to them,
then they can experience cognitive dissonance that removes focus from
the information.

If we use singular "they/them" pronouns instead of "he/him" or "she/her"
pronouns, then we can avoid this congitive load on the reader.

Using singular "they" is also incredibly efficient. Choosing a gendered
pronoun is usually arbitrary between "he" or "she". Using constructs
such as "he or she", "s/he", or "(s)he" are more complicated than
singular "they".

When choosing a gendered pronoun, that pronoun no longer applies to
nearly half of possible readers. Even if we alternated between "he/him"
and "she/her" perfectly evenly, we would still expect male and female
readers to experience an incorrect pronoun half the time. However, some
readers will not prescribe to either of these binary genders. Those
readers hence suffer an incorrect pronoun the entire time. Singular
"they" applies to every reader.

Perhaps due to similar reasons, official style guides have changed their
stance on singuler "they" in recent years. For example, the APA style
guide changed their official recommendation in 2019 [1]. The MLA
handbook also references helpful ways to use singular "they" [2]. While
not fully endorsing it, the Chicago Manual of Style has removed its
blanket ban on singular "they" [3] (the previous recommendation was to
only use "it" as a singular non-gendered pronoun).

[1] https://apastyle.apa.org/blog/singular-they
[2] https://style.mla.org/using-singular-they/
[3] https://libraries.indiana.edu/chicago-manual-style-singular-pronoun-they

While not all styleguides are updating their recommendations, we can
make a choice as a project to adopt the concept because of the
efficiencies above, as well as the benefits of increased inclusion.

To futher justify singular "they" as an acceptable grammatical concept,
I include the careful research of brian m. carlson who collected their
thoughts on this matter [4] (lightly edited):

  Singular "they" has been used by native English speakers as part of
  the language for over half a millennium and is widely used and
  understood. This usage is specified in Merriam Webster[5]:

    The use of they, their, them, and themselves as pronouns of
    indefinite gender and indefinite number is well established in
    speech and writing, even in literary and formal contexts.

  Wiktionary notes[6] (references omitted):

    Usage of they as a singular pronoun began in the 1300s and has been
    common ever since, despite attempts by some grammarians, beginning
    in 1795, to condemn it as a violation of traditional (Latinate)
    agreement rules. Some other grammarians have countered that criticism
    since at least 1896. Fowler's Modern English Usage (third edition)
    notes that it "is being left unaltered by copy editors" and is "not
    widely felt to lie in a prohibited zone." Some authors compare use of
    singular they to widespread use of singular you instead of thou.

  Linguists fit roughly into two camps: prescriptive and descriptive.
  The former specify rules for people to use, and the latter document
  language as it is actually used without forming a judgment.

  Some prescriptivists think it is acceptable, and some do not. But
  descriptivists will rightly note that it is and has been commonly
  used in English across countries, cultures, and contexts for an
  extended period of time and is therefore generally accepted by most
  English speakers as a normal part of the language.  Since we are
  writing text for an English language audience who are mostly not
  linguists, we should probably consider using the language that most
  people will use in this context.

[4] https://lore.kernel.org/git/YKrk4dEjEm6+48ji@camp.crustytoothpaste.net/
[5] https://www.merriam-webster.com/dictionary/they
[6] https://en.wiktionary.org/wiki/they

Since singular "they" might be unfamiliar to some, we also list an
option to rephrase writing to use singular "you" or plural "they". We
can use singular "you" to refer to the reader instead of an abstract
user. Plural "they" works if we refer to multiple abstract users instead
of one. The English language does not have gendered versions of these
terms.

If we refer to a specific person, then using a gendered pronoun is
appropriate. There can also be other cases where it is inappropriate for
us to update the existing examples within the Git codebase, such as:

* References to real people (e.g. Linus Torvalds, "the Git maintainer").
  Do not misgender real people. If there is any doubt to the gender of a
  person, then use singular "they".

* References to fictional people with clear genders (e.g. Alice and
  Bob).

* Sample text used in test cases (e.g t3702, t6432).

* The official text of the GPL license contains uses of "he or she", but
  modifying the license this way is not within the scope of the Git
  project.

* Literal email messages in Documentation/howto/ should not be edited
  for grammatical concerns such as this, unless we update the entire
  document to fit the standard documentation format. If such an effort is
  taken on, then the authorship would change and no longer refer to the
  exact mail message.

* External projects consumed in contrib/ should not deviate solely for
  style reasons. Recommended edits should be contributed to those
  projects directly.

Other cases within the Git project were cleaned up by the previous
changes.

Helped-by: Junio C Hamano <gitster@pobox.com>
Signed-off-by: Derrick Stolee <dstolee@microsoft.com>
Signed-off-by: Junio C Hamano <gitster@pobox.com>

---
## [ropenscilabs/gendercoder@2d10761fff...](https://github.com/ropenscilabs/gendercoder/commit/2d10761fffe705941ff9fcb990ada40c09fc357a)
##### 2021-06-10 07:02:00 by Lingtax

converts outputs from male and female to man or boy and woman or girl

---
## [eatmyvenom/hyarcade@b01a4523cb...](https://github.com/eatmyvenom/hyarcade/commit/b01a4523cb43be78b813c2562e03fca57275b4a5)
##### 2021-06-10 08:18:20 by eatmyvenom

force mw bot mode

diverge mw mode

logs

bruh

Auhdashdidhusdisuhadafshuhfudis

ae

why did it have to break

dgfafdafddasfdas

aaaaaaaaaaaaaaaa

hhhh

eawaeeaeaaeeaseaeaeaaeeaeaeaeaaeeaeaeaaeaeaeeaeaaeeaeaaeaeaeeaeaaeaeaeeaeaaeeaeeaeeaeaeaea

WHAT THE FUCK

gggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggg

im going to hurl my pc into a fucking dump

i wish the void of emotion i feel could be replaced with void of life

fix logs

---
## [mrakgr/The-Spiral-Language@7b4c5a80c1...](https://github.com/mrakgr/The-Spiral-Language/commit/7b4c5a80c1d7ae4dfe2ab123aff7b37e46c114a0)
##### 2021-06-10 16:21:47 by Marko GrdiniÄ‡

"2:40pm. https://github.com/dhruvramani/Transformers-RL/blob/master/layers.py

This is a different library than the one I am looking at. It is ridiculously complex.

https://github.com/lucidrains/x-transformers
> pip install x-transformers

Let me get an example to run.

```py
import torch
from x_transformers import XTransformer

model = XTransformer(
    dim = 512,
    enc_num_tokens = 256,
    enc_depth = 6,
    enc_heads = 8,
    enc_max_seq_len = 1024,
    dec_num_tokens = 256,
    dec_depth = 6,
    dec_heads = 8,
    dec_max_seq_len = 1024,
    tie_token_emb = True      # tie embeddings of encoder and decoder
)

src = torch.randint(0, 256, (1, 1024))
src_mask = torch.ones_like(src).bool()
tgt = torch.randint(0, 256, (1, 1024))
tgt_mask = torch.ones_like(tgt).bool()

loss = model(src, tgt, src_mask = src_mask, tgt_mask = tgt_mask) # (1, 1024, 512)
loss.backward()
```

This runs.

Let me take a look at the source for a bit.

2:50pm. Hmmm, I thought the library would be huge, but it is only 900 LOC. This is not too much given that it has a couple of papers implemented as a part of it.

```py
new_mems = list(map(lambda t: t[..., -self.max_mem_len:, :].detach(), new_mems))
```

Huh, what are these 3 dots here?

https://stackoverflow.com/questions/772124/what-does-the-ellipsis-object-do

Ah, I see. Moving on.

3:05pm. https://arxiv.org/abs/2001.04451
Reformer: The Efficient Transformer

Let me read this. I am starting to come to a decision. The transformers are too complex for me to reimplement them on my own while using the library as a reference.

If I had a couple of weeks or a full month, that might be a decent intermediate learning project, but right now I want to keep pushing forward. For once, I am just going to take a black boy component and just plug it in, and hope the library does not have bugs in it. I'll just go with what works best in the end. If the transformers are too slow or start giving me trouble then I'll either go back to MLPs or roll my own.

It is not just that they would be hard to implement. But all the testing that would need to be done in order to have an assurance of their performance would be quite hard.

Do I feel like working on an equivalent of a char RNN all over again? Hell no.

```
src = torch.randint(0, 256, (1, 1024))
src_mask = torch.ones_like(src).bool()
tgt = torch.randint(0, 256, (1, 1024))
tgt_mask = torch.ones_like(tgt).bool()

loss = model(src,mask=src_mask)
print(loss.shape)
```

Hmmm, it actually takes it a second to run this.

Let me set the depth to 1.

3:35pm. https://github.com/jerrodparker20/adaptive-transformers-in-rl

Let me check this out.

https://arxiv.org/abs/2004.03761
Adaptive Transformers in RL

https://github.com/lucidrains/compressive-transformer-pytorch

Lucidrains is a beast. But I myself am thinking that transformers might be too much and that I'd be better off changing the serializer so it does ints in binary.

4:05pm. I am seeing some how-to implement transformer tutorials on the net. If I am really serious I should go for them.

But no. I can't just pick one from the library. Transformers are too complex, and for such an integral piece, I'd want to implement them myself so I can understand them.

It is out of the question to take something I do not need, just because I am attracted by its fanciness.

4:10pm. I'll definitely be using transformers for something in the future - just not poker.

Instead of transformers I'd be better off doing a new serializer that can serialize int in binary format.

Even if I used MLPs, it is not like inputs of 1-2k elements which I would need to get max sequence length in HU NL would be that large.

Forget it.

If I was forced to say clearly what I want here, that would be neurochips. I'd implement RNNs on them and add transformer like memory mechanisms on the side.

I would have used transformers if all of the library examples that I could find weren't so ridiculously complex. The complexity expense of transformers is something I do not need right now.

4:15pm. I know that in ML, nothing ever comes for free. I could take out the transformers and run them, and they would fail to work. Then what?

With those libraries, I'd have to look under the hood. But by the time I am doing that, I might as well be implementing my own thing. Is there a single thing in ML that you can just take, and have it work right off the bat?

And this RL we are talking about, transformers did not work on it originally.

4:25pm. That is just an excuse. It is an excuse for me to not do unnecessary work.

4:35pm. https://www.reddit.com/r/reinforcementlearning/comments/ns7xmj/r_reinforcement_learning_as_one_big_sequence/

https://arxiv.org/abs/2102.11972
Do Transformer Modifications Transfer Across Implementations and Applications?

Let me check this out. I am still trying to make a decision.

4:50pm. Let me just ask on the RL sub.

5:15pm. https://www.reddit.com/r/reinforcementlearning/comments/nwq2uo/d_would_transformers_be_expected_to_work_better/

Maybe I'll get some interesting replies from this.

5:40pm. I am thinking. My mood is so low right now.

The thought of wasting 2 week trying out transformers on some toy game is throwing my motivation into the trash.

But now that I think about it, there is no way transformers will beat MLPs on a game with short sequences like Leduc. To get a significant benefit, it would have to be on a game with longer sequences like Holdem.

So I am thinking about compromising. I am done with Leduc. I want to put it behind me.

Instead why don't I train an MLP player with short fixed windows, and then pit a transformer against it?

Why am I obsessed about trying it out on Leduc anyway? I can train a transformer on NL Holdem. Against something like a random player, it should see improvement pretty quickly.

I can benchmark MLPs and transformers that way.

And if deep stack Holdem is too big, I can get the essence of the game by making the flop the final round and shrinking the stack sizes. Hell, I can shrink the deck as well. It would be bigger than Leduc, but it might still be a tangible toy task for tabular agents.

6:05pm. Done with lunch.

Yes, I should do transformers, but it should be a gradual process. Since there is no point in pitting them on Leduc, I have no need to fall into my old habit of doing side quests. Instead their evaluation should be done as part of the main one. I'll reimplement them from scratch and try them out piece by piece. There is no need to make a fuss over this.

6:10pm. It is going to happen - my next step is NL Hodem. I am going to start work on it tomorrow and nothing will stop me from finishing it this time.

I'll do Holdem, the UI for it, and the uniform player. Once that is done I will get serious about training agents to master it. At the end of this road I will have my first real world power. The foundation I've attained is solid, now it is time for the first seeds to sprout.

6:15pm. Today was lackluster. I'll finish reading the paper and then close. Though I've looked at many things today, I haven't been focusing on my studies at all. I was mired in indecision the whole time. My thoughts were wandering.

I just had to consider where the transformers fit into my plans. My fault was trying to push them into the Leduc-shaped hole and getting depressed when the peg did not fit.

I am tired and fatigued from this long journey. NL Holdem, just this one thing I want to do in life, and then lightning can strike me for all I care. I will do it even if I die and become a ghost.

I do not know what lies beyond it, but I hope that the power I will attain will be enough to tackle the challenge."

---
## [matjsilva/jarparur@1f1294f8ac...](https://github.com/matjsilva/jarparur/commit/1f1294f8acae4bf5f007f8a9ff3bc6371401cbad)
##### 2021-06-10 17:27:04 by matjs

Not alone anymore

Ok, this update is HUGE! It's not just a system, but three!

Let's start from the beginning, where i was on the happy start from the last 12 hours i passed trying to solve and connect a bunch of features.
Alright, the first thing i did after the last update was to move freely across the proto world of Jarparur as if it was the most happy thing that happened to me, well, it felt pretty empty so i realized i could add some folks. Yea, that's what i did actually.

[+] characterCreator => createNpcs() -> The first HUGE addition: NPC Generator and NPC Manager, two extremely important parts of this thing that i refer as a Game Engine, the NPC Generator do as its name, generates new dudes that spawn at different location according to its race and class, for example, an orc can only spawn at "Deserto dos Ossos" because it's the orcish country in Jarparur's lore, so it makes sense, and so on. The NPC Manager also does as its name (what a surprise, a good name for a good function wow) this piece of the engine here can tell me where a folk is, what is his or her name (oh, i almost forgot to tell, i added a genre attribute, which influences the name of the creature), how is the folk feeling, an so on. This is just the first.

[+] Global Time => Oh yeah, finally the world can spin around its axis. Day an night doesn't really exist yet, this thing that i called global time is just a really REALLY big integer that is interpreted as seconds and is formatted to a readable string, the fact that it's just a huge integer makes possible for me to easily manipulate it, just by adding some seconds to the global time because the player has moved to another place or to add some days because the player has moved to another country, also, it helps with World Events (shhhhh nobody knows about this yet!), that's it. Time.

[+] Special Event Screens => Finally you can drop into Sul or Deserto dos Ossos and see its description and languages, or see the secret description of somebody's house, who knows, also, you can enter at hacker mode if you can find the special command to enter at items console manager (trust me, it isn't 'items'), or enter at the NPC manager to see with mAGiC where somebody is or how is somebody feeling. Don't know, just use it.

[*] characterCreator => solved some save bugs

[*] inventorySystem => loop bugs, solved with boolean magic

[*] prologue => here comes the CREATE A WORLD

[*] worldData => got rid of "day", "month" and "year", now we have "globalTime", much better!

Jokes aside, that was a pretty good experience, this game as it self is starting to feel very special for me, i'm learning a lot, that's what matters. See you (who reads this horrible text for no specific reason) next time.

---
## [tsipa/bpf-next@f40bf32dc1...](https://github.com/tsipa/bpf-next/commit/f40bf32dc1e22f315a08d4646542ece49998b623)
##### 2021-06-10 19:03:47 by Maciej Å»enczykowski

bpf: do not change gso_size during bpf_skb_change_proto()

This is technically a backwards incompatible change in behaviour,
but I'm going to argue that it is very unlikely to break things,
and likely to fix *far* more then it breaks.

In no particular order, various reasons follow:

(a) I've long had a bug assigned to myself to debug a super rare kernel
crash on Android Pixel phones which can (per stacktrace) be traced back
to bpf clat ipv6 to ipv4 protocol conversion causing some sort of ugly
failure much later on during transmit deep in the GSO engine, AFAICT
precisely because of this change to gso_size, though I've never been able
to manually reproduce it.
I believe it may be related to the particular network offload support
of attached usb ethernet dongle being used for tethering off of an
IPv6-only cellular connection.  The reason might be we end up with more
segments than max permitted, or with a gso packet with only one segment...
(either way we break some assumption and hit a BUG_ON)

(b) There is no check that the gso_size is > 20 when reducing it by 20,
so we might end up with a negative (or underflowing) gso_size or
a gso_size of 0.  This can't possibly be good.
Indeed this is probably somehow exploitable (or at least can result
in a kernel crash) by delivering crafted packets and perhaps triggering
an infinite loop or a divide by zero...
As a reminder: gso_size (mss) is related to mtu, but not directly
derived from it: gso_size/mss may be significantly smaller then
one would get by deriving from local mtu.  And on some nics (which
do loose mtu checking on receive, it may even potentially be larger,
for example my work pc with 1500 mtu can receive 1520 byte frames
[and sometimes does due to bugs in a vendor plat46 implementation]).
Indeed even just going from 21 to 1 is potentially problematic because
it increases the number of segments by a factor of 21 (think DoS,
or some other crash due to too many segments).

(c) It's always safe to not increase the gso_size, because it doesn't
result in the max packet size increasing.  So the skb_increase_gso_size()
call was always unnecessary for correctness (and outright undesirable, see
later).  As such the only part which is potentially dangerous (ie. could
cause backwards compatibility issues) is the removal of the
skb_decrease_gso_size() call.

(d) If the packets are ultimately destined to the local device, then
there is absolutely no benefit to playing around with gso_size.
It only matters if the packets will egress the device.  ie. we're
either forwarding, or transmitting from the device.

(e) This logic only triggers for packets which are GSO.  It does not
trigger for skbs which are not GSO.  It will not convert a non-GSO mtu
sized packet into a GSO packet (and you don't even know what the mtu is,
so you can't even fix it).  As such your transmit path must *already* be
able to handle an mtu 20 bytes larger then your receive path (for ipv4
to ipv6 translation) - and indeed 28 bytes larger due to ipv4 fragments.
Thus removing the skb_decrease_gso_size() call doesn't actually increase
the size of the packets your transmit side must be able to handle.
ie. to handle non-GSO max-mtu packets, the ipv4/ipv6 device/route mtus
must already be set correctly.  Since for example with an ipv4 egress mtu
of 1500, ipv4 to ipv6 translation will already build 1520 byte ipv6 frames,
so you need a 1520 byte device mtu.  This means if your ipv6 device's
egress mtu is 1280, your ipv4 route must be 1260 (and actually 1252,
because of the need to handle fragments).  This is to handle normal non-GSO
packets.  Thus the reduction is simply not needed for GSO packets,
because when they're correctly built, they will already be the right size.

(f) TSO/GSO should be able to exactly undo GRO: the number of packets
(TCP segments) should not be modified, so that TCP's mss counting works
correctly (this matters for congestion control).
If protocol conversion changes the gso_size, then the number of TCP segments
may increase or decrease.  Packet loss after protocol conversion can result
in partial loss of mss segments that the sender sent.  How's the sending
TCP stack going to react to receiving ACKs/SACKs in the middle of the
segments it sent?

(g) skb_{decrease,increase}_gso_size() are already no-ops for GSO_BY_FRAGS
case (besides triggering WARN_ON_ONCE). This means you already cannot
guarantee that gso_size (and thus resulting packet mtu) is changed.
ie. you must assume it won't be changed.

(h) changing gso_size is outright buggy for UDP GSO packets, where framing
matters (I believe that's also the case for SCTP, but it's already excluded
by [g]).  So the only remaining case is TCP, which also doesn't want it
(see [f]).

(i) see also the reasoning on the previous attempt at fixing this
(commit fa7b83bf3b156c767f3e4a25bbf3817b08f3ff8e) which shows
that the current behaviour causes TCP packet loss:

  In the forwarding path GRO -> BPF 6 to 4 -> GSO for TCP traffic, the
  coalesced packet payload can be > MSS, but < MSS + 20.

  bpf_skb_proto_6_to_4() will upgrade the MSS and it can be > the payload
  length. After then tcp_gso_segment checks for the payload length if it
  is <= MSS. The condition is causing the packet to be dropped.

  tcp_gso_segment():
    [...]
    mss = skb_shinfo(skb)->gso_size;
    if (unlikely(skb->len <= mss)) goto out;
    [...]

Thus changing the gso_size is simply a very bad idea.
Increasing is unnecessary and buggy, and decreasing can go negative.

Cc: Dongseok Yi <dseok.yi@samsung.com>
Cc: Daniel Borkmann <daniel@iogearbox.net>
Cc: Willem de Bruijn <willemb@google.com>
Fixes: 6578171a7ff0 ("bpf: add bpf_skb_change_proto helper")
Signed-off-by: Maciej Å»enczykowski <maze@google.com>

---
## [ivanmixo/BeeStation-Hornet@1405821172...](https://github.com/ivanmixo/BeeStation-Hornet/commit/1405821172a86f948711881af641b9d384719ab2)
##### 2021-06-10 19:46:46 by bluezorua

adds a new clown mask, the madman, a madness combat reference (#4356)

* funni reference

* fixes bug

* oh fuck oh shit im stupid

---
## [burgmaister-game/chatty@2a031424ab...](https://github.com/burgmaister-game/chatty/commit/2a031424ab0549b3c669760d5a98f35b276e6d54)
##### 2021-06-10 21:11:54 by Pawel Kuznik

Credited my wonderful and supportive girlfriend forgiving my valuable feedback.

---

# [<](2021-06-09.md) 2021-06-10 [>](2021-06-11.md)

