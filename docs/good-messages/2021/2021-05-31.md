# [<](2021-05-30.md) 2021-05-31 [>](2021-06-01.md)

2,957,743 events, 1,453,293 push events, 2,372,093 commit messages, 155,666,551 characters


## [JohnFulpWillard/fulpstation@f4a04ca124...](https://github.com/JohnFulpWillard/fulpstation/commit/f4a04ca1248ebd02b7f682f57a8770436097116e)
##### 2021-05-31 05:31:32 by SgtHunk

Updates Selenestation with directionals (#172)

* a lot of directional

commiting before i experiment more

* stuff before i continue

* SHOULD BE ALL???

* i dont want to play with you anymore

updatepaths my beloved locket gif

* yo mama

gets rid of strays and fixes a few fucked-up displays

Co-authored-by: Enricode <SgtHunk@users.noreply.github.com>

---
## [wooga/macos-security.groovy@8be2798ba0...](https://github.com/wooga/macos-security.groovy/commit/8be2798ba0e6b0b0fa7862425990d6554ea46df4)
##### 2021-05-31 09:14:13 by Manfred Endres

add JNI lib for interacting with macOS security framework

Description
===========

The `security` commandline tool on macOS is a horrible tool.
It has a very ugly interface and most commands have weird ways of
returnning values. Sometimes stuff gets pushed to `stdout` and
sometimes to `stderr`. Not all features are supported, like getting
the actual status of a keychain. To have more control over the
execution and the output I decided to move some of the core keychain
handling logic into a JNI lib.
At the moment this means most APIs that provide basic keychain
logic (`create`, `lock`, `unlock`, etc) are now implemented with
a JNI lib. The rest of the functionality is still provided with
the `security` cli wrapper commands.

JNI
---

JNI on its own is a tough bit of software. I decided to use the
[nokee] gradle plugin suite to build package a JNI jar with a native
libraty written in pure `c`. This is all contained in a seperate
gradle subproject.

```

 ┌────────────────┐
 │                │
 │ macos-security │
 │                │
 └───────┬────────┘
         │
         │        ┌────────────────────┐
         │        │                    │
         └───────►│ macos-security-jni │
                  │                    │
                  └────────────────────┘
```

This new project contains two major wrappers.

* MacOsSecurity
* KeychainRef

The `MacOsSecurity` class is the main native interface and provides
the raw JNI methods. I decided to wrap the functions provided by
the apple [Security Framework] as close as possible without to much
JNI back and forth. Means when possible only use scalar parameters
and simple types JNI supports out of the box.

`KeychainRef` on the other hand contains methods that allow to pass
more jave natural objects like `File`, `List<>` etc. These will
be translated back and forth. My main intention is that
`MacOsSecurity` should not be used directly from consuming libraries.

The `KeychainRef` object is also interesting because of a second fact.
This object is mainly an interface. It's the main object returned
and or passed to `MacOsSecurity`. The kechain references in the
[Security Framework] are memory managed with a reference counting
model. The moment we pass a keychain ref to JAVA land we are responsible
of `releasing` it again. To make this as easy as possible I decided
to create this interface. The `c` code will instanciate an object
which implements this interface which is not constructable from Java.
This object has a `finalizer` which calls into JNI again to release
the reference. This makes it easy to use these references without
thinking to much about the underlying memory management model.

Breaking Chagnes
----------------

This patch will introduce breaking API changes. I thought about this
long and think to leverage this new JNI interface best we should use
the `KeychainRef` as the main handle to pass keychain references.
The old API used `File` objects since keychains are just files on the
harddisc. But I like the fact to use a strict datatype to describe
a keychain rather that a generic `File` object. I updated everything
to use the new object. From the outside it is quite easy to move from
the `File` based API to the new one. The `KeychainRef` interface has
static methods to either create or open (get) keychain references from
files. The underlying files don't have to exists to fetch a valid
`KeychainRef`.

Publish Changes
---------------

I updated also the publish flow slightly since we need to publish
two jars at the same time. I didn't want to open a new repo for
the JNI part as this belongs together in a single repo anyways.
I moved the main publish configuration for gradle into a file
`publish-helper.gradle` which is sourced by the two gradle projects.
The rest is taken care of by the maven-publish/nexus publish plugins.

This patch is rather big for my taste. I don't think breaking it up
makes a lot of sense here.

Changes
=======

* ![ADD] ![GRADLE] subproject `macos-security-jni`
* ![ADD] `JNI` library to expose [Security Framework] functions to Java
* ![BREAK] Use `KeychainRef` as the base keychain objects rather than `File`
* ![IMPROVE] publish to publish multiple projects (jars) at the same time

[Security Framework]: https://developer.apple.com/documentation/security/keychain_services/keychains?language=objc

---
## [VedantPol/CODEFORCES_ROUND_SOLUTIONS@21d379513f...](https://github.com/VedantPol/CODEFORCES_ROUND_SOLUTIONS/commit/21d379513f20f9df863e898bdf2a2065bb113312)
##### 2021-05-31 14:00:01 by vedant pol

Mishka and Game Codeforces Round #365 (Div. 2)

Mishka is a little polar bear. As known, little bears loves spending their free time playing dice for chocolates. Once in a wonderful sunny morning, walking around blocks of ice, Mishka met her friend Chris, and they started playing the game.

Rules of the game are very simple: at first number of rounds n is defined. In every round each of the players throws a cubical dice with distinct numbers from 1 to 6 written on its faces. Player, whose value after throwing the dice is greater, wins the round. In case if player dice values are equal, no one of them is a winner.

In average, player, who won most of the rounds, is the winner of the game. In case if two players won the same number of rounds, the result of the game is draw.

Mishka is still very little and can't count wins and losses, so she asked you to watch their game and determine its result. Please help her!

Input
The first line of the input contains single integer n n (1 ≤ n ≤ 100) — the number of game rounds.

The next n lines contains rounds description. i-th of them contains pair of integers mi and ci (1 ≤ mi,  ci ≤ 6) — values on dice upper face after Mishka's and Chris' throws in i-th round respectively.

Output
If Mishka is the winner of the game, print "Mishka" (without quotes) in the only line.

If Chris is the winner of the game, print "Chris" (without quotes) in the only line.

If the result of the game is draw, print "Friendship is magic!^^" (without quotes) in the only line.

Examples
inputCopy
3
3 5
2 1
4 2
outputCopy
Mishka
inputCopy
2
6 1
1 6
outputCopy
Friendship is magic!^^
inputCopy
3
1 5
3 3
2 2
outputCopy
Chris

---
## [mrakgr/The-Spiral-Language@80d998f8e4...](https://github.com/mrakgr/The-Spiral-Language/commit/80d998f8e4ca14e91d6289594b03391cb5d2f08a)
##### 2021-05-31 17:06:35 by Marko Grdinić

"2:05pm. Sigh. My stress is high, and I am not doing much. I keep going in circles in my mind.

Right now the idea that has bubbled to the top is to use the infinity norm to normalize the gradients in each individual layer.

I am starting to think that anything that requires clipping from above is the wrong choice.

But since signSGD gives poor generalization, there should be a compromise between it and SGD. Per layer infinity norm normalization is the answer. The weakness is that outlier might slow down the whole net, but with regular SGD, those outliers would lead to blowups anyway.

The best choice might be to interpolate between the signSGD update and the infinity norm one. Something like 0.1 signSGD and 0.9 infNorm.

That way I have a lower bound on how much I want to update and the upper bound. This might work just a bit better than the either of the two.

```fs
let inf_norm = 0.5
let f x = float (sign x) * 0.1 + x / inf_norm * 0.9
f 0.001
```

```
val inf_norm : float = 0.5
val f : x:float -> float
val it : float = 0.1018
```

I am playing around with it. This is good.

Now the actual gradient information will be taken into account, but it won't dominate the optimization. By using this update, I'll get the best of both worlds, and the stability that I require.

https://pytorch.org/docs/stable/linalg.html#torch.linalg.norm

I'll be able to use this.

I am going to have to be careful to flatten the weight tensor first before using this. Don't want to sum things up.

2:45pm. Why does the Spiral server shut down sometimes? I don't understand that.

TODO: Why is the Spiral server shutting down sometimes?

I'll add a TODO for the time being.

3:55pm. I had to spend some time in bed.

At any rate, I've collected all my experiences and worked out the semi-tabular method.

Now that I look at it, it resembles an EM procedure. It feels similar to k-means.

It should be particularly stable. In fact, if it is merely training the head of the value net, I won't even need backprop.

Now where do I start? I'd rather call it a day here, but I should do a bit more.

```
// Since the rest failed, I'll try multiple critic steps.

// Update: No, I am having too much trouble dealing with gradient descent.
// Tabular RL and supervised ML are on one side, but this is entirely on the other
// in terms of difficulty. It is not optimizing properly and I can berely even tell
// whether the learning rate is off, or whether I should let it run for longer or if
// it is collapsing, or if there is a bug in the code on top of all of that.

// I should also be using weighted values in the value net and that is not compatible
// with backprop.

// I am crap the useless variance matching idea and use an update that I've come up with
// which interpolates between signSGD and infinity norm normalization over the layers.
packages: |core2-
modules:
    serialization/
        dense/
            array
        sparse/
            int
    utils-
    sampling
    nodes-
    leduc
    train
    agent/
        uniform
        neural
    uniform_test
    neural_test
```

I am going to wipe all this and start from scratch. I'll forget what I've been doing for the past 10 days, and reconsider everything. To make Python interop smoother, I think I might go forward with the plan to extend the language so it can compile library functions.

```
inl policy (epsilon : f32) (leduc_net {policy value schema}) (l : ra _ _) =
    inl len = length l
    inl policy_size = serialization.dense.array.size schema.policy
    inl value_size = serialization.dense.array.size schema.value
    inl actions_size = serialization.sparse.int.size schema.actions
    inl data_value : obj = $"torch.zeros(!len,!value_size)"
    l |> am.iteri fun b (player_state, (leduc_state (p1,p2,community_card)), pid, actions) =>
        inl seq = pl2_observations player_state pid |> obs_to_array
        inl pot = if pid = p1.id then p1.pot else p2.pot
        schema.value.pickle (leduc_policy_input {pid= $"!pid"; pot= $"!pot"; seq},if pid = p1.id then p2.card else p1.card) (0, $"!data_value[!b,:].numpy()")
    inl action_indices : a u64 (a u64 _) = l |> am.generic.map fun (_,_,_,actions) => am.map schema.actions.pickle actions
    inl policy_raw : obj = $"!policy.forward(!data_value[:,:!policy_size])"
    inl policy_mask : obj = $"torch.full((!len,!actions_size),float('-inf'))"
    am.iteri (fun b => am.iter (fun a => $"!policy_mask[!b,!a] = 0")) action_indices
    inl policy_log_probs : obj = $"torch.log_softmax(!policy_mask + !policy_raw.cpu(),dim=-1)"
    inl policy_probs : obj = $"torch.exp(!policy_log_probs.detach())"

    inl sample_probs : obj =
        if epsilon = 0 then policy_probs else
        inl sample_probs : obj = $"torch.empty_like(!policy_probs)"
        am.iteri (fun b l =>
            inl valid_size = f32 (length l)
            loop.for' (from:0 nearTo: actions_size) fun a =>
                inl x = if $"!policy_mask[!b,!a]" = 0f32 then epsilon / valid_size + (1.0 - epsilon) * $"!policy_probs[!b,!a]" else 0
                $"!sample_probs[!b,!a] = !x"
            ) action_indices
        sample_probs
    inl sample_indices : a u64 i64 = $"torch.distributions.Categorical(!sample_probs).sample().numpy()"

    inl l' : a _ _ =
        am.mapi (fun b a =>
            inl policy : f32 = $"!policy_probs[!b,!a]"
            inl sample : f32 = $"!sample_probs[!b,!a]"
            log_prob_from {policy sample}, schema.actions.unpickle a
            ) sample_indices
    l', fun (r : a u64 r2) =>
        if length r = 0 then r else
        inl value_raw : obj = $"!value.forward(!data_value).cpu()"
        inl value_sampled : obj = $"!value_raw.detach().clone()"
        inl is_value_training = $"!value.training"
        inl value_signal : obj = if is_value_training then $"torch.zeros_like(!value_raw)" else $"None"

        // The multiplication by the regret probs is what makes the algorithm CFR instead vanilla PG.
        let regret_prob player_state,_,pid,_ =
            inl prob = pl2_probs player_state pid
            inl env_prob = prob.chance +@ prob.op
            exp (-prob.self.sample + (env_prob.policy - env_prob.sample))

        am.iteri2 (fun b a (r2 r) =>
            inl sample : f32 = $"!sample_probs[!b,!a]"
            inl q : f32 = $"!value_sampled[!b,!a]"
            if nan_is q then failwith "The value prediction is nan."
            inl dif : f32 = $"!r - !q"
            inl regret_prob = regret_prob (index l b)
            if is_value_training then $"!value_signal[!b,!a] -= !dif * !regret_prob"
            $"!value_sampled[!b,!a] += !dif / !sample"
            ) sample_indices r
        // $"print(torch.mean(!value_signal))"
        if is_value_training then $"!value_raw.backward(!value_signal)"

        inl means : obj = $"torch.einsum('ab,ab->a',!value_sampled,!policy_probs)"
        if $"!policy.training" then
            inl regret_prob_times_pids : obj =
                am.generic.mapi (fun b (_,_,pid,_ as a) => regret_prob a * if pid = 0 then 1 else -1f32) l
                |> fun (x : a _ _) => $"torch.from_numpy(!x)"
            $"!policy_log_probs.backward(!regret_prob_times_pids.unsqueeze(-1) * (!means.unsqueeze(-1) - !value_sampled))"
        $"!means.numpy()" : a u64 r2
```

This was annoying enough to write, but every time I make a change I have to wait 20s for Cython to recompile the thing. This really bothers me when everything is broken and I have to push forward to debug.

4:05pm. Thankfully everything I suspicious of has to do with NNs. The game and the training scheme I doubt have bugs. They might, I am rarely thorough in anything. But it is not the problem.

The biggest problem overall is that the code is hard to interact with because the feedback loop is so slow. I am really feeling the disadvantages of working in two different languages right now.

4:10pm. So for this next part, I should write out the optimizer and the NN policy function like the one above in pure Python. As a matter of principle, I should minimize the amount of interaction I do between Spiral and PyTorch. I've been worrying about making sure those serializers do not end up as closures. Who gives a rat's ass about that. Put all the Spiral stuff into closures and move on.

I've severely underestimated how easy it would be to get these algorithms to work. I am getting my ass kicked here. And my dumb optimization ideas aren't making things any better.

Forget the past 5 weeks, and do it anew. That is the way to go here.

NNs are hard, but they should not be overly hard with the right approach.

4:15pm. Instead of ranting here, let me do some programming. First let me see how the SGD works. That will tell me how the layer norming one should go.

4:40pm.

```
// Since the past attempts failed, I am going to try something else here.
// Inspired by the signSGN papers, for the optimizer I am going to make an update
// that interpolates signSGD + infinity norm gradient normalization.
// For the value network, since I absolutely need the values to act as weighted
// moving averages, I am going to semi-tabular CFR in the value head.

// Instead of one hot vectors like in full tabular, the semi tabular will have
// probabilistic vectors. I'll take a log softmax over the body, exp it and use
// that as the input to the head which be optimized using the tabular algorithm.

// For getting the gradients for the value body, I'll use the absolute value differential
// between the given and the predicted error, center it with regards to the probabilistic mean
// and use that as the backwards input for the log softmax.

// Iterating updating the value head and body will make this an EM procedure similar to k-means.
// The main reason why I am going for this besides being able to weight the values is because
// right now I can't tell at all whether the value update works. If I had a tabular agent, this
// kind of debugging would be a lot easier.

// The issue with the value net is that it needs to learn the reward magnitudes in the weights,
// so I cannot use something like signSGD (which is Adam with full batch learning) to stabilize it.
// This is a source of many of my headaches. NNs are good for probability distributions, but bad
// for learning large ranges of values.

// With this method I'll be able to optimize the value head and get something useful even without
// necessarily optimizing the body.

// The actor on the other hand will be a lot easier to deal with as I will be able to use plain
// backprop with the aforementioned optimizer.
packages: |core2-
modules:
    serialization/
        dense/
            array
        sparse/
            int
    utils-
    sampling
    nodes-
    leduc
    train
    agent/
        uniform
        neural
    uniform_test
    neural_test
```

Now I am ranting in the package file instead.

5:05pm. Let me go to bed for another hour. Then I'll call it a day. I do not feel like starting at all. What I want to do right now is strongly desire and dream about the new method.

Forget the lost time. I spent years writing and then rewriting a language. A few weeks here or there to get a handle on deep learning will not kill me.

5:25pm.

```py
import numpy as np
p = np.array([1/3,1/3,1/3])
def f(xm):
    return xm - p * xm.sum()
x = np.array([1,2,4],dtype=np.float)
f(x - x.mean())
f(x - (x * p).sum())
f(x)
```
```
>>> f(x - x.mean())
array([-1.33333333, -0.33333333,  1.66666667])
>>> f(x - (x * p).sum())
array([-1.33333333, -0.33333333,  1.66666667])
>>> f(x)
array([-1.33333333, -0.33333333,  1.66666667])
```

Right now I am thinking about grad centering. This here is a bit interesting.

```py
p = np.array([1/4,1/4,1/2])
def f(xm):
    return xm - p * xm.sum()
x = np.array([1,2,4],dtype=np.float)
f(x - x.mean())
f(x - (x * p).sum())
f(x)
```
```
>>> f(x - x.mean())
array([-1.33333333, -0.33333333,  1.66666667])
>>> f(x - (x * p).sum())
array([-1.4375, -0.4375,  1.875 ])
>>> f(x)
array([-0.75,  0.25,  0.5 ])
```

The first update is right out. Clearly it is not right if it results in the same credit as if the probabilities were uniform.

Between the second and the third which should I go for?

```py
p = np.array([0,0,1])
def f(xm):
    return xm - p * xm.sum()
x = np.array([1,2,4],dtype=np.float)
f(x - (x * p).sum())
f(x)
```
```
>>> f(x - (x * p).sum())
array([-3., -2.,  5.])
>>> f(x)
array([ 1.,  2., -3.])
```

Now this is super interesting isn't it?

I definitely do not want to be pushing up the first two elements in disfavor of the last. The last one is the one I supposed to be increasing!

```py
f(x - (x * p).sum())
```

So this update is the one that I should be using.

My initial impulse to center the gradients like this for the body was on the mark. I am surprised that in all cases it sums up to 0 either way though.

5:35pm. Let me think. What am I going to do tomorrow?

1) The optimizer.
2) The policy function for the new player.

I should just focus on doing this last one in pure Python and get it out of my system.

Actually, I won't need to extend the language. I can do all the Python parts in a script, inluding the loop that is currently in `neural_test` and just call it. That seems like a much more streamlined solution. In Spiral I'll have to pack the serializers into a record along with the game and whatever else I need. My CPS magic will pave the way after that.

So I should not be reserved and make the Python code my main focus.

5:40pm. In the `main` function I should return the game and the serializers.

```py
import numpy as np
import pyximport
pyximport.install(language_level=3,setup_args={"include_dirs":np.get_include()})
from neural_test import main
print(main())
```

Instead of just printing that useless `None` that `main` call should generate whatever I need for to interface with the Leduc game. I should be extending this file.

5:50pm. Let me stop here for the day. I can continue obsessing for the rest of the day like this, and I probably will, but I put in my daily effort. I might not have done much, but I was not playing games on the side. My focus was on the task at hand. I hate it that I can't just get an idea and start cracking, but going through the motions first in my mind is an integral part of the process for me.

5:55pm. I should look into the clustering approach to learning value functions that I've thought up. I wonder if this has something to do with episodic memory?

Let me look that up and I'll stop for the day then.

https://arxiv.org/abs/1805.07603
Episodic Memory Deep Q-Networks

Let me take a look at this.

```py
import numpy as np
p = np.array([1/4,1/4,1/2])
def f(xm):
    return xm - p * xm.sum()
x = np.array([1,2,4],dtype=np.float)
q = x - (x * p).sum()
w = q - (q * p).sum()
f(x - (x * p).sum())
```
```
>>> q = x - (x * p).sum()
>>> q
array([-1.75, -0.75,  1.25])
>>> w = q - (q * p).sum()
>>> w
array([-1.75, -0.75,  1.25])
```

Yeah, centering an already centered array results in the input.

6:45pm. Done with lunch.

http://www.cs.toronto.edu/~fritz/absps/dh97.pdf
Using EM for Reinforcement Learning

This is an old paper by Hinton.

https://arxiv.org/abs/2010.00304
Reinforcement Learning Using Expectation Maximization Based Guided Policy Search for Stochastic Dynamics

Let me check this paper out.

7:05pm. Forget it. I am done for the day."

---
## [Liyajoseph/ICS2O-Unit3-03-HTML-1@234c3291ed...](https://github.com/Liyajoseph/ICS2O-Unit3-03-HTML-1/commit/234c3291edd0bb6e135623d86276c3d41b231221)
##### 2021-05-31 17:31:53 by Liyajoseph

<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>Hello, World</title>
    <link href="style.css" rel="stylesheet" type="text/css" />
  </head>
  <body class="body">
    <h1 class="header2">Hello, World</h1>
    <div class="row">
  <div class="box1">
    <h2>Column</h2>
    <p>Life is full of moments of joy, pleasure, success and comfort punctuated by misery, defeat, failures and problems. There is no human being on Earth, strong, powerful, wise or rich, who has not experienced, struggle, suffering or failure. You have to work hard to reach to the highest position.</p>
  </div>

  <div class="box1">
    <h2>Column</h2>
    <img src="./images/hsja.jpg" alt="Bible verse wallpaper" width="60%" height="30%">
  </div>

  <div class="box1">
    <h2>Column</h2>
    <p>Life is full of moments of joy, pleasure, success and comfort punctuated by misery, defeat, failures and problems. There is no human being on Earth, strong, powerful, wise or rich, who has not experienced, struggle, suffering or failure. You have to work hard to reach to the highest position.</p>
  </div>
</div>
<h5 class="footer">Hope you have a nice day</h5>
    <script src="script.js"></script>
  </body>
</html>

---
## [Liyajoseph/ICS2O-Unit3-03-HTML-1@702802c470...](https://github.com/Liyajoseph/ICS2O-Unit3-03-HTML-1/commit/702802c47054ecfc16f26966e2eb10849a1e7d6b)
##### 2021-05-31 17:38:25 by Liyajoseph

<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>Hello, World</title>
    <link href="style.css" rel="stylesheet" type="text/css" />
  </head>
  <body class="body">
    <h1 class="header2">Hello, World</h1>
    <div class="row">
  <div class="box1">
    <h2>Column1</h2>
    <p>Life is full of moments of joy, pleasure, success and comfort punctuated by misery, defeat, failures and problems. There is no human being on Earth, strong, powerful, wise or rich, who has not experienced, struggle, suffering or failure. You have to work hard to reach to the highest position.</p>
  </div>
  <div class="box1">
    <h2>Column2</h2>
    <img src="./images/hsja.jpg" alt="Bible verse wallpaper" width="60%" height="30%">
  </div>
  <div class="box1">
    <h2>Column3</h2>
    <p>Joy, pleasure, success, and comfort are interspersed throughout life, as are agony, defeat, failures, and troubles. No human person, no matter how powerful, intelligent, or wealthy, has ever lived without struggle, suffering, or failure. To get to the top, you must put in a lot of effort.</p>
  </div>
</div>
<h5 class="footer">Hope you have a nice day</h5>
    <script src="script.js"></script>
  </body>
</html>

---
## [Xagula/Xagula@f8e9b90b39...](https://github.com/Xagula/Xagula/commit/f8e9b90b398c5e3d200bf0ed557fc96a11abddf5)
##### 2021-05-31 17:38:43 by Xagula

Update README.md

Hey! I am Xagula my real name is Sandro just friends call me Xagula. I am newbie in software development, I am looking for acquire new skills and learn new things, I am learning everything from internet and self teaching my self! I know basics of python, know how to use html and css and i will be honest I have solid knowladge in both and if i miss something in any of them I know where to find that information! I started learning solidity bcs i want to become blockchain developer I am very motivated watched some tutorials on youtube and i will be happy if someone hires me as junior developer and teach me new skills so i will do some work for them. I am very motivated, I know I should done it earlier to start educating myself and I am regreting the seconds I lost in gaming, yes i am gamer well i wasted to much time on gaming and got nothing from them kinda i was addicted to pleasure and fun now i had left behind that harmfull behavior and got back on track. I have some and even solid experience in different fields especially in science also i can call myself kinda educated one just it neds to be updated. 
So here i am 20 year old Sandro who looks a mentor to guide him as a developer. i have some skills and i am sure i will develope more ones i am very motivated person now i had some lazy past but it changed even when i was lazy i acquired knowladge which is very helpful forme today i am sure with this mindset (not mode) i will crush everything i will put my hand on so look forward for feedback!

---
## [Liyajoseph/ICS2O-Unit3-03-HTML-1@acca25ae7a...](https://github.com/Liyajoseph/ICS2O-Unit3-03-HTML-1/commit/acca25ae7a3891b104c2bc50401918856dbf573e)
##### 2021-05-31 17:48:21 by Liyajoseph

<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>Hello, World</title>
    <link href="style.css" rel="stylesheet" type="text/css" />
  </head>
  <body class="body">
    <h1 class="header2">Hello, World</h1>
    <div class="row">
  <div class="box1">
    <h2>Column1</h2>
    <p>Life is full of moments of joy, pleasure, success and comfort punctuated by misery, defeat, failures and problems. There is no human being on Earth, strong, powerful, wise or rich, who has not experienced, struggle, suffering or failure. You have to work hard to reach to the highest position.</p>
  </div>
  <div class="box1">
    <h2>Column2</h2>
<img src="./images/hsja.jpg" alt="Bible verse wallpaper" width="60%" height="30%">
  </div>
  <div class="box1">
    <h2>Column3</h2>
    <p>Joy, pleasure, success, and comfort are interspersed throughout life, as are agony, defeat, failures, and troubles. No human person, no matter how powerful, intelligent, or wealthy, has ever lived without struggle, suffering, or failure. To get to the top, you must put in a lot of effort.</p>
  </div>
</div>
<h5 class="footer">Hope you have a nice day</h5>
    <script src="script.js"></script>
  </body>
</html>

---
## [Liyajoseph/ICS2O-Unit3-03-HTML-1@5f6dd2e931...](https://github.com/Liyajoseph/ICS2O-Unit3-03-HTML-1/commit/5f6dd2e931d9b29ed315813a2fd5dacce2734171)
##### 2021-05-31 17:56:51 by Liyajoseph

<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>Hello, World</title>
    <link href="style.css" rel="stylesheet" type="text/css" />
  </head>
  <body class="body">
    <h1 class="header">Hello, World</h1>
<div class="row">
  <div class="column">
    <h2>Column</h2>
    <p>Life is full of moments of joy, pleasure, success and comfort punctuated by misery, defeat, failures and problems. There is no human being on Earth, strong, powerful, wise or rich, who has not experienced, struggle, suffering or failure. You have to work hard to reach to the highest position.</p>
  </div>
  <div class="column">
    <h2>Column</h2>
    <img src="./images/webarebears.wallpaper.jpg"
    alt="We Bare Bears Wallpaper" width="100%">
  </div>
</div>
  <img src="./images/pretty.jpg" alt="Sunset wallpaper" width="500px" height="2000px">
  </body>
</html>

---
## [byxor/NeverScript@c4911038bf...](https://github.com/byxor/NeverScript/commit/c4911038bf629393a294788d96e3a51e4acbd408)
##### 2021-05-31 18:05:31 by byxor

Licenses are for chumps. Do whatever the hell you want with this repo (with respect to your morals).

---

# [<](2021-05-30.md) 2021-05-31 [>](2021-06-01.md)

