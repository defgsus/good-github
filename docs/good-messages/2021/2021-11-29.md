# [<](2021-11-28.md) 2021-11-29 [>](2021-11-30.md)

3,473,605 events, 1,785,670 push events, 2,686,115 commit messages, 214,944,471 characters


## [NiloGregginz33/Seperating-kids-from-fighting-in-group-size-N](https://github.com/NiloGregginz33/Seperating-kids-from-fighting-in-group-size-N)@[0fef7a2d1b...](https://github.com/NiloGregginz33/Seperating-kids-from-fighting-in-group-size-N/commit/0fef7a2d1baaff231001287eb9c5433dabf484a3)
#### Monday 2021-11-29 00:05:11 by Manav Naik

Update README.md

Part 1: Get a job as a swim coach




Part 2I thought that the kids in the swimming class I taught were fighting but they were simply interacting with each other (They actually were fighting like straight up punching each other and then I spent like 2-3 years trying to figure out the math so I lost sight of what its use case was). I take psychiatric medication but don't like admitting what its for because I honestly don't know at this point but the people in my everyday life say I'm easier to interact with when I take my medication but not so much when I don't. I also made an honest mistake getting annoyed at this little kid on Omegle when in reality I was just annoyed because I had the hidden intention of trying to talk to this girl who was into math and performed professional background checks or something. How to hash user identities is coming up next (jk). The drone thing is supposed to be a physical game to play with like people you know like sending a carrier pigeon since I literally grew up with like hella birds that we would raise to sell to exotic bird owners but the more people I explain the human tracking part to the more people think its a weapon. Pigeons carry mathy code that might not be that useful to people who like math. People need a mechanism to send back the pigeon...

---
## [Vortetty/2Dsolang](https://github.com/Vortetty/2Dsolang)@[bae8b422b0...](https://github.com/Vortetty/2Dsolang/commit/bae8b422b0e1d2a59f50e9f517050526555652aa)
#### Monday 2021-11-29 01:09:43 by Vortetty

MORE WINDOWS SHIT LKJBSVCPILUJHBESLKJBA:JBCPUVBELLAJOIAPIO PLEASE KILL ME I HATE WINDOWS
(really though i just added better windows instructions and something to ensure curses is installed)

---
## [eliotbaez/dinosaur](https://github.com/eliotbaez/dinosaur)@[665e2894f5...](https://github.com/eliotbaez/dinosaur/commit/665e2894f5b7f5d2653325dc221fb6f54f7a10f2)
#### Monday 2021-11-29 07:39:54 by Eliot Baez

Finally attempt to make the dinosaur jump

I say attempt, because I failed on this attempt. From what I can see
from the debug data, the sensors are reading the individual arms of the
cactus as separate obstacles, and that's throwing off all the
calculations for identifying obstacles. I'm sure there's a way to fix
this, there's always a way. I just need to find out how.

I really home I don't have to go back to the running mean approach from
my very first prototype. That will be very resource intensive,
especially now that there are four photoresistors being polled in
parallel.

Maybe what I'll do is determine a safe amount of time to wait so that
the cactus is probably still the same color so that the arduino still
recognizes it.

Or I could use optics to blur the light coming into the photoresistor
(using something diffuse like hot-melt glue, which I have an abundance
of). I know commit messages are meant more for explaining the changes in
the code, but my brain is really on its last leg tonight and I really
need a reliable way to write notes to myself about this code. This is
the way.

---
## [ksh93/ksh](https://github.com/ksh93/ksh)@[5f850edf6e...](https://github.com/ksh93/ksh/commit/5f850edf6e0ef8e44836fac1bcd0168604a7a3bf)
#### Monday 2021-11-29 08:01:07 by Martijn Dekker

Fix defining types conditionally and/or in subshells (re: 1dc18346)

This commit mitigates the effects of the hack explained in the
referenced commit so that dummy built-in command nodes added by the
parser for declaration/assignment purposes do not leak out into the
execution level, except in a relatively harmless corner case.

Something like

	if false; then
		typeset -T Foo_t=(integer -i bar)
	fi

will no longer leave a broken dummy Foo_t declaration command. The
same applies to declaration commands created with enum.

The corner case remaining is:

$ ksh -c 'false && enum E_t=(a b c); E_t -a x=(b b a c)'
ksh: E_t: not found

Since the 'enum' command is not executed, this should have thrown
a syntax error on the 'E_t -a' declaration:
ksh: syntax error at line 1: `(' unexpected

This is because the -c script is parsed entirely before being
executed, so E_t is recognised as a declaration built-in at parse
time. However, the 'not found' error shows that it was successfully
eliminated at execution time, so the inconsistent state will no
longer persist.

This fix now allows another fix to be effective as well: since
built-ins do not know about virtual subshells, fork a virtual
subshell into a real subshell before adding any built-ins.

src/cmd/ksh93/sh/parse.c:

- Add a pair of functions, dcl_hactivate() and dcl_dehacktivate(),
  that (de)activate an internal declaration built-ins tree into
  which check_typedef() can pre-add dummy type declaration command
  nodes. A viewpath from the main built-ins tree to this internal
  tree is added, unifying the two for search purposes and causing
  new nodes to be added to the internal tree. When parsing is done,
  we close that viewpath. This hides those pre-added nodes at
  execution time. Since the parser is sometimes called recursively
  (e.g. for command substitutions), keep track of this and only
  activate and deactivate at the first level.

- We also need to catch errors. This is done by setting libast's
  error_info.exit variable to a dcl_exit() function that tidies up
  and then passes control to the original (usually sh_exit()).

- sh_cmd(): This is the most central function in the parser. You'd
  think it was sh_parse(), but $(modern)-form command substitutions
  use sh_dolparen() instead. Both call sh_cmd(). So let's simply
  add a dcl_hacktivate() call at the beginning and a
  dcl_deactivate() call at the end.

- assign(): This function calls path_search(), which among many
  other things executes an FATH search, which may execute arbitrary
  code at parse time (!!!). So, regardless of recursion level,
  forcibly dehacktivate() to avoid those ugly parser side effects
  returning in that context.

src/cmd/ksh93/bltins/enum.c: b_enum():

- Fork a virtual subshell before adding a built-in.

src/cmd/ksh93/sh/xec.c: sh_exec():

- Fork a virtual subshell when detecting typeset's -T option.

Improves fix to https://github.com/ksh93/ksh/issues/256

---
## [thedogwiththedataonit/Blockchaingame](https://github.com/thedogwiththedataonit/Blockchaingame)@[fd8e4fd6de...](https://github.com/thedogwiththedataonit/Blockchaingame/commit/fd8e4fd6de56389ad7caa9b02ba28c62af8f0cbf)
#### Monday 2021-11-29 08:13:07 by Junseo Park

FIXED THE FUCKING HASHING PROCESS AND TRANSACTIONS AND TREE HOLY SHIT

---
## [Tekiniggemann/GNOD-music-recommender](https://github.com/Tekiniggemann/GNOD-music-recommender)@[3a4a2be8fc...](https://github.com/Tekiniggemann/GNOD-music-recommender/commit/3a4a2be8fceac5393dba08e40999333a97ffa02b)
#### Monday 2021-11-29 08:25:54 by Tekin

Add files via upload

Introduction
You have been hired as a Data Analyst for Gnod.

Gnod is a site that provides recommendations for music, art, literature and products based on collaborative filtering algorithms. Their flagship product is the music recommender, which you can try at www.gnoosic.com. The site asks users to input 3 bands they like, and computes similarity scores with the rest of the users. Then, they recommend to the user bands that users with similar tastes have picked.

Gnod is a small company, and its only revenue stream so far are adds in the site. In the future, they would like to explore partnership options with music apps (such as Deezer, Soundcloud or even Apple Music and Spotify). But for that to be possible, they need to expand and improve their recommendations.

That’s precisely where you come. They have hired you as a Data Analyst, and they expect you to bring a mix of technical expertise and business mindset to the table.

Jane, CTO of Gnod, has sent you an email assigning you with your first task.

The Challenge
This is an e-mail Jane - CTO of Gnod - sent over your inbox in the first weeks working there.

Copy
Dear xxxxxxxx,
 
We are thrilled to welcome you as a Data Analyst for Gnoosic!
 
As you know, we are trying to come up with ways to enhance our music recommendations. One of the new features we'd like to research is to recommend songs (not only bands). We're also aware of the limitations of our collaborative filtering algorithms, and would like to give users two new possibilities when searching for recommendations:
 
- Songs that are actually similar to the ones they picked from an acoustic point of view.
- Songs that are popular around the world right now, independently from their tastes.
 
Coming up with the perfect song recommender will take us months - no need to stress out too much. In this first week, we want you to explore new data sources for songs. The internet is full of information and our first step is to acquire it do an initial exploration. Feel free to use APIs or directly scrape the web to collect as much information as possible from popular songs. Eventually, we'll need to collect data from millions of songs, but we can start with a few hundreds or thousands from each source and see if the collected features are useful. 
 
Once the data is collected, we want you to create clusters of songs that are similar to each other. The idea is that if a user inputs a song from one group, we'll prioritize giving them recommendations of songs from that same group.
 
On Friday, you will present your work to me and Marek, the CEO and founder. Full disclosure: I need you to be very convincing about this whole song-recommender, as this has been my personal push and the main reason we hired you for!
 
Be open minded about this process: we are agile, and that means that we define our products and features on-the-go, while exploring the tools and the data that's available to us. We'd love you to provide your own vision of the product and the next steps to be taken.
 
Lots of luck and strength for this first week with us!
 
Jane
Have fun and enjoy the ride!

---
## [reaPeR1010/android_kernel_motorola_sanders](https://github.com/reaPeR1010/android_kernel_motorola_sanders)@[1ae2a992a6...](https://github.com/reaPeR1010/android_kernel_motorola_sanders/commit/1ae2a992a69704b149865df311ab8a656dead87c)
#### Monday 2021-11-29 11:32:21 by Masahiro Yamada

BACKPORT: modpost: file2alias: go back to simple devtable lookup

Commit e49ce14150c6 ("modpost: use linker section to generate table.")
was not so cool as we had expected first; it ended up with ugly section
hacks when commit dd2a3acaecd7 ("mod/file2alias: make modpost compile
on darwin again") came in.

Given a certain degree of unknowledge about the link stage of host
programs, I really want to see simple, stupid table lookup so that
this works in the same way regardless of the underlying executable
format.

Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
Acked-by: Mathieu Malaterre <malat@debian.org>
Link: https://git.kernel.org/linus/ec91e78d378cc5d4b43805a1227d8e04e5dfa17d
Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
Signed-off-by: reaPeR1010 <reaPeR10x10x@gmail.com>

---
## [GnomeModder/EnforcerMod](https://github.com/GnomeModder/EnforcerMod)@[65f3fd807c...](https://github.com/GnomeModder/EnforcerMod/commit/65f3fd807c5991f63eb4c7d407217cf758b0b5d8)
#### Monday 2021-11-29 12:24:29 by TimeSweeper

ok robit achievement networked. fuckin config mismatch holy hell
speaking of configs, moved them out of enforcermain. slowly slowly de-retarding that script
but also adding more retarding. hacked golem deflecting to stone wisps as well, wip
oh yeah golem deflecting is finally fucking networked

---
## [DataScienceScotland/sg-wellbeing-economy-monitor](https://github.com/DataScienceScotland/sg-wellbeing-economy-monitor)@[2c8b899753...](https://github.com/DataScienceScotland/sg-wellbeing-economy-monitor/commit/2c8b899753bf609e9d894998fc7eb3063c5062e1)
#### Monday 2021-11-29 14:42:22 by Polly Le Grand

Update server.R

Adding in year charts for healthy life expectancy (female and male)

---
## [aerosayan/voidstar-lang](https://github.com/aerosayan/voidstar-lang)@[9f209a0eeb...](https://github.com/aerosayan/voidstar-lang/commit/9f209a0eeb654efd4e96c445a0c6c385a504c036)
#### Monday 2021-11-29 18:19:09 by aerosayan

exm:mod:major language modification

- I realized that people don't like to learn new things.
  They prefer things that they are familiar with.

  So, my fancy language would not have been used considering
  that it was weird and kind of annoying to read.
- I was asking too much from the language users.
  I was asking them to learn a new language, give up case
  sensitivity, give up most of the amazing features of the other
  languages, learn a new and cryptic syntax that isn't standard,
  believe my claims that it would be safe, yada ... yada.
- NO! My idea was horrible in practice!
- I'm now learning from BASIC, and keeping things standard.

---
## [Maetrim/DDOBuilder](https://github.com/Maetrim/DDOBuilder)@[5a94cb3b8a...](https://github.com/Maetrim/DDOBuilder/commit/5a94cb3b8ae2334219153a0bd04e5f2c988eee01)
#### Monday 2021-11-29 19:25:15 by Maetrim

Build 1.0.0.147

Fix: The Epic Destiny preview pane should no longer acquire a tree you have spent points in in a previously saved build (Reported by Laur)
---If you have since saved a build with 4 trees present. Empty the 1st tree of points spent, switch it to "No Selection" and save, close and reopen the file.
Fix: Applying and revoking a tier 5 in any epic destiny tree will now correctly unlock other epic destiny tree tier 5's (Reported by Hawkwier)
Fix: Primal Scream now has a Stance associated with it and now also applies to Dexterity and is a Morale bonus (Reported by Hawkwier)
Fix: "Bombardier: Efficient Heighten" now correctly costs 2ap (Reported by Guntharm)
Fix: Rapid Shot ranged power bponus now only correctly applies to Long and Short bows. (Reported by Aquoia)
Fix: All Sorcerer Elemental Apotheosis enhancement now state that they remove element proection while active (Reported by ASilver)
Fix: Feat "Stunning Fist" now correctly requires "Flurry of blows to be trained (Reported by Laur)
Fix; the old Fatesinger slef buff Hyms removed
Items:
---Explorer of the Depths (Helmet - Minor Artifact) (Reported missing by Guntharm)
---Perfected Bracers of the Demon's Consort
---Perfected Bloodstone
---Perfected Torc of Prince Raiyum-de II
---Perfected Vulkoorim Pendant

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[f19c44bd29...](https://github.com/mrakgr/The-Spiral-Language/commit/f19c44bd29382b91d4adc51b1327cf61424fd85c)
#### Monday 2021-11-29 20:22:42 by Marko Grdinić

":35am. Went to bed earlier than usual yesterday at 10:30pm. Had a hell of a time falling asleep. Let's see, any contacts or mail? Nope, Spiral is buried. Maybe I made a mistake.

Why did I make Spiral? It wasn't really for myself. I just thought that it was justice given the future flow of events. And I did not really think that current day algorithms are so weak that I could not beat a toy game. Spiral might be justice, but in the future I want to do things for myself. I need to cast away all notions of morality.

Right now my stress is maximum. It feels like late 2016 all over again, and I feel like I have nothing. But if I had art skills it would not be boring to work on a game.

I guess what this situation is forcing me is to really form a view on art itself.

https://www.reddit.com/r/MachineLearning/comments/r3g7kd/d_looking_for_a_sponsor_for_a_functional/

The bugger deleted it. I really meant to archive some of these comments, but I guess he sensed saying Python is good for the next 20 years would not look good in hindsight. Smart of him. But positive or negative it does not change that I will get zero support from other people for this.

9:45am. I need to crystalize the vision of me being good at art and music, and working on games. It does not matter if fiction is a parasite on other people's free time and money. It is precisely because it is that that I should make it.

An interesting story is about one of my old acquaintances from the school days. He started a game company, was not successful at first and only became so once he got into gambling style games. I am thinking it is probably gatcha. I have no idea what his company is, but I do not care enough to disbelieve this story. Right now he is rich and could probably pursue AI should he want to instead I at 34 can't even afford a computer upgrade right now. He on the other hand has a wife and kid.

All I have is this pen tablet, my rig which is old by now, and my brain which is getting older by the day.

I am not interested in working on gatcha or things like that, but my first project should be the most ideal learning experience.

9:55am. While art is not easy, for most people programming would be the hard part and I have that down.

10am. Sigh, I wish I could live for a million years so I could take advantage of this hard won experience properly. It is all up to luck.

I've thought about it, and if neuroscience can get the low level primitives right, then evolutionary algorithms should be able to recover the high level stuff. If I could get the right hardware, I could get to the AGI level myself. Right now, it is a waste of time to think about this. It is a waste of time to keep track of ML as those advances are all fake. But it will serve to push the next wave, much like gaming did to push GPUs.

Whether it is Deepmind or OpenAI, they should have the hardware necessary to actually discover some good algorithms and finally give them to the world at large. The reason I am in this situation of having no good choices when it comes to RL is all because of them. The ML community deserves a lot of the blame, but ultimately, it is I who made the choice to buy into the hype.

Because I loved functional programming. But had I looked there might have been more constructive ways of satisfying that urge. This will be my second farewell to Spiral. Next time we ride, it will be all the way to the end.

Now, I won't hesitate. Let me take out my pen and start cracking.

10:05am. Let me just say one thing. I really hate OneDrive. Yesterday, I meant to stop it from backing the Documents up and what happened is that I actually put most of the files on the cloud by accident!

This is the first time I've ever cursed Microsoft for wasting my time with its shit. Today when I woke up most of my desktop was missing!

It still is.

https://www.tenforums.com/general-support/147272-how-move-everything-onedrive-local-computer-drive.html

This is a pain the ass. Don't tell my I have to copy the documets and the desktop manually?

https://www.google.com/search?q=onedrive+deleted+my+desktop

https://www.addictivetips.com/windows-tips/move-the-desktop-folder-out-of-onedrive-on-windows-10/

Oh, so there is a restore button somewhere. Let me give it a try.

Sigh, I can't find the restore anywhere.

https://www.reddit.com/r/onedrive/comments/90r5k2/onedrive_deleted_all_my_files_from_my_pc_hard/

10:20am. The files are there on OneDrive, just why aren't they one the desktop? I know I deselected the folder to backup and then reactivated it again. Why did it move them all the way to OneDrive?

https://www.reddit.com/r/onedrive/comments/r18svf/why_did_onedrive_just_move_all_my_folders_from_my/

> Best way is to right click the OneDrive folder and to tell it to keep all locally which makes sure you have everything on local drive and it keeps OneDrive always synced, so you have a exact copy in both places. The other way, not recommended, is to right click the folder and then change location to your local drive through properties sub menu, which means you have to manually sync/ update OneDrive and I don’t recommend this

Where is that damn option?

10:30am. Ok, I have no choice. I can't waste any more time with this. Let me copy the OneDrive folders that I am interested locally, and then I will unlink the thing from my account.

Actually, I would not mind it backing up my desktop, since I keep my journo there, but now that it wiped half it, I do not trust the program anymore.

10:35am. This will take a while. The full thing is 3.5gb. Ommiting the BG2 And My Blender Projects brings it down to 900mb. These are just save data for various things, but damn this stuff does not belong on the cloud. I am going to give up on the Blender and BG2 files. Yesterday I just copied the model I was working on because I suddenly lost access to all my stuff.

10:40am. Ok, forget this. Just leave it in the background and it will be done in an hour or so. Let me fire up Blender. First of all the ear.

Yesterday I realized something huge. The angle of the ear is completely off. I have it at a flat angle and to make matters worse its backside is oval instead of half oval.

I completely lack common sense when it comes to art. I know it felt like a monkey when I looked at it from the front, but it should have occurred to me that the angle is off if I considered how the headphones I am using fit on my own ear.

10:45am. I need to invest in getting some better references. I looked at the ear from the front and sideways, but did not realize what the angle was at all. Just how did I miss this?

https://youtu.be/VzMAh66ofq0?t=635

When did he twist the ear?

https://youtu.be/VzMAh66ofq0?t=640

Ah here. Damn.

What am I doing? I should be using the pose tool, not the move tool for this. That is what I planned during the night. Let me stick to it.

11am. Agggghhhhh...

The pose did work, but it made a huge mess of the head. I'd have been better off cutting the ear off as a separate object, rotating it and then putting it back in. I was wrong about my choice of tool here.

Ok, now the OneDrive backup is done. Let me copy the BG folder.

...

11:20am. No it not working. I am having trouble getting the union to stick. Probably because the ear mesh has holes in it. While looking from the inside I saw some disconnected particles.

Maybe I did the wrong thing by slicing off the ear. What about the twist tool? Fuck, I ended up saving with the messed up ear previously. Now the whole thing is a wreck.

11:25am. Ok, I somehow managed to twist the ear into shape. At this point I really wish I was drawing instead of messing with sculpting. Right now I am a bit sad and angry so I am not in optimal mental state to be sculpting.

Am I underestimating the effort or others, or am I good at programming? I have no idea. But it does not seem like my work on Spiral really means much to others. During the job application spree, some of the replies that I have gotten based on my resume, how much of that is related to that project? Could I have done better just by trojaning Google and Facebook in small and white font?

11:40am. I have no idea what I am doing. I went through all this effort just to rotate the ear, but I am still going to have to redo it so it matches Flycat's.

11:50am. Now I am messing with OneDrive. The File Explorer is randomly crashing because of it.

11:55am. Let me wipe all the ear detail. It is just making me hesitate. I'll redo the ear from scratch so it matched the Flycats'. Every ref I've seen online seem to have a different ear shape.

12:05pm. Getting the ear to fall into shape is a nightmare. Right now my morale is not exactly high, and I am finding myself erasing the old instead of making the new. Let me have breakfast here. I keep thinking negative thought like how easier this would be in 2d.

1pm. Done with breakfast. Let me chill a bit and then I will resume. I'll work on the ear for a while, and if I am still feeling down, I'll switch to drawing.

2pm. Done with chores.

It is time to resume. I do not know whether it is pathetic or not, but after all the disappointments and setbacks that I had, I want a path that will not fail me again. There is nothing worse than power that cannot be applied in the real world. At least with games I can contiue my development in a positive manner.

In games there could be a dream of gradually scaling AI against the player. It was too much all or nothing. It is too much where any little piece can fail.

Let me bring up some references. I am just mucking about from imagination. Unlike the various body parts, the ear is not something I've paid attention to in regular life.

https://www.youtube.com/results?search_query=sculpt+ear

Why don't I look at some of these vids? I need something to lift my spirit. I can't get a grasp on how Flycat did it.

https://youtu.be/WKfL_NmFUWU?t=2
Sculpt Ears | EASY Blender Tutorial

I never watched any of PIXXO 3D's tutorials. Oh, and this way of doing it is interesting. The ear is aculpted on a slab. This is how I should imagine it. Right now I am wrestling with how to angle it properly. I spent plenty of time mucking about, but I am getting punished for not getting the big things right.

https://youtu.be/WKfL_NmFUWU?t=54

I should really learn how to use the annotation tool, because the way I am doing it just by moving things around is trash. When I worked on the hand and the boot I completely lost track of proportions. This part of the reason why I want to skip sculpting and get back to drawing. There if I mess up I'll be able to more easily adjust.

The ear is so strange. I am having trouble breaking it down into simpler forms.

https://youtu.be/WKfL_NmFUWU?t=99

This is actually pretty informative. He says that it is amistake to draw the ear like a semi circle. This is how I've been doing it in the past.

2:25pm. https://youtu.be/WKfL_NmFUWU?t=168

Looking at this is lifting my spirit in fact.

2:35pm. I am getitng lost in thought over some Simulacrum scenes. Right now I am thinking what to do after the MC is a Transcendi and has no peer. I figured out a way to make it interesting. This particular scene makes me emotional. I think I'll be able to capture the feeling of longing for power at every step of the way

https://youtu.be/WKfL_NmFUWU?t=640

Looking it from the top I definitely did not angle the ear correctly. It is just too hard to follow Flycat directly. His technique is too refined.

The way he pulled out the ear is quite interesting. Such a way never actually occured to me.

3:10pm. https://www.reddit.com/r/reinforcementlearning/comments/lhmnl8/from_a_high_level_can_some_one_tell_me_the_cons/

Am looking up some ES threads on the RL sub. I really should have ditched gradients completely and tried this instead. And indeed, I would expect this to work better than PPO given all that I know so far. Using ES for trading stocks. Sigh, just sigh. I can only sigh at this point. I'll be doing a lot of that in the future. Sigh, if only I could get that AI chip, my options would go up quite a bit.

https://arxiv.org/abs/1703.03864
Evolution Strategies as a Scalable Alternative to Reinforcement Learning

> We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.

> it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.

I think these advantages might more than make up for the 10x drop in sample efficiency. And actually, why couldn't this be combined with RL AC methods?

I did go down the wrong path. Just out of curiosity, what would be the ideal way to implement this on the GPU. Would it be possible to actually implement something like a poker simulator on a GPU and train it all there?

Why am I even asking this? I thought about this and concluded that I need an AI chip simply due to warp divergence issues. I am better off doing something else.

Looking to what would the characteristics of this be on AI chips, I see that ES in particular would be able to make use of the matrix multiply capabilities of these chips. I might not have been able to invent backprop, but I could definitely invent ES.

Ok, ok...let me actually read the paper.

https://eng.uber.com/deep-neuroevolution/

> We do so by computing the gradient of network outputs with respect to the weights (i.e. not the gradient of error as in conventional deep learning), enabling the calibration of random mutations to treat the most sensitive parameters more delicately than the least, thereby solving a major problem with random mutation in large networks.

This might be pretty smart actually. I do not think I understood what this was when I read this blog post years ago.

>  Setting the stage for further innovation with ES, we provide deeper insight into its relationship to SGD through a comprehensive study that examines how close the ES gradient approximation actually comes to the optimal gradient for each mini-batch computed by SGD on MNIST, and also how close this approximation must be to perform well. We show that ES can achieve 99 percent accuracy on MNIST if enough computation is provided to improve its gradient approximation, hinting at why ES will increasingly be a serious contender in Deep RL, where no method has privileged access to perfect gradient information, as parallel computation increases.

I should look this up.

https://eng.uber.com/accelerated-neuroevolution/

> Today, we are releasing open source code that makes it possible to conduct such research much faster and cheaper. With this code, the time it takes to train deep neural networks to play Atari, which takes ~1 hour on 720 CPUs, now takes ~4 hours on a single modern desktop.

Sigh, let me read this blog post.

> It turns out that modern, high-end desktops, which have dozens of virtual cores, themselves act like a modest computing cluster. If evaluations are properly executed in parallel, a run that takes 1 hour on 720 cores can be run on the CPUs of a 48-core personal computer in 16 hours, which is slower, but not prohibitively so.

48 cores? Oh lol. Who has that? My own has only 4 cores.

> The first customized TensorFlow operation sped up the GPUs significantly. It is built specifically for heterogeneous neural network computation in RL domains where episodes are of different length, as is true in Atari and many simulated robot learning tasks.

This is actually pretty interesting. This was a problem for me in poker.

3:45pm. https://twitter.com/togelius/status/1129326917906247682
> Guess who won the AAMAS 2019 best paper award? @giuse_tweets, me and @eXascaleInfolab did, that's who. Or rather, our paper "Playing Atari with Six Neurons" did. We are very happy 😁

https://log2.ch/2019/exploring-a-pixel-maze-with-evolution-strategies/

https://arxiv.org/abs/1712.06564
On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent

https://vimeo.com/250400191#_=_
Evolving to Learn through Synaptic Plasticity

https://arxiv.org/abs/1806.01363
Playing Atari with Six Neurons

Let me read this last paper. Then I'll look into the ES paper. Then I'll look into the paper that combines ES with output sensitivity.

3:55pm. This latest paper does use an ES.

4pm. I am not really focusing on the paper too much so I am having difficulty understanding it. But this is novel to me.

4:05pm. It uses the natural gradient to optimize the net.

> The goal of this work is not to propose a new generic feature extractor for Atari games, nor a novel approach to beat the best scores from the literature. Our declared goal is to show that dividing feature extraction from decision making enables tackling hard problems with minimal resources and simplistic methods, and that the deep networks typically dedicated to this task can be substituted for simple encoders and tiny networks while maintaining comparable performance.

> We empirically evaluated our method on a set of well-known Atari games using the ALE benchmark. Tight performance restrictions are posed on these evaluations, which can run on common personal computing hardware as opposed to the large server farms often used for deep reinforcement learning research.

Sigh, this might be an initial way of doing it that I've been dreaming about.

Let me read the ES paper from OpenAI next. If ES is doing finite differences it might not be what I am thinking of.

4:20pm. Hmmm, actually it is pretty much like finite difference. I thought a bit during the night and realized that there is no chance that this would have issues with exploding and vanishing gradients and such.

4:25pm. I'll admit the way it just multiplies the return by the noise during the gradient update is interesting. I like this way of doing it.

> To reduce variance, we use antithetic sampling Geweke [1988], also known as mirrored sampling Brockhoff et al. [2010] in the ES literature: that is, we always evaluate pairs of perturbations , −, for Gaussian noise vector .

This is a good idea too.

> By not requiring backpropagation, black box optimizers reduce the amount of computation per episode by about two thirds, and memory by potentially much more. In addition, not explicitly calculating an analytical gradient protects against problems with exploding gradients that are common when working with recurrent neural networks. By smoothing the cost function in parameter space, we reduce the pathological curvature that causes these problems: bounded cost functions that are smooth enough can’t have exploding gradients. At the extreme, ES allows us to incorporate non-differentiable elements into our architecture, such as modules that use hard attention [Xu et al., 2015].

They say as much about not having problems with exploding and vanishing gradients. I could really improve these methods using output sensitivies. I could bring in the covariance stuff from KFAC.

The paper has a lot of good insight on ES, such as that its ability to optimize is not so much dependent on network size as it is on the complexity of the problem.

> Black box optimization methods are uniquely suited to low precision hardware for deep learning. Low precision arithmetic, such as in binary neural networks, can be performed much cheaper than at high precision. When optimizing such low precision architectures, biased low precision gradient estimates can be a problem when using gradient-based methods. Similarly, specialized hardware for neural network inference, such as TPUs [Jouppi et al., 2017], can be used directly when performing optimization using ES, while their limited memory usually makes backpropagation impossible.

> By perturbing in parameter space instead of action space, black box optimizers are naturally invariant to the frequency at which our agent acts in the environment. For MDP-based reinforcement learning algorithms, on the other hand, it is well known that frameskip is a crucial parameter to get right for the optimization to succeed [Braylan et al., 2005]. While this is usually a solvable problem for games that only require short-term planning and action, it is a problem for learning longer term strategic behavior. For these problems, RL needs hierarchy to succeed [Parr and Russell, 1998], which is not as necessary when using black box optimization.

This is in fact an issue in poker. Consider a hand with a small number of big bets vs many small bets that ultimately amount to the same reward in the end. The later would have far higher variance. I needed to switch to TD methods because of that which spiked training effort immensely.

The fact that it could have discrete stuff is also good news because in more complex games I am essentially dealing with an unbounded action space.

5pm. Though the problem is really always the same, how to scale these things.

Let me read the neuroevolution paper that considers sensitivities.

https://arxiv.org/abs/1712.06563
Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients

It is this one.

> Note that calculating s in practice requires taking the average absolute value of the gradient over a batch of data (the absolute value reflects that we care about the magnitude of the slope and not its sign); unfortunately this cannot be efficiently calculated within popular tensor-based machine learning platforms (e.g. TensorFlow or PyTorch), which are optimized to compute gradients of an aggregate scalar (e.g. average loss over many examples) and not aggregations over functions of gradients (e.g. summing the absolute value of per-example gradients).

This would not be hard to do in Spiral's old library.

6pm. Done with lunch. I've realized something huge. I've been thinking about the above paper.

At first I thought about the output sensitivity, and how if I wanted to pass in the reward I'd have to do it at every timestep which degrade the method to being PG, but I realized in fact that after I sum up the sensitivities I can just multiply it all by the total reward in the end. That would be remarkable!

I am not sure if the paper itself does that, but it probably does.

8:20pm. Sigh, I am just skimming back and forth and can't figure it out. Nevermind that.

I am thinking in which scenarios might way of doing things would be superior.

In a game like poker where the rewards come all at the end of the hand, the sensitivity multiplying way would have an absolute advantage in variance reduction over policy gradients. There is simply no way it could be worse.

Ok, so it would have higher variance that with a TD trained AC. But training those critcis in such a method is insanely expensive! Especially on something like holdem.

6:25pm. ES itself has some inbuilt variance reduction in that the gradients get centered automatically. This is easy to see...actually, no. There isn't a multiplication by the action probabilities to have such an effect.

6:30pm. It is fine, the softmax centers the grads on its own. There is no need to fret about this.

6:35pm. Actually, if I added a bunch of meaningless actions and noise timesteps, the sensitivities might get washed out. There are different kinds of variance, and you trade one kind for another. Unlike ES, my idea is not truly invariant to temporal length. Still it is definitely better than raw PG, which nobody would really use.

Ah, to make my idea work rather than uniform sensitivity over all the timesteps, what I need is some kind of attention mechanism to distribute the rewards. But that is just RUDDER in the end which is hard.

So my idea is a wash, it is not really better that ES.

The fact that RL methods can't really beat the randomized search ones is a damning indictement of them. What about something like CMA-ES? What if I used that for poker along with refined features.

6:45pm. https://arxiv.org/abs/1712.06564
On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent
> Evolving neural networks, more commonly called neuroevolution, has a long history preceding the recent Salimans et al. [2017] result [Floreano et al., 2008, Yao, 1999]. In fact, it is important to note that over the history of RL, there are periods during which neuroevolution was the leading method on popular benchmarks.

7:10pm. https://youtu.be/7J5KK-tYoXc
RLVS 2021 - Day 5 - Evolutionary Reinforcement Learning

Let me watch this. I really completely left sculpting behind. Even if ES gives me some hope it is not enough. To really try it out I'd want that AI chip.

https://rl-vs.github.io/rlvs2021/

Interesting stuff here.

7:25pm. > 7.1. Why the GA is faster than ES The GA is faster than ES for two main reasons: (1) for every generation, ES must calculate how to update its neural network parameter vector θ. It does so via a weighted average across many (10,000 in Salimans et al. 2017) pseudooffspring (random θ perturbations) weighted by their fitness. This averaging operation is slow for large neural networks and large numbers of pseudo-offspring (the latter is required for healthy optimization), and is not required for the Deep GA. (2) The ES requires require virtual batch normalization to generate diverse policies amongst the pseudooffspring, which is necessary for accurate finite difference approximation (Salimans et al., 2016). Virtual batch normalization requires additional forward passes for a reference batch–a random set of observations chosen at the start of training–to compute layer normalization statistics that are then used in the same manner as batch normalization (Ioffe & Szegedy, 2015). We found that the random GA parameter perturbations generate sufficiently diverse policies without virtual batch normalization and thus avoid these additional forward passes through the network

I should keep GAs in mind. With ES I probably would have tried without virtual batch norm, but that might end up being a mistake. GAs seems to have more intrinsic roboustness.

7:40pm. Nevermind that video. It is 2h and I do not feel like watching it all.

https://rl-vs.github.io/rlvs2021/evolving-agents.html

Let me watch this instead.

8pm. https://youtu.be/rEawyh46Ukc?t=350
> Can we train such complex heterogenous architectures end-to-end through evolution alone?

8:30pm. https://youtu.be/rEawyh46Ukc?t=2152

They are looking into Hebbian plasticity stuff. These experiments they are doing are on toy games.

8:55pm. Let me close here. Ok, there just no helping it. I am just out of energy working for things that aren't within my reach. Who knows if I'll be able to reach my goals with the first gen of AI chips.

With Spiral I sought risk, and now I need to be averse to it. When one door closes another opens. I can be somebody quite great even without great computational power at my disposal. I'll live by acquiring the gamedev expertise.

Right now the learning algos are the hard part, but in the future they will be easy. Once that happens making games remain the hard part much like programming. I should get that later part out of the way.

I want to sell Spiral, but I can't sell something nobody wants to buy. I could look into PyTorch and ask them if they need a hand, but if I want money my instinct is to avoid working for free as much as possible. The repuation benefits are not worth it.

8:55pm. The evolutionary algo stuff is interesting. I am tempted to try it, but I know it will be a very tough uphill battle to get them to scale on just my computer. They will absolutely be worth using on AI chips though.

My expectation that figuring out the true learning algos will happen through evolving them from the right set of low level primitives. This is in the worst case if the neuroscience guys do not manage to figure it out themselves.

9:05pm. There is no question that I am pathetic right now, but I can stop being that if I can change. Right that being whipped by the potential future I pursued, with a few years of practice I will gain the power to take on anything. I can become a true master of illusion.

9:15pm. I am burdening myself with too much crap beyond my control. A few more years to learn sculpting, drawing, painting and music while I make Heaven's Key and more will not kill me. Money is not something I need. If I get good at the art stuff I should get some Patreon bucks. I am not looking for millinos here, even 1k month should allow me to upgrade my rig and get that AI chip eventually. It is worth working towards.

Today I was really determined, but somehow did not do much. Tomorrow I am going to go all out on the ear. I am going to master the female mesh and then move on to drawing. I won't give up."

---
## [Perkedel/Kaded-fnf-mods](https://github.com/Perkedel/Kaded-fnf-mods)@[24e74cb1cc...](https://github.com/Perkedel/Kaded-fnf-mods/commit/24e74cb1ccca9347be0a24164cea263d007d43a6)
#### Monday 2021-11-29 21:01:09 by Joel Robert Justiawan

[skip ci] keh

_meta.json some fixups. I forgot some, I know!

dialogue box name reference fix! yes remember that song field in Chart JSON is id. no more uppercase or lower casing nor even dash to space anymore.

markicop

update GameJolter @TentaRJ https://github.com/TentaRJ/GameJolt-FNF-Integration . HEY LOOK! it has toast?!??!?!?! whoah!!! how?!?!?!??!? I HAVE BEEN WANTING TO MAKE MY OWN TOAST TOO ASWELL!!! TEACH ME!!! lemme see the code.

oops! duplicate option watermark lol!

I cannot fix the touch screen / mouse option menu selection. pls help. you can only now click the same option that being selected without being crazy. Yeah you can select other unselected option, but if it has to scroll, it goes crazy!

I thought stage swags dynamic data type is exageration. Well, bug in Haxe. I set the datatype for those only to FlxSprite. Therefore it should also accept any class datatype that inherits from that too right? NOPE!!! if it is child class based on FlxSprite, it's fine. BUT when you going to use special method from that child class **IT ERRORS!!!** method not found. WTF? SO it has to be only whatever FlxSprite had? wait wait. maybe... ONE OF THESE TYPE! either FlxSprite, or any following... haha yess!

placeholder offset attempt fix faile
other offset faileleirlereirere
Jakarta Fair readjust offset

add Note type changer in Charting State

---
## [rorgoroth/mingw-cmake-env](https://github.com/rorgoroth/mingw-cmake-env)@[c798598e5b...](https://github.com/rorgoroth/mingw-cmake-env/commit/c798598e5b6013d9a74ed5b59e0bd7213d898d6e)
#### Monday 2021-11-29 21:04:58 by Ray Griffin

packages/quake3e-urt: Update patch

God this shit is annoying.
Download from server doesn't seem to work if the binary name was wrong, or compiled outside of this project, worked fine building with msvs but it seems faking the Q3 version works.

---
## [microsoft/terminal](https://github.com/microsoft/terminal)@[f2ebb21bd1...](https://github.com/microsoft/terminal/commit/f2ebb21bd13b20db38305136d34fa0778baf7920)
#### Monday 2021-11-29 21:10:51 by Mike Griese

Add snap-layouts support to the Terminal (#11680)

Adds snap layout support to the Terminal's maximize button. This PR is
full of BODGY, so brace yourselves.

Big thanks to Chris Swan in #11134 for building the prototype.
I don't believe this solves #8795, because XAML islands can't get
nchittest messages

- The window procedure for the drag bar forwards clicks on its client
  area to its parent as non-client clicks.
- BODGY: It also _manually_ handles the caption buttons. They exist in
  the titlebar, and work reasonably well with just XAML, if the drag bar
  isn't covering them.
- However, to get snap layout support, we need to actually return
  `HTMAXBUTTON` where the maximize button is. If the drag bar doesn't
  cover the caption buttons, then the core input site (which takes up
  the entirety of the XAML island) will steal the `WM_NCHITTEST` before
  we get a chance to handle it.
- So, the drag bar covers the caption buttons, and manually handles
  hovering and pressing them when needed. This gives the impression that
  they're getting input as they normally would, even if they're not
  _really_ getting input via XAML.
- We also need to manually display the button tooltips now, because XAML
  doesn't know when they've been hovered for long enough. Hence, the
  `_displayToolTip` `ThrottledFuncTrailing`

## Validation
Minimized, maximized, restored down, hovered the buttons slowly, moved
the mouse over them quickly, they feel the same as before. But now with
snap layouts appearing.

## TODO!
* [x] I'm working on getting the ToolTips on the caption buttons back. Alas, I needed a demo of this _today_, so I'll fix that tomorrow morning.
* [x] mild concern: I should probably test Win 10 to make sure there wasn't weird changes to the message loop in win11 that means this is broken on win10.
* [x] I think I used the wrong issue number for tons of my comments throughout this PR. Double check that. Should be #9443, not #9447. 

Closes #9443
I thought this took care of #8587 ~as a bonus, because I was here, and the fix is _now_ trivial~, but looking at the latest commit that regressed.

Co-authored-by: Chris Swan <chswan@microsoft.com>

---
## [JustLenard/TheOdinProject-FreeCodeCamp-Learning-Projects](https://github.com/JustLenard/TheOdinProject-FreeCodeCamp-Learning-Projects)@[11f8f185d2...](https://github.com/JustLenard/TheOdinProject-FreeCodeCamp-Learning-Projects/commit/11f8f185d2aee8a0491e91d4d04a98271881dcf7)
#### Monday 2021-11-29 22:12:38 by Len

OH MY GOSHHHHHH. IT WOOOOOOOOOOOOOOOOOOOOOOOOORKSSSSSSSS. HOOOOOLY SHIT. FUCCCCCCCKING FINALLLLY DUDDDDDE. It only took FOREVER. But I did itgit add .git add . Freaking did it. I am so opening a bear after this. Found the fucking bug and I can play against the bot on console. This is such a relief. Such a feels good too. Now with translating all of this to JS....

---
## [ShibaPuppyClassic/ShibaPuppyClassic](https://github.com/ShibaPuppyClassic/ShibaPuppyClassic)@[ae22183bf5...](https://github.com/ShibaPuppyClassic/ShibaPuppyClassic/commit/ae22183bf5db79a55542a12d1be6323580171f68)
#### Monday 2021-11-29 23:41:29 by ShibaPuppyClassic

ShibaPuppyClassic

💎 SHIBA PUPPY CLASSIC | FAIR LAUNCH 💎
 📌Launch on 30 NOV, 10:00 UTC


The Future of the NFT Marketplace is here.

Create, Discover, Trade and Take Advantage of NFT

ShibaPuppyClassic is a decentralized NFT Marketplace for Creating and Selling NFTs on the Binance Smart Chain; Our goal is to create the most User-Friendly & Interoperable NFT platform that buys back its holders.

Shiba and Doge lovers will love this high and deflationary lunar yield!
We will make the Biggest Community Project

The developer is very experienced and is responsible for a number of very successful tokens in BSC. He applies all his knowledge to this contract to ensure 100% SAFU, carpet resistant and durable. The team looks solid and handles the telegram community really well, I'm pretty sure it's really going to the moon!

Huge hype for this coin. will trend on Dextools for days and hot pair on coinecko

Our mission is to make BSC safer and stronger. With doxxed developer and 100% safe project for you to invest.

Other projects like Doge, Shiba and each other are also part of the ecosystem.
Expect collaboration, cross-project rewards, and integration.

This is going to be a big, game changer and we - as investors who have been cheated in the past - are all waiting for this to happen.

TOKENOMIC
Total supply: 100,000,000,000
Tax: 16%
Crazy 6% Hyper Buyback
3% Auto-LP
6% Marketing & Development
1% Charity

The purchase fee will be 13%:
5% Buyback Passive Income
5% for Marketing
2% LP auto
1% Charity

The cost of selling will be 16%:
Passive Income Buy Back 6%
6% for Marketing & Development
3% Auto LP
1% Charity

🔔 Countdown:  https://countingdownto.com/?c=3950527


🌍 Social media : ✅

🌐 Website: https://shibapuppy.site

🟢 Telegram: https://t.me/shibaPC

🔵 Twitter: https://twitter.com/ShibaPuppyclass

🟣 Discord: https://www.discord.com/ShibaPuppyClassic
🟡 Medium: https://medium.com/@ShibaPuppyClassic

🟤 Github: (Audit)

⚫ Gitbook: (in progres)

---

# [<](2021-11-28.md) 2021-11-29 [>](2021-11-30.md)

