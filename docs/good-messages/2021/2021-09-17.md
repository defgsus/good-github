# [<](2021-09-16.md) 2021-09-17 [>](2021-09-18.md)

3,081,422 events, 1,633,961 push events, 2,416,550 commit messages, 178,446,929 characters


## [Buildstarted/linksfordevs](https://github.com/Buildstarted/linksfordevs)@[f280511ee8...](https://github.com/Buildstarted/linksfordevs/commit/f280511ee81a18eb56c589ed577703e2ae5752e0)
#### Friday 2021-09-17 00:09:14 by Ben Dornis

Updating: 9/17/2021 12:00:00 AM

 1. Added: Can We Solve Linear Algebra Problems at Extreme Scale and Low Precisions?
    (https://nhigham.com/2021/09/14/can-we-solve-linear-algebra-problems-at-extreme-scale-and-low-precisions/)
 2. Added: You wanted WebSockets? | daniel.haxx.se
    (https://daniel.haxx.se/blog/2021/09/16/you-wanted-websockets/)
 3. Added: How We Made Playable Quotes for the Game Boy
    (https://joel.franusic.com/playable_quotes_for_game_boy)
 4. Added: CSS Mirror Editing in Edge DevTools for VS Code
    (https://christianheilmann.com/2021/09/16/css-mirror-editing-in-edge-devtools-for-vs-code/)
 5. Added: Don't fear the pointer — Bitfield Consulting
    (https://bitfieldconsulting.com/golang/pointers)
 6. Added: Slowing 47.181.162.115&c=1&t=44455.7118645833
    (http://techref.massmind.org/techref/other/incompetence.htm)
 7. Added: Notes on the small web
    (https://felix.plesoianu.ro/web/in-the-small.html)
 8. Added: Process Scheduling In Linux
    (https://codingkaiser.blog/2021/09/16/process-scheduling-in-linux/)
 9. Added: XTDB — Open Time Store
    (https://xtdb.com/articles/strength-of-the-record.html)
10. Added: What’s the value of an advanced technical degree? — Mark Simithraaratchy
    (https://marksimi.com/writing/whats-the-value-of-an-advanced-technical-degree)
11. Added: How to Lead a More Rational Life with Bayes' Theorem
    (https://x-team.com/blog/bayes-theorem/)
12. Added: Building OwnFlask - A Flask(like) Python Framework
    (https://bhavaniravi.com/blog/building-own-flask-1/)
13. Added: Hamtips, or why I still run the Technical Phone Screen as the Hiring Manager
    (http://alexeymk.com/2021/09/15/hamtips.html)
14. Added: Ten Years of Fukushima Disinformation | Skeptical Inquirer
    (https://skepticalinquirer.org/2021/06/ten-years-of-fukushima-disinformation/)
15. Added: I Tried to Launch a Side Project Within 30 Days and Failed Hopelessly
    (https://robinmartijn.nl/i-tried-to-launch-a-side-project-within-30-days-and-failed-hopelessly)
16. Added: How to ask for help - Stavros' Stuff
    (https://www.stavros.io/posts/how-to-ask-for-help/)
17. Added: advanced shell packaging: resholve YADM's nixpkg
    (https://t-ravis.com/post/nix/advanced_shell_packaging_resholve_yadm/)
18. Added: Syntax highlighting is backwards
    (https://www.benkuhn.net/syntax/)
19. Added: Work on interesting problems. Not interesting technologies - Part 2
    (https://ruky.me/2021/09/15/work-on-interesting-problems-not-interesting-technologies-part-2/)

Generation took: 00:09:05.2461560

---
## [Swag-Engine/Swag-Engine](https://github.com/Swag-Engine/Swag-Engine)@[cde9e30dfc...](https://github.com/Swag-Engine/Swag-Engine/commit/cde9e30dfc0c5c458dfbde184e82013965aa89d3)
#### Friday 2021-09-17 00:33:09 by KadeDeveloper

make botplay based on time instead of fuckin stupid ass posistionsstgast

---
## [alliedmodders/sourcepawn](https://github.com/alliedmodders/sourcepawn)@[dfae6e3b2a...](https://github.com/alliedmodders/sourcepawn/commit/dfae6e3b2a687316e1ccf4c2f030c34f457eedf3)
#### Friday 2021-09-17 02:30:48 by David Anderson

Remove the text-based assembler.

This improves compilation performance by about 10%, and eliminates a
huge pile of legacy code. Previously, the code generator emitted a text
stream containing assembly instructions. This was then either written to
disk (if -a was given on the command-line), or re-parsed to emit actual
instructions.

This system did have certain advantages, namely, it's easy to debug and
it's easy to rewind the assembly stream. But it had many disadvantages.
It's slow, it's a lot of code, it's difficult to extend, and it required
a lot of hacks to make symbol references and the data section work.

The new code generator emits a binary stream without the text processing
step. It is much more straightforward. We already have smxviewer to
inspect files, so debuggability is not lost.

The new generator uses code from the experimental branch, namely, Label.
Labels greatly simplify backpatching by embedding linked lists within
the codegen stream. This unfortunately breaks the ability to "rewind"
the codegen process (which was called "staging" mode). Staging mode was
used to make sure ternary operators were emitted correctly.  For now, we
emit some relatively gross but harmless instructions to paper over the
problem. Once the analysis phase has been refactored we'll be able to
annotate expressions before codegen begins, and provide a priori
information to eliminate the need for staging hacks.

A new option, --no-verify, has been added as well. When verification
fails it's useful to still emit a .smx file so it can be debugged in
smxviewer.

It's also worth noting that this patch removes a large number of global
variables. sc_labnum, glb_declared, the text stream (gAsmBuffer),
curseg, and the particularly horrible code_idx are now gone.

---
## [comps/ptef](https://github.com/comps/ptef)@[c79e5c00f3...](https://github.com/comps/ptef/commit/c79e5c00f33fbc8c95158e16817b931a83047996)
#### Friday 2021-09-17 02:34:37 by Jiri Jaburek

go back to .copr instead of rpkg

rpkg has several major issues that make it hard to use in upstream
of this project:

1) It too much presumes a rpkg-centric workflow, annotated repository
   tags, RPM subpackage directory structure, etc.
   Even the default examples lead an unsuspecting poor developer
   into {{{ git_dir_* }}} instead of {{{ git_* }}}, causing the
   developer to spend 2 days figuring out git_dir_* is not what
   they want.
   It presumes the project is completely fine with polluting its root
   dir with several rpkg metadata and many spec files.

2) It breaks too easily when deviated from standard workflow.
   The official docs claim several locations for rpkg.conf, with
   the highest prio being $gitroot/.rpkg/rpkg.conf. This is very
   obviously bullshit. Further, user_macros inside [rpkg] in said
   config is ignored, causing rpkg.macros to be always read from
   ./rpkg.macros, not from the configured location.
   This seems to magically work in COPR, but only with the default
   rpkg.macros name, changing the option to anything else breaks
   it.

3) Just untested design - if you don't specify {{{ git_name }}}
   in your spec (because you want to write Name: manually), various
   pieces of functionality break. Same with missing {{{ git_version }}}
   when you want your own user macro for generating version, named
   ie. {{{ my_version }}}. The custom macro works, but the absence
   of {{{ git_version }}} causes the source tarball to lack version
   altogether, generating ie. ptef-.tar.gz.
   IOW *you* not using a jinja2 macro breaks some global state in rpkg.
   Madness!

4) It broke COPR - the COPR rpkg logic parses package name from rpkg
   macros, *creating* (or updating) a package in COPR with that name,
   *ignoring* the original package name the build was run from.
   This results in weird overlaps with per-RHEL-version packages in
   COPR (using different spec files), where all packages would appear
   without builds, but a "shared" ptef package would be updated by
   these per-RHEL builds, potentially overwriting built RPMs.

5) It was just not worth it - we use it here just to prepare a source
   tarball and that's it. We can do that in about the same code amount
   doing it manually and don't have to deal with the BS above.
   Downstream ptef packaging wouldn't be able to use rpkg as it was
   configured here anyway.

Signed-off-by: Jiri Jaburek <comps@nomail.dom>

---
## [facebook/flow](https://github.com/facebook/flow)@[e0f81af916...](https://github.com/facebook/flow/commit/e0f81af9163dc008643462f803bedc134c51ac89)
#### Friday 2021-09-17 06:22:46 by Mike Vitousek

[new-env] Use new environment to power checking

Summary:
This is where we start to link everything together.

This diff makes `statement.ml` use the new environment module to handle checking when the new env is enabled. We do this by functorizing `statement`, `type_inference_js`, and many similar modules over `Env`, and providing the new env as an alternate implementation of the env signature. By doing this, we don't need to change the API used by statement.ml to access the environment much, other than passing in locations in some places where we didn't before. (Nearly all of the changes to `statement` in this diff are just the result of formatting changes by indenting the entire module one level.) When `type_inference_js`'s API is used, callers now need to choose between the old and new env implementations.

The meat of the actual changes in this diff is `new_env.ml` itself. The new env now has to do a number of things: it needs to use the env_builder to find writes for variables that the env builder is aware of, but it also needs to be able to handle internal variables, which don't exist from the env builder's perspective. It also needs to handle heap refinements (which at this point aren't handled by the env builder, though that will change), types (likewise), and contextual information about scope (e.g. are we currently at the module toplevel). Since the original env.ml already handles these things, the new env itself actually `include`s the entire implementation from the original env. It then shadows the definitions of functions relevant to the new env's behavior, e.g. `var_ref`, `set_var`, etc. In many cases, it needs to decide whether to use the new_env's infrastructure or the old env, by checking if the variable to be modified is internal or ordinary; that's why there's a lot of branching on the kind of Reason.name is passed in. (In cases where we don't branch on the name, it's because the name is a `string` and will always be an ordinary variable. That's an annoying inconsistency in the API but this isn't the place to solve it.)

Why not build a new implementation to handle internal vars etc? It sounded like a good idea, since it's pretty unwieldy to include the entire behavior of the old environment to handle a few internal variables and some state-tracking that could be handled elsewhere. However, it turns out that some internal variables really do depend on the complex behavior of the environment. For example, `super` and `this` are considered to be undeclared let-bound variables in derived class constructors, which are declared when `super()` is called, and accessing them (other than calling `super()`) before that is a TDZ error. `maybe_exhaustively_checked` also depends on the behavior of `merge_env`/`copy_env` to determine whether it's definitely initialized or not, which affects the result of exhaustiveness checks. Eventually, all of the internal variables will need to be handled of course, but they'll be handled in different ways--`maybe_exhaustively_checked` will probably go away, `this`/`super`/probably `exports` can be handled by the `env_builder`, and `return`/`yield` likely should be treated, not as variables, but as other unrelated contextual information (especially since they'll need to be locally inferred). So for now I think maintaining the original env's behavior is the best way to maintain support for these features while we work on the early stages of LTI.

All that said, with this diff we can now actually do some checking of programs using the new env, woohoo! This diff includes some test cases. There are differences between the new env's behavior here and the old env, as a result of combining provider-checks and open tvars. In particular, this part of `test.ml` causes an error:
```
var x = c ? 42 : "a'";

var y = 42

if (typeof x === 'string') {
  (x: string);
  y = x;
}
```
With the new env, when we assign `x` to `y`, we get an error. The assignment does a subtyping check against the type of the provider for `y`, which is 42, and since `x` is a string we blame the assignment. This is, of course, the desired behavior for us to eventually reach, but it's currently inconsistent--if `y`'s initializer was a function call instead, we wouldn't get the error, but `x` would instead flow into the function's tout:

```

var x2 = f(42); // where f : number => string
(x2: string); // <- error: number (from y below) is incompatible with string
x2 = y; // where y : number
```

With LTI, this will be fixed since the tout for the function will be resolved, and we'll get an error when we try to write to `x2`. However, to land this without LTI, the fix will be to have a separate tvar for the general type of a variable, which will happen in another diff.

Lastly, this diff creates a new_env mode for ./runtests, with -e. this results in the following:
`Passed: 148, Failed: 157, Skipped: 264, Errored: 159`
which is pretty decent for where we're at imo (it skips annotate_exports and friends)

Reviewed By: panagosg7

Differential Revision: D30616141

fbshipit-source-id: 890e7583ba4754d09771bdc3585a5dc4787e41e4

---
## [GnomeModder/EnforcerMod](https://github.com/GnomeModder/EnforcerMod)@[9bf45f670f...](https://github.com/GnomeModder/EnforcerMod/commit/9bf45f670fb55ef6e3807b158d83641f5f7cb26f)
#### Friday 2021-09-17 06:59:11 by TimeSweeper

fuck you sneed you should have never existed

well trolled, a troll so hard it even made the dev mad

---
## [youknowthechuck/pig](https://github.com/youknowthechuck/pig)@[c73716d1f4...](https://github.com/youknowthechuck/pig/commit/c73716d1f40432525788a04602520ce8a4537352)
#### Friday 2021-09-17 07:05:31 by Connor Hollis

Fucked it all up on purpose:
- Commented out the path node array reference in the path data component. This is because ECS and arrays = bad???
- Because of that I need a different way of accessing the path nodes. For now it's just finding the game object with the nodes on it. 100% certain that isn't thread safe or burst compileable. I don't really get where you're supposed to put global data in ECS yet.
- Made the m_length variable in the path proxy public, so we can grab it in the path travel system.
- Clean up pass on System.Collections.Generic, added using references
- Clean up on some spelling and other nitpicky bullshit
- Fixed up travel system to use the reference to the proxy game object. Stupid code for stupid people, LIKE ME.

---
## [TwinkleInstituteAB/MBBS-abroad](https://github.com/TwinkleInstituteAB/MBBS-abroad)@[3ba37bd26d...](https://github.com/TwinkleInstituteAB/MBBS-abroad/commit/3ba37bd26dbeaed53bbae49b57209f1b82199e93)
#### Friday 2021-09-17 12:02:42 by TwinkleInstituteAB

Create Getting Admission in Russia Medical Colleges 2021

Russia is protected amongst these international locations place the literacy charge is 99% nearly every man or woman holds a college diploma and modernization is up to the mark. Usually, college students pick to study MBBS in russia due to the fact of the modernization and widespread strategies that are maintained. If you have no clue about the strategies which are maintained through scientific universities of Russia for MBBS, then you are on the proper web page we will inform you with each bit of information.

Here we will be going to discuss the factors involving general schooling in Russia that will assist you to apprehend why Russia is the most favoured preference by using all the Indian clinical students.

Things that make Russia the solely first-class vacation spot for scientific education:

Literacy rate:
If a scholar used to be to figure out about the use of a to pursue MBBS Russia, they want to appear at the literacy rate. In Russia, the literacy fee is about 99.6%, and it suggests that right here humans residing there are extra involved in their schooling and additionally strive to be beneficiant about things. Hence when it comes to MBBS find out about Russia, a pupil will be going to get an exceptional environment to study. Along with it, they will be dealing with all the skilled human beings around them.

Advanced techniques and Standards of teaching:
Introduction to the superior strategies has surrounded the campus to beautify the requirements that are being maintained. No single character can say that the requirements are now not up to that stage of college students which is no longer being furnished as they deserve. All-round college students will be going to analyze the modern-day equipment, and they will recognize How these superior strategies will assist college students in a most terrific way to apprehend subjects. Whenever a pupil is coming for Medical Education in Russia, they will be going to get the excessive degree of instructing which will assist them to elevate their well known to some extent. There will be the improvement of persona as well, and they will no longer go via with any form of a challenge at all.

Educational atmosphere:
The academic surroundings are additionally essential in a student’s life. Russian scientific universities grant extraordinary surroundings, and each school is committed to educating with so a lot of compassion. They in no way like to compromise with the time as punctuality is the first and fundamental rule to be observed utilizing them at the top. All the lecturers, students, workers, and all the human beings working around are so dedicated that they do now not like to compromise with any of the matters that make matters higher for college students analyzing in Russia and offering college students with the satisfactory find out about atmosphere. Students will be getting the delivered advantages with acceptable studying if they are being supplied with the great find out about atmosphere.

Great Opportunities:
As it is the pronouncing as properly as real somewhere, possibilities continually knock the door of these humans who are ready for them and seize them every time they knock at their door. Hence if a scholar is coming to study mbbs abroad in russia, he will no longer sense like possibilities are now not there for him. When you are pursuing MBBS, you will have masses of possibilities to control your costs as well, and after completion of commencement Russian scientific universities will furnish college students with a one 12 monthly internship application as well. In internship duration, as nicely as a man or woman will be capable to analyze and acquire journey to exercise further. Hence, college students will be in a position to apprehend how they want to nurture humans in their environment and how they can effortlessly recognize how an affected person is taking care of.

Worldwide Recognitions:
Usually, all the Russian clinical universities supplying MBBS find out about in Russia are identified via MCI, WHO, UNESCO, and different scientific councils. And the clinical universities have maintained their well-known training in a manner that no different University can even contact it. Worldwide consciousness will be supplied to these MBBS universities which are providing college students with Top type clinical training alongside skilled and high-quality faculties. Hence, these universities are eligible for the reputation they have acquired.

These are some of the matters that make the excessive trendy of MBBS Admission In Russia that no other college can beat, and there will be no trouble created to any scholar analyzing there. Also, the high-quality lodging services are being supplied to the college students and they will be going to get all the possibilities that can trade their existence with a vibrant and profitable future. Hence deciding on MBBS In Russia is no longer at all an awful preference however the high-quality choice ever made by way of any science student.

An Academic 12 months in Russian clinical universities begins in September. There are two semesters held in every educational 12 months that are September to January month and February to June month. 1st-semester examinations commence in January whereas second-semester tests begin in June month. The duration of MBBS direction in Russia is of six years (English medium).

---
## [JosephJomama/tgstation](https://github.com/JosephJomama/tgstation)@[ddab8ef27e...](https://github.com/JosephJomama/tgstation/commit/ddab8ef27e0eaa5b184cfc275a3a7a8418566f39)
#### Friday 2021-09-17 12:43:36 by JosephJomama

Adds seafood tab to crafting menu

so when you ask the chef for seafood he knows what the fuck you're even talking about

---
## [samphire/testmaker](https://github.com/samphire/testmaker)@[3ee47f2add...](https://github.com/samphire/testmaker/commit/3ee47f2add09bdfbbc240a036401c294e6d71449)
#### Friday 2021-09-17 13:35:32 by samphire

Insane nonsense messing around with material, or is it mui, or is it date-io or whatever nonsense that fails over and over and over and over again for hours. Node sucks, react sucks, material ui sucks ass.

---
## [rosskush/pyemu](https://github.com/rosskush/pyemu)@[8f386803e0...](https://github.com/rosskush/pyemu/commit/8f386803e08a2c674baa81d32b4114bbda990e3d)
#### Friday 2021-09-17 14:53:34 by Parallels

more fucking bullshit fixes for flopy changing and changing and changing.  Does no one believe backward compat?  Why do we nee to have PlotMapView that does exactly the same thing as ModelMap?  Why does modelgrid have use xcellcenters instead of xcentergrid?  Is one name cryptic name some how better than the other? what a giant pain the ass for anyone downstream

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[2c547dad45...](https://github.com/mrakgr/The-Spiral-Language/commit/2c547dad45319db2c0eb8d2843df850746228980)
#### Friday 2021-09-17 16:24:42 by Marko Grdinić

"10:30am. During the night there was heavy rain and thunder so I stayed in bed. My stress was sky high so I kept having nightmares, but now I feel refreshed. I should have gone to bed earlier instead of 1am, but I did not feel like it.

Ender Lilies was very good. I do not have a single complaint about the game. 9.5/10.

10:35am. No mail to speak of.

10:40am. Let me read the Hagure Idol chapter and then I'll watch the rest of neurosymbolic videos. Right now I am engrossed in the possibilities of this, so much that I am not sure what to do with probabilistic programming. I guess I'll put that on the backburner unless I get part time work.

https://www.youtube.com/watch?v=m78qYOdK4Tg
More-Neural Symbolic Concept Learning | Christopher Manning

Let me watch this.

https://youtu.be/m78qYOdK4Tg?t=1017

I remember this. This is just another hack. I am yet to see anything remotely impressive as LNNs.

https://youtu.be/m78qYOdK4Tg?t=1322

I'll have to check out how NSCL works.

https://youtu.be/m78qYOdK4Tg?t=1712
> Geoff: Extrapolating the spectacular performance of GPT3 into the future suggest that the answer to life, the universe and everything is just 4.398 trillion parameters.

Geoff is probably being sarcastic.

11:30am. Forget this. Let me go for the LNN paper. Nothing in the field can possibly be more impressive than it.

There is no need to look anymore.

> Inputs are initial truth value bounds for each of the neurons in the network; in particular, neurons pertaining to predicate atoms may be populated with truth values taken from KB data. Additionalinputs may take the form of injected formulae representing a query or specific inference problem. Outputs are typically the final computed truth value bounds at one or more neurons pertaining to specific atoms or formulae of interest. In other problem contexts, the outputs of interest may instead be the neural parameters themselves — serving as a form of inductive logic programming (ILP) — after learning with a given loss function and input training data set.

This is a new-new thing. LNNs are a big deal. They aren't some architecture hack, but a fundamental breakthrough that changes the way I will be doing programming.

I meant to look into probabilistic programming, but if I am to do variational inference, NNs will no doubt come into play and I will have all the same issues as before. Whereas this changes things at exactly where the pain point is. I'll look into ways of of combining LNNs and prob prog, but for now my focus will be LNNs. Regular NNs failed me, but these might be powerful enough to get me through the poker hurdle.

11:40am. I'll look into ILP later. For now, let me focus on this paper. It is important enough that I can dedicate a whole week just to it and its follow ups.

11:50am. Something is coming back to me. Those soft dual bools in Omega remind me of the lower and upper bounds being used here. Could they be related?

> Many candidate neural activation functions can accommodate the classical truth functions of logical connectives, each varying in how it handles inputs strictly between 0 and 1. For instance, min{x, y} is a suitable activation function for real-valued conjunction x⊗y, but so are x·y and max{0, x+y−1}.

Hmmm, what about the softmax? Or maybe softmin in this case.

> The choice of activation function corresponds to the implemented real-valued logic; Gödel, product, and Łukasiewicz logic are common examples.

This kind of refinement is what I've been badly missing with regular NNs. Every activation seems to work similarly there and there is no rhime or reason to them beyond how they propagate gradients.

5/48. The fact that this method requires bidirectional inference only serves to increase my convinction in it. I'd long have thought than the real method would require back and forth sweeps.

12:15pm. I am thinking myself on the side. Softmax would correspond to something like a XOR...no, these activation are on a per neuron basis. Working with distributions is beyond the scope of this paper.

In order to get that I'd need to extend the framework so it covers it.

11/48. Here is the appendix. After I am done with the paper, I'll look up some follow ups that use these nets to do RL.

12:35pm. 12/48. I am not focusing too deeply on the sentences. This is complex stuff and I will have to internalize it through practice.

12:50pm. Let me get breakfast here.

1:45pm. Let me resume.

2:55pm. 41/48. Had to take a break. Let me finish the paper. Let me also grab Hollow Knight while I am at it. I meant to set it to download earlier.

3:10pm. Done. Let me check out some of the RL specific follow ups.

https://scholar.google.com/scholar?cites=8528841725427555404&as_sdt=2005&sciodt=0,5&hl=hr

It has 21 citations, let me go through them.

Hmmm, there isn't much in terms of follow up. Let me take a look at the talk links.

3:20pm. I have no idea what is going on with Arxiv. The download is too slow. Though this might be due to me downloading HK in the background.

Let me take a look at the other talk on LNNs by some african guy.

https://www.youtube.com/watch?v=SweBzdCCQO8
Ndivhuwo Makondo: Logical Neural Networks

Let me watch this.

3:30pm. There is probably something going on with Arxiv seeing I have enough bandwidth to watch the video without issue.

3:50pm. https://youtu.be/SweBzdCCQO8?t=1708

I did think about this. I did consider the possibility that NNs could be made to act as logic systems and that maybe it would be possible to propagate probabilities. Only I had no idea how to actually implement such a system and went on to what was there.

If I cultivate this I'll be able to make a decent player using only a small number of parameters and then scale it afterwards. Poker is the kind of game that needs reasoning instead of just pattern matching too and I knew regular NNs are not a good fit. I just hoped that I could find tricks to make it work.

But no amount of tricks can make up from having wrong principles.

Prob programming is an option, but it should not be my job to crack nested inference. And if I use VI to get scaling I'll run into the same issues when using regular NNs as when I was doing regular deep RL.

I am unlucky that I did not go down this route before, but I am really lucky that I discovered this in the first place. There is a lot of hope of attaining something using this.

4pm. I can allow myself to feel some relief. I had my dose of humility in the last month what with throwing in the towel and actually applying to jobs for the first time in my life. I had actually accepted defeat. I was forced to admit that my odds of winning with the approach I was taking was 0 despite putting in over 6 months of effort.

But now I can dream again.

I will dedicate myself to mastering neural logic and that will be the tool to break into 3/5 in ML where I can do useful things with it. Once I get the AI chips and implement it all there, I will be able to move to 4/5.

I'll start programming tomorrow. Right now, let me watch the video and read the paper.

https://arxiv.org/abs/2106.05387
Eye of the Beholder - Improved Relation Generalization for Text-based Reinforcement Learning Agents

This one I mean. This was the paper referred to in the talk I think.

4:20pm. https://youtu.be/SweBzdCCQO8?t=2391

Being able to do downward inference will be really useful for modeling the opponent's hand.

4:30pm. https://paperswithcode.com/paper/logical-neural-networks

Hmmm, I am surprised that nobody tried implementing this yet.

Should I consider that a red flag?

I'll believe in it. The story is good enough. I should consider it my homework to implement this.

https://youtu.be/SweBzdCCQO8?t=3043
> NM: One of the biggest challenges with classical systems since they work with true or false values the rules that engineer hand craft must be perfect. But it is actually very difficult to come up with perfect rules.

https://youtu.be/SweBzdCCQO8?t=3183

Comparison with Bayesian networks.

https://youtu.be/SweBzdCCQO8?t=3265

I need to remember inductive logic programming.

5:05pm. Let me go for the paper.

https://arxiv.org/abs/2103.02363
Reinforcement Learning with External Knowledge by using Logical Neural Networks

That paper I linked to had nothing to do with it.

https://arxiv.org/abs/2008.02429
Foundations of Reasoning with Uncertainty via Real-valued Logics

I'll also take a look at this.

5:35pm. Done with the 'Reinforcement Learning with External Knowledge by using Logical Neural Networks' paper.

It is only 4 pages. I had to put in Alexander Gray as one of the authors while searching. It seems scholar failed me.

Let me unexcite myself a bit. Supposing I use these nets, but init the randomly much like regular ones. In that case do I really have a particular advantage over other approaches? Do I really need the inference step for the opponent's hand?

Yeah, I think I do need that last one so I can calculate the EV properly. But can't I use prob prog with VI to get the same effect? I could train a VAE to do this task.

Right now, there are no libraries for LNNs so I'd have to implement it all on my own. I should think about it carefully before I decide on a course of action.

I have to cultivate one of the two approaches.

Ultimately, the pathway to skill is to increase your understanding. Just throwing data at NNs will lead me to stagnate.

5:45pm. Let me read that other paper.

> Formalization of the idea of real-valued logics is old and fundamental, going back to the origins of formal logic. It is not well known that Boole himself invented a probabilistic logic in the 19th century [2], where formulas were assigned real values corresponding to probabilities.

I had no idea.

6pm. https://cdn.codeground.org/nsr/downloads/global/cambridge-workshop/A%20New%20Paradigm%20of%20Logical%20Neural%20Networks-Ryan%20Riegel.pdf

I should keep hese slides in mind. I am not sure the paper had an optimizer spelled out.

6pm. Let me stop here on time for once. It is 6pm and the work day is over.

I have no choice. It would be too rash to just dive into LNNs. Instead I should thoroughly play with prob prog just as per plan. I should see how far I can push that. Then I should see how far I can push LNNs. After that I'll either pick out the dominant approach or combine the two.

6:05pm. Anything I could try has its disadvantages. No matter what I approach I pick, if I just randomnly initialize the layers and let them train, I'll run into the sample efficiency problems much like before.

I should solve the problem in the small with a combination of RL and VI first.

6:10pm. So what is the plan for tomorrow? I wasn't exactly too focused today.

More Pyro tutorials. Zenna's examples.

I need to transition from just reading stuff to actual programming. If prob prog turns out to be too hard to do anything with in practice, I'll switch to neurosymbolic approaches. I need to see how much I can go with the current standard methods before I decide on that first.

I should think about translating the poker game from Spiral into Julia. Or alternatively, I should make a Julia backend for Spiral because I just know it won't do a good job of inlining monadic code.

But before I do that, I'd like to think of some RL situations I could use prob prog in.

Think...Julia should have something related to doing RL on a small scale. I should try out the PPLs on that. That will tell me where I stand very quickly. Some grid world stuff should be enough. If PPL approaches flounder against deep RL or tabular ones then they are just a waste of time and I should move to neurosymbolic ones.

Yeah, that should be my goal. It would be one thing if I got part time work on PPLs, but since I do not have anybody to give me income to work on random things, I need to focus on what could give me real world gains.

Even though the realization that probability theory extends regular logic to handle uncertainty, I do not need the features PPLs offer me. My primary concern is to increase the sample efficiency of my agents and nothing else.

I should not lose sight of that."

---
## [LumberKing/Tianxia](https://github.com/LumberKing/Tianxia)@[619c86f5e9...](https://github.com/LumberKing/Tianxia/commit/619c86f5e9801e0856bb5a66df02cd7adc308262)
#### Friday 2021-09-17 16:46:26 by Silversweeper

Balance/optimization/polishing (part 5 of ???) + Fujiwara improvements (1 of ???)

General:
- Started work on consolidating assorted namespaces into soh/kom/mr/sri/tianxia namespaces. Will be WIP for a long time since all old events (and references to them) need to be dealt with.
- Several instances of "taoist" are now "confucian" or both.

Artefacts:
- Assorted Korean book artefacts now actually check for e.g. Koreanic + Buddhist/Taoist/Confucian rather than Koreanic OR Buddhist/Taoist/Confucian.

Bloodlines:
- The Very Best bloodline, Hwarang refounding bloodline, and both generic Grand Chancellor bloodlines now count as created bloodlines.

CBs:
- Fixed some potential cases of sacriligious Chrysanthemum Throne usurpation.

Societies:
- Confucians can now join the Underworld Triad and the Hwarang, and are treated as Taoists as far as the WotRS goes.

scripted_effects:
- Fixed a few leftover references to offmap_china.
- Removed preset attribute values for randomly generated Council Eunuchs. After all, we don't really need them to be incredibly competent...
- Made the randomly generated Council Eunuchs less likely to get nifty traits. Again, competence isn't necessarily what we want or need...

Religion:
- Added +1 piety/month while at peace to the Taoist religion.
- Started work on adding Confucianism. SRS, holy order, branch traits, god names, general localisation TODO.
- Updated intermarriages to account for Confucianism.

Decisions:
- Removed irrelevant decision to restore the Manichean leadership; it's around in all start dates in vanilla since a while back.

Events:
- Reduced the chance of Ainu WL consorts spawning with inheritable traits a bit.
- Tweaked Iyomante rewards and logic slightly.
- Guests can now get a hangover at the Iyomante even with a middling amount of sake.
- Dharmics that are unlikely to possess elepants no longer gift elepants as part of the Business focus trade route events.
- "Civlized" pagans no longer gift warriors as part of the Business focus trade route events.
- Slightly changed Court Mu and Gut event rewards and logic.
- Gim Daeseong should no longer get an event talking about himself in third person if he goes to Bulguksa.
- Slightly adjusted Bulguksa/Seokguram pilgrimage logic and rewards.
- Adjusted Chuseok logic and rewards slightly, and added an option to potentially convert to Confucian.

History:
- Prince Michihito's birth certificate no longer requires time travel to be accurate.
- Added and fixed several Nanke Fujiwaras.
- Added and fixed several Kyoke Fujiwaras.
- Added and fixed several Shikike Fujiwaras.
- Updated assorted history files in Japan due to being able to land more Fujiwaras.
- Confucius' descendants are now Confucian.
- Some notable Confucians are now Confucian.
- Minor adjustments outside Japan.

Localisation:
- Commented out a bunch of random localisation strings that seem completely irrelevant to Tianxa, and removed some that most definitely aren't needed.
- Moved changed vanilla localisation into the correct file and deleted several vanilla files rendered irrelevant by this. JD still TODO.
- Cleaned up several unused files.
- Fixed several instances of the game being "literally unplayable" due to typos.
- Fixed some very old localisation strings that really needed work.
- Improved Ainu localization slightly.
- Updated some inconsistent, old, and/or confusing localisation.
- Improved Mu, Gut, Chuseok, and Bulguksa pilgrimage localisation slightly.

---
## [LleidaHack/hackeps-2021](https://github.com/LleidaHack/hackeps-2021)@[0b462ffb95...](https://github.com/LleidaHack/hackeps-2021/commit/0b462ffb9599866ae9ba24d41becede7ee606496)
#### Friday 2021-09-17 18:02:45 by Jordi Ricard Onrubia Palacios

Merge pull request #50 from JordiROP/master

fixed bullshit over fucking shit

---
## [Cosinusjustafonction/training](https://github.com/Cosinusjustafonction/training)@[d70256ae17...](https://github.com/Cosinusjustafonction/training/commit/d70256ae174387220b14098ce9c1b346c5da7d23)
#### Friday 2021-09-17 19:26:52 by ilyass

USACO 2017 US Open Contest, Bronze Problem 1. The Lost Cow

Farmer John has lost his prize cow Bessie, and he needs to find her!
Fortunately, there is only one long path running across the farm, and Farmer John knows that Bessie has to be at some location on this path. If we think of the path as a number line, then Farmer John is currently at position x and Bessie is currently at position y (unknown to Farmer John). If Farmer John only knew where Bessie was located, he could walk directly to her, traveling a distance of |x−y|. Unfortunately, it is dark outside and Farmer John can't see anything. The only way he can find Bessie is to walk back and forth until he eventually reaches her position.

Trying to figure out the best strategy for walking back and forth in his search, Farmer John consults the computer science research literature and is somewhat amused to find that this exact problem has not only been studied by computer scientists in the past, but that it is actually called the "Lost Cow Problem" (this is actually true!).

The recommended solution for Farmer John to find Bessie is to move to position x+1, then reverse direction and move to position x−2, then to position x+4, and so on, in a "zig zag" pattern, each step moving twice as far from his initial starting position as before. As he has read during his study of algorithms for solving the lost cow problem, this approach guarantees that he will at worst travel 9 times the direct distance |x−y| between himself and Bessie before he finds her (this is also true, and the factor of 9 is actually the smallest such worst case guarantee any strategy can achieve).

Farmer John is curious to verify this result. Given x and y, please compute the total distance he will travel according to the zig-zag search strategy above until he finds Bessie.

---
## [newstools/2021-daily-post-nigeria](https://github.com/newstools/2021-daily-post-nigeria)@[184a2b9045...](https://github.com/newstools/2021-daily-post-nigeria/commit/184a2b90456c2b41b685a79380584d9e5e1bba52)
#### Friday 2021-09-17 19:45:34 by Billy Einkamerer

Created Text For URL [dailypost.ng/2021/09/17/a-sister-set-me-up-with-her-boyfriend-to-confirm-my-virginity-nkechi-blessing/]

---
## [xiaomi-sdm678/android_kernel_xiaomi_mojito](https://github.com/xiaomi-sdm678/android_kernel_xiaomi_mojito)@[83571b8bd4...](https://github.com/xiaomi-sdm678/android_kernel_xiaomi_mojito/commit/83571b8bd4a8dabd215b974ea93f3626637ea8cb)
#### Friday 2021-09-17 20:49:28 by Peter Zijlstra

sched/core: Fix ttwu() race

Paul reported rcutorture occasionally hitting a NULL deref:

  sched_ttwu_pending()
    ttwu_do_wakeup()
      check_preempt_curr() := check_preempt_wakeup()
        find_matching_se()
          is_same_group()
            if (se->cfs_rq == pse->cfs_rq) <-- *BOOM*

Debugging showed that this only appears to happen when we take the new
code-path from commit:

  2ebb17717550 ("sched/core: Offload wakee task activation if it the wakee is descheduling")

and only when @cpu == smp_processor_id(). Something which should not
be possible, because p->on_cpu can only be true for remote tasks.
Similarly, without the new code-path from commit:

  c6e7bd7afaeb ("sched/core: Optimize ttwu() spinning on p->on_cpu")

this would've unconditionally hit:

  smp_cond_load_acquire(&p->on_cpu, !VAL);

and if: 'cpu == smp_processor_id() && p->on_cpu' is possible, this
would result in an instant live-lock (with IRQs disabled), something
that hasn't been reported.

The NULL deref can be explained however if the task_cpu(p) load at the
beginning of try_to_wake_up() returns an old value, and this old value
happens to be smp_processor_id(). Further assume that the p->on_cpu
load accurately returns 1, it really is still running, just not here.

Then, when we enqueue the task locally, we can crash in exactly the
observed manner because p->se.cfs_rq != rq->cfs_rq, because p's cfs_rq
is from the wrong CPU, therefore we'll iterate into the non-existant
parents and NULL deref.

The closest semi-plausible scenario I've managed to contrive is
somewhat elaborate (then again, actual reproduction takes many CPU
hours of rcutorture, so it can't be anything obvious):

					X->cpu = 1
					rq(1)->curr = X

	CPU0				CPU1				CPU2

					// switch away from X
					LOCK rq(1)->lock
					smp_mb__after_spinlock
					dequeue_task(X)
					  X->on_rq = 9
					switch_to(Z)
					  X->on_cpu = 0
					UNLOCK rq(1)->lock

									// migrate X to cpu 0
									LOCK rq(1)->lock
									dequeue_task(X)
									set_task_cpu(X, 0)
									  X->cpu = 0
									UNLOCK rq(1)->lock

									LOCK rq(0)->lock
									enqueue_task(X)
									  X->on_rq = 1
									UNLOCK rq(0)->lock

	// switch to X
	LOCK rq(0)->lock
	smp_mb__after_spinlock
	switch_to(X)
	  X->on_cpu = 1
	UNLOCK rq(0)->lock

	// X goes sleep
	X->state = TASK_UNINTERRUPTIBLE
	smp_mb();			// wake X
					ttwu()
					  LOCK X->pi_lock
					  smp_mb__after_spinlock

					  if (p->state)

					  cpu = X->cpu; // =? 1

					  smp_rmb()

	// X calls schedule()
	LOCK rq(0)->lock
	smp_mb__after_spinlock
	dequeue_task(X)
	  X->on_rq = 0

					  if (p->on_rq)

					  smp_rmb();

					  if (p->on_cpu && ttwu_queue_wakelist(..)) [*]

					  smp_cond_load_acquire(&p->on_cpu, !VAL)

					  cpu = select_task_rq(X, X->wake_cpu, ...)
					  if (X->cpu != cpu)
	switch_to(Y)
	  X->on_cpu = 0
	UNLOCK rq(0)->lock

However I'm having trouble convincing myself that's actually possible
on x86_64 -- after all, every LOCK implies an smp_mb() there, so if ttwu
observes ->state != RUNNING, it must also observe ->cpu != 1.

(Most of the previous ttwu() races were found on very large PowerPC)

Nevertheless, this fully explains the observed failure case.

Fix it by ordering the task_cpu(p) load after the p->on_cpu load,
which is easy since nothing actually uses @cpu before this.

Fixes: c6e7bd7afaeb ("sched/core: Optimize ttwu() spinning on p->on_cpu")
Reported-by: Paul E. McKenney <paulmck@kernel.org>
Tested-by: Paul E. McKenney <paulmck@kernel.org>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lkml.kernel.org/r/20200622125649.GC576871@hirez.programming.kicks-ass.net
Signed-off-by: azrim <mirzaspc@gmail.com>
Signed-off-by: Anush02198 <Anush.4376@gmail.com>

---
## [Danielhongjin/AOH2R](https://github.com/Danielhongjin/AOH2R)@[fcd32f9f65...](https://github.com/Danielhongjin/AOH2R/commit/fcd32f9f6501fdcbd5ffacefea85242dd7ce74f5)
#### Friday 2021-09-17 21:42:18 by Danielhongjin

1.36
Simplified Chinese localization of items has begun thanks to CandiFantasia. Most item names and item descriptions have not been translated yet, save for certain descriptions. If you'd like to contribute, swing by our discord.

Hard mode regen from 0.85% to 0.65%.
Outpost heal aura from 2.5% hp and 1.5% mp to 3.25% hp and 2.0% mp.
Round end is now signified by a random horn sound for your listening pleasure.

Experimental change made to boss behaviors: Boss animation periods will prevent boss cooldowns from progressing. May or may not break some behaviors. Should stop bosses from dumping abilities right away after a lengthy ability.
New boss round 39, boss rush.
New boss Void Spirit replaced old boss Antimage - Watch out for his counter. Don't run up or right when he uses his revenge.
Tinker boss round removed, all higher round bosses shifted down one level.
Abyssal Underlord slightly chunkier, shockwave is also a chunker.
Tidehunter boss no longer has a heart of tarrasque.

Undying boss zombie hp from 1300 to 800. Spawns more zombies now.

Dynamo-Core mana reduction from 30% to 20%.
Warrior's Seal magic resistance nerfed from 25% to 20%.

Wisp tether nerfed healing ratio. Tether cooldown now 0.
Wisp relocate cooldown from 30/25/20 to 30/20/10.
Wisp agility and intelligence stat gain increased by 1.

---
## [newstools/2021-eye-witness-news](https://github.com/newstools/2021-eye-witness-news)@[69963a6b7a...](https://github.com/newstools/2021-eye-witness-news/commit/69963a6b7a6423afe34cb899074c8e2e1ce2f147)
#### Friday 2021-09-17 23:17:59 by Billy Einkamerer

Created Text For URL [ewn.co.za/2021/09/17/there-were-earlier-attempts-on-nomia-ndlovu-s-boyfriend-s-life-claims-his-sister]

---

# [<](2021-09-16.md) 2021-09-17 [>](2021-09-18.md)

