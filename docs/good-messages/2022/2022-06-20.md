# [<](2022-06-19.md) 2022-06-20 [>](2022-06-21.md)

1,699,227 events recorded by [gharchive.org](https://www.gharchive.org/) of which 1,699,227 were push events containing 2,656,243 commit messages that amount to 199,542,157 characters filtered with [words.py@e23d022007...](https://github.com/defgsus/good-github/blob/e23d022007992279f9bcb3a9fd40126629d787e2/src/words.py) to these 28 messages:


## [treckstar/yolo-octo-hipster](https://github.com/treckstar/yolo-octo-hipster)@[884bbf92b2...](https://github.com/treckstar/yolo-octo-hipster/commit/884bbf92b27cc9c587138ca135136b53cbbe99f1)
#### Monday 2022-06-20 02:22:03 by treckstar

People listen up don't stand so close, I got somethin that you all should know. Holy matrimony is not for me, I'd rather die alone in misery.

---
## [HexerMaster1929/HexHub](https://github.com/HexerMaster1929/HexHub)@[0fe6b984f7...](https://github.com/HexerMaster1929/HexHub/commit/0fe6b984f787526cc5d2d720afbdb9b8446e82a3)
#### Monday 2022-06-20 03:03:34 by Mark

Update Notification (Public For Exploit Makers).lua

Fuck My life ass shit i keep messing stuff up

---
## [knight00/LFLists](https://github.com/knight00/LFLists)@[4c4fe013b4...](https://github.com/knight00/LFLists/commit/4c4fe013b453e19b0f137983f95c35d51842b83f)
#### Monday 2022-06-20 03:18:52 by pyrQ

Rush whitelist: Added new Rush cards + official releases

From "Go Rush Deck - Galactica Arrive":
- Cosmo Titan
- Sorapuyo
- Transamu Klein
- Transamu Arrive
From "Megaroad Pack":
- Strong Strike Dragon Metagiastar F
- Sevens Road Magician (alt art)
- Sevens Road Witch (alt art)
- Magic Curtain
- Darkness Road
- Sevens Road Protection
- Omega Guitarna the Supreme Shining Superstar
- Princess Omega the Supreme Shining Superstar
- A.I. Bear Can
- Ama Lilith
- Psychic Omega Blast
- Cross Target
- Featuring Omega
- Shoulder Phone Nyan Nyan
- Oriental Tiger
- Imptailing Crisis
The "Yu-Gi-Oh! Rush Duel LP Volume 1" promo:
- Accel Wonder Pebble

In addition, the cards from "Go Rush Deck - Galactica Arrive", "Go Rush Deck - Jointech Attack", "Go Rush Deck Bonus Cards", the "Yu-Gi-Oh! SEVENS Luke! Explosive Supremacy Legend!! Volume 3" promo, the "Saikyō Jump June 2022" promo, and the "Yu-Gi-Oh! SEVENS My Road Academy Volume 1" promo are now out of pre-release.

ProjectIgnis/BabelCDB@3fc186d566c63b864da5f989dcdd75e66d566c1c

---
## [knight00/LFLists](https://github.com/knight00/LFLists)@[48bfc997e1...](https://github.com/knight00/LFLists/commit/48bfc997e12635dc9cedfad818d252595f7ec4f1)
#### Monday 2022-06-20 03:18:52 by pyrQ

Rush whitelist: Added new Rush cards + "Deck Modification Pack - Galaxy of Fate!!" official release

From "Go Rush Deck Bonus Cards":
- Pitch-Black Warwolf
- Alien Shocktrooper
- Dark Factory of Mass Production
- Tribute Doll
From "Go Rush Deck - Galactica Arrive":
- Galactica Amnesia
- Rebirth Cycle
- Heaven Gancel
- Strange Traveler
- Bright Sentinel
- Shadow Sentinel
- Asteroeva
- Galactica Force
- Vacua Annihilation
- Nebula Power
From "Go Rush Deck - Jointech Attack":
- Gadget Soldier
- Lightning Braver
- Jointech Bumper
The "Saikyō Jump June 2022" promo:
- Direct Dive Dragon
From "Megaroad Pack":
- Magician of Dark Sevens
- Darkness Zerorogue
- Darkness Fource Seeker
- Darkness Rogue
- Road Magic - Dark Night
- Magic Fire Guard

In addition, the cards from "Deck Modification Pack - Galaxy of Fate!!" are now out of pre-release.

ProjectIgnis/BabelCDB@fccfed3fec4a655ae05ddf5a3a0a4d6cd446b856

---
## [knight00/LFLists](https://github.com/knight00/LFLists)@[bea8df21a4...](https://github.com/knight00/LFLists/commit/bea8df21a4d947d805abc59a8d4645c747448800)
#### Monday 2022-06-20 03:18:52 by Naim

Rush whitelist: Added new Rush cards

From "Megaroad Pack":
- The Half-Body Crawling Around
- Ghoulish Gal
- The Thing in the Purple Mirror
- The Doll of Dread
- Nightmare Knock
- Frightening Fan Mail
- Zombie Carnival
- Zombie Fireworks
- Surprising Zombie Victory
- Gradually Approaching Footsteps
- The Cursed Cat Counting Dishes
- Terror Phone Number
- Cursed Skeletal Dragon Diarga
- The Strange Specter of Celestial Severance
- Sacrificial Summon
- Progress Potter
- Mother's Storm
- Excitagain
From "Deck Modification Pack - Requiem of Destruction!!":
- Dian Keto the Cure Maiden
- Cremation Dog Nitro
- Dynamo Might
- Chemicalize Salamander
- Rice Terrace Ripple
- Ewekai Aquasheep
- Ewekai Thunderlambda
- Ewekai Airaries
- Ewekai Waveschaf
- Melo Melo Meeeg☆Uuultra Beam
- Splendid Floor Master
- Tutumes Dark Witch
- Ichthyosteguard
- Sunbathing Kappa
- Kappa's Gas
- Miginagi the Talismanic Warrior
- Rainy Megalopolis
- Bubble Kingdom
- Nuvia the Wicked Mischief Maker
- Sannomiya Golden G Robo MK-III
- Alien 33
- The Three Warp-Granny Sisters
- Alien Count of the White Dwarf, St. Germain
- 300 Light-Year Red Cloak
- Third Coming of the Reptilian Count
- Area 33
- The Three Moonlit Mystery Geckos
The "Jump Victory Carnival 2022" promo:
- Luster Dragon

ProjectIgnis/BabelCDB@37423207dd2d2f6048aca60c240ff3b21e7f0d9b

---
## [jhamby/gnv-bash](https://github.com/jhamby/gnv-bash)@[5f2bc76a88...](https://github.com/jhamby/gnv-bash/commit/5f2bc76a884607cb4804927d9b234ce1d9e6f0ff)
#### Monday 2022-06-20 03:55:50 by Jake Hamby

Commit work so far on bash 5.1 port.

Removed as many VAX-specific hacks as I could find.

Shortened config_vms.h by moving enabling default bash features
into config_h.com (along with passing through "#include" lines).

Removed "VM pipes" from the build, at least for now. My theory is
that setting the pipe buffer size and pipe buffer quota to 65535
will provide enough of a buffer for pipelines (possibly requiring
user quotas to have their bytlm raised to 400000 bytes or more),
without the added overhead of a complex pipe add-on designed for
throughput. I can add them back later if they are faster.

My plan is to use the VMS C RTL poll() function, which supports
files, pipes, mailboxes, sockets, etc., in place of the local
implementation of poll() and select() that requires intercepting
all of the file APIs (open, close, read, write). We still need
any special handling related to terminal driver mapping to termios,
but not the workaround for not having a suitable poll().

I removed some lines from "config_vms.h_in" (which gets copied
into "config_vms.h", along with the CPU architecture) that disable
a number of compiler warnings that I'm interested in. That means
that the build is currently failing to compile error.c, until I
add a few small patches for error.c and error.h to make the
command_errstr() function static, to fix a const pointer mismatch
with the const char * error string array. %CC-W-NOTCONSTQUAL is
one of the warnings that I want enabled, to fix these occurrences.

---
## [clayne/mpv](https://github.com/clayne/mpv)@[3d459832a8...](https://github.com/clayne/mpv/commit/3d459832a88a9bd2835b339cf6ca98f84aad0115)
#### Monday 2022-06-20 05:02:10 by Dudemanguy

x11: support xorg present extension

This builds off of present_sync which was introduced in a previous
commit to support xorg's present extension in all of the X11 backends
(sans vdpau) in mpv. It turns out there is an Xpresent library that
integrates the xorg present extention with Xlib (which barely anyone
seems to use), so this can be added without too much trouble. The
workflow is to first setup the event by telling Xorg we would like to
receive PresentCompleteNotify (there are others in the extension but
this is the only one we really care about). After that, just call
XPresentNotifyMSC after every buffer swap with a target_msc of 0. Xorg
then returns the last presentation through its usual event loop and we
go ahead and use that information to update mpv's values for vsync
timing purposes. One theoretical weakness of this approach is that the
present event is put on the same queue as the rest of the XEvents. It
would be nicer for it be placed somewhere else so we could just wait
on that queue without having to deal with other possible events in
there. In theory, xcb could do that with special events, but it doesn't
really matter in practice.

Unsurprisingly, this doesn't work on NVIDIA. Well NVIDIA does actually
receive presentation events, but for whatever the calculations used make
timings worse which defeats the purpose. This works perfectly fine on
Mesa however. Utilizing the previous commit that detects Xrandr
providers, we can enable this mechanism for users that have both Mesa
and not NVIDIA (to avoid messing up anyone that has a switchable
graphics system or such). Patches welcome if anyone figures out how to
fix this on NVIDIA.

Unlike the EGL/GLX sync extensions, the present extension works with any
graphics API (good for vulkan since its timing extension has been in
development hell). NVIDIA also happens to have zero support for the
EGL/GLX sync extensions, so we can just remove it with no loss. Only
Xorg ever used it and other backends already have their own present
methods. vo_vdpau VO is a special case that has its own fancying timing
code in its flip_page. This presumably works well, and I have no way of
testing it so just leave it as it is.

---
## [GsoSoft/Rocket.Chat](https://github.com/GsoSoft/Rocket.Chat)@[5a37518e42...](https://github.com/GsoSoft/Rocket.Chat/commit/5a37518e42dec114e0ed1a71b5d103b4a62e9b58)
#### Monday 2022-06-20 05:55:51 by Ben Wiederhake

[FIX] Client-generated sort parameters in channel directory  (#25768)

## Proposed changes (including videos or screenshots)
- In the web-client, sorting the channel directory by "Last Message" raises the error "Invalid sort parameter provided".

I don't think a screenshot is necessary, but if you'd like one anyway, here it is:

![Bildschirmfoto_2022-06-06_12-54-34](https://user-images.githubusercontent.com/2690845/172147996-e9942daf-6819-4eee-afa4-b1c6bce7a650.png)


## Issue(s)
Closes #25695

## Steps to test or reproduce

- Open the web client, i.e. navigate your browser to `https://rocketchat.$DOMAIN/home`
- Click the "Directory" button in the top left, (or just navigate directly to `https://rocketchat.$DOMAIN/directory/channels`)
- In the table header, click on "Last message" once
- In the table header, click on "Last message" again

Expected behavior: Clicking "Last message" turns on and then toggles sorting by the date of the last message, either first ascending and then descending, or the other way around.

Actual behavior: The first click sorts the messages in ascending order (good!), the second click shows a red warning box "FIXME", the table content disappears, and is replaced by throbbing grey placeholders.

### "Good" request (ascending order):

`https://rocketchat.domain.org/api/v1/directory?query=%7B%22type%22%3A%22channels%22%2C%22text%22%3A%22%22%2C%22workspace%22%3A%22local%22%7D&sort=%7B%22lastMessage%22%3A1%7D&count=25`

More easily readable GET parameters:

```
query | {"type":"channels","text":"","workspace":"local"}
sort | {"lastMessage":1}
count | 25
```

Response:
```
{"result":[{"_id":"AAAAAAAAAAAAAAAAA","name":"foobar","fname":"foobar","t":"c","usersCount":10,"ts":"2019-01-01T00:00:00.000Z","default":false,"lastMessage":{"_id":"AAAAAAAAAAAAAAAAA","rid":"AAAAAAAAAAAAAAAAA","msg":"Hello, World!","ts":"2019-01-01T00:00:00.000Z","u":{"_id":"AAAAAAAAAAAAAAAAA","username":"normaluser","name":"normaluser"},"unread":true,"_updatedAt":"2019-01-01T00:00:00.000Z","urls":[],"mentions":[],"channels":[]},"description":"Obviously, this JSON response is heavily censored."}],"count":25,"offset":0,"total":52,"success":true}
```

(Obviously, this JSON response is heavily censored, but you get the gist: It was successful.)

### "Bad" request (descending order):

`https://rocketchat.domain.org/api/v1/directory?query=%7B%22type%22%3A%22channels%22%2C%22text%22%3A%22%22%2C%22workspace%22%3A%22local%22%7D&sort=%7B%22lastMessage%22%3A0%7D&count=25`

More easily readable GET parameters:

```
query | {"type":"channels","text":"","workspace":"local"}
sort | {"lastMessage":0}
count | 25
```

Response:
```
{"success":false,"error":"Invalid sort parameter provided: \"{\"lastMessage\":0}\" [error-invalid-sort]","errorType":"error-invalid-sort","details":{"helperMethod":"parseJsonQuery"}}
```

## Further comments

Version on the server where I noticed this: `https://rocketchat.$DOMAIN/api/info` returns `{"version":"4.8","success":true}`. According to the "Releases" page, this version appeared 5 days ago, so I believe this is up to date.

### The journey to here

For some reason, the descending order uses the wrong magic number "0", and the server can't interpret that. However, this *used* to work, so I'm not very sure about this.

The error message appears in the source code of the entire organization exactly once: https://github.com/RocketChat/Rocket.Chat/blob/31ae30f30ad71d9e5a1b0cad494b3471a7dd8807/apps/meteor/app/api/server/helpers/parseJsonQuery.ts#L42
So I'll guess that this is the line of code that generated this particular message.

A few lines above we see that the server only knows 1 and -1 as magic numbers for the sorting:
https://github.com/RocketChat/Rocket.Chat/blob/31ae30f30ad71d9e5a1b0cad494b3471a7dd8807/apps/meteor/app/api/server/helpers/parseJsonQuery.ts#L35
This matches the observed bug: The browser sends 1 (which works) and 0 (which doesn't work).

Generally, it seems that the web client actually uses the strings "asc" and "desc" internally, which are hard to mix up. So I assume that it's the conversion of that is broken somehow.

Exactly this seems to be the case here:
https://github.com/RocketChat/Rocket.Chat/blob/31ae30f30ad71d9e5a1b0cad494b3471a7dd8807/apps/meteor/client/views/directory/hooks.js#L11

The code around it produces exactly the kind of query seen in the network log, and can also produce the buggy parameter `sort: 0`. This either fixes bug #25695, or a different bug of the same kind.

I am not sure how to add tests for this, can someone do this for me or show me where to start? I'm actually just an end-user and "accidentally" found the fix for the bug while writing the bug report ^^'

EDIT: Rebased for convenience, and to re-check CI.

---
## [treckstar/yolo-octo-hipster](https://github.com/treckstar/yolo-octo-hipster)@[4c98fff719...](https://github.com/treckstar/yolo-octo-hipster/commit/4c98fff7195b52355eeb7a9bc3c8115f79a6f956)
#### Monday 2022-06-20 06:22:04 by treckstar

Life is one big road with lots of signs. So when you riding through the ruts, don't complicate your mind. Flee from hate, mischief and jealousy. Don't bury your thoughts, put your vision to reality. Wake Up and Live!

---
## [goldbergmoshe/dexie.js-web](https://github.com/goldbergmoshe/dexie.js-web)@[738c3f97cf...](https://github.com/goldbergmoshe/dexie.js-web/commit/738c3f97cf902a510217ffd14760aa4527bd9bfc)
#### Monday 2022-06-20 06:40:04 by The Web Dev Hub

Fix Vue.md Example Code

Hi,

In the example code of Dexie for Vue, I have corrected a mistake that prevents the code from running.

For some reason you guys left "item.age" instead of "friend.age". I have corrected it in this PR.

Thanks for the amazing lib!

---
## [Yosorable/Grasscutter](https://github.com/Yosorable/Grasscutter)@[fbaeaee4b5...](https://github.com/Yosorable/Grasscutter/commit/fbaeaee4b5aa82fe10897b60ea642d4428e8abd8)
#### Monday 2022-06-20 07:38:11 by Kimi

another translation patches because i fucked it up

i hate myself

---
## [BlackSilverUfa/data](https://github.com/BlackSilverUfa/data)@[a5f6b3fee4...](https://github.com/BlackSilverUfa/data/commit/a5f6b3fee4192420c1f5f7f9b8daba853ba97c0d)
#### Monday 2022-06-20 08:51:26 by Jenkins

Запись стрима 1508120809

* Первый взгляд 2022 — My Demon Wife (демо) [100%]
* Первый взгляд 2022 — Loveland (демо) [100%]
* Первый взгляд 2022 — Eternal Evil [100%]
* Первый взгляд 2022 — Andy's Apple Farm (демо) [100%]
* Первый взгляд 2022 — Despair: Blood Curse (демо) [100%]
* Первый взгляд 2022 — Oxide: Room 104 [100%]

---
## [ImLonely13/kernel_xiaomi_merlinx](https://github.com/ImLonely13/kernel_xiaomi_merlinx)@[72d572bbed...](https://github.com/ImLonely13/kernel_xiaomi_merlinx/commit/72d572bbedda912ed91d129bcb236ebe0ac27e64)
#### Monday 2022-06-20 10:46:41 by Peter Zijlstra

sched/core: Fix ttwu() race

Paul reported rcutorture occasionally hitting a NULL deref:

  sched_ttwu_pending()
    ttwu_do_wakeup()
      check_preempt_curr() := check_preempt_wakeup()
        find_matching_se()
          is_same_group()
            if (se->cfs_rq == pse->cfs_rq) <-- *BOOM*

Debugging showed that this only appears to happen when we take the new
code-path from commit:

  2ebb17717550 ("sched/core: Offload wakee task activation if it the wakee is descheduling")

and only when @cpu == smp_processor_id(). Something which should not
be possible, because p->on_cpu can only be true for remote tasks.
Similarly, without the new code-path from commit:

  c6e7bd7afaeb ("sched/core: Optimize ttwu() spinning on p->on_cpu")

this would've unconditionally hit:

  smp_cond_load_acquire(&p->on_cpu, !VAL);

and if: 'cpu == smp_processor_id() && p->on_cpu' is possible, this
would result in an instant live-lock (with IRQs disabled), something
that hasn't been reported.

The NULL deref can be explained however if the task_cpu(p) load at the
beginning of try_to_wake_up() returns an old value, and this old value
happens to be smp_processor_id(). Further assume that the p->on_cpu
load accurately returns 1, it really is still running, just not here.

Then, when we enqueue the task locally, we can crash in exactly the
observed manner because p->se.cfs_rq != rq->cfs_rq, because p's cfs_rq
is from the wrong CPU, therefore we'll iterate into the non-existant
parents and NULL deref.

The closest semi-plausible scenario I've managed to contrive is
somewhat elaborate (then again, actual reproduction takes many CPU
hours of rcutorture, so it can't be anything obvious):

					X->cpu = 1
					rq(1)->curr = X

	CPU0				CPU1				CPU2

					// switch away from X
					LOCK rq(1)->lock
					smp_mb__after_spinlock
					dequeue_task(X)
					  X->on_rq = 9
					switch_to(Z)
					  X->on_cpu = 0
					UNLOCK rq(1)->lock

									// migrate X to cpu 0
									LOCK rq(1)->lock
									dequeue_task(X)
									set_task_cpu(X, 0)
									  X->cpu = 0
									UNLOCK rq(1)->lock

									LOCK rq(0)->lock
									enqueue_task(X)
									  X->on_rq = 1
									UNLOCK rq(0)->lock

	// switch to X
	LOCK rq(0)->lock
	smp_mb__after_spinlock
	switch_to(X)
	  X->on_cpu = 1
	UNLOCK rq(0)->lock

	// X goes sleep
	X->state = TASK_UNINTERRUPTIBLE
	smp_mb();			// wake X
					ttwu()
					  LOCK X->pi_lock
					  smp_mb__after_spinlock

					  if (p->state)

					  cpu = X->cpu; // =? 1

					  smp_rmb()

	// X calls schedule()
	LOCK rq(0)->lock
	smp_mb__after_spinlock
	dequeue_task(X)
	  X->on_rq = 0

					  if (p->on_rq)

					  smp_rmb();

					  if (p->on_cpu && ttwu_queue_wakelist(..)) [*]

					  smp_cond_load_acquire(&p->on_cpu, !VAL)

					  cpu = select_task_rq(X, X->wake_cpu, ...)
					  if (X->cpu != cpu)
	switch_to(Y)
	  X->on_cpu = 0
	UNLOCK rq(0)->lock

However I'm having trouble convincing myself that's actually possible
on x86_64 -- after all, every LOCK implies an smp_mb() there, so if ttwu
observes ->state != RUNNING, it must also observe ->cpu != 1.

(Most of the previous ttwu() races were found on very large PowerPC)

Nevertheless, this fully explains the observed failure case.

Fix it by ordering the task_cpu(p) load after the p->on_cpu load,
which is easy since nothing actually uses @cpu before this.

Fixes: c6e7bd7afaeb ("sched/core: Optimize ttwu() spinning on p->on_cpu")
Reported-by: Paul E. McKenney <paulmck@kernel.org>
Tested-by: Paul E. McKenney <paulmck@kernel.org>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lkml.kernel.org/r/20200622125649.GC576871@hirez.programming.kicks-ass.net
Signed-off-by: ImLonely13 <gabutuhaku@gmail.com>

---
## [uclouvain/osis-portal](https://github.com/uclouvain/osis-portal)@[797852bc38...](https://github.com/uclouvain/osis-portal/commit/797852bc3880c027afb3be7cb9cc445602d072e9)
#### Monday 2022-06-20 12:43:11 by mathieuzen

OSIS-6757 in every life we have some trouble, but when you worry you make it double

=> don't worry be happy

---
## [ropery/william-shakespeare_romeo-and-juliet](https://github.com/ropery/william-shakespeare_romeo-and-juliet)@[018983bca5...](https://github.com/ropery/william-shakespeare_romeo-and-juliet/commit/018983bca58d1cf04128063ffbaabdd87fb9d410)
#### Monday 2022-06-20 12:46:26 by dreethyweirdo

[Editorial] when (he -> I) shall die

(Manuscripts: I Q2-3, F; hee Q4, Otway)

Juliet is saying "let me have Romeo to myself as long as I am alive, and
when I die then I will share him with the whole world as a source of
light that will put the sun to shame. Q4 'he' for 'I' in 21, adopted by
many eds., too suddenly changes the focus to Romeo's death, something,
as Delius points out, Juliet 'cannot, in her present happiness,
conceive'. Accepting 'he', Dover Wilson (NS) paraphrases: 'if, gentle
night, you will give him to me now, you may have him when he is dead to
make stars of.'"

Add to the above, some commenters point out that 'die' in "the
Elizabethan meaning, sexual climax/orgasm, is plainly most on her mind".

---
## [tannerhelland/PhotoDemon](https://github.com/tannerhelland/PhotoDemon)@[202103f4c8...](https://github.com/tannerhelland/PhotoDemon/commit/202103f4c8157da049730bc0f21431967d491709)
#### Monday 2022-06-20 16:24:15 by Tanner

File > Export > Color lookup: this is actually gonna work!

I honestly didn't know if this homebrew LUT creation strategy would work, but it does!  Yay!

Here's the gist:

3D LUTs are used in a number of industries - video editing, game development, photography, etc.  LUT files exist in a bunch of different formats, and they're basically just giant tables that map colors from one domain to another.  Such tables are extremely helpful for taking complex color transforms with a ton of steps and reducing them into a single table that applies *all* those changes at once - e.g. in a game pipeline you might bump up brightness, reduce yellow tones, increase contrast, improve clarity at high and low ranges of the green spectrum, give everything a slightly violet tint, then tone-map that into a final screen-ready gamut. - but doing all those steps separately takes forever, so instead at development time you use Photoshop (or dedicated color-grading software) to create a LUT that performs all of those steps on every color in the spectrum (or a reasonably representative subset of colors), stores the final mapping of each color in a giant list, and then at run-time you can just apply that LUT to each frame to apply your huge list of edits in a single uniform pass.  Whether you're doing 100 adjustments or 1 doesn't matter - a LUT merges all those changes into a single mapping table that does it for you.

LUTs are also used extensively for color-grading photos and film, because once you develop a signature "look" you can simply merge the full pipeline of edits into a single LUT, and with one click you get that "look" on any arbitrary photo, video, whatever.  LUTs are one of the few photo editing things that works across almost any software and/or platform because at the end of the day, LUTs are pretty much just text files with encoded RGB tables inside.  So even if your software doesn't support e.g. a Curves tool, you can let users load a LUT created in software that *does* support curve adjustments and then apply it, because software doesn't care how a LUT was created - it just uses the embedded tables to convert all colors to new values.

So applying LUTs is the easy part.  PhotoDemon supports a number of LUT formats and lets you apply them to images the same way any other photo editor does.  But creating LUTs is a different story, and there was no way to *create* new LUTs inside PD... until this commit.

Photoshop limits LUT creation to adjustment layers specifically - you have to set up 1+ adjustment layers on an image, and then you can export the resulting merged adjustment-layer-transform into a new LUT file.  This is cool, obviously, and relatively easy to support because the possible range of edits is small.  But PhotoDemon doesn't provide adjustment layers (yet) and even Photoshop only supports a subset of its full Adjustment tool library as adjustment layers.  Wouldn't it be better if you could just edit a photo using ANY AND EVERY TOOL in the app, and then the app would magically reverse-engineer a LUT for you, encompassing all the changes you'd made?

That's what I've attempted to do in PhotoDemon, and by god, it works.  Mostly.  It's a little slow right now due to huge numbers of classes used in the necessary data structs (so the code is fast, but class teardown is like 60 seconds for the hundreds of thousands of classes that get created), so I'll need to rewrite some data structures either using lightweight classes or by converting them to array-driven methods.  But that's easy stuff compared to the work that's already been done!

Now for the caveats.

LUTs encode 1:1 mapping between colors, so they cannot encode area-driven effects (like blur, distort filters, etc).  This means they are best at encoding edits from Adjustment menu tools, but you don't really need to care about that - if you use any Effects, PhotoDemon can still auto-create LUTs for you!  But if the same color gets mapped to multiple output colors (due to a blur effect or similar), the quality of the LUT will suffer.

For the next caveat, I need to describe PD's LUT creator works.  Basically, the algorithm starts by comparing the final, edited image state to its original, unmodified state.  A huge tree of all represented colors is constructed, and the algorithm analyzes how each color has changed.  From that list of changes, it constructs a full-gamut LUT, directly using relevant color changes where it can and interpolating changes from similar colors for any parts of the color spectrum that the current image doesn't include.  This encodes most "normal" adjustment patterns very well, but can produce weird results if your source image has a very limited palette (e.g. it's grayscale, or mostly a single color tone, etc).

So for best results, if you intend to export a LUT you'll want to perform your adjustments/effects/etc on a photo with reasonably good color diversity - lots of dark and bright tones of as many different colors as possible.  This gives the LUT creator more information to work with, and the resulting LUT file will be more applicable to any type of image.

Next up is resolving the damn VB6 class teardown perf issue, then looking at an improved interpolation strategy that provides more accurate coverage of massive state changes (like "invert all colors").  I also want to write a new Render effect for generating color test patterns, which would help immensely for improving gamut coverage.

I need this tool available so that I can finally create a default set of LUTs to ship with PhotoDemon.  I want to provide similar LUTs to Photoshop's default set, but they copyright their LUTs (which seems silly - can you really copyright a list of numbers?  idk).  So I can't just ship Photoshop's files outright - but I can certainly make my own set of edits that produce a similar result to theirs, then create my own LUT files and ship *those*.  So that's what I'm gonna do.

Anyway, I legitimately didn't know if this strategy would work, so I'm pretty stoked to have a workable path forward for this feature.

---
## [MMMiracles/tgstation](https://github.com/MMMiracles/tgstation)@[763a10d1cc...](https://github.com/MMMiracles/tgstation/commit/763a10d1cc44c91720101d422d8709ad1aa0644d)
#### Monday 2022-06-20 17:10:26 by distributivgesetz

Resonance cascade polishening, bugfixes and better logging (#67488)

This PR rewrites almost all messages related to cascade events. Some messages felt kinda clunky to read or could have been written better. Overall, the new messages add to the experience as a cascade being a terrifying event in a way that I felt the old ones missed, and they make the event feel overall a lot sharper.

While looking at the resonance cascade code, I noticed that there a lot of stuff about cascades in the air which was not touched on. So, as I do, this PR evolved into a polish and roundup PR for cascades. There was a lot of stuff still hanging out relating to the event, and although the big backend of it sits, there was still a bit left to be completed. Therefore this PR deserves more the title of the "Resonance cascade POLISHENING" instead of the "REFLAVAHRING". But yeah, you ever go on a massive tangent before?

---
## [nayvcake/titus-executor](https://github.com/nayvcake/titus-executor)@[3590f8629d...](https://github.com/nayvcake/titus-executor/commit/3590f8629d0142c208009d0f3362edd13580329e)
#### Monday 2022-06-20 17:27:02 by Kyle Anderson

RFC: TitusMainContainerVolume mount support

I expect a common pattern in Titus will be to
"Share this volume between the main container".

This would power things like Fuse mounts from a fuse sidecar,
or just sharing /mydata to things like nginx or whatever.

Our experience when working with this kind of pattern in
stock kubelet is kinda painful, becuase it means making
and EmptyDir, making an init container to copy files over
from the main one to that dir, and then sharing that EmptyDir
volume with another container.

But we control the runtime, we can make the common thing easy,
and I bet 90% of the kinds of volumes we will want will be like this.

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[5ae00fabd0...](https://github.com/mrakgr/The-Spiral-Language/commit/5ae00fabd0b547c66058c135a3491f322dfa4818)
#### Monday 2022-06-20 17:27:35 by Marko Grdinić

"///

Half the year is over and I've spent six month cultivating my [art skills](https://twitter.com/Ghostlike). Right now I am recovering from a failed attempt and illustrating the flying golden city, so it is the ideal time to do the halfly review. It is an interesting thing. In programming, if you want to save time, the first choice is always to find a library that does what you want and make use of it. Languages, and all other considerations are secondary to that. Though pro programmers are generally weak and start to panic whenever it is time to learn something new, switching languages is not a big deal.

In art, switching 3d software, generally is a big deal. So far, I've used many different 3d software: Blender, Moi, Clarisse, Zbrush, Substance Painter, Houdini, and just today I gave 3ds Max a try. All of these softwares have different navigation. It is like trying to drive several different types of cars if every one of them had different ways of steering and their pedals had their positions swapped. I have no idea why they do this, but it makes context switching between them quite hard and it constantly disrupts my muscle memory. A few days ago I tried Clarisse after not using it for a few months and if there was a camera in my room it would show me practicing the super saiyan transformation.

I had the idea of trying out kitbashing. This is not something I could ever imagine failing at. Kitbashing is just getting existing models from a library and putting them around the scene instead of taking the longer way, which is modeling the assets yourself. The [kitbashed models](https://kitbash3d.com/products/manhattan?_pos=1&_psq=manh&_ss=e&_v=1.0) are done by pros and look great. The particular kit I linked to has 12.5m polys, and the .blend file is 2.6gb. When I try to open it, Blender lags a lot, and even converting it to bounding boxes in the viewport does not help much. I tried making use of them in Clarisse, but realistically, I am at my limit here. I do not have room in my head for much more than Blender and Moi. The models are simply too high poly, and are unusable without significant hardware upgrades to my rig. Maybe if I got the 64-core Threadripper and 128gb I could upgrade my style so it incorporates kitbashing, but right now I'll just make do with what I have.

Me trying to make use of procedural city generation and kitbashing is more a matter of principle than actual need. Modeling things by yourself can be quite fast depending on the amount of detail you are going for. And I definitely don't need much.

It is just frustrating to fail in such a manner, to do what should be the right thing, only for the wrong thing to end up being right. Unlike images which you can downscale very easily, 3d models aren't so straightforward, and I am better modeling them on my own. This is a good lesson for me, 2d artists just get their references and draw. From here on out, I'll get my refs and model. After I am done with the flying city scene, I'll move on to character modeling and finally bring Euclid along with the rest of the NPCs into light. After that I'll move on to music.

I've finally had some success with making use of ML as well. While my reinforcement learning attempts were a complete disaster, I got lucky this time and found something good almost right off the bat. It started with me playing with the [Deep Dream Generator](https://deepdreamgenerator.com/) and finding the results interesting, and then looking for something I could run on my own rig. I found [CAST](https://github.com/zyxElsa/CAST_pytorch) and managed to adapt it so it can do style transfer even on my weak rig. You can see examples of it in action on my Twitter profile. My plan so far is to do some low-mid poly modeling, render it using Cycles/Luxcore and then do style transfer to get rid of the plastic 3d look. For charas, I will spend some time sculpting them myself in 3d, style transferring and then painting over the image by hand until I am satisfied. Last week the plan was to get a char modeling program, but given how poorly the urban procedural generation experiment went, I am very skeptical of taking any more shortcuts.

Right now my skills are just about good enough to be an apprentice illustrator, but definitely not something like a mangaka. I know my limits which is why I am aiming for a VN, rather than a manga. If I went for a manga, all my time would be consumed by drawing even if I were good at it. I am lazy and want to give myself breathing room. I'll make a resolution: 1 scene or 1 character a week, over time that kind of effort will pile up and my skills will grow from their current low 3/5 rank. I admire steadiness and consistency rather than artfulless, so that is what I will aspire to reach. I have a lot of breadth, and in the next six months I'll aim to exchange that for depth in a particular style.

In case you are wondering, 5/5 is the pinnacle of human achievement, and I'd put people like Kim Jung Gi and Murata Yusuke at such a level. I am not even going to try beating them, getting to peak 3/5 would be enough for me. Though 5/5 is the limit of humans, it is not the limit in any absolute sense. And it has actually been exceeded at the start of the year by [diffusion models](https://www.youtube.com/results?search_query=dalle+2). Honestly, when OpenAI said that GPT-3 was too dangerous to release, I thought that deep learning had jumped the shark. AlphaGo was an exciting moment, but the rest of the RL achievements were huge circus acts, more like ambushes than legitimate achievements. The same goes for pro beating poker bots.

But what they've done here I find admirable. If I could run something like DALLE 2 on my home rig, or even rent it cheaply, it would completely change the way I did art and I probably would not have bothered investing so much into 3d skills. I'd put it at low rank 6. At full rank 6 you'd have a proper memory system that you could tell it - 'This is a sketch of Euclid, make it anime'. And 'Generate him from the sides and the back in several different versions, I'll tell you which ones are suitable.' To be full rank 6 the system should be capable of one shot learning much like humans. With the current methods, acquiring new knowledge in its long term memory would require a costly optimization step. In fact, you can only throw the whole dataset at it, not teach it targeted lessons.

That is the trouble. DALLE 2 has 5b parameters, so absolutely no home rig could ever run it. 5b params would require 20gb just to store the weights. It would probably require a cluster just for a forward pass on a decently sized image. AI chips are supposedly here, but I haven't felt them making any impact, so who knows when we are going to get enough computing power to play with non-toy models. The style transfer net that I got was a fortuitous encounter, and I am not likely to get another.

It is an arms race, and it does not feel like I've made a single step forward with my own efforts. I look at OpenAI and Deepmind which I see as competitors, and it feels like they are far ahead. Sure they have big brains and pockets, but none of the fundamental concerns that I've had with deep learning methods have changed.

Deepmind might have made AlphaGo, and I still couldn't run it. In contrast, 8 years after the Deep Blue and Kasparov match, you could get something as good at it on your home computer.

You get new training methods and architectures, you have advances in generative modeling, you have proof of how great matrix multiplication truly is, but none of those advances will change that deep learning is not capable of continuous and long term learning. There are a lot of brains doing research, but there were no sparks of genius produced. When it happens the Singularity will ignite.

It is going to happen. GPUs won't stay dominant forever and the hardware necessary to properly cultivate ML skills will arrive. With strong enough hardware, there will be ways of having it itself tell us how to optimally use it. Right now we are good at making use of GPUs, but at the same time restricted to what they are good at. I feel the timeline is a bit wonky, the memristor breakthroughs failed to properly manifest, if they had we could have had terabyte non-volatile memory devices suitable for all sorts of tasks, and especially AI.

If you look at the current GPU development, it is struggling. NVidia is silently ramping up the power consumption to get the performance increases inline with predictions and that way of getting improvements is shallow, it won't last for long. In fact, what you'd want to see is power consumption going in the other direction.

The main low hanging fruit to be plucked in ML is sparsity, GPUs aren't suited for it, but AI chips are so that is where the river will flow. They will also pave the way for true multi-core processing. 7 years ago when I got my rig, I opted for a 4 core CPU, if I got one now for the same price it would be 8 cores. If Moore's Law was alive a 64 core CPU would be mid end instead of very high end. AI chips will bridge the gap instead. Being able to use them will bring a change in mindset. It is a lot easier to learn something if the circumstances force you to do it, so having to do async programming should be beneficial for research into asynchronous learning algorithms. Getting rid of layerwise hierarchy is similar to getting rid of temporal hierarchy, so following that path should give insights into the true nature of long term and continuous learning.

For me, style transfer is the most convenient route of getting back into ML. If I can make some money through Heaven's Key I could reinvest that into better hardware, and ML development. The CAST net given by the author is only 7m params, using a bigger net, and trying out different training methods that I have in mind should lead to improvements. I wanted to do this through RL. As I said, it is a race, and I sure was pissed to not be allowed to even run out of the gate while everyone else seemingly raced off. In my desperation I thought of getting a job, but it does not make sense for me to become a programmer only to set the money on fire trying to make RL work. Deepmind and OpenAI sure aren't spending their own money on their experiments. The way I want to win at ML is to establish a base, and then enlarge it by reinvesting the profits. A relationship where I am the only one putting in effort and getting nothing in return would quickly turn sour. So if I became a programmer, I'd be forced down the doomed path of the normie. Making money only to become a NPC is not worth it.

The Singularity is everything to me. It is a war. If you look at the world today, it is split into anti and pro technology factions. These aren't formal factions, but in general you have people who use technology do so begrudgingly, and see it only as fashion. They live the same they would have a thousand years ago, except they have fancy clothes, are fat, and have smartphones. They aren't consumed by technology and look down on those who have.

The pro technology faction agents believe in it. Even if they don't understand its potential, they trust it, more so than they do other people. They've given their souls to the future.

In the next few decades this long war will reach its conclusion and a decisive victor will emerge. I know which side I will be on.

I am frustrated with my own failures and the way the timeline is progressing, but I am not 60 years old, but 35, so in all likelihood I'll be able to afford the time needed to play the game properly and live through to the end of the era of man. The long term is scary when you are human. Tomorrow, I'll be fine, but who knows what will happen to me 20 years down the road. Long term, I'll only be able to feel safe with the power of the machines at my disposal.

I need resources to develop my ML skills further.

Becoming an artist in this situation seems absurd, but take a look at the latest diffusion models and consider what they could be capable of with a few more upgrades. So a hybrid artist/ML class does not seem at all ridiculous to develop in the current day. On the 3d side, we will get neural based renderers and content producing analogues of DALLE. It does not seem absurd that in the future NNs will be capable of making animations. It should be obvious that to reach rank 6 in art, one has to retire his own pen and rely on machines as much as possible.

I want a path that sustains me on its own. I'd rather make 1k as an artist than 10k as a programmer so I will do whatever I can to make this work in the most direct way possible. Art will give me an opportunity to slowly increase my following and hence income, while making use of ML. Programming itself will be the last field of all to succumb to ML, so ironically it will not give me opportunities to make use of it. Rather I expect to be able to make use of those skills along the way in art.

When it is time to start Heaven's Key, I'll announce it on Twitter. I'll probably post the story on Royal Road or ScribbleHub instead of here in order to garner readers.

I'll aim to begin by the end of the year. Right now what I want to internalize is proper self reliance in my 3d modeling by doing more scenes and chara modeling. When I have a finely polished technique in this area, I will move to music studies. No doubt music itself will take me at least 3 months to grasp. I am really looking forward to finding out what kind of NN work has been done there.

One thing I also look forward to is a bear market in BTC. The big run-up, the blow off top, the months-long consolidation and failed rally, followed by the break in the last two months greatly resemble what happened in late 2000 to tech stocks. It was a fun party for those in BTC, but after it gets crushed the GPU prices will be able to feel some relief, meaning in the future I'll actually be able to afford a decent upgrade. If you are in it now, it is not worth praying for a recovery like in late 2018 when it got crushed to below 4k. An asset like BTC you only get into when it breaks out to new highs and don't hold it for long. There will be a bull market in something else, so it is better to find and hold that. The only thing waiting for the BTC crowd is years of bear market action from here on out, that is one party you'd want to miss.

///

I'll be 35 in two days when I post it on the Simulacrum blog. Let me put the above into the Google docs tool.

6:50pm. Put it through the spell checker. Time for lunch.

7:15pm. Let me close here. Tomorrow I will do it as the circumstances ordered. Moi and Blender for modeling and Scatter 5 for scattering, my own buildings only. I'll also do the trees if needed. The way I did the cover was the ideal workflow, so I'll have more of that. It does not matter if I do not get to the goal in the theoretically most optimal fashion. From here on out I will definitely make progress. It is fine if it takes me a week. I won't let this time go down the drain. Any progress towards the goal is good enough. I do not need highly detailed models to make it look good. I will find my own way."

---
## [BlueHoodPC/BlueHoodPC](https://github.com/BlueHoodPC/BlueHoodPC)@[a021d2c953...](https://github.com/BlueHoodPC/BlueHoodPC/commit/a021d2c9530f99608c0e65657cfca5833cae14e3)
#### Monday 2022-06-20 18:26:15 by BlueHoodPC

aihunter for minecraft

*Minecraft Manhunt and speedrunner vs. terminator are originated from Dream (Youtube), this is a recreation of his idea in the form of a datapack. This datapack is for everyone to be able to play very easily, without the need of opening a server, set up plugin and an alt account for bot. I recommend you to use the original plugin if you want: https://youtu.be/O60zJzhYGEA (link in his video description)



Have you been wanting to try out Minecraft Manhunt but your friends just do not play Minecraft (or you are just very alone)?

Here is the thing you would ask for, the AI Hunter that will chase you down NO MATTER WHAT.

Fair Warning: it is not easy

[AI Hunter] Minecraft Manhunt but you have no friend... (speedrunner vs. terminator) Minecraft Data Pack

Your Goal:

-To kill the Ender Dragon before it kills you

How to start your challenge:

1) install this data pack in a newly created survival world, with "allow cheats" on (tutorial from wiki)

2) make sure your render distance is 8 or above

3) type /reload or /function aihunter:menu
[AI Hunter] Minecraft Manhunt but you have no friend... (speedrunner vs. terminator) Minecraft Data Pack

4) click <[​start]>
[AI Hunter] Minecraft Manhunt but you have no friend... (speedrunner vs. terminator) Minecraft Data Pack

once you click <[​start]> a hunter will spawn at where you are, and you have 60 to 30 seconds ahead before the hunter starts its move (vary by difficulty). Normally one death means game over for manhunt, but you could set the rule yourself. Hunter will still chase you after you respawn.

*if you want to make hunter ignore certain players or only target certain player, you need to do the following before starting your challenge:
Players who are in the team "aihunter" will not be targeted by hunters. So after installing this datapack, enter this command: "/team join aihunter <NAME>" replace <NAME> into a player id. Any player that is NOT in the team will be targeted by hunters. (doesn't affect players in creative/ spectator mode)

note: Hunters can't hunter players in team aihunter, but players in that team can still hurt hunters.

use "/team list aihunter" to check who is in the team. use "/team leave <NAME>" to remove a player from any team.

---
## [Tiktodz/android_kernel_asus_sdm636](https://github.com/Tiktodz/android_kernel_asus_sdm636)@[fc09593053...](https://github.com/Tiktodz/android_kernel_asus_sdm636/commit/fc0959305369887314de71b54d81b3fd948e4ea3)
#### Monday 2022-06-20 19:09:20 by Dan Pasanen

power: don't ever reboot to verity red

* We get it, shit's broken. We're flashing custom stuff, shit's bound
  to break. Don't pop this annoying screen up, we're not even using
  verity anyway.

Change-Id: Icd77b70ec1df9108a4ba9e7fd8cb9623b35b78db
Signed-off-by: Albert I <kras@raphielgang.org>

---
## [redxlegion/redlegion-blog](https://github.com/redxlegion/redlegion-blog)@[fa978774a9...](https://github.com/redxlegion/redlegion-blog/commit/fa978774a98ee8ed6a239abd1b0f34aa7d35708d)
#### Monday 2022-06-20 20:35:20 by redlegion

Removed extra bracket

Stupid dumb shit. God dammit.

---
## [andro951/WeaponEnchantments](https://github.com/andro951/WeaponEnchantments)@[67868c09df...](https://github.com/andro951/WeaponEnchantments/commit/67868c09df88c6a62b1809e8d838fe741e7a40c0)
#### Monday 2022-06-20 20:46:21 by andro951

0.2.0.7(Released 20JUN22)
      -FIXED GOD SLAYER ON SERVERS..........FINALLY!!!!
      -Fixed World Ablaze's final tier on fire debuff not working on servers.
     -Fixed One for all not working on servers
PS I hate servers with a passion.  Had to get that off my chest.  Enjoy all, I know I will.

---
## [czotomo/diesel](https://github.com/czotomo/diesel)@[448df6b615...](https://github.com/czotomo/diesel/commit/448df6b61566dbd419554fc82abd018357848846)
#### Monday 2022-06-20 20:49:21 by Georg Semmler

Address #3173

This is a tricky one. It seems like the behaviour described in that
issue should work out of the box, but it doesn't. I've spend some time
to investigate various solutions to make this work, but I came to the
conclusion that the current behaviour is the correct one.

The underlying issue here is that such an query promotes the inner
`Nullable<_>` of the field onto the outer `Queryable` wrapper. Without
`Selectable` that would require a select clause like
`.select((table::column.assume_not_null(),).nullable())`. This is
technically a safe pattern, but requires the usage of the "advanced"
`assume_not_null()` method to forcibly convert types to their not null representation.

Possible solutions tried to make the enable constructs shown in that
issue:

* I've tried to make the `impl Selectable for Option<T>` return the
following `SelectExpression`:
`dsl::Nullable<dsl::AssumeNotNull<T::SelectExpression>>`
where `AssumeNotNull` converts each tuple element to the corresponding
not nullable expression, while `Nullable` wraps the tuple itself into a
nullable type wrapper.
* I've tried to apply a similar approach like that one above, but only
for derived impls by manipulating the generated code for a optional
field with `#[diesel(embed)]`

Both solutions require changes to our sql type system, as for example
allowing to load a non nullable value into a `Option<T>` to enable their
usage in a more general scope as the presented example case.
(See the added test cases for this). That by itself would be fine in my
opinion, as this is always a safe operation. Unfortunately the
`AssumeNotNull` transformation would be applied recursively for all
sub-tuples, which in turn would cause trouble with nested joins (again
see the examples). We would be able to workaround this issue by allowing
the `FromSql<ST, DB> for Option<T>` impl for non-nullable types to catch
null values, which in turn really feels like a bad hack. (You would like
to get an error there instead, but nested nullable information are
gone.)
All in all this lead me to the conclusion that the current behaviour is
correct.

This PR adds a few additional tests (an adjusted version of the test
from the bug report + more tests around nested joins) and does move
around some code bits that I noticed while working on this.

I think the official answer for the bug report would be: Either wrap the
inner type also in an `Option` or provide a manual `Selectable` impl
that does the "right" thing there.

---
## [open-telemetry/opentelemetry-ruby](https://github.com/open-telemetry/opentelemetry-ruby)@[45429c7d53...](https://github.com/open-telemetry/opentelemetry-ruby/commit/45429c7d537807aad94003f7568650e4a7dc16d2)
#### Monday 2022-06-20 21:09:07 by Andrew Hayworth

Split CI builds by gems at top-level (#1249)

* fix: remove unneeded Appraisals for opentelemetry-registry

It's not actually doing anything, so we skip it.

* ci: remove ci-without-services.yml

We're going to bring back these jobs in the next few commits, but we can delete it right now.

* ci: remove toys/ci.rb

We're going to replicate this in Actions natively, so that we can get
more comprehensible build output.

* ci: replace toys.rb functionality with an explosion of actions + yaml

This replaces the "test it all in a loop" approach that `toys/ci.rb` was
taking, by leveraging some more advanced features of GitHub Actions.

To start, we construct a custom Action (not a workflow!) that can run
all the tests we were doing with `toys/ci.rb`. It takes a few different
inputs: gem to test, ruby version to use, whether or not to do rubocop,
etc. Then, it figures out where in the repo that gem lives, sets up ruby
(including appraisals setup, if necessary), and runs rake tests (and
then conditionally runs YARD, rubocop, etc).

Then, over in `ci.yml`, we list out all of the gems we currently have
and chunk them up into different logical groups:

- base (api, sdk, etc)
- exporters
- propagators
- instrumentation that requires sidecar services to test
- instrumentaiton that doesn't require anything special to test

For most groups, we set up a matrix build of operating systems (ubuntu,
macos, and windows) - except for the "instrumentation_with_services"
group, because sidecar services are only supported on linux.

For each matrix group (gem + os), we then have a build that has multiple
steps - and each step calls the custom Action that we defined earlier,
passing appropriate inputs. Each step tests a different ruby version:
3.1, 3.0, 2.7, or jruby - and we conditionally skip the step based on
the operating system (we only run tests against ruby 3.1 for mac /
windows, because the runners are slower and we can't launch as many at
once).

Notably, we have a few matrix exclusions here: things that wont build on
macos or windows, but there aren't many.

Finally, each group also maintains a "skiplist" of sorts for jruby -
it's ugly, but some instrumentation just doesn't work for our Java
friends. So we have a step that tests whether or not we should build the
gem for jruby, and then the jruby step is skipped depending on the
answer. We can't really use a matrix exclusion here because we don't use
the ruby version in the matrix at all - otherwise we'd have a *huge*
explosion of jobs to complete, when in reality we can actually install +
test multiple ruby versions on a single runner, if we're careful.

The net effect of all of this is that we end up having many different
builds running in parallel, and if a given gem fails we can *easily* see
that and get right to the problem. Builds are slightly faster, too.

The major downsides are:
- We need to add new gems to the build list when we create them.
- We can't cache gems for appraisals, which adds a few minutes onto the
  build times (to be fair, we weren't caching anything before)
- It's just kinda unwieldy.
- I didn't improve anything around the actual release process yet.

Future improvements could be:
- Figuring out how to cache things with Appraisals, because I gave up
  after a whole morning of fighting bundler.
- Dynamically generating things again, because it's annoying to add gems
  to the build matrices.

* feat: add scary warning to instrumentation_generator re: CI workflows

* fix: remove testing change

* ci: Add note about instrumentation_with_services

---
## [QTNeen/space-station-14](https://github.com/QTNeen/space-station-14)@[949fbd0194...](https://github.com/QTNeen/space-station-14/commit/949fbd019404b32fded90f37e3f6a7548790e55e)
#### Monday 2022-06-20 21:44:16 by Emisse

Bagel Update 13.7 (#8690)

* fuck shit ass shit

* Add files via upload

---
## [Vexylius/My_Fork_of_Echo13](https://github.com/Vexylius/My_Fork_of_Echo13)@[91795aa57f...](https://github.com/Vexylius/My_Fork_of_Echo13/commit/91795aa57f5ecc4aeee81e91a50e08de0d960be5)
#### Monday 2022-06-20 22:39:19 by TheNeoGamer42

Arrowhead-Class Long Range Scoutship (#111)

* woo

* i kinda fucking forgot i moved an entire room and that there was a wall there now

* god fucking damnet the other wall too

* wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwires

* you know this really throws a wrench in my plans, you know? I hate wrenches in my plans.

* slightly less sane piping to infuriate both myself and others. also a waste hookup for that portable scrubber I threw in.

---
## [p-lucero/s3shortlink](https://github.com/p-lucero/s3shortlink)@[8f7e1b18e3...](https://github.com/p-lucero/s3shortlink/commit/8f7e1b18e31f8cee4e8cf83971d745aeaf57dd5c)
#### Monday 2022-06-20 22:52:12 by p-lucero

Add lots of untested code that I'm not totally sure about.

- create actually has an implementation now
- everything is based on boto3 because apparently the original boto is
  deprecated. I have no idea how boto3 handles credentialing, as I haven't even
  tried to run this yet, so just assume for now it does something magical.
- - related to this: remove the credential-taking arguments from the argparse
    setup, we shouldn't be getting those on the command line either way to
    avoid badness in the bash history.
- util has become naming, as I'm trying to avoid meaningless filenames in this
  project
- add the HTML template back in its own file and generalize it so that it can
  accept arbitrary delay - the string templating here is a little bit hacky but
  /shrug
- uploading actual files is now supported as well, in case you want to
  self-host audio or video using this tool (I have no idea if s3 supports that
  very well or provides a good experience, but it seems like a neat idea to me)
- run everything through yapf

---

# [<](2022-06-19.md) 2022-06-20 [>](2022-06-21.md)

