# [<](2022-03-14.md) 2022-03-15 [>](2022-03-16.md)

1,750,885 events recorded by [gharchive.org](https://www.gharchive.org/) of which 1,750,885 were push events containing 2,795,112 commit messages that amount to 198,821,327 characters filtered with [words.py@e23d022007...](https://github.com/defgsus/good-github/blob/e23d022007992279f9bcb3a9fd40126629d787e2/src/words.py) to these 40 messages:


## [dvsdude2/doom](https://github.com/dvsdude2/doom)@[56e1d35338...](https://github.com/dvsdude2/doom/commit/56e1d35338d5b001a83003619802d388e5b634ec)
#### Tuesday 2022-03-15 00:21:44 by dvsdude2

latest changes

@@ -55,7 +55,9 @@
this is me learning what what I can do with setting my own
key bindings, did lots of work and learning, just messing with
the dashboard. thought it might be useful to have a keybinding
for it. have not tried this yet just fixed up what I had.
so work still in progress.

@@ -78,7 +80,13 @@
this ...this is what I want to talk about. this was my own idea.
and I was able to make it happen. an elfeed icon that opens rss
from the dashboard. IT WORKED! So this is the story.
I have been using an rss feed for quit a while now mainly news-flash.
worked well. one of those programs you set it then almost forget it.
simply because it does what you want and just keeps on doing it well.
so the idea of switching to elfeed wasn't something I was looking for.
if its not broke why fix it was the mentality. plus elfeed did not look
that appealing. did a quick try did not like it so left it and moved on.
as time past i would come across people like distro tube, mike zamasky,
system crafters, all give good reviews of elfeed. read articles once
again favourable reviews. So thought I would give it anouther shot.
needed something to figit with to try and buffer the 24 hr propaganda
that is beeing forced on us, this is what I chose. So to make a long
story longer. since rss feeds are what I start my day with, usually
one of the first things I run. it would make sense to put quick way of
doing that from the dashboard. after starring at my dashboard it hit me
yes a icon/shortcut that opened elfeed from the dashboard. would make
sense. so after looking at the code I was sure I could do this and
just like that I wrote out what I thought it should look like and
copied it to my config with only one miss step, I was able to get it up
and running so the TL/DR is I'am proud of this cause I did it all myself.
AND IT WORKED! an original idea made to come to life.great!

@@ -345,10 +353,10 @@
this was something that I realised that when I changed what I thought
was just corfu's completions it affected the mini-buffer as well.
took a while to notice the difference that happened. needed to switch
back because I find orderless completions in mini buffer way more
useful than the completions at point.

@@ -495,11 +503,6 @@
this just me moving the old elfeed block to the bottom of the config.

@@ -623,8 +626,15 @@
consists of thee different settings.
the first on the "toggle comment" to me is the most important.
at some point an update or something changed the way M-; commented
lines the new behaviour was when you tried to comment out or comment a
line it would jump the cursor to the top header. super annoying. not bad
if small block but something like org-block that large it would move the
entire page up leaving you unable to see what you had just done. the
problem as far as I could see was the command was actually DWIM which is
do what I mean and it was not just for commenting. started trying to
change my all ready learned key and change it to gc for nerd commenter
then I came accross this in my internet wonderings grabbed it right away
this fixed that and I'am much happier now that I can go back to what I
was already doing. great ..like it.
the "ytdl" is new. not much to say here other than it something i'm
looking into. had liked the fuzzy version of this in the terminal.
so just something left here to remind my self. always looking at my
config file. more than most other files.
the third thing is something at first just thought what ever, this is
just an accessory to make your emacs fancier. then realised that I
could use this and I like it. It is useful not just for looks.
the thing i'm talking about is "beacon" it flashes the cursor
when ever it has jumped showing you were in the buffer it is.

@@ -697,8 +707,45 @@
lots going on here as well...first part is me filling out my
mpv.el package config,left this undone till I could go over it more
thrall. basically expanding thing as I learn them. here I figured out
that one can use spc m l l which will be up a list of types of links.
by choosing mpv and filling in the path to a video you can just click
the link and it opens said video in mpv controlled by an org file your
in. mainly time stamping different places in the video with notes that
you can than just click the time stamp to get there.
the next part was me removing a feeble try at number the completions so
one could just hit the appropriat number to finish the completions.
but after tweaking the Ispell auto complete it not as needed now the
completions seem straight forward, no need for this as of now maybe in
future i'll think of this but if not needed now no use in keeping it
here.
the next chunk is for deft, people on emacs are how can I say.....
fucking nutters about their notes, there is such a vast array of
different ways to take ,keep,search notes. It'll make your head swim
came across deft in the process assumed it was just anouther variant.
fired it up just for shits and giggles. realised what it actually did
which to me is it a stand alone note searcher using key word searches.
the reason I was looking at any of these, was the fact that my org
folder is getting harder to find things. and just by fluke I happened on
to this which is just what I was looking for. Now I can leave all the org-roam
and zettelkasten note taking for anouther time.
fter that comes my new elfeed config block. made my own keybinding.
because I can...installed elfeed goodies as well as elfeed-org.
and added there settings as per default. Was able to update feed with
my feed list from newsflash rss feed reader, then made an org-mode file
for all the feeds that can be used by elfeed this all worked well.

---
## [FernandoJ8/fulpstation](https://github.com/FernandoJ8/fulpstation)@[c449fbb56c...](https://github.com/FernandoJ8/fulpstation/commit/c449fbb56c7cb57fc9d8c0db32be0b66e6d7293b)
#### Tuesday 2022-03-15 00:26:27 by SgtHunk

Fixes Solitaire runtimes + missing APCs (#488)

* solitaire fixes

* fuck you bar decals

---
## [MacBlaze1/tgstation](https://github.com/MacBlaze1/tgstation)@[759d24ab14...](https://github.com/MacBlaze1/tgstation/commit/759d24ab14af0ab22ae9642e9190c3db91e16516)
#### Tuesday 2022-03-15 00:35:30 by san7890

Four Corners, Red Rover: An Exploration in Decal Trends [MDB IGNORE] (#65290)

* Four Corners, Red Rover: An Exploration in Decaled Trends

You there! What exactly is wrong with this photograph?!

You don't need to tell me, I've boxed it out. There's four individual corners for the decalling. This is weird. You may be asking: Why don't they use the "full tile" turf decals? Let me demonstrate.

Look at the difference between the one at left and the one in the middle. The turf decal totally smothers the nice contrast lines afforded to use by the base turf, causing it to have smooth, clammy exterior. This is probably why no mapper ever uses the full turf decal, much to the chagrin of people who stare at how big the size of this repo is.

Now, what's that on the right? Why, it's the new sprite (and pathing I made) to help counter-act this issue! This perfectly lines up with the contrast lines of the base turf, allowing us to have a non-flattened visualization, while not having four fucking turf decals a turf load upon initialization. How epic!

I've also added "contrasted" variants of the "half" and "anticorner" turf decals for future use. I probably won't go through and update this in this PR, but the opportunity remains available.

I may or not map this change across all the maps. We shall see.

* neutral corners

we love vsc

* no wait

i forgot a bunch of potential edgecases so we'll have to go back. yellow should be fine but neutral, dark, blue, and green should get a second look over

* recheck

found some stuff, probably missed out on others. let us commence forth

* MISTAKE

nearly a fucko bwoingo

* final pass

it compiles and i've had enough, someone else can probably figure it out from this point onwards

* #65230 goated my timbs

now we wait for linters to fail

* YOU DIDN'T SAY THAT THE FIRST TIME

LINTERS AAFAFAFF

---
## [MacBlaze1/tgstation](https://github.com/MacBlaze1/tgstation)@[884c1eeb62...](https://github.com/MacBlaze1/tgstation/commit/884c1eeb62e1c970b2b6edc425f36c924b9f48ee)
#### Tuesday 2022-03-15 00:35:30 by 小月猫

fixes wallmounts (#65408)

closes #65393 (Engineering Cyborgs can't place APC or Air alarm frames on walls anymore)
fixes the code error in #64428 (afc1e44ee2922a316feb958249f7806568953bbe)

basically what occured is that he typed out the T(turf) attackby proc to input the screwdriver as an arg rather then the wallmount, remember, you want the WALLMOUNT to hit the wall to place it, not the screwdriver, that just creates runtimes and doesnt place anything

EDIT: actually re-reading it, what it was actually doing was using the screwdriver as the user arg, and trying to smash the user into the wall, thats actually kinda funny

borgo wallmounting is a good thing, good borgos need their treats

---
## [cindychip/opentitan](https://github.com/cindychip/opentitan)@[29b8d2c3e7...](https://github.com/cindychip/opentitan/commit/29b8d2c3e7fde48a117a31241c508bd4325f5b88)
#### Tuesday 2022-03-15 00:39:43 by Rupert Swarbrick

[dv,verilator] Make multiple sim_ctrl extensions play nicely

I'd finally got annoyed enough about not being able to pass "-t" in
the middle of a command line to figure out what was going on. It turns
out that by default getopt_long rearranges its arguments to put all
positional args at the end. That's nice, because it allows you code to
easily support stuff like

   my_program -a -b positional0 -c -d positional1

and, post-parse, it will find positional0 and positional1 as the last
two arguments. (If long enough in the tooth, you might remember having
to do "my_program -a -b -c -d positional0 positional1" for some
programs: this is what getopt fixes for us!)

Unfortunately, this behaviour plays havoc when more than one parser
wants to look at argv at once. For example, suppose you have

   my_program --some-args ARG --no-args

and you parse this twice. The first parser understands --no-args and
the second understands --some-args. With the default behaviour and ":"
at the start of the optstring, the first parser will ignore the
unknown --some-args argument and move the positional ARG to the end.
But then the second parser sees

  my_program  --some-args --no-args ARG

and tries to pass "--no-args" as the value to "--some-args". Much
confusion ensues...

Fortunately, we can pass '-' at the start of optstring to disable this
behaviour. The result is harder to parse if you're interested in
positional arguments (which is why this isn't the default behaviour)
but works when you have multiple parsers that have to place nicely
together.

Signed-off-by: Rupert Swarbrick <rswarbrick@lowrisc.org>

---
## [staten-island-tech/vue-project-2-ainu](https://github.com/staten-island-tech/vue-project-2-ainu)@[aaab2719a7...](https://github.com/staten-island-tech/vue-project-2-ainu/commit/aaab2719a745d2dd0269a41e878d4db8515a9af4)
#### Tuesday 2022-03-15 01:21:47 by darrenh6

12 Stout Street

Yuh
Real Rx
I used to wake up in my room in the morning
Put on my dirty shoes in the morning
Heard momma crying last night
Think the lights finna go out
Only thing on my mind is hitting a lick
Her Friend in prison for doing some shit
Say I'mma go to prison for doing some shit
Only thing on my mind is booming a bitch
12 Stout Street, I hated that house
I had to learn early on bein' a man about
My momma ain't never buy me shit
I sold drugs and robbed for all my shit
Momma said, "Baby that was years ago"
"Don't stress about shit that happened years ago"
This shit'd take a bitch years to know
I cried in the cold till my tears was froze
I hit a lick to help my momma out
How the fuck my mom the one to kick me out?
How the fuck you gonna send me out to the streets?
How the fuck you gonna say I can't come home to sleep?
How the fuck I come out your pussy and you
Choose your husband like you knew that Friend before me?
How the fuck you gon' turn your back on me?
How the fuck you gon' leave me flat on E?
How you gon' do that knowing they killed my dad?
You supposed to be my mom and my dad
I wish that fucking house would burn down
I couldn't tell you then but shit, I'll tell you now
For so many years, I held it down
I never in my life wanted to sell drugs
I would've been cool with playing games and shit
But instead I'm running with the gang and shit
Robberies done turned into shootings
Your son done did a gang and shit
It'd take a year to explain this shit
We don't stay safe, we stay dangerous
They took my brother, that fucked me up
Perc after perc, they fucking me up
Thousand percs later, still don't do nothing
Shits barely working, they're supposed to make me numb
Had flashbacks to when I was young
Bitches used to laugh and call me a bum
I was with Face, shot my first gun
Before Neo or Jet Li, I was the one
My momma ain't see it but the streets did
Said I wouldn't be shit, streets made me shit
Going through withdrawal, got me sick
I'm stretched back to back, I'm 'bout to flip
Don't look at me funny, you don't know shit 'bout me
Stood on the block with dreams of an Audi
Had a nightmare sleeping in my Audi
A Friend caught me lacking and pulled me out it
Big ass pistol to my mouthpiece
And it happened in front of 12 Stout Street

---
## [bestony/gutenberg](https://github.com/bestony/gutenberg)@[3ea2d42b0a...](https://github.com/bestony/gutenberg/commit/3ea2d42b0a6a206663735a47f9796bd42eda2186)
#### Tuesday 2022-03-15 01:28:50 by Dennis Snell

Blocks: Remember raw source block for invalid blocks. (#38923)

Part of #38922

When the editor is unable to validate a block it should preserve the
broken content in the post and show an accurate representation of that
underlying markup in the absence of being able to interact with it.

Currently when showing a preview of an invalid block in the editor we
attempt to re-generate the save output for a block given the attributes
we originally parsed. This is a flawed approach, however, because by
the nature of being invalid we know that there is a problem with those
attributes as they are.

In this patch we're introducing the `__unstableBlockSource` attribute on 
a block which only exists for invalid blocks at the time of this patch. That 
`__unstableBlockSource` carries the original un-processed data for a block
node and can be used to reconstruct the original markup without using
garbage data and without inadvertently changing it through the series
of autofixes, deprecations, and the like that happen during normal block loading.

The noticable change is in `block-list/block` where we will be showing
that reconstruction rather than the re-generated block content. Previously
it was the case that the preview might represent a corrupted version of the
block or show the block as if emptied of all its content. Now, however,
the preview sould accurately reflect the HTML in the source post even
when it's invalid or unrecognized according to the editor.

Further work should take advantage of the `__unstableBlockSource`
property to provide a more consistent and trusting experience for
working with unrecognized content.

---
## [GolfinhoVoador/tgstation](https://github.com/GolfinhoVoador/tgstation)@[079f8ac515...](https://github.com/GolfinhoVoador/tgstation/commit/079f8ac51554bb338ac5826c9d06c8d4bc10be80)
#### Tuesday 2022-03-15 01:52:57 by LemonInTheDark

Adds moveloop bucketing, uses queues for the singulo rather then sleeps (#64418)

Adds a basic bucketing system to move loops.

This should hopefully save a lot of cpu time, and allow for more load while gaining better smoothness.

The idea is very similar to SStimer, but my implementation is much more simple, since I have to worry less about long delays and redundant buckets.
Insertion needs to be cheaper too, since I'm making a system that by design holds a lot of looping things

It comes with some minor tradeoffs, we can't have constant rechecking of loops if a move "fails", not that we really want that anyway
We also lose direct control over the timer var, but I think that's better, don't want people manipulating that directly
Not that it even really worked very well back when we did have it
Removes the sleep from singularity code

Rather then using sleep to store the state of our iteration, we instead queue the iteration in a list.
We then use a custom singulo processing subsystem to call our "digest" proc several times per full eat, with the hope of staying on top of
our queue
This rarely happens because the queue is too large, god why is a sm powered singulo 24x24 tiles.

I've also A: cached our dist checks, and B: Added dist checks to prevent attempting to pull things out of range
This might look a bit worse, but it saves a lot of work

Oh right and I made the singulo unable to eat while it still has tiles to digest. The hope is to prevent
overwork and list explosion.

Hopefully this will prevent singulo server stoppage, though I've seen some other worrying things in testing.

---
## [whofagg0t/my-things](https://github.com/whofagg0t/my-things)@[03d4cf17ef...](https://github.com/whofagg0t/my-things/commit/03d4cf17efe4cf687c90c8638f3cac13cd905aef)
#### Tuesday 2022-03-15 02:19:37 by whofagg0t

idk if this works or no hhksjghkjagbkjhkg

ok fuck you

---
## [san7890/bruhstation](https://github.com/san7890/bruhstation)@[11c2dec0c3...](https://github.com/san7890/bruhstation/commit/11c2dec0c3b67bf72f243e3cf3fdeaebed1b68c0)
#### Tuesday 2022-03-15 02:22:50 by san7890

Mapper's Delight: Directional Poster Spawners [MDB IGNORE]

A lot of my PRs have been focused on having a fire lit under the seat of my pants. However, this PR is based off one conversation I had with someone.

"Haha, mapper. All you do is var_edits."

It was a bit more verbose than that, but it really did get me thinking about how fucking massive our files our thanks to these var_edits. This PR adds directional helpers to both A) Help mappers map the correct things with as little var edits as possible and B) Lessen the amount of space each fucking map file is thanks to however many extra bytes are taken up with pixel_x = 32. we have a lot of posters.

Bluntly put, this PR adds directional helpers to all generic /random poster spawners. i would add them to every single poster in the game, but that's a lot of work for unique posters, and someone can probably come up with a better idea. Good luck with that, this is just a good first step.

---
## [chaldeaprjkt/kernel_xiaomi_vayu](https://github.com/chaldeaprjkt/kernel_xiaomi_vayu)@[af8728964b...](https://github.com/chaldeaprjkt/kernel_xiaomi_vayu/commit/af8728964b61e891d1b549fad3ab9392e51ba9ce)
#### Tuesday 2022-03-15 02:42:37 by Peter Zijlstra

sched/core: Fix ttwu() race

Paul reported rcutorture occasionally hitting a NULL deref:

  sched_ttwu_pending()
    ttwu_do_wakeup()
      check_preempt_curr() := check_preempt_wakeup()
        find_matching_se()
          is_same_group()
            if (se->cfs_rq == pse->cfs_rq) <-- *BOOM*

Debugging showed that this only appears to happen when we take the new
code-path from commit:

  2ebb17717550 ("sched/core: Offload wakee task activation if it the wakee is descheduling")

and only when @cpu == smp_processor_id(). Something which should not
be possible, because p->on_cpu can only be true for remote tasks.
Similarly, without the new code-path from commit:

  c6e7bd7afaeb ("sched/core: Optimize ttwu() spinning on p->on_cpu")

this would've unconditionally hit:

  smp_cond_load_acquire(&p->on_cpu, !VAL);

and if: 'cpu == smp_processor_id() && p->on_cpu' is possible, this
would result in an instant live-lock (with IRQs disabled), something
that hasn't been reported.

The NULL deref can be explained however if the task_cpu(p) load at the
beginning of try_to_wake_up() returns an old value, and this old value
happens to be smp_processor_id(). Further assume that the p->on_cpu
load accurately returns 1, it really is still running, just not here.

Then, when we enqueue the task locally, we can crash in exactly the
observed manner because p->se.cfs_rq != rq->cfs_rq, because p's cfs_rq
is from the wrong CPU, therefore we'll iterate into the non-existant
parents and NULL deref.

The closest semi-plausible scenario I've managed to contrive is
somewhat elaborate (then again, actual reproduction takes many CPU
hours of rcutorture, so it can't be anything obvious):

					X->cpu = 1
					rq(1)->curr = X

	CPU0				CPU1				CPU2

					// switch away from X
					LOCK rq(1)->lock
					smp_mb__after_spinlock
					dequeue_task(X)
					  X->on_rq = 9
					switch_to(Z)
					  X->on_cpu = 0
					UNLOCK rq(1)->lock

									// migrate X to cpu 0
									LOCK rq(1)->lock
									dequeue_task(X)
									set_task_cpu(X, 0)
									  X->cpu = 0
									UNLOCK rq(1)->lock

									LOCK rq(0)->lock
									enqueue_task(X)
									  X->on_rq = 1
									UNLOCK rq(0)->lock

	// switch to X
	LOCK rq(0)->lock
	smp_mb__after_spinlock
	switch_to(X)
	  X->on_cpu = 1
	UNLOCK rq(0)->lock

	// X goes sleep
	X->state = TASK_UNINTERRUPTIBLE
	smp_mb();			// wake X
					ttwu()
					  LOCK X->pi_lock
					  smp_mb__after_spinlock

					  if (p->state)

					  cpu = X->cpu; // =? 1

					  smp_rmb()

	// X calls schedule()
	LOCK rq(0)->lock
	smp_mb__after_spinlock
	dequeue_task(X)
	  X->on_rq = 0

					  if (p->on_rq)

					  smp_rmb();

					  if (p->on_cpu && ttwu_queue_wakelist(..)) [*]

					  smp_cond_load_acquire(&p->on_cpu, !VAL)

					  cpu = select_task_rq(X, X->wake_cpu, ...)
					  if (X->cpu != cpu)
	switch_to(Y)
	  X->on_cpu = 0
	UNLOCK rq(0)->lock

However I'm having trouble convincing myself that's actually possible
on x86_64 -- after all, every LOCK implies an smp_mb() there, so if ttwu
observes ->state != RUNNING, it must also observe ->cpu != 1.

(Most of the previous ttwu() races were found on very large PowerPC)

Nevertheless, this fully explains the observed failure case.

Fix it by ordering the task_cpu(p) load after the p->on_cpu load,
which is easy since nothing actually uses @cpu before this.

Fixes: c6e7bd7afaeb ("sched/core: Optimize ttwu() spinning on p->on_cpu")
Reported-by: Paul E. McKenney <paulmck@kernel.org>
Tested-by: Paul E. McKenney <paulmck@kernel.org>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Link: https://lkml.kernel.org/r/20200622125649.GC576871@hirez.programming.kicks-ass.net

---
## [dnhatn/elasticsearch](https://github.com/dnhatn/elasticsearch)@[37ea6a8255...](https://github.com/dnhatn/elasticsearch/commit/37ea6a8255623d41be7df11440610ffa958ce50e)
#### Tuesday 2022-03-15 02:51:43 by Nik Everett

TSDB: Support GET and DELETE and doc versioning (#82633)

This adds support for GET and DELETE and the ids query and
Elasticsearch's standard document versioning to TSDB. So you can do
things like:
```
POST /tsdb_idx/_doc?filter_path=_id
{
  "@timestamp": "2021-12-29T19:25:05Z", "uid": "adsfadf", "v": 1.2
}
```

That'll return `{"_id" : "BsYQJjqS3TnsUlF3aDKnB34BAAA"}` which you can turn
around and fetch with
```
GET /tsdb_idx/_doc/BsYQJjqS3TnsUlF3aDKnB34BAAA
```
just like any other document in any other index. You can delete it too!
Or fetch it.

The ID comes from the dimensions and the `@timestamp`. So you can
overwrite the document:
```
POST /tsdb_idx/_bulk
{"index": {}}
{"@timestamp": "2021-12-29T19:25:05Z", "uid": "adsfadf", "v": 1.2}
```

Or you can write only if it doesn't already exist:
```
POST /tsdb_idx/_bulk
{"create": {}}
{"@timestamp": "2021-12-29T19:25:05Z", "uid": "adsfadf", "v": 1.2}
```

This works by generating an id from the dimensions and the `@timestamp`
when parsing the document. The id looks like:
* 4 bytes of hash from the routing calculated from routing_path fields
* 8 bytes of hash from the dimensions
* 8 bytes of timestamp
All that's base 64 encoded so that `Uid` can chew on it fairly
efficiently.

When it comes time to fetch or delete documents we base 64 decode the id
and grab the routing from the first four bytes. We use that hash to pick
the shard. Then we use the entire ID to perform the fetch or delete.

We don't implement update actions because we haven't written the
infrastructure to make sure the dimensions don't change. It's possible
to do, but feels like more than we need now.

There *ton* of compromises with this. The long term sad thing is that it
locks us into *indexing* the id of the sample. It'll index fairly
efficiently because the each time series will have the same first eight
bytes. It's also possible we'd share many of the first few bytes in the
timestamp as well. In our tsdb rally track this costs 8.75 bytes per
document. It's substantial, but not overwhelming.

In the short term there are lots of problems that I'd like to save for a
follow up change:
1. ~~We still generate the automatic `_id` for the document but we don't use
   it. We should stop generating it.~~ Included in this PR based on review comments.
2. We generated the time series `_id` on each shard and when replaying
   the translog. It'd be the good kind of paranoid to generate it once
   on the primary and then keep it forever.
3. We have to encode the `_id` as a string to pass it around
   Elasticsearch internally. And Elasticsearch assumes that when an id
   is loaded we always store as bytes encoded the `Uid` - which *does*
   have nice encoding for base 64 bytes. But this whole thing requires
   us to make the bytes, base 64 encode them, and then hand them back to
   `Uid` to base 64 decode them into bytes. It's a bit hacky. And, it's
   a small thing, but if the first byte of the routing hash encodes to
   254 or 255 we `Uid` spends an extra byte to encode it. One that'll
   always be a common prefix for tsdb indices, but still, it hurts my
   heart. It's just hard to fix.
4. We store the `_id` in Lucene stored fields for tsdb indices. Now
   that we're building it from the dimensions and the `@timestamp` we
   really don't *need* to store it. We could recalculate it when fetching
   documents. In the tsdb rall ytrick this'd save us 6 bytes per document
   at the cost of marginally slower fetches. Which is *fine*.
5. There are several error messages that try to use `_id` right now
   during parsing but the `_id` isn't available until after the parsing
   is complete. And, if parsing fails, it may not be possible to know
   the id at all. All of these error messages will have to change,
   at least in tsdb mode.
6. ~~If you specify an `_id` on the request right now we just overwrite
   it. We should send you an error.~~ Included in this PR after review comments.
7. We have to entirely disable the append-only optimization that allows
   Elasticsearch to skip looking up the ids in lucene. This *halves*
   indexing speed. It's substantial. We have to claw that optimization
   back *somehow*. Something like sliding bloom filters or relying on
   the increasing timestamps.
8. We parse the source from json when building the routing hash when
   parsing fields. We should just build it from to parsed field values.
   It looks like that'd improve indexing speed by about 20%.
9. Right now we write the `@timestamp` little endian. This is likely bad
   the prefix encoded inverted index. It'll prefer big endian. Might shrink it.
10. Improve error message on version conflict to include tsid and timestamp.
11. Improve error message when modifying dimensions or timestamp in update_by_query
12. Make it possible to modify dimension or timestamp in reindex.
13. Test TSDB's `_id` in `RecoverySourceHandlerTests.java` and `EngineTests.java`.

I've had to make some changes as part of this that don't feel super
expected. The biggest one is changing `Engine.Result` to include the
`id`. When the `id` comes from the dimensions it is calculated by the
document parsing infrastructure which is happens in
`IndexShard#pepareIndex`. Which returns an `Engine.IndexResult`. To make
everything clean I made it so `id` is available on all `Engine.Result`s
and I made all of the "outer results classes" read from
`Engine.Results#id`. I'm not excited by it. But it works and it's what
we're going with.

I've opted to create two subclasses of `IdFieldMapper`, one for standard
indices and one for tsdb indices. This feels like the right way to
introduce the distinction, especially if we don't want tsdb to cary
around it's old fielddata support. Honestly if we *need* to aggregate on
`_id` in tsdb mode we have doc values for the `tsdb` and the
`@timestamp` - we could build doc values for `_id` on the fly. But I'm
not expecting folks will need to do this. Also! I'd like to stop storing
tsdb'd `_id` field (see number 4 above) and the new subclass feels like
a good place to put that too.

---
## [pastthepixels/InfiniteShooter](https://github.com/pastthepixels/InfiniteShooter)@[d4fbfdb75a...](https://github.com/pastthepixels/InfiniteShooter/commit/d4fbfdb75ab33cfa2ec072cfbc5040f2ef4496e2)
#### Tuesday 2022-03-15 04:22:28 by PastThePixels

Made some loose code to make the docking station arrive after each level

NOTE 1: I also, finding bosses too difficult, lowered the damage of all bosses by about a half

NOTE 2: My plan is to have upgrades persist only per-run as to fully remove that feeling of being too powerful for the first level. So instead, the goal is to display the upgrades screen as part of a thing that happens when you board a ship after each level. It looks a bit clunky now, but I am to fix that in the future.

NOTE 3: So note-to-self here: the upgrades screen still makes upgrades persist with the userdata.txt file. I'll need to edit it so it only edits scores/userdata per run. Also, I'll need to make the experience a bit more smooth.

---
## [Morokite/Skyrat-tg](https://github.com/Morokite/Skyrat-tg)@[41aa1d2ee4...](https://github.com/Morokite/Skyrat-tg/commit/41aa1d2ee421161505284504f4d6f76faf51b0f7)
#### Tuesday 2022-03-15 05:56:04 by SkyratBot

[MIRROR] Adds a colorblind accessability testing tool [MDB IGNORE] (#11995)

* Adds a colorblind accessability testing tool (#65217)

* Adds a colorblind accessability testing tool

I keep finding myself worrying about if things I create will be parsable
for colorblind people. So I've made a debug tool for approximating
different extreme forms of colorblindness.

It's very very much a hack. We can't do the proper correction required
to actually deal directly with long medium and short wavelengths of
light, so we need to rely on approximations. Part of that means say,
bright things being brighter then they ought to be. S not how people
actually experience things, but it's not something we can do anything
about in byond.

Anyway uh, it works by taking color matrixes, and using the plane master
grouping system floyd added to apply them to most all parts of the game
you would want to color correct.

There's some slight fragility here, but I couldn't think of a better way
of handling it.

We also need to deal with planes that have BLEND_MULTIPLY as their
blendmode, since that fucks up the filter. I've come up with a hack for
it, since I wanted to avoid breaking anything.

Oh and since I want it to apply to huds too I added plane masters to
represent them. I think that's about it.

Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

* Adds a colorblind accessability testing tool

Co-authored-by: LemonInTheDark <58055496+LemonInTheDark@users.noreply.github.com>
Co-authored-by: Mothblocks <35135081+Mothblocks@ users.noreply.github.com>

---
## [enjarai/Mystical-Index](https://github.com/enjarai/Mystical-Index)@[1d0d368c30...](https://github.com/enjarai/Mystical-Index/commit/1d0d368c30b8149b37ded03aaf707ed99d0fe657)
#### Tuesday 2022-03-15 06:42:12 by enjarai

changes™ (yknow what, fuck you. *makes your index mystical*)

---
## [treckstar/yolo-octo-hipster](https://github.com/treckstar/yolo-octo-hipster)@[cde4792ccf...](https://github.com/treckstar/yolo-octo-hipster/commit/cde4792ccf910a27158164ad5dffe435d6b2e220)
#### Tuesday 2022-03-15 07:00:03 by treckstar

People listen up don't stand so close, I got somethin that you all should know. Holy matrimony is not for me, I'd rather die alone in misery.

---
## [projects-nexus/android_kernel_xiaomi_lavender-LTO](https://github.com/projects-nexus/android_kernel_xiaomi_lavender-LTO)@[89a00805cc...](https://github.com/projects-nexus/android_kernel_xiaomi_lavender-LTO/commit/89a00805ccc867b71b7f639f84ec84fbeafaeeb1)
#### Tuesday 2022-03-15 07:12:27 by Maciej Żenczykowski

FROMGIT: bpf: Do not change gso_size during bpf_skb_change_proto()

This is technically a backwards incompatible change in behaviour, but I'm
going to argue that it is very unlikely to break things, and likely to fix
*far* more then it breaks.

In no particular order, various reasons follow:

(a) I've long had a bug assigned to myself to debug a super rare kernel crash
on Android Pixel phones which can (per stacktrace) be traced back to BPF clat
IPv6 to IPv4 protocol conversion causing some sort of ugly failure much later
on during transmit deep in the GSO engine, AFAICT precisely because of this
change to gso_size, though I've never been able to manually reproduce it. I
believe it may be related to the particular network offload support of attached
USB ethernet dongle being used for tethering off of an IPv6-only cellular
connection. The reason might be we end up with more segments than max permitted,
or with a GSO packet with only one segment... (either way we break some
assumption and hit a BUG_ON)

(b) There is no check that the gso_size is > 20 when reducing it by 20, so we
might end up with a negative (or underflowing) gso_size or a gso_size of 0.
This can't possibly be good. Indeed this is probably somehow exploitable (or
at least can result in a kernel crash) by delivering crafted packets and perhaps
triggering an infinite loop or a divide by zero... As a reminder: gso_size (MSS)
is related to MTU, but not directly derived from it: gso_size/MSS may be
significantly smaller then one would get by deriving from local MTU. And on
some NICs (which do loose MTU checking on receive, it may even potentially be
larger, for example my work pc with 1500 MTU can receive 1520 byte frames [and
sometimes does due to bugs in a vendor plat46 implementation]). Indeed even just
going from 21 to 1 is potentially problematic because it increases the number
of segments by a factor of 21 (think DoS, or some other crash due to too many
segments).

(c) It's always safe to not increase the gso_size, because it doesn't result in
the max packet size increasing.  So the skb_increase_gso_size() call was always
unnecessary for correctness (and outright undesirable, see later). As such the
only part which is potentially dangerous (ie. could cause backwards compatibility
issues) is the removal of the skb_decrease_gso_size() call.

(d) If the packets are ultimately destined to the local device, then there is
absolutely no benefit to playing around with gso_size. It only matters if the
packets will egress the device. ie. we're either forwarding, or transmitting
from the device.

(e) This logic only triggers for packets which are GSO. It does not trigger for
skbs which are not GSO. It will not convert a non-GSO MTU sized packet into a
GSO packet (and you don't even know what the MTU is, so you can't even fix it).
As such your transmit path must *already* be able to handle an MTU 20 bytes
larger then your receive path (for IPv4 to IPv6 translation) - and indeed 28
bytes larger due to IPv4 fragments. Thus removing the skb_decrease_gso_size()
call doesn't actually increase the size of the packets your transmit side must
be able to handle. ie. to handle non-GSO max-MTU packets, the IPv4/IPv6 device/
route MTUs must already be set correctly. Since for example with an IPv4 egress
MTU of 1500, IPv4 to IPv6 translation will already build 1520 byte IPv6 frames,
so you need a 1520 byte device MTU. This means if your IPv6 device's egress
MTU is 1280, your IPv4 route must be 1260 (and actually 1252, because of the
need to handle fragments). This is to handle normal non-GSO packets. Thus the
reduction is simply not needed for GSO packets, because when they're correctly
built, they will already be the right size.

(f) TSO/GSO should be able to exactly undo GRO: the number of packets (TCP
segments) should not be modified, so that TCP's MSS counting works correctly
(this matters for congestion control). If protocol conversion changes the
gso_size, then the number of TCP segments may increase or decrease. Packet loss
after protocol conversion can result in partial loss of MSS segments that the
sender sent. How's the sending TCP stack going to react to receiving ACKs/SACKs
in the middle of the segments it sent?

(g) skb_{decrease,increase}_gso_size() are already no-ops for GSO_BY_FRAGS
case (besides triggering WARN_ON_ONCE). This means you already cannot guarantee
that gso_size (and thus resulting packet MTU) is changed. ie. you must assume
it won't be changed.

(h) changing gso_size is outright buggy for UDP GSO packets, where framing
matters (I believe that's also the case for SCTP, but it's already excluded
by [g]).  So the only remaining case is TCP, which also doesn't want it
(see [f]).

(i) see also the reasoning on the previous attempt at fixing this
(commit fa7b83bf3b156c767f3e4a25bbf3817b08f3ff8e) which shows that the current
behaviour causes TCP packet loss:

  In the forwarding path GRO -> BPF 6 to 4 -> GSO for TCP traffic, the
  coalesced packet payload can be > MSS, but < MSS + 20.

  bpf_skb_proto_6_to_4() will upgrade the MSS and it can be > the payload
  length. After then tcp_gso_segment checks for the payload length if it
  is <= MSS. The condition is causing the packet to be dropped.

  tcp_gso_segment():
    [...]
    mss = skb_shinfo(skb)->gso_size;
    if (unlikely(skb->len <= mss)) goto out;
    [...]

Thus changing the gso_size is simply a very bad idea. Increasing is unnecessary
and buggy, and decreasing can go negative.

Fixes: 6578171a7ff0 ("bpf: add bpf_skb_change_proto helper")
Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
Cc: Dongseok Yi <dseok.yi@samsung.com>
Cc: Willem de Bruijn <willemb@google.com>
Link: https://lore.kernel.org/bpf/CANP3RGfjLikQ6dg=YpBU0OeHvyv7JOki7CyOUS9modaXAi-9vQ@mail.gmail.com
Link: https://lore.kernel.org/bpf/20210617000953.2787453-2-zenczykowski@gmail.com

(cherry picked from commit 364745fbe981a4370f50274475da4675661104df https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next.git/commit/?id=364745fbe981a4370f50274475da4675661104df )
Test: builds, TreeHugger
Bug: 158835517
Bug: 188690383
Signed-off-by: Maciej Żenczykowski <maze@google.com>
Change-Id: I0ef3174cbd3caaa42d5779334a9c0bfdc9ab81f5
Signed-off-by: ImPrashantt <prashant33968@gmail.com>

---
## [ThePainkiller/sojourn-station](https://github.com/ThePainkiller/sojourn-station)@[83c7559f7c...](https://github.com/ThePainkiller/sojourn-station/commit/83c7559f7c9d151b330239652f0b66ba3463584a)
#### Tuesday 2022-03-15 07:19:34 by WilliamNelson37

Fratellis and Co

Finishes the Fratellis

Removed Low level code that allowed staff to vore people (why the fuck)

Removed Low level code that allowed staff to regurgitate vored people (god's light has diminished)

---
## [Chemlight/Hyper-Station-13](https://github.com/Chemlight/Hyper-Station-13)@[5d96c13de2...](https://github.com/Chemlight/Hyper-Station-13/commit/5d96c13de288fd1d735a392ceffd991d7ecc17f2)
#### Tuesday 2022-03-15 08:33:11 by sarcoph

holy fucking shit i am so tired of refactoring

to do list:
- ensure all useBackends use context instead of props
- ensure all uis that need it get their `{ act, data, config }` from useBackend(context)
- fix the rest of the bugs

i do not have the energy for this.

---
## [StarStation13/StarStation13](https://github.com/StarStation13/StarStation13)@[eeb5465931...](https://github.com/StarStation13/StarStation13/commit/eeb546593148ce940e9adac2c663c453d6557247)
#### Tuesday 2022-03-15 09:27:41 by vincentiusvin

Ordnance Content Update: Scientific Papers (#62284)

How do I play/test/operate this?

Download NT Frontier on any modular computers. It should debrief you on what experiments are available and how to publish.
If you want to do a bomb experiment, make sure it's captured by the doppler array (as usual) and then print the experiments into a disk and publish it.
If you want to do a gas experiment, make the gas and either pump it into a tank and 1) overpressurize it with a "clear" gas like N2 or 2) overpressurize tanks with the gas itself. Make sure you do the overpressurizing in the compressor machine. When tanks are destroyed/ejected leaked gas will get recorded. Print it into a disk and publish it.
For publication, the file needs to be directly present inside the computer's HDD. This means you need to copy it first with the file manager.
Fill the data (if desired, it will autofill with boiler plate if you dont) and send away!
Doing experiments unlock nodes, while doing them well unlocks boosts (which are discounts but slightly more restrictive) which are purchaseable with NT Frontier.
If you are testing this and have access to admin tools, there are various premade bombs under obj/effect/spawner/newbomb

A doc I wrote detailing the why and what part of this PR.
https://hackmd.io/JOakSYVMSh2zU2YL5ju_-Q

---

# Intro

## The Problem(s)

Ordnance, (previously toxins) seems to lack a lot of content and things to do. The gameplay loop consists of making a bomb and then sending it off for credits or using it to refine cores. Ordnance at it's inception originally relies on players experimenting and finding the perfect mix over multiple rounds, but once the recipe for a "do-everything" mix got out, the original charm of individual discoveries becomes meaningless.

Another issue with ordnance is the odd difficulty curve. As a new player, ordnance is almost impossible to decipher, but once you watch a tutorial or read a wiki and can mail a 50k into space, there pretty much isn't anything else to do. Most players will be satisfied at this point without the gameplay loop encouraging them to understand or play more. The only thing you can do afterwards is to sink your teeth in and understand why that particular mix explodes the way it does. This again has a significant difficulty curve, but if you do that, the department doesn't acknowledge or reward that in any way. There are pretty much two huge spikes, with the latter one not really existing inside the department.

TLDR:
* The content being same-y over rounds.
* Odd difficulty curve: 
    1. A new player is oblivious to everything. 
    2. Those in the middle can repeat the final goal consistently without needing to understanding why
    3. There is nothing to justify spending more time in the department after reaching the midgame.

## Abstract

Scientific Papers aim to add a framework to run multiple experiments in ordnance. Adding more experiments scattered across various atmospheric aspects might allow players of various knowledge levels to still have something engaging to do. A new player should have an easier challange than to mail a 50K. While those that already can make bombs should have an easier time understanding why their bombs explode the way it does. Once they fully understand why, they can set their sights on taking advantage of another reaction to set their bomb off or hone one particular reaction down.

## Goals

* Have some intro-level challanges for new players.
* Have some semblance of late-game challanges for more experienced players.
* Explain the mechanics better for those in the middle of the road.
* Incentivize trying new things out in the department.
* Better integrate Ordnance with Experisci

## Boundaries / Dont's

* Do not incentivize people to learn ordnance by using PvP loots.
* Do not shake or change the reaction system by a huge amount.
* Disincentivize having a single god-mix that does everything.
****

# Main design pillars

## A. Framework surrounding the experiments

### A.1. New experiments

Add new experiments to the ExperiSci module. These will come in two flavours: New explosions to do, and various gas synthesis experiments. Both of these are actually supported by the map layout of ordnance right now, but there is no reason to do anything outside of making a 50k as fast as possible.

### A.2. Rewards for experiments: Cash and Techweb Boosts.

Scientific papers will add a separate experiment handling system. A single experiment will be graded into various tiers, each tier corresponding to the explosion size or amount of gas made.  Doing any tier of a specific experiment will unlock the discount for that specific reactions. A single explosion **WILL NOT** do multiple experiments (or even tiers) at once.

On publication, a partner can be selected. A single partner only has a specific criteria of experiments they want. The experiments will then be graded on "how good they are done", with the criteria being more punishing as tier increases. Publication will then reward scientific cooperation with the partnered partner. Players can spend this cooperation on techweb boosts. Techweb boosts are meant to be subservient to discount from experiments and will not shave a node's price to be lower than 500 points.

**Experiments will only unlock nodes, discounts are handled through this boost system.**
This is more for maintainability than anything.

### A.3. On Tedium

*This is a note on implementation more than anything, but I think this helps explains why several things are done.*

Due to the nature of atmospheric reactions in the game (they're all linear), tedium is a very important thing to consider. An experiment should have a sweet spot to aim for, but there should not be a point where further mastery is stopped dead on it's track with a reward cap.

Scientific Papers attempts to discourage this behaviour by having the "maximum score" scale off to infinity but with the rewards being smaller and smaller. The sweet spot is always there to aim for and should be well communicated with players, but on their last submission of an experiment topic players should be encouraged to do their best. There should always be a reward for pushing the system to it's limit as long as it doesn't completely nullify the other subdepartments. This is the reason why there is a hard limit on the number of publications and why the score calculation is a bit more complex than it needed to be.

## B. Gas Synthesis (Early-Mid Game)

Scientific papers will add one new machine that requests a tank to release x amounts of y gas. This will be accomplished by adding a tank pumping machine which will either burst or explode a tank, releasing the gas inside. The gas currently requested are BZ, Nitryl, Halon and Nob.

The overarching goal of this compressor machine is to present a gas synthesis challange for the players and to get them more accustomed to how a tank explodes. The gas synthesis part can always be changed in order to reflect the current state of atmospheric reactions.

## C. Explosion Changes (Mid-Late Game)

### C.1 Cause and effect.

The main theme of the explosion changes is establishing cause and effect of explosions. Reactions that happens inside a tank that's going to explode will be recorded and forwarded to a doppler array. Some experiments will require only a single cause to be present (think of it as isolating a variable). This is currently implemented for nobliumformation and pressure based bombs. Having other reactions occuring besides noblium formation will fail the first one, while having any reactions at all will fail the second one. 

Adding more explosions here will be a slight challange because as of now the game has only two reactions that can reliably make an explosion.

### C.2 Tools upgrade.

Doppler array has now been retrofitted to state the probable cause of an explosion, be it reactions or just overpressurization on gas merging. These should help intermediate players figure out what is causing an explosion.

Added a new functionality to the implosion compressor:
Basically performs the gas merging and reaction that TTV does in a machine and reports the results back as if someone uses an analyzer on them. Here to give players feedback so they can try and understand what is actually going on in a bomb.

## D. Player Interaction

There should be more room for more than 1 player to play ordnance simultaneously. Previously players are also able to split tasks, but this rarely happens because tritium synthesis needs only the gas chamber to be reconfigured. Now, different players can pick different experiments and work on them. Players can also do joint tasks on one single experiment. Gases like noblium will need tritium production and also a cooling module online.

Ordnance can also coordinate with their parent department on what they really need, be it money or research bonuses.

# Potential Changes

The best-case changes that can be implemented if the current roster of content isn't enough is more reactions that can be used in bombs. Eliminating bombs entirely goes against the spirit of the subdepartment, while adding new ones will need a lot of care and consideration.

Another possible change is to implement a "gas payload" bomb. Bombs that has a set number of unreacting gas inside that will increase the heat capacity, reduce the payload, and neccesitates more bespoke mixes.

Adding more gas synthesis experiments is discouraged. The main focus of ordnance should be bombs, with gas synthesis being a side project for ordnance. These are present to ease the introduction to bombs and provide some side content. 
There should be a somewhat well-justified goal in adding new synthesis experiments: e.g. BZ is there as a "tutorial" gas, Nitryl to introduce players to cooling/heating mixes, Halon to a more efficient tritium production, and Nob as a nudge to nobformation bombs and mastery over other aspects.

# Conclusion / Summary

Add more experiments to ordnance that players can take, accomplish this by:
1. Making the players perform gas synthesis or make bombs.
2. Have them collect the data, see if it fits the criteria. Explain why if it fits and why if it doesn't.
3. Have the player publish a paper.

Reward them based on how well did they do, give players agency both on the experiment phase and also publication phase.


---
TLDR: Added new experiment to toxins, added the framework for those experiments existing. Experiments comes in gas synthesis and also bombs but with more parameters. Experiments needs to be published through papers, various choices to be made there.

Implementation notes:

Because of how paper works, ordnance experiments are handled outside of experiment_handler components. My reasoning for this is twofold:

The experiments will be completed manually on publication and if the experiment isn't unlocked yet it will still be completed.
Experiment handler datums have several procs which require an atom-level parent, and I figured this is the most sensible and cleanest way to implement this without changing the experiment handler datum too much.

Small change to /obj/machinery/proc/power_change() signal ordering to adjust the state first and then send the signal. Didn't found any other usage of this signal except mine but barge down my door if it broke something.

Rewrote the ttv merge_gases() code to be slightly more readable.
A small code improvement for thermomachine to use tofixed (my fault).

Ordnance have been updated to enable the publication of papers
Several new explosive and gas synthesis experiments have been added to ordnance
Anomaly compressor has been TGUIzed and now supports simulating the reaction of the gases inside the ttv.
New tank compressor machine for toxins. You can overpressurize tanks with exotic gases and complete experiments.
Several techweb nodes are locked and require toxin experiments to complete.
Toxins can purchase boosts for various techweb nodes.
You no longer need to anchor doppler arrays for it to work.
Doppler array and implosion compressor now supports deconstruction, implosion compressor construction added.
Doppler now emits a red light to denote it's direction and it being on. Doppler not malf.
Implosion compressor renamed to anomaly refinery.
Created a new program tab "Science" for the downloader app. Removed Robotics.
Reworked the code for bombspawner (used in the cuban pete arcade game)

---
## [m4cey/remote-spotify](https://github.com/m4cey/remote-spotify)@[08ef448c45...](https://github.com/m4cey/remote-spotify/commit/08ef448c45ae918bdd08b35db505da59962fe24a)
#### Tuesday 2022-03-15 10:52:23 by m4cey

reset those nifty fucking tokens omg I fucking hate this bug it drove me mad

---
## [huggingface/datasets](https://github.com/huggingface/datasets)@[a8fa7cfe95...](https://github.com/huggingface/datasets/commit/a8fa7cfe95e06c8a667c4d7c5b7c7287b7e9ac4f)
#### Tuesday 2022-03-15 10:52:51 by RenChu Wang

Multi-GPU support for `FaissIndex` (#3721)

* 🎉 This commit fixes huggingface/datasets#3716

This commit adds handling for faiss indices that run on multiple GPUs.

* 🤕 Stupid mistake in that index isn't returned in the function handling device.

Now it's fixed. Hopefully the PR isn't merged yet!

* 🗎 Updated documents to reflect changes I made in the code.

Update `device`'s document to include negative numbers and lists.

* 1️⃣ The line should not exceed length: 119

It seems that this is what circle CI checked anyways.

* 🥴 Apply suggestions from code review

Missed it the first time :)

Co-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>

* 🛠️ Fixed my fixes.

Updated code to address concerns.

* 🇫 Update src/datasets/search.py

Using f-strings in docs.

Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>

Co-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>
Co-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>

---
## [sunnykamboz/sunnykambozinfo](https://github.com/sunnykamboz/sunnykambozinfo)@[b1961eff4e...](https://github.com/sunnykamboz/sunnykambozinfo/commit/b1961eff4ee9839c63f768da8f8f44f14a99954a)
#### Tuesday 2022-03-15 10:54:14 by Sunny Kamboz

Sunny Kamboz Indian Singer

Sunny Kamboz is an Indian Singer | Songwriter | Composser | Music Produce. He was born on 17 March 1994 in Patiala. He is the active ceo and founder of "SK Music Studio", a digital marketing company. The best way to keep up is to learn from and support each other as we continue pushing the boundaries of innovation in marketing.
He is a graduate ( B.A. Arts) student at Government Mohindra College.
Sunny Kamboz is an Influencer, fashion blogger, and Social media star. Sunny Kamboz is yet unmarried. He also has not posted any picture of her girlfriend in the social media. Therefore, we can believe that the star is still single.

---
## [Abel3047/Tunnel](https://github.com/Abel3047/Tunnel)@[b4223635de...](https://github.com/Abel3047/Tunnel/commit/b4223635de275f6c455ffc15f50892fa5873e5a7)
#### Tuesday 2022-03-15 11:20:55 by Bernard Pixel

Changed TunnelId type and add a note file

-So I was having trouble working with the idGenerator I created so I
decided to change TunnelId to string so that it fits. I didn't want to
use slong or something just to store a number so string is the best bet.
-Following the reflective changes need to be done cause of TunnelId I
wanted to make notes as I progressed over the thoughts I was having with
the whole project in its entirety. I didnt want to put it in the README
so I made my own text file called note to myself.
-I also added a Task list token called Note so where-ever notes are
being made it will be captured by the Task List

---
## [prince-rudh/Rudhra](https://github.com/prince-rudh/Rudhra)@[4a7bbca02c...](https://github.com/prince-rudh/Rudhra/commit/4a7bbca02c8f3829141cbe8196dc006efaa78c5f)
#### Tuesday 2022-03-15 12:12:12 by Prince Rudh

📢 Rudhra Version 3.0 Available Now! …

# Contributing to Prince-Rudh

When contributing to this repository, please first discuss the change you wish to make via issue,
email, or any other method with the owners of this repository before making a change. 

Please note we have a code of conduct, please follow it in all your interactions with the project.

## Pull Request Process

1. Ensure any install or build dependencies are removed before the end of the layer when doing a 
   build.
2. Update the README.md with details of changes to the interface, this includes new environment 
   variables, exposed ports, useful file locations and container parameters.
3. Increase the version numbers in any examples files and the README.md to the new version that this
   Pull Request would represent. The versioning scheme we use is [ReadMe](https://github.com/prince-rudh/Rudhra2.0/blob/master/README.md).
4. You may merge the Pull Request in once you have the sign-off of two other developers, or if you 
   do not have permission to do that, you may request the second reviewer to merge it for you.

## Code of Conduct

### Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, gender identity and expression, level of experience,
nationality, personal appearance, race, religion, or sexual identity and
orientation.

### Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
  address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

### Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

### Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

### Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at AsenaDev. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.


### Warning ⚠️

This project is open source. So you are responsible for the changes you make.
It is your responsibility to use these codes. We are not responsible for any bad things you make.

##

---
## [nevimer/Skyrat-tg](https://github.com/nevimer/Skyrat-tg)@[d96e7b7e27...](https://github.com/nevimer/Skyrat-tg/commit/d96e7b7e278dd0226a4de8d9463edda37af709f9)
#### Tuesday 2022-03-15 13:25:55 by SkyratBot

[MIRROR] Makes Ants glow, puts a min on ant screaming & shoe permeability, & other ant-related things. [MDB IGNORE] (#11821)

* Makes Ants glow, puts a minimum on ant screaming and shoe permeability, and other ant-related things. (#64786)

I found out how emissives work and my first thought was "damn ants should glow that would look sick"
So now they do.

Also, having less than 5u ants in your body will make you not scream, so 0.0001u ants will no longer have that tiny chance of making someone scream for their life.

If an ant pile has a max damage value less than 1, then they won't be able to bite through your shoes. This is the same threshold as the second tier ant icon.

Makes the giant ant a hostile mob with the neutral faction, meaning they will attack anything not in the neutral faction.

* Makes Ants glow, puts a min on ant screaming & shoe permeability, & other ant-related things.

Co-authored-by: Wallem <66052067+Wallemations@users.noreply.github.com>

---
## [avar/git](https://github.com/avar/git)@[5ef11419ff...](https://github.com/avar/git/commit/5ef11419ffe1163bb2d9515f87f06cdf03c50f1c)
#### Tuesday 2022-03-15 13:34:01 by Ævar Arnfjörð Bjarmason

Makefile: have "header dep" 1=1 depend on *.[cs] file

Change the rules added when "COMPUTED_HEADER_DEPENDENCIES" was
introduced in dfea575017d (Makefile: lazily compute header
dependencies, 2010-01-26) so that there is a 1=1 mapping between the
dependencies we declare for a given file if we have the *.o.d file,
and whether or not that *.o.d file exists.

Right now this doesn't change anything, because despite what
dfea575017d seems to have been confused about on the topic of $(LIB_H)
v.s. $(GENERATED_H) dependencies it's OK for a given foo.o to
over-depend on headers it doesn't actually need (including generated
ones!), since we'll be compiling it anyway. The worst case for an
over-dependency is that we'd needlessly create one of the
$(GENERATED_H) files. See f53df0bdf6d (Makefile: remove an out-of-date
comment, 2021-09-23) for further discussion about that inaccuracy.

This change doesn't really change anything meaningful *yet*, rather it
sets us up for being able to use generated header files which will be
used in a *lot* of places, such making the copy/pasting in advice.[ch]
generated, or to have builtin.h be generated.

That hasn't been a practical problem so far, since
{command,config,hook}-list.h are each included in only one *.c
file. By having the sort of rule added in d3fd1a66679 (Makefile:
correct the dependency graph of hook-list.h, 2021-12-17) we've been
able to compile *.o files that required a generated header, as can be
seen with:

    for f in add bugreport ; do git clean -dxf; make builtin/$f.o; done

But if we were to add such a widely-used generated header (think a
generated builtin.h) our current way of dealing with this would be
come very fragile. We'd either need to have all *.o files depend on
all generated headers, or manually and exhaustively list our
dependencies.

This change sets us up for being able to get the best of both
worlds. Now by adding this to the "dep_template" here (using a new
$(COMMON_GENERATED_H)):

    $(call dot_o_targets_template,$(1),s sp): $(or $(wildcard $(2)),$(COMMON_GENERATED_H))

We would with COMPUTED_HEADER_DEPENDENCIES=yes have all *.o files
depend on whatever is in $(COMMON_GENERATED_H), but only until we
compile them once, for any subsequent incremental compilation we'd use
the generated *.o.d files to ascertain our actual header dependencies.

The trade-off is that for the "for"-loop above we'd over-depend on
that $(COMMON_GENERATED_H) with a fresh build, but I think that's
OK. A bit of over work beats a compilation failure as an edge case,
more important is that we won't redundantly over-build the entire
project if one of these generated headers changes, just those things
that need the header.

Since I'm changing this change the .depends directories to take
advantage of the template added in 0b6d0bc9246 (Makefiles: add and use
wildcard "mkdir -p" template, 2022-03-03), and create them in a tree
under a new .build directory. Now we'll no longer create all possible
.depend directories just to compile a single file, and keeping track
of these becomes easier: we just need to "rm -rf .build" in the
"clean" rule.

Signed-off-by: Ævar Arnfjörð Bjarmason <avarab@gmail.com>

---
## [ibratabian17/android_kernel_lenovo_msm8916](https://github.com/ibratabian17/android_kernel_lenovo_msm8916)@[ff17892aa6...](https://github.com/ibratabian17/android_kernel_lenovo_msm8916/commit/ff17892aa67ecbef97ee3b261ed4872d23ea579e)
#### Tuesday 2022-03-15 13:39:46 by Masahiro Yamada

modpost: file2alias: go back to simple devtable lookup

commit ec91e78d378cc5d4b43805a1227d8e04e5dfa17d upstream.

Commit e49ce14150c6 ("modpost: use linker section to generate table.")
was not so cool as we had expected first; it ended up with ugly section
hacks when commit dd2a3acaecd7 ("mod/file2alias: make modpost compile
on darwin again") came in.

Given a certain degree of unknowledge about the link stage of host
programs, I really want to see simple, stupid table lookup so that
this works in the same way regardless of the underlying executable
format.

Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
Acked-by: Mathieu Malaterre <malat@debian.org>
[nc: Omit rpmsg, sdw, fslmc, tbsvc, and typec as they don't exist here
     Add of to avoid backporting two larger patches]
Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
Signed-off-by: Sasha Levin <sashal@kernel.org>
Signed-off-by: Kevin F. Haggerty <haggertk@lineageos.org>
Change-Id: Ic632eaa7777338109f80c76535e67917f5b9761c

---
## [poettering/systemd](https://github.com/poettering/systemd)@[de90700f36...](https://github.com/poettering/systemd/commit/de90700f36f2126528f7ce92df0b5b5d5e277558)
#### Tuesday 2022-03-15 14:53:49 by Lennart Poettering

pid1: set SYSTEMD_NSS_DYNAMIC_BYPASS=1 env var for dbus-daemon

There's currently a deadlock between PID 1 and dbus-daemon: in some
cases dbus-daemon will do NSS lookups (which are blocking) at the same
time PID 1 synchronously blocks on some call to dbus-daemon. Let's break
that by setting SYSTEMD_NSS_DYNAMIC_BYPASS=1 env var for dbus-daemon,
which will disable synchronously blocking varlink calls from nss-systemd
to PID 1.

In the long run we should fix this differently: remove all synchronous
calls to dbus-daemon from PID 1. This is not trivial however: so far we
had the rule that synchronous calls from PID 1 to the dbus broker are OK
as long as they only go to interfaces implemented by the broke itself
rather than services reachable through it. Given that the relationship
between PID 1 and dbus is kinda special anyway, this was considered
acceptable for the sake of simplicity, since we quite often need
metadata about bus peers from the broker, and the asynchronous logic
would substantially complicate even the simplest method handlers.

This mostly reworks the existing code that sets SYSTEMD_NSS_BYPASS_BUS=
(which is a similar hack to deal with deadlocks between nss-systemd and
dbus-daemon itself) to set SYSTEMD_NSS_DYNAMIC_BYPASS=1 instead. No code
was checking SYSTEMD_NSS_BYPASS_BUS= anymore anyway, and it used to
solve a similar problem, hence it's an obvious piece of code to rework
like this.

Issue originally tracked down by Lukas Märdian. This patch is inspired
and closely based on his patch:

       https://github.com/systemd/systemd/pull/22038

Fixes: #15316
Co-authored-by: Lukas Märdian <slyon@ubuntu.com>

---
## [ZMRamsey/Fighting-Game](https://github.com/ZMRamsey/Fighting-Game)@[7b95b93241...](https://github.com/ZMRamsey/Fighting-Game/commit/7b95b932415f97fad1fda979bd941118b830cedd)
#### Tuesday 2022-03-15 15:53:11 by Shaun Kennie

some pausey stuff and funny

jfc the dude behind me makes me want to hurt everyone, like shut up dude you genuinely make everyone elses life worse

---
## [anthonyra/blockchain-etl](https://github.com/anthonyra/blockchain-etl)@[4153b8af1e...](https://github.com/anthonyra/blockchain-etl/commit/4153b8af1e1c984a9515a7d1cfcb09adf602e1f9)
#### Tuesday 2022-03-15 16:09:17 by Anthony Anderson

ara/performance-debugging

Added timers to be_db_block

typo

oops

Return maps

using milliseconds instead

drilling deeper

Clear logs, more fluff

shrug

maybe ok missed

clarification added

did I break it bad

q_raw_transactions

q_raw_transaction v2

q_raw_transaction v2.1

q_raw_transaction v3

q_raw_transaction v3.1

q_raw_transaction v3.2

q_raw_transaction v3.3

Better place for time comparison

added deeper performance timers

typo

changed units

typo

link logs better

remove logs

try raw pmap to json

fixed typo

clarified logs

Fixed warnings

add to_detail_json

fixed typo

commented out for a little

updated core to include required export

debugging

remove pmap from detailed

dumb it real down

big oops

again

ugh one day

ffs

shrug

closer

kiss

clean up

oof

uhhh

pleasse

ffsa

added more logs

pmap still no worky

again last one

facepalm

okay finally last one

until next time

day 2

debugging snapshot in ledger

rebar.lock updated

I'll be a real boy one day

fix?

bin or not to bin

fix?

closer so close

mvp

damn autocorrect

given'er the beans

cleaning up logs

Better log locations and pmap?

wooo

fixed crashes in state_channel_close

start work on batch_pmap

batch_pmap prep

batch_pmap prep again

debugging preps

oops

batch_pmap issues

Add batch partition list

fixed typo

Check both versions p_list and performance

ahah

better debugging

hi ho hi ho

preps for test run

haha

jebus

fix length printout

real test

jokes on me

switch back to pmap

pmap batch feature

pmap finalized

more pmap support

exports again

dumb dumb

more more

ugh

one thing at a time

hacky

try this one

oops

woot

integrity tests

commas

erlang operators

debugging txn integrity issues

compare keys

better integrity checks

integrity checks

map checks

more logs

more digging

diggy diggy

close enough

last attempt

last last one

commaaa

add compare_lists for integrity checks

forgot completion message for compare_lists

commmon

oops

binary issue?

syntax error

better compare_lists

syntax error hunting

eh

closer

more catches

syntax

huh

add error tuple

save log space

okay maybe now

wildcards I tell yea

Better?

third time the charm

ffs

damn

how deep can you go

I think it's all inclusive now

all this work

damn logs cleanup

Less noise

sort lists first

better sort locations

testing missing is_valid

find is_valid in Opts

finding is_valid

is_valid test

is_valid not working

ugh

WitnessInfo

damn it

it works

see if channels available

again

maybe this time?

haha

damn dig deeper

ffs

cooommmamaaa

drilling down

see if I can recreate channels

next step to recreate channels

ugh

Get what I need from ledger

get it to work

Get regions to work

damnit

one step at a time

challengee region

remove extra log

oops

one day

hard code resolution

dumb dumby

catch the error

syntax

please work

ffs

step it all the way back

mixed them up ugh

find challengeeRegion

ugghh

keep it running

and again

spring cleaning

oops

does this work?

trying again

Where does it die?

closer to finding error

error catching

learning to spec

its my fault!

common baby

adding more specs

soooo close

---
## [Divine-Journey-2/main](https://github.com/Divine-Journey-2/main)@[132f70b386...](https://github.com/Divine-Journey-2/main/commit/132f70b38666cf276dfffebd2c1230abadfc040f)
#### Tuesday 2022-03-15 16:22:02 by Atricos

Update 2.12.0

- Completely reworked the Mob Loot Fabricator (implemented by WaitingIdly). It's now available in Chapter 23 instead of 26 and it's much cheaper to build. The player can now choose to generate items from 18 different categories by inserting a "catalyst" item into its input slot. Instead of costing Mana, it now costs Life Essence to run. However one of the categories also generates Life Essence liquid back. It can also generate EvilCraft Blood, Vengeance Essence, Will, Gaia Spirits, DivineRPG Souls and boss drops, all Thaumcraft Vis Crystals, all Twilight Forest boss trophies, and more! Moved its quest to Chapter 23, explained the details there, and rearranged the Chapter.
- Updated version number and changelog.

---
## [avar/git](https://github.com/avar/git)@[0f0b7976c8...](https://github.com/avar/git/commit/0f0b7976c8cfbf2ab2e811c9f20a8837ae4b778c)
#### Tuesday 2022-03-15 17:02:59 by Ævar Arnfjörð Bjarmason

advice-type.h: add a generated list of advice, like hook-list.h

Make the addition of new advice to advice.[ch] simpler and more
fool-proof by generating the list of advice enums from
Documentation/config/advice.txt, instead of requiring new additions to
be hardcoded across the three files (*.c, *.h and *.txt).

See cfe853e66be (hook-list.h: add a generated list of hooks, like
config-list.h, 2021-09-26) for similar prior art, including a similar
addition to CMakeLists.txt.

In subsequent commits we'll also start auto-generating the mapping
between enum labels and config keys (in advice.c), but for now we save
ourselves the duplication here.

Let's leave a "GENERATED_ADVICE_H" variable and other
scaffolding (case ... esac) in place in the shellscript to make that
later addition easy. The lack of "case...esac" indentation here is
intentional, it makes managing the whitespace in the generated code
easier.

While the "To add a new advice" comment is still describing a process
that's needed, let's remove it entirely, in subsequent commits we'll
replace the remaining manual parts of the process, and having to
update that comment as we go along would result in needless churn.

As discussed in a preceding commit that mentioned the
"$(COMMON_GENERATED_H)" being added here, we need to ensure that the
code which depend on this new header won't fail if we haven't built
advice-type.h yet.

That's now done by having *.{o,s,sp} files depend on
"$(COMMON_GENERATED_H)" under COMPUTE_HEADER_DEPENDENCIES , but only
if we haven't compiled it once before, and can thus benefit from the
accurate dependencies the compiler generated. This means that
e.g. this works:

    $ git clean -qdxf; make grep.o; git clean -dxf '*.h' '*.o'; make grep.o
    GIT_VERSION = 2.35.1.475.g534253553cd.dirty
        * new build flags
        MKDIR -p .build/dep
        GEN advice-type.h
        CC grep.o
    Removing advice-type.h
    Removing grep.o
        CC grep.o

I.e. note how we have grep.o over-depend on advice-type.h, but only
the first time around. The second time around we looked at the
dependency graph in ".build/dep/grep.o.d". Doing the same with
e.g. remote.o (which does use the generated header) will end in:

    Removing advice-type.h
    Removing remote.o
        GEN advice-type.h
        CC remote.o

I.e. in that case we do still depend on advice-type.h post-compilation:

    $ grep  -c advice-type.h .build/dep/remote.o.d
    2
    $

We do need to hardcode one special-case: The advice-type.h header is
the only generated header that's included in another header, so for
"make hdr-check" we need to have advice.hco depend on advice-type.h.

[TODO other commit msg]:

so that there is a 1=1 mapping between the
dependencies we declare for a given file if we have the *.o.d file,
and whether or not that *.o.d file exists.

Right now this doesn't change anything, because despite what
dfea575017d seems to have been confused about on the topic of $(LIB_H)
v.s. $(GENERATED_H) dependencies it's OK for a given foo.o to
over-depend on headers it doesn't actually need (including generated
ones!), since we'll be compiling it anyway. The worst case for an
over-dependency is that we'd needlessly create one of the
$(GENERATED_H) files. See f53df0bdf6d (Makefile: remove an out-of-date
comment, 2021-09-23) for further discussion about that inaccuracy.

This change doesn't really change anything meaningful *yet*, rather it
sets us up for being able to use generated header files which will be
used in a *lot* of places, such making the copy/pasting in advice.[ch]
generated, or to have builtin.h be generated.

That hasn't been a practical problem so far, since
{command,config,hook}-list.h are each included in only one *.c
file. By having the sort of rule added in d3fd1a66679 (Makefile:
correct the dependency graph of hook-list.h, 2021-12-17) we've been
able to compile *.o files that required a generated header, as can be
seen with:

    for f in add bugreport ; do git clean -dxf; make builtin/$f.o; done

But if we were to add such a widely-used generated header (think a
generated builtin.h) our current way of dealing with this would be
come very fragile. We'd either need to have all *.o files depend on
all generated headers, or manually and exhaustively list our
dependencies.

Since I'm changing this change the .depends directories to take
advantage of the template added in 0b6d0bc9246 (Makefiles: add and use
wildcard "mkdir -p" template, 2022-03-03), and create them in a tree
under a new .build directory. Now we'll no longer create all possible
.depend directories just to compile a single file, and keeping track
of these becomes easier: we just need to "rm -rf .build" in the
"clean" rule.

[more TODO]

This change sets us up for being able to get the best of both
worlds. Now by adding this to the "dep_template" here (using a new
$(COMMON_GENERATED_H)):

    $(call dot_o_targets_template,$(1),s sp): $(or $(wildcard $(2)),$(COMMON_GENERATED_H))

We would with COMPUTED_HEADER_DEPENDENCIES=yes have all *.o files
depend on whatever is in $(COMMON_GENERATED_H), but only until we
compile them once, for any subsequent incremental compilation we'd use
the generated *.o.d files to ascertain our actual header dependencies.

The trade-off is that for the "for"-loop above we'd over-depend on
that $(COMMON_GENERATED_H) with a fresh build, but I think that's
OK. A bit of over work beats a compilation failure as an edge case,
more important is that we won't redundantly over-build the entire
project if one of these generated headers changes, just those things
that need the header.

Signed-off-by: Ævar Arnfjörð Bjarmason <avarab@gmail.com>

---
## [mawrick26/SM8250](https://github.com/mawrick26/SM8250)@[da582abe72...](https://github.com/mawrick26/SM8250/commit/da582abe7273a540f21074b25b0e94f51e87bb03)
#### Tuesday 2022-03-15 17:22:42 by George Spelvin

lib/sort: make swap functions more generic

Patch series "lib/sort & lib/list_sort: faster and smaller", v2.

Because CONFIG_RETPOLINE has made indirect calls much more expensive, I
thought I'd try to reduce the number made by the library sort functions.

The first three patches apply to lib/sort.c.

Patch #1 is a simple optimization.  The built-in swap has special cases
for aligned 4- and 8-byte objects.  But those are almost never used;
most calls to sort() work on larger structures, which fall back to the
byte-at-a-time loop.  This generalizes them to aligned *multiples* of 4
and 8 bytes.  (If nothing else, it saves an awful lot of energy by not
thrashing the store buffers as much.)

Patch #2 grabs a juicy piece of low-hanging fruit.  I agree that nice
simple solid heapsort is preferable to more complex algorithms (sorry,
Andrey), but it's possible to implement heapsort with far fewer
comparisons (50% asymptotically, 25-40% reduction for realistic sizes)
than the way it's been done up to now.  And with some care, the code
ends up smaller, as well.  This is the "big win" patch.

Patch #3 adds the same sort of indirect call bypass that has been added
to the net code of late.  The great majority of the callers use the
builtin swap functions, so replace the indirect call to sort_func with a
(highly preditable) series of if() statements.  Rather surprisingly,
this decreased code size, as the swap functions were inlined and their
prologue & epilogue code eliminated.

lib/list_sort.c is a bit trickier, as merge sort is already close to
optimal, and we don't want to introduce triumphs of theory over
practicality like the Ford-Johnson merge-insertion sort.

Patch #4, without changing the algorithm, chops 32% off the code size
and removes the part[MAX_LIST_LENGTH+1] pointer array (and the
corresponding upper limit on efficiently sortable input size).

Patch #5 improves the algorithm.  The previous code is already optimal
for power-of-two (or slightly smaller) size inputs, but when the input
size is just over a power of 2, there's a very unbalanced final merge.

There are, in the literature, several algorithms which solve this, but
they all depend on the "breadth-first" merge order which was replaced by
commit 835cc0c8477f with a more cache-friendly "depth-first" order.
Some hard thinking came up with a depth-first algorithm which defers
merges as little as possible while avoiding bad merges.  This saves
0.2*n compares, averaged over all sizes.

The code size increase is minimal (64 bytes on x86-64, reducing the net
savings to 26%), but the comments expanded significantly to document the
clever algorithm.

TESTING NOTES: I have some ugly user-space benchmarking code which I
used for testing before moving this code into the kernel.  Shout if you
want a copy.

I'm running this code right now, with CONFIG_TEST_SORT and
CONFIG_TEST_LIST_SORT, but I confess I haven't rebooted since the last
round of minor edits to quell checkpatch.  I figure there will be at
least one round of comments and final testing.

This patch (of 5):

Rather than having special-case swap functions for 4- and 8-byte
objects, special-case aligned multiples of 4 or 8 bytes.  This speeds up
most users of sort() by avoiding fallback to the byte copy loop.

Despite what ca96ab859ab4 ("lib/sort: Add 64 bit swap function") claims,
very few users of sort() sort pointers (or pointer-sized objects); most
sort structures containing at least two words.  (E.g.
drivers/acpi/fan.c:acpi_fan_get_fps() sorts an array of 40-byte struct
acpi_fan_fps.)

The functions also got renamed to reflect the fact that they support
multiple words.  In the great tradition of bikeshedding, the names were
by far the most contentious issue during review of this patch series.

x86-64 code size 872 -> 886 bytes (+14)

With feedback from Andy Shevchenko, Rasmus Villemoes and Geert
Uytterhoeven.

Link: http://lkml.kernel.org/r/f24f932df3a7fa1973c1084154f1cea596bcf341.1552704200.git.lkml@sdf.org
Signed-off-by: George Spelvin <lkml@sdf.org>
Acked-by: Andrey Abramov <st5pub@yandex.ru>
Acked-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Reviewed-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Cc: Geert Uytterhoeven <geert@linux-m68k.org>
Cc: Daniel Wagner <daniel.wagner@siemens.com>
Cc: Don Mullis <don.mullis@gmail.com>
Cc: Dave Chinner <dchinner@redhat.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

---
## [TerraLindaHighSchool/memorium-memorium-production-team](https://github.com/TerraLindaHighSchool/memorium-memorium-production-team)@[97ed4ef350...](https://github.com/TerraLindaHighSchool/memorium-memorium-production-team/commit/97ed4ef35003ac75fbc7f016dd93a6b2599de8ec)
#### Tuesday 2022-03-15 17:52:58 by WoofMeister

i hate you stupid github you need to die jk lol not really haha funny also i put angyboi inside unity bc hawt

---
## [ar-sht/whichrewardscard](https://github.com/ar-sht/whichrewardscard)@[0b86f77f32...](https://github.com/ar-sht/whichrewardscard/commit/0b86f77f32c41e30500d73bd70d1e11d3f60a554)
#### Tuesday 2022-03-15 18:59:04 by Ari Shtein

Added CSS & Fixed weird idiot stuff in HTML

- CSS changes
  - Results look less like crap
  - Submit button is no longer weird
  - Navbar is lowercase and Comfortaa is used for cleaner look

- HTML changes
  - All supported airlines are an option
  - Fixed rambly incorrect 'About' section
  - New icon displayed in the browser tab
  - Converted minilogo to transparent PNG

I think that't about it, gonna come back to this soon to add cookies and
and databases and all that as soon as I learn how.

Ok, thanks to the US government (Bureau of Transportation Statistics)
for all the data I got from them.

Finally, hope you enjoy looking at how awful the JS was future me!

---
## [SkyN9ne/git](https://github.com/SkyN9ne/git)@[96bfb2d8ce...](https://github.com/SkyN9ne/git/commit/96bfb2d8ce221d31b3da08a49a455e316dbef7fb)
#### Tuesday 2022-03-15 22:01:26 by Jeff King

run-command: unify signal and regular logic for wait_or_whine()

Since 507d7804c0 (pager: don't use unsafe functions in signal handlers,
2015-09-04), we have a separate code path in wait_or_whine() for the
case that we're in a signal handler. But that code path misses some of
the cases handled by the main logic.

This was improved in be8fc53e36 (pager: properly log pager exit code
when signalled, 2021-02-02), but that covered only case: actually
returning the correct error code. But there are some other cases:

  - if waitpid() returns failure, we wouldn't notice and would look at
    uninitialized garbage in the status variable; it's not clear if it's
    possible to trigger this or not

  - if the process exited by signal, then we would still report "-1"
    rather than the correct signal code

This latter case even had a test added in be8fc53e36, but it doesn't
work reliably. It sets the pager command to:

  >pager-used; test-tool sigchain

The latter command will die by signal, but because there are multiple
commands, there will be a shell in between. And it's the shell whose
waitpid() call will see the signal death, and it will then exit with
code 143, which is what Git will see.

To make matters even more confusing, some shells (such as bash) will
realize that there's nothing for the shell to do after test-tool
finishes, and will turn it into an exec. So the test was only checking
what it thought when /bin/sh points to a shell like bash (we're relying
on the shell used internally by Git to spawn sub-commands here, so even
running the test under bash would not be enough).

This patch adjusts the tests to explicitly call "exec" in the pager
command, which produces a consistent outcome regardless of shell. Note
that without the code change in this patch it _should_ fail reliably,
but doesn't. That test, like its siblings, tries to trigger SIGPIPE in
the git-log process writing to the pager, but only do so racily. That
will be fixed in a follow-on patch.

For the code change here, we have two options:

  - we can teach the in_signal code to handle WIFSIGNALED()

  - we can stop returning early when in_signal is set, and instead
    annotate individual calls that we need to skip in this case

The former is a simpler patch, but means we're essentially duplicating
all of the logic. So instead I went with the latter. The result is a
bigger patch, and we do run the risk of new code being added but
forgetting to handle in_signal. But in the long run it seems more
maintainable.

I've skipped any non-trivial calls for the in_signal case, like calling
error(). We'll also skip the call to clear_child_for_cleanup(), as we
were before. This is arguably the wrong thing to do, since we wouldn't
want to try to clean it up again. But:

  - we can't call it as-is, because it calls free(), which we must avoid
    in a signal handler (we'd have to pass in_signal so it can skip the
    free() call)

  - we'll only go through the list of children to clean once, since our
    cleanup_children_on_signal() handler pops itself after running (and
    then re-raises, so eventually we'd just exit). So this cleanup only
    matters if a process is on the cleanup list _and_ it has a separate
    handler to clean itself up. Which is questionable in the first place
    (and AFAIK we do not do).

  - double-cleanup isn't actually that bad anyway. waitpid() will just
    return an error, which we won't even report because of in_signal.

Signed-off-by: Jeff King <peff@peff.net>
Signed-off-by: Junio C Hamano <gitster@pobox.com>

---
## [AllieLikesAstolfo/tgstation](https://github.com/AllieLikesAstolfo/tgstation)@[4051ad647e...](https://github.com/AllieLikesAstolfo/tgstation/commit/4051ad647e3e94ea5c722cee18cecf350270ab6f)
#### Tuesday 2022-03-15 22:32:59 by LemonInTheDark

Space drifting fixes and cleanup (#64915)

* Fixes infi pushing off something in space

Right now you can just push "into" a dense object forever, and depending
on your move rate, just kinda glide

We can fix that by checking if we're trying to push "off" something
we're moving towards

* Makes pushing off something shift it instantly

Currently if you kick off something in space it waits the delay of the
move to start drifting. Looks dumb, let's not

* Updates backup movement to properly account for directional windows. GOD I HATE DIRECTIONAL DENSITY SHOOOOOT MEEEEEEEEEEEEEEEEEEE

* Uses range instead of orange so standing on the same tile as a directional counts properly, rather then suddenly entering a drift state. I hate it here

* Ensures all args are named, updates implementations of the proc with the new arg

---
## [benbennybenben/ben_2022](https://github.com/benbennybenben/ben_2022)@[d59b331327...](https://github.com/benbennybenben/ben_2022/commit/d59b3313272af29de36f5805c1e41b42d1d39b71)
#### Tuesday 2022-03-15 22:49:09 by Ben Gregory

Merge branch 'master' of https://github.com/benbennybenben/ben_2022
this is first push, fuck you, just push this shit

---
## [tgstation/tgstation](https://github.com/tgstation/tgstation)@[770ef81a1f...](https://github.com/tgstation/tgstation/commit/770ef81a1fb271572d711e7a05dbce62564ca3b0)
#### Tuesday 2022-03-15 22:49:27 by John Willard

makes podpeople call parent (#65362)


About The Pull Request

kinda fucked up that it doesnt.
Also while checking this PR I noticed other species also don't, kinda screwed up world we live in...
Why It's Good For The Game

Parent's spec_life is what checks if you have nobreath, and in which case it will remove all your oxygen damage and, if in crit, give you brute damage instead. Not having this makes you basically not take damage while in crit, which I think shouldn't be the case.
Changelog

cl
fix: Podpeople now take self-respiration into account when taking damage from critical condition, like most other species.
/cl

---

# [<](2022-03-14.md) 2022-03-15 [>](2022-03-16.md)

