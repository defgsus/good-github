# [<](2022-01-07.md) 2022-01-08 [>](2022-01-09.md)

1,410,369 events recorded by [gharchive.org](https://www.gharchive.org/) of which 1,410,369 were push events containing 1,954,127 commit messages that amount to 123,562,945 characters filtered with [words.py@e23d022007...](https://github.com/defgsus/good-github/blob/e23d022007992279f9bcb3a9fd40126629d787e2/src/words.py) to these 23 messages:


## [Fikou/tgstation](https://github.com/Fikou/tgstation)@[96b81f6c7f...](https://github.com/Fikou/tgstation/commit/96b81f6c7f40fb9e103646e642a0e554a3841c18)
#### Saturday 2022-01-08 00:11:33 by Wallem

Refactors Sign Language & Fixes Inconsistency (#62836)

Refactors Sign Language code so instead of copy-pasting the same giant wall of checks we can just use a proc.
Also now checks to see if your limb is disabled, which fixes people with disabled robotic limbs being able to sign still.
Finally, the tongue only has ORGAN_UNREMOVABLE if you attained it from the trait. I've been told that the tongue could be attained from meateors and I think that's funny as hell so I swapped that over.

---
## [AllyMarthaJ/mcm4csharp](https://github.com/AllyMarthaJ/mcm4csharp)@[e1a9508f3a...](https://github.com/AllyMarthaJ/mcm4csharp/commit/e1a9508f3a1002480140da92f353891cb8f6828f)
#### Saturday 2022-01-08 01:05:47 by AllyMarthaJ

Note to self, Ally sucks at coding at night. Debugging + fixes.

---
## [chrisbobbe/zulip-mobile](https://github.com/chrisbobbe/zulip-mobile)@[d7d91dcfb6...](https://github.com/chrisbobbe/zulip-mobile/commit/d7d91dcfb63eae31cc39526337649743699da6a7)
#### Saturday 2022-01-08 01:42:25 by Chris Bobbe

keyboard-avoiding ios: Begin forking RN's KeyboardAvoidingView

From
  node_modules/react-native/Libraries/Components/Keyboard/KeyboardAvoidingView.js
with v0.64.2 of `react-native`, adjusting only the license pointer
in the copyright header.

We'll make the fork usable soon, not in this pure copy-paste commit.

We don't like this implementation (see, e.g., 70eca0716, which took
a lot of painstaking investigation), but we want to fix #5162 as
soon as we can.

For a not-quite-perfect attempt at reimplementing it from scratch in
our style (and with #5162 fixed), see my branch
`reimplement-keyboard-avoiding-view` on GitHub.

We can't easily make a jank-free solution in a normal React Native
way because we can't have perfect information about the layout on
every frame. React Native exposes it to us by consuming event
listeners and providing asynchronous query functions, and we end up
having to learn about different aspects of the layout at different
times.

The best hope for a jank-free solution is to use native iOS APIs. If
we're feeling adventurous and we find the time, we should really try
hard to make React Native play along with iOS's
`keyboardLayoutGuide` API (iOS 15+).

The first four minutes of this video should be really useful
background on the `keyboardLayoutGuide` feature:
  https://developer.apple.com/videos/play/wwdc2021/10259/

It works within iOS's "Auto Layout" system:
  https://developer.apple.com/library/archive/documentation/UserExperience/Conceptual/AutolayoutPG/index.html

But then we'd have to go and

(a) Make a native component:
      https://reactnative.dev/docs/native-components-ios

(b) Figure out how to make React Native's propagate-from-JavaScript
    layout system (Yoga) not fight with the native iOS layout
    system, including "Auto Layout" and `keyboardLayoutGuide`.
    Possibly we can pick up some clues from
    `react-native-safe-area-context` for this?

From another angle, since the jank is regularly seen on screen
orientation changes between portrait and landscape, we could
consider just unsupporting the landscape orientation. On very common
device types, like phones, it's just not as easy to use the app in,
especially for composing messages.

---
## [jkcarney/wakatime-bot](https://github.com/jkcarney/wakatime-bot)@[d42c87b1b3...](https://github.com/jkcarney/wakatime-bot/commit/d42c87b1b30f757b85a74ac67ce1576e6d0ea08b)
#### Saturday 2022-01-08 01:44:30 by Joshua Carney

Merge pull request #5 from jkcarney/Auth0

HOLY SHIT AUTH0 FINALLY FUCKIN WORKS

---
## [cvasqxz/VVVVVV](https://github.com/cvasqxz/VVVVVV)@[57dca99a4e...](https://github.com/cvasqxz/VVVVVV/commit/57dca99a4e9fb07185de90cd70def42b17549fb6)
#### Saturday 2022-01-08 01:58:52 by Misa

Ax OverlaySurfaceKeyed(), set proper foregroundBuffer blend mode

So, earlier in the development of 2.0, Simon Roth (I presume)
encountered a problem: Oh no, all my backgrounds aren't appearing! And
this is because my foregroundBuffer, which contains all the drawn tiles,
is drawing complete black over it!

So he had a solution that seems ingenius, but is actually really really
hacky and super 100% NOT the proper solution. Just, take the
foregroundBuffer, iterate over each pixel, and DON'T draw any pixel
that's 0xDEADBEEF. 0xDEADBEEF is a special signal meaning "don't draw
this pixel". It is called a 'key'.

Unfortunately, this causes a bug where translucent pixels on tiles
(pixels between 0% and 100% opacity) didn't get drawn correctly. They
would be drawn against this weird blue color.

Now, in #103, I came across this weird constant and decided "hey, this
looks awfully like that weird blue color I came across, maybe if I set
it to 0x00000000, i.e. complete and transparent black, the issue will be
fixed". And it DID appear to be fixed. However, I didn't look too
closely, nor did I test it that much, and all that ended up doing was
drawing the pixels against black, which more subtly disguised the
problem with translucent pixels.

So, after some investigation, I noticed that BlitSurfaceColoured() was
drawing translucent pixels just fine. And I thought at the time that
there was something wrong with BlitSurfaceStandard(), or something.
Further along later I realized that all drawn tiles were passing through
this weird OverlaySurfaceKeyed() function. And removing it in favor of a
straight SDL_BlitSurface() produced the bug I mentioned above: Oh no,
all the backgrounds don't show up, because my foregroundBuffer is
drawing pure black over them!

Well... just... set the proper blend mode for foregroundBuffer. It
should be SDL_BLENDMODE_BLEND instead of SDL_BLENDMODE_NONE.

Then you don't have to worry about your transparency at all. If you did
it right, you won't have to resort this hacky color-keying business.

*sigh*

---
## [elireisman/codespaces_dotfiles](https://github.com/elireisman/codespaces_dotfiles)@[d6104e8e77...](https://github.com/elireisman/codespaces_dotfiles/commit/d6104e8e77f8b07859b18886ced75e0385f95f6b)
#### Saturday 2022-01-08 02:02:23 by Jakob Homan

Required in all bashrc files.

An author ought to consider himself, not as a gentleman who gives a private or eleemosynary treat, but rather as one who keeps a public ordinary, at which all persons are welcome for their money. In the former case, it is well known that the entertainer provides what fare he pleases; and though this should be very indifferent, and utterly disagreeable to the taste of his company, they must not find any fault; nay, on the contrary, good breeding forces them outwardly to approve and to commend whatever is set before them. Now the contrary of this happens to the master of an ordinary. Men who pay for what they eat will insist on gratifying their palates, however nice and whimsical these may prove; and if everything is not agreeable to their taste, will challenge a right to censure, to abuse, and to d—n their dinner without controul.

To prevent, therefore, giving offence to their customers by any such disappointment, it hath been usual with the honest and well-meaning host to provide a bill of fare which all persons may peruse at their first entrance into the house; and having thence acquainted themselves with the entertainment which they may expect, may either stay and regale with what is provided for them, or may depart to some other ordinary better accommodated to their taste.

As we do not disdain to borrow wit or wisdom from any man who is capable of lending us either, we have condescended to take a hint from these honest victuallers, and shall prefix not only a general bill of fare to our whole entertainment, but shall likewise give the reader particular bills to every course which is to be served up in this and the ensuing volumes.

The provision, then, which we have here made is no other than Human Nature. Nor do I fear that my sensible reader, though most luxurious in his taste, will start, cavil, or be offended, because I have named but one article. The tortoise—as the alderman of Bristol, well learned in eating, knows by much experience—besides the delicious calipash and calipee, contains many different kinds of food; nor can the learned reader be ignorant, that in human nature, though here collected under one general name, is such prodigious variety, that a cook will have sooner gone through all the several species of animal and vegetable food in the world, than an author will be able to exhaust so extensive a subject.

An objection may perhaps be apprehended from the more delicate, that this dish is too common and vulgar; for what else is the subject of all the romances, novels, plays, and poems, with which the stalls abound? Many exquisite viands might be rejected by the epicure, if it was a sufficient cause for his contemning of them as common and vulgar, that something was to be found in the most paltry alleys under the same name. In reality, true nature is as difficult to be met with in authors, as the Bayonne ham, or Bologna sausage, is to be found in the shops.

But the whole, to continue the same metaphor, consists in the cookery of the author; for, as Mr Pope tells us—

    “True wit is nature to advantage drest;
    What oft was thought, but ne'er so well exprest.”
 
The same animal which hath the honour to have some part of his flesh eaten at the table of a duke, may perhaps be degraded in another part, and some of his limbs gibbeted, as it were, in the vilest stall in town. Where, then, lies the difference between the food of the nobleman and the porter, if both are at dinner on the same ox or calf, but in the seasoning, the dressing, the garnishing, and the setting forth? Hence the one provokes and incites the most languid appetite, and the other turns and palls that which is the sharpest and keenest.

In like manner, the excellence of the mental entertainment consists less in the subject than in the author's skill in well dressing it up. How pleased, therefore, will the reader be to find that we have, in the following work, adhered closely to one of the highest principles of the best cook which the present age, or perhaps that of Heliogabalus, hath produced. This great man, as is well known to all lovers of polite eating, begins at first by setting plain things before his hungry guests, rising afterwards by degrees as their stomachs may be supposed to decrease, to the very quintessence of sauce and spices. In like manner, we shall represent human nature at first to the keen appetite of our reader, in that more plain and simple manner in which it is found in the country, and shall hereafter hash and ragoo it with all the high French and Italian seasoning of affectation and vice which courts and cities afford. By these means, we doubt not but our reader may be rendered desirous to read on for ever, as the great person just above-mentioned is supposed to have made some persons eat.

Having premised thus much, we will now detain those who like our bill of fare no longer from their diet, and shall proceed directly to serve up the first course of our history for their entertainment.

---
## [Zeodexic/tsorcRevamp](https://github.com/Zeodexic/tsorcRevamp)@[de746cf2a2...](https://github.com/Zeodexic/tsorcRevamp/commit/de746cf2a22026a9c4e8f1f6d447ae8c9f4fd265)
#### Saturday 2022-01-08 02:52:09 by timhjersted

Fairly large amount of changes

-Hero of Lumia reworked into a boss with new attacks; chance to spawn in HM only (scripted event planned but not implemented)
-Taurus Knight (SHM mini-boss) has new attacks
-Hellkite Dragon, Ancient Jungle Wyvern and Jungle Wyvern Juvenile resprited, with attack tweaks
-Hydris Necromancer resprited, with reworked stats and ally spawns
-Hollow enemies no longer spawn in corruption or crimson
-Stat and spawn tweaks for several enemies
-More enemies have HM or SHM stat variants
-More enemies have dusts or sprites to telegraph attacks (some sprites removed in favor of dusts only; looks better and easier to add more)
-Cursed Skulls blocked, for a fresh, less annoying dungeon experience amiright?
-Enemies now spawn in the catacombs (fixed bug)
-Hypnotic Disrupter projectile now causes crippled debuff
-Powerful Curse buildup gives crimson drain buff for shorter duration
-The Destroyer resprites added but need to be implemented

---
## [nine7nine/linux-cachyos-nspa](https://github.com/nine7nine/linux-cachyos-nspa)@[05c7048476...](https://github.com/nine7nine/linux-cachyos-nspa/commit/05c704847648d58fae56c10cda797b34ec0d8a43)
#### Saturday 2022-01-08 04:03:10 by ninez

Remove Xanmod's stupid autogroup patch.

 this feature was always half-baked and shouldn't be enabled on any system that
 may be using RT apps like Jackd... it tends to degrade performance.

 nice feature in theory. not that great in practice. certainly not a sane
 default. I also wonder what the point of this patch is; the feature can already
 be enabled via kernel commandline or sysctl. this patch is pointless.

Signed-off-by: ninez <johnstonljordan@gmail.com>

---
## [jtpg0/nu-fetch](https://github.com/jtpg0/nu-fetch)@[d8f3525a14...](https://github.com/jtpg0/nu-fetch/commit/d8f3525a14ce22b8dfb3adcc40ebcb4800869746)
#### Saturday 2022-01-08 05:07:36 by jtpg

i think this will also make arch work for artix but i dont care hahah fuck you jews

---
## [myerfire/Yarn](https://github.com/myerfire/Yarn)@[7102f86173...](https://github.com/myerfire/Yarn/commit/7102f86173f24d40d3b69882a60e3044a0a530b9)
#### Saturday 2022-01-08 08:00:37 by myer

better way to get picture (arrays suck javascript fuck you)

---
## [theodorecooper/awesome-ddos-tools](https://github.com/theodorecooper/awesome-ddos-tools)@[88612c405c...](https://github.com/theodorecooper/awesome-ddos-tools/commit/88612c405c947f647d01a242de8a350fe86e33d0)
#### Saturday 2022-01-08 10:05:56 by Theodore Cooper

Add a 700+ starred stupid trash.

https://github.com/Ha3MrX/DDos-Attack
You should try this shitty script I just found.
"It's fake, I think it's a joke. Look at the code. Well everyone using this is prob like 5 years old and don't even know how to write a head title in HTML lol"

"Thanks Ha3MrX,
Ive been looking for DDOS tools for so so so long and all the shit i try and get are a virus or fake but thanks this script works and the source code is so easy to install ;)
I cant wait to use it some more, if people talk shit well.... Thanks to u i can hit people with 169Gb/per second. but lol i love the 69 but im so glad this powerful took works."

---
## [djfox20/Codeacademy-Projects](https://github.com/djfox20/Codeacademy-Projects)@[6c17d4124c...](https://github.com/djfox20/Codeacademy-Projects/commit/6c17d4124c5fdefc9b3b9531207476854c7b1b78)
#### Saturday 2022-01-08 11:25:04 by ⚜Adolfo Quevedo

Create LISTS_review.py

WORKING WITH LISTS IN PYTHON
Review
In this lesson, we learned how to:

Add elements to a list by index using the .insert() method.
Remove elements from a list by index using the .pop() method.
Generate a list using the range() function.
Get the length of a list using the len() function.
Select portions of a list using slicing syntax.
Count the number of times that an element appears in a list using the .count() method.
Sort a list of items using either the .sort() method or sorted() function.
As you go through the exercises, feel free to use print() to see changes when not explicitly asked to do so.

Instructions
1.
Our friend Jiho has been so successful in both the flower and grocery business that she has decided to open a furniture store.

Jiho has compiled a list of inventory items into a list called inventory and wants to know a few facts about it.

First, how many items are in the warehouse?

Save the answer to a variable called inventory_len.

Checkpoint 2 Passed

Stuck? Get a hint
2.
Select the first element in inventory. Save it to a variable called first.

Checkpoint 3 Passed

Stuck? Get a hint
3.
Select the last element from inventory. Save it to a variable called last.

Checkpoint 4 Passed

Stuck? Get a hint
4.
Select items from the inventory starting at index 2 and up to, but not including, index 6.

Save your answer to a variable called inventory_2_6.

Checkpoint 5 Passed

Stuck? Get a hint
5.
Select the first 3 items of inventory. Save it to a variable called first_3.

Checkpoint 6 Passed

Stuck? Get a hint
6.
How many 'twin bed's are in inventory? Save your answer to a variable called twin_beds.

Checkpoint 7 Passed

Stuck? Get a hint
7.
Remove the 5th element in the inventory. Save the value to a variable called removed_item.

Checkpoint 8 Passed

Stuck? Get a hint
8.
There was a new item added to our inventory called "19th Century Bed Frame".

Use the .insert() method to place the new item as the 11th element in our inventory.

Checkpoint 9 Passed

Stuck? Get a hint
9.
Sort inventory using the .sort() method or the sorted() function.

Remember, the sorted() function doesn’t change the original list — it creates a new list with the elements properly sorted. If you use sorted() you’ll have to set inventory equal to the value returned by sorted().

Print inventory to see the result.

---
## [TheDisabledKid/FNF-PsychEngine](https://github.com/TheDisabledKid/FNF-PsychEngine)@[cd83cffb4f...](https://github.com/TheDisabledKid/FNF-PsychEngine/commit/cd83cffb4f7906e4621c6d9412ecf2f81c8ad863)
#### Saturday 2022-01-08 12:15:57 by ShadowMario

Merge pull request #1053 from ArfieCat/main

remember my god damn volume already

---
## [mrakgr/The-Spiral-Language](https://github.com/mrakgr/The-Spiral-Language)@[c06e7db320...](https://github.com/mrakgr/The-Spiral-Language/commit/c06e7db3202741a22ff5abf097fc19bdc36c0eda)
#### Saturday 2022-01-08 12:17:52 by Marko Grdinić

"11:50am. Since I got up this late, let me just see if I can find any transfer attribute videos.

https://youtu.be/TLwL1UTdtHg
[Tut] Field & Transfer Attribute node Explained - Blender Geometry Nodes 3.0 Field

Bradley had it.

Let me go for it.

https://youtu.be/2qWnRRkV9Zk
[Tut] 2 Uses of Capture Attribute Node Explained - Blender Geometry Nodes 3.0 Field

I got this in the video. I'll leave it until after I am finished.

https://youtu.be/TLwL1UTdtHg?t=352

Ah, so capture attribute does not transfer to different geometries.

I am going to have to play with this example a bit. After I've watched the video, I am going to try to replicate it without looking just to see whether I have understood.

https://youtu.be/TLwL1UTdtHg?t=532

Actually, I think I understand it completely. There is no real need to play with it. I'll just move to the next thing.

https://youtu.be/TLwL1UTdtHg?t=543

He says he is using the index mode. How is that useful?

12:05pm. Ok, I think I see it.

Also I've finally figured out what the viewer node is supposed to be doing.

12:10pm. I do not feel a need for it, but let me watch the capture attribute tutorial.

Hmmm, if I have points, even if endpoint selector failed me, could I just compare based on the index of it and use that as the selector?

https://youtu.be/2qWnRRkV9Zk?t=249

Ah, I can't forget about this node. I could very easily hack it with the mesh to points and make a mistake that way.

https://youtu.be/2qWnRRkV9Zk?t=288

I ran into this problem yesterday and figured it out.

https://youtu.be/2qWnRRkV9Zk?t=387

I really need to figure out what align euler to vector is doing.

https://youtu.be/X80M0q0gxIE
Quaternion or Euler, understanding rotation in Blender

I'll watch this after I am done.

https://youtu.be/2qWnRRkV9Zk?t=476

I wouldn't have thought mixing edge and face positions would work. How does that make sense?

Hmmm, why did he split the edges?

https://youtu.be/2qWnRRkV9Zk?t=538

Normals are associated with the face. I totally thought they were associated with the vertices.

12:30pm. Focus me, focus. Stop reading the liches thread.

https://youtu.be/X80M0q0gxIE
Quaternion or Euler, understanding rotation in Blender

Let me watch this.

https://youtu.be/0E1K9j9zoik
[Tut] Align Euler to Vector Node Explained - Blender Geometry Nodes 3.0 Field

I do not like that animation video, as I am not interested in amination. Let me watch this again. It is Bradley once more.

12:40pm. https://youtu.be/0E1K9j9zoik?t=183

This is rather instructive. Fine I'll subscribe to your channel.

I see now why normals can be converted to rotation using this. Because they just point in a certain direction.

12:45pm. https://youtu.be/0E1K9j9zoik?t=274

This is a good point, just what is rotation doing here?

Actually, does that connect to the rotation attribute for the geometry?

https://youtu.be/0E1K9j9zoik?t=316

Ohhh, I did not expect the factor to go into the negative. Yeah, the factor is not the factor between rotation and the vector. That can't be it.

12:50pm. https://youtu.be/0E1K9j9zoik?t=438

Not what I expected. I thought that the inputs should be all zeroes in order to make it point towards the origin...

Actually if you think that there is already rotation...

No way, there is no wait that is right.

Rather the normals themselves have to be pointing towards the origin.

If I put all zeroes as a vector it would not have any basis for rotation...

But didn't I assume that rotation is attached to the original geometry. Then it should work, right?

Ah, I'll play around with it later. Right now let me just watch it.

1pm. https://youtu.be/0E1K9j9zoik?t=690

I had no idea these constraints existed. This is ridiculous. The amount of things I do not know in Blender despite 3 full months of studying it is crazy.

https://youtu.be/0E1K9j9zoik?t=783

I didn't understand the previous part with two euler nodes, but I understand this.

https://youtu.be/57FaqP_Q36w
EVERY NODE in the Erindale Toolkit - Geometry Nodes

I won't watch this, but I'll keep the addon in mind. Let me move on.

1:10pm. By move on I mean have breakfast. This was a nice morning session.

Let's see, I really should play with subdivide edges in order to increase my understanding on how interpolating between faces and points works. Right now it does not make sense because there should be a different amount of them, so I do not know how Blender knows to decide between collections of different sizes. I need to play with that example. Maybe with align euler as well as I do not understand why adding that second vector makes a difference.

I should investigate that.

1:15pm. Sigh, I really want to start doing my own things, but I also want to increase my understanding. A few days ago I was complaining that the phyisics sims tutorials were extremely tedious, but I am findings these quite interesting. I am quite into this.

Well, I'll get through it at some point. There is not an infinite amount of tutorials to go through. Let me have breakfast here."

---
## [Moffein/SniperClassic](https://github.com/Moffein/SniperClassic)@[f1548b617e...](https://github.com/Moffein/SniperClassic/commit/f1548b617e568e9dd694b7b2163b5e0d6df59515)
#### Saturday 2022-01-08 12:59:15 by TheTimeSweeper

oh yea the rad quintel skin.

feel like we've kinda abandoned bruh's shit, eventually this will get in and your his will be appreciated as it should be

---
## [blahblahbal/ExxoAvalonOrigins](https://github.com/blahblahbal/ExxoAvalonOrigins)@[d3af068daf...](https://github.com/blahblahbal/ExxoAvalonOrigins/commit/d3af068daf7f679524fb8cb0ba0749373ba3eeb7)
#### Saturday 2022-01-08 15:48:22 by Brian Hansen

A lot more stuff lol

-Buffed Vertex of Excalibur a little
-Buffed Armageddon Slime
-Phantasm lasers no longer sound across the entire world
-Juggernauts no longer spawn in the dungeon, and also spawn less often
-Placeable evil altars can now be picked back up
-Armageddon Slime's defeat now properly saves
-Dark Matter spreading no longer spreads contagion grass (lol)
-Viris gores are now properly scaled down
-Peace Candles can now be crafted from Bismuth
-Sulphur now glows with Spelunker effect
-Holybird, Sweetstem, Barfbush, and Bloodberry spawns are now reduced
-Dust from Ultrabright Razorblade Bullets has been reduced
-Oblivion Ore generating upon entering SHM has increased size of the patches
-Spirit Poppies now drop at a lower rate from Dungeon Spirits
-Blah Staff has been completely fixed (tweaks may come at some point)
-Eye of Oblivion now correctly requires an Alien Apparatus instead of the Device
-Vampire Harpy Wings can now be crafted from Dark Matter Tokens

---
## [Aliscans/crawl](https://github.com/Aliscans/crawl)@[a95d5098ad...](https://github.com/Aliscans/crawl/commit/a95d5098adf834630e626d9093620f0033ea286c)
#### Saturday 2022-01-08 16:17:30 by hellmonk

feat: invert the wanderer paradigm

Wn is a really cool "random start" option, but has a couple annoyances.
You can get some truly awful starts due to the way skill points are
assigned, and are basically guaranteed to have awful stats due to the
way stat points are assigned. This commit tries to rectify those problems
and adds some new things that Wn can get at game start.

The new model assigns the equipment first, based entirely on species
apts, then assigns stats and skills afterward depending on what we got.
It tends to lead to more lopsided (ie useful) stat distributions and
higher levels of skill in things that are useful, though these are by no
means guaranteed.

Along with changing the model this commit makes the following
adjustments to Wn starting equipment, offering a wider mix of cool
things:

- "good weapon" can get an upgraded base type or vorpal brand instead of enchant
- "good armour" can roll chain instead of +2 scale, and gives acid scales for
  non-chain users
- "good shield" now rolls an enchanted buckler or prot buckler, instead
  of a kite shield
- "good magic school" rolls a 3-spell randbook instead of a background
  starting book
- "good spellcasting" rolls a 4-spell any-school randbook (capped at
  level 3)
- "good stealth" can give either a +2 dagger and a consumable or a plain
  dagger and a mix of atropa and datura darts
- "decent armour" can role chain or ring, with a plain aux armour for odd
  shapes (all scales and tla are too strong for decent items)
- "decent dodging" and "decent stealth" can give an aux armour if a good
  armour wasn't received
- "decent fighting" can give an extra weapon plus if a weapon is
  available
- it is possible to start with a hunting sling and sling skill
- throwing can start with boomerangs
- it is possible to get aux armour (and likely for oddly shaped species)
- a wider variety of potions and scrolls can be rolled
- a wider variety of evocables can be rolled
- Wn that start with sandblast also start with stones

[ Committer's notes: Thanks to gammafunk, kate, and PleasingFungus for
  input in this overhaul. ]

---
## [RESPULTE/PyTree](https://github.com/RESPULTE/PyTree)@[53b5eec359...](https://github.com/RESPULTE/PyTree/commit/53b5eec359dedbf3b62609f4a1673c8af6cc8fa5)
#### Saturday 2022-01-08 18:05:10 by RESPULTE

I've honestly thought of punching a hole in my comp a terrifying amount of times after finding and debugging rogue infinite while loop for 5 hours str8, should almost be done now, some more clean up with the overall structure of the code and a few more user-friendly methods and that should do it. Note-to-self: never trust a while-loop >>:(

---
## [oumajgad/BlackICE](https://github.com/oumajgad/BlackICE)@[34f29170b8...](https://github.com/oumajgad/BlackICE/commit/34f29170b8986ab62175a548b33330bba5c14fd6)
#### Saturday 2022-01-08 20:09:53 by Dsafe1

holy shit lua doesnt preserve order of insertion i hate my life

---
## [visionizer/esque](https://github.com/visionizer/esque)@[b7491cd570...](https://github.com/visionizer/esque/commit/b7491cd57018e7a40ef12edeb7a241ea9a834dad)
#### Saturday 2022-01-08 20:28:41 by Clemens Schütz

[build] Finished the Python-based build system

This commit introduces further changes in `y.py`.
From now on, it is the primary build system for Esque.
While some may wonder why this is neccessary, a simple
look at more complex projects, such as the linux kernel
serves an answer quickly. The Makefiles have devolved,
from a once great build system, to an unreadable
mess. Python is in today's age, in my opinion, the
Lingua Franca of programmers. I have yet to meet
a person who did not understand, at least partly, python
code.
A kernel on it's own is a complex project, one that
we should not try to overcomplicate by using a build-
system that is guaranteed to be unreadable for many
after a certain amount of time.

---
## [MrJamesGaming/FtcRobotController](https://github.com/MrJamesGaming/FtcRobotController)@[566a245763...](https://github.com/MrJamesGaming/FtcRobotController/commit/566a245763404529185a2c9011fa9515d98941ce)
#### Saturday 2022-01-08 20:36:10 by ProDCG

LearnOpenCV Home Getting Started Courses Resources AI Consulting About Search for: Search here... SEARCH BUTTON Skip to primary navigation Skip to main content Skip to primary sidebar Skip to footer Subscribe for More Download Code (C++ / Python) Getting Started with OpenCV Introduction To OpenCV Read, Display and Write an Image using OpenCV Reading and Writing Videos using OpenCV Image Resizing with OpenCV Cropping an Image using OpenCV Image Rotation and Translation Using OpenCV Annotating Images Using OpenCV Color spaces in OpenCV (C++ / Python) Image Filtering Using Convolution in OpenCV Image Thresholding in OpenCV Blob Detection Using OpenCV ( Python, C++ ) Edge Detection Using OpenCV Mouse and Trackbar using OpenCV GUI Contour Detection using OpenCV Simple Background Estimation in Videos using OpenCV (C++/Python) Deep Learning with OpenCV DNN Module: A Definitive Guide Subscribe for More Download Code (C++ / Python) Color spaces in OpenCV (C++ / Python) Changes in color by varying Illumination Changes in color by varying Illumination In this tutorial, we will learn about popular colorspaces used in Computer Vision and use it for color based segmentation. We will also share demo code in C++ and Python.  In 1975, the Hungarian Patent HU170062 introduced a puzzle with just one right solution out of 43,252,003,274,489,856,000 (43 quintillion) possibilities. This invention now known as the Rubik’s Cube took the world by storm selling more than 350 million by January 2009.  So, when a few days back my friend, Mark, told me about his idea of building a computer vision based automated Rubik’s cube solver, I was intrigued. He was trying to use color segmentation to find the current state of the cube. While his color segmentation code worked pretty well during evenings in his room, it fell apart during daytime outside his room!  He asked me for help and I immediately understood where he was going wrong. Like many other amateur computer vision enthusiasts, he was not taking into account the effect of different lighting conditions while doing color segmentation. We face this problem in many computer vision applications involving color based segmentation like skin tone detection, traffic light recognition etc. Let’s see how we can help him build a robust color detection system for his robot.  The article is organized as follows:  First we will see how to read an image in OpenCV and convert it into different color spaces and see what new information do the different channels of each color space provide us. We will apply a simple color segmentation algorithm as done by Mark and ponder over its weaknesses. Then we will jump into some analytics and use a systematic way to choose: The right color space. The right threshold values for segmentation. See the results The different color spaces In this section, we will cover some important color spaces used in computer vision. We will not describe the theory behind them as it can be found on Wikipedia. Instead, we will develop a basic intuition and learn some important properties which will be useful in making decisions later on.  Let us load 2 images of the same cube. It will get loaded in BGR format by default. We can convert between different colorspaces using the OpenCV function cvtColor() as will be shown later.  Download Code To easily follow along this tutorial, please download code by clicking on the button below. It's FREE!  Download Code 1 #python 2 bright = cv2.imread('cube1.jpg') 3 dark = cv2.imread('cube8.jpg') 1 //C++ 2 bright = cv::imread('cube1.jpg') 3 dark = cv::imread('cube8.jpg') The first image is taken under outdoor conditions with bright sunlight, while the second is taken indoor with normal lighting conditions.  Two cubes under varying Illumination Figure 1 : Two images of the same cube taken under different illumination The RGB Color Space The RGB colorspace has the following properties  It is an additive colorspace where colors are obtained by a linear combination of Red, Green, and Blue values. The three channels are correlated by the amount of light hitting the surface. Let us split the two images into their R, G and B components and observe them to gain more insight into the color space.  BGR Components Figure 2 : Different Channels Blue ( B ), Green ( G ), Red ( R ) of the RGB color space shown separately Observations If you look at the blue channel, it can be seen that the blue and white pieces look similar in the second image under indoor lighting conditions but there is a clear difference in the first image. This kind of non-uniformity makes color based segmentation very difficult in this color space. Further, there is an overall difference between the values of the two images. Below we have summarized the inherent problems associated with the RGB Color space:  significant perceptual non-uniformity. mixing of chrominance ( Color related information ) and luminance ( Intensity related information ) data.  Official OpenCV Courses Start your exciting journey from an absolute Beginner to Mastery in AI, Computer Vision & Deep Learning! Learn More The LAB Color-Space The Lab color space has three components.  L – Lightness ( Intensity ). a – color component ranging from Green to Magenta. b – color component ranging from Blue to Yellow. The Lab color space is quite different from the RGB color space. In RGB color space the color information is separated into three channels but the same three channels also encode brightness information. On the other hand, in Lab color space, the L channel is independent of color information and encodes brightness only. The other two channels encode color.  It has the following properties.  Perceptually uniform color space which approximates how we perceive color. Independent of device ( capturing or displaying ). Used extensively in Adobe Photoshop. Is related to the RGB color space by a complex transformation equation. Let us see the two images in the Lab color space separated into three channels.  1 #python 2 brightLAB = cv2.cvtColor(bright, cv2.COLOR_BGR2LAB) 3 darkLAB = cv2.cvtColor(dark, cv2.COLOR_BGR2LAB) 1 //C++ 2 cv::cvtColor(bright, brightLAB, cv::COLOR_BGR2LAB); 3 cv::cvtColor(dark, darkLAB, cv::COLOR_BGR2LAB); LAB Components Figure 3 : The Lightness ( L ), and color components ( A, B ) in LAB Color space. Observations It is pretty clear from the figure that the change in illumination has mostly affected the L component. The A and B components which contain the color information did not undergo massive changes. The respective values of Green, Orange and Red ( which are the extremes of the A Component ) has not changed in the B Component and similarly the respective values of Blue and Yellow ( which are the extremes of the B Component ) has not changed in the A component. The YCrCb Color-Space The YCrCb color space is derived from the RGB color space and has the following three compoenents.  Y – Luminance or Luma component obtained from RGB after gamma correction. Cr = R – Y ( how far is the red component from Luma ). Cb = B – Y ( how far is the blue component from Luma ). This color space has the following properties.  Separates the luminance and chrominance components into different channels. Mostly used in compression ( of Cr and Cb components ) for TV Transmission. Device dependent. 1 #python 2 brightYCB = cv2.cvtColor(bright, cv2.COLOR_BGR2YCrCb) 3 darkYCB = cv2.cvtColor(dark, cv2.COLOR_BGR2YCrCb) The two images in YCrCb color space separated into its channels are shown below  1 //C++ 2 cv::cvtColor(bright, brightYCB, cv::COLOR_BGR2YCrCb); 3 cv::cvtColor(dark, darkYCB, cv::COLOR_BGR2YCrCb); YCrCb Components Figure 4 : Luma ( Y ), and Chroma ( Cr, Cb ) components in YCrCb color space. Observations Similar observations as LAB can be made for Intensity and color components with regard to Illumination changes. Perceptual difference between Red and Orange is less even in the outdoor image as compared to LAB. White has undergone change in all 3 components. The HSV Color Space The HSV color space has the following three components  H – Hue ( Dominant Wavelength ). S – Saturation ( Purity / shades of the color ). V – Value ( Intensity ). Let’s enumerate some of its properties.  Best thing is that it uses only one channel to describe color (H), making it very intuitive to specify color. Device dependent. The H, S and V components of the two images are shown below.  1 #python 2 brightHSV = cv2.cvtColor(bright, cv2.COLOR_BGR2HSV) 3 darkHSV = cv2.cvtColor(dark, cv2.COLOR_BGR2HSV) 1 //C++ 2 cv::cvtColor(bright, brightHSV, cv::COLOR_BGR2HSV); 3 cv::cvtColor(dark, darkHSV, cv::COLOR_BGR2HSV); HSV Components Figure 5 : Hue ( H ), Saturation ( S ) and Value ( V ) components in HSV color space. Observations The H Component is very similar in both the images which indicates the color information is intact even under illumination changes. The S component is also very similar in both images. The V Component captures the amount of light falling on it thus it changes due to illumination changes. There is drastic difference between the values of the red piece of outdoor and Indoor image. This is because Hue is represented as a circle and red is at the starting angle. So, it may take values between [300, 360] and again [0, 60]. How to use these color spaces for segmentation The simplest way Now that we have got some idea about the different color spaces, lets first try to use them to detect the Green color from the cube.  Step 1 : Get the color values for a particular color Find the approximate range of values of green color for each color space. For doing this, I’ve made an interactive GUI where you can check the values of all the color spaces for each pixel just by hovering the mouse on the image as shown below :  Pixel color in different color spaces Figure 6 : Demo showing a pixel and its value in different color spaces for the Outdoor image. Step 2 : Applying threshold for segmentation Extract all pixels from the image which have values close to that of the green pixel. We can take a range of +/- 40 for each color space and check how the results look like. We will use the opencv function inRange for finding the mask of green pixels and then use bitwise_and operation to get the green pixels from the image using the mask.  Also note that for converting one pixel to another color space, we first need to convert 1D array to a 3D array.  1 #python 2 bgr = [40, 158, 16] 3 thresh = 40 4   5 minBGR = np.array([bgr[0] - thresh, bgr[1] - thresh, bgr[2] - thresh]) 6 maxBGR = np.array([bgr[0] + thresh, bgr[1] + thresh, bgr[2] + thresh]) 7   8 maskBGR = cv2.inRange(bright,minBGR,maxBGR) 9 resultBGR = cv2.bitwise_and(bright, bright, mask = maskBGR) 10   11 #convert 1D array to 3D, then convert it to HSV and take the first element 12 # this will be same as shown in the above figure [65, 229, 158] 13 hsv = cv2.cvtColor( np.uint8([[bgr]] ), cv2.COLOR_BGR2HSV)[0][0] 14   15 minHSV = np.array([hsv[0] - thresh, hsv[1] - thresh, hsv[2] - thresh]) 16 maxHSV = np.array([hsv[0] + thresh, hsv[1] + thresh, hsv[2] + thresh]) 17   18 maskHSV = cv2.inRange(brightHSV, minHSV, maxHSV) 19 resultHSV = cv2.bitwise_and(brightHSV, brightHSV, mask = maskHSV) 20   21 #convert 1D array to 3D, then convert it to YCrCb and take the first element 22 ycb = cv2.cvtColor( np.uint8([[bgr]] ), cv2.COLOR_BGR2YCrCb)[0][0] 23   24 minYCB = np.array([ycb[0] - thresh, ycb[1] - thresh, ycb[2] - thresh]) 25 maxYCB = np.array([ycb[0] + thresh, ycb[1] + thresh, ycb[2] + thresh]) 26   27 maskYCB = cv2.inRange(brightYCB, minYCB, maxYCB) 28 resultYCB = cv2.bitwise_and(brightYCB, brightYCB, mask = maskYCB) 29   30 #convert 1D array to 3D, then convert it to LAB and take the first element 31 lab = cv2.cvtColor( np.uint8([[bgr]] ), cv2.COLOR_BGR2LAB)[0][0] 32   33 minLAB = np.array([lab[0] - thresh, lab[1] - thresh, lab[2] - thresh]) 34 maxLAB = np.array([lab[0] + thresh, lab[1] + thresh, lab[2] + thresh]) 35   36 maskLAB = cv2.inRange(brightLAB, minLAB, maxLAB) 37 resultLAB = cv2.bitwise_and(brightLAB, brightLAB, mask = maskLAB) 38   39 cv2.imshow("Result BGR", resultBGR) 40 cv2.imshow("Result HSV", resultHSV) 41 cv2.imshow("Result YCB", resultYCB) 42 cv2.imshow("Output LAB", resultLAB) 1 //C++ code 2 cv::Vec3b bgrPixel(40, 158, 16); 3 // Create Mat object from vector since cvtColor accepts a Mat object 4 Mat3b bgr (bgrPixel); 5   6 //Convert pixel values to other color spaces. 7 Mat3b hsv,ycb,lab; 8 cvtColor(bgr, ycb, COLOR_BGR2YCrCb); 9 cvtColor(bgr, hsv, COLOR_BGR2HSV); 10 cvtColor(bgr, lab, COLOR_BGR2Lab); 11 //Get back the vector from Mat 12 Vec3b hsvPixel(hsv.at<Vec3b>(0,0)); 13 Vec3b ycbPixel(ycb.at<Vec3b>(0,0)); 14 Vec3b labPixel(lab.at<Vec3b>(0,0)); 15   16 int thresh = 40; 17   18 cv::Scalar minBGR = cv::Scalar(bgrPixel.val[0] - thresh, bgrPixel.val[1] - thresh, bgrPixel.val[2] - thresh) 19 cv::Scalar maxBGR = cv::Scalar(bgrPixel.val[0] + thresh, bgrPixel.val[1] + thresh, bgrPixel.val[2] + thresh) 20   21 cv::Mat maskBGR, resultBGR; 22 cv::inRange(bright, minBGR, maxBGR, maskBGR); 23 cv::bitwise_and(bright, bright, resultBGR, maskBGR); 24   25 cv::Scalar minHSV = cv::Scalar(hsvPixel.val[0] - thresh, hsvPixel.val[1] - thresh, hsvPixel.val[2] - thresh) 26 cv::Scalar maxHSV = cv::Scalar(hsvPixel.val[0] + thresh, hsvPixel.val[1] + thresh, hsvPixel.val[2] + thresh) 27   28 cv::Mat maskHSV, resultHSV; 29 cv::inRange(brightHSV, minHSV, maxHSV, maskHSV); 30 cv::bitwise_and(brightHSV, brightHSV, resultHSV, maskHSV); 31   32 cv::Scalar minYCB = cv::Scalar(ycbPixel.val[0] - thresh, ycbPixel.val[1] - thresh, ycbPixel.val[2] - thresh) 33 cv::Scalar maxYCB = cv::Scalar(ycbPixel.val[0] + thresh, ycbPixel.val[1] + thresh, ycbPixel.val[2] + thresh) 34   35 cv::Mat maskYCB, resultYCB; 36 cv::inRange(brightYCB, minYCB, maxYCB, maskYCB); 37 cv::bitwise_and(brightYCB, brightYCB, resultYCB, maskYCB); 38   39 cv::Scalar minLAB = cv::Scalar(labPixel.val[0] - thresh, labPixel.val[1] - thresh, labPixel.val[2] - thresh) 40 cv::Scalar maxLAB = cv::Scalar(labPixel.val[0] + thresh, labPixel.val[1] + thresh, labPixel.val[2] + thresh) 41   42 cv::Mat maskLAB, resultLAB; 43 cv::inRange(brightLAB, minLAB, maxLAB, maskLAB); 44 cv::bitwise_and(brightLAB, brightLAB, resultLAB, maskLAB); 45   46 cv2::imshow("Result BGR", resultBGR) 47 cv2::imshow("Result HSV", resultHSV) 48 cv2::imshow("Result YCB", resultYCB) 49 cv2::imshow("Output LAB", resultLAB) Intial result for green color on Outdoor Image Figure 7 : RGB looks good, May be we are just wasting time here. Some more results So, it seems that the RGB and LAB are enough to detect the color and we dont need to think much. Lets see some more results.  initial result for green color on Indoor image Figure 8 : Applying the same threshold to the Indoor image fails to detect the green cubes in all the color spaces. So, the same threshold doesn’t work on the dark image.  Doing the same experiment to detect the yellow color gives the following results.  initial result for yellow color on Outdoor Image Figure 9 : Trying to detect the yellow pieces using the same technique and threshold ( for yellow ) obtained from bright image. HSV and YCrCb are still not performing well. Initial result for yellow color on Indoor image Figure 10 : Trying to detect the yellow pieces using the threshold obtained from bright cube. All color spaces fail again. But why is it that the results are so bad? This is because we had taken a wild guess of 40 for the threshold. I made another interactive demo where you can play with the values and try to find one that works for all the images. Check out the screenshot. But then there will be cases where another image comes and it doesn’t work again. We cannot just take some threshold by trial and error blindly. We are not using the power of the color spaces by doing so.  We need to have some methodical way to find the correct threshold values.  demo for playing around with the color spaces Figure 11 : Screenshot of the demo for playing around with the different values to detect the particular color in all color spaces for a given image. Some Data Analysis for a Better Solution Step 1 : Data Collection I have collected 10 images of the cube under varying illumination conditions and separately cropped every color to get 6 datasets for the 6 different colors. You can see how much change the colors undergo visually.  Changes in color by varying Illumination Figure : Showing changes in color due to varying Illumination conditions Step 2 : Compute the Density plot Check the distribution of a particular color say, blue or yellow in different color spaces. The density plot or the 2D Histogram gives an idea about the variations in values for a given color. For example, Ideally the blue channel of a blue colored image should always have the value of 255. But practically, it is distributed between 0 to 255.  I am showing the code only for BGR color space. You need to do it for all the color spaces.  We will first load all images of blue or yellow pieces. 1 #python 2 B = np.array([]) 3 G = np.array([]) 4 R = np.array([]) 5 im = cv2.imread(fi) Separate the channels and create and array for each channel by appending the values from each image. 1 #python 2 b = im[:,:,0] 3 b = b.reshape(b.shape[0]*b.shape[1]) 4 g = im[:,:,1] 5 g = g.reshape(g.shape[0]*g.shape[1]) 6 r = im[:,:,2] 7 r = r.reshape(r.shape[0]*r.shape[1]) 8 B = np.append(B,b) 9 G = np.append(G,g) 10 R = np.append(R,r) Use histogram plot from matplotlib to plot the 2D histogram 1 #python 2 nbins = 10 3 plt.hist2d(B, G, bins=nbins, norm=LogNorm()) 4 plt.xlabel('B') 5 plt.ylabel('G') 6 plt.xlim([0,255]) 7 plt.ylim([0,255]) Observations : Similar Illumination density plot similar lighting Figure 13 : Density Plot showing the variation of values in color channels for 2 similar bright images of blue color density plot similar lighting Figure 14 : Density Plot showing the variation of values in color channels for 2 similar bright images for the yellow color It can be seen that under similar lighting conditions all the plots are very compact. Some points to be noted are :  YCrCb and LAB are much more compact than others In HSV, there is variation in S direction ( color purity ) but very little variation in H direction. Observations : Different Illumination density plot different illumination Figure 15 : Density Plot showing the variation of values in color channels under varying illumination for the blue color density plot different illumination Figure 16 : Density Plot showing the variation of values in color channels under varying illumination for the yellow color As the Illumination changes by a large amount, we can see that :  Ideally, we want to work with a color space with the most compact / concentrated density plot for color channels. The density plots for RGB blow up drastically. This means that the variation in the values of the channels is very high and fixing a threshold is a big problem. Fixing a higher range will detect colors which are similar to the desired color ( False Positives ) and lower range will not detect the desired color in different lighting ( False Negatives ). In HSV, since only the H component contains information about the absolute color. Thus, it becomes my first choice of color space since I can tweak just one knob ( H ) to specify a color as compared to 2 knobs in YCrCb ( Cr and Cb ) and LAB ( A and B ). Comparing the plots of YCrCb and LAB shows a higher level of compactness in case of LAB. So, next best choice for me becomes the LAB color space. Final Results In this last section, I will show the results for detecting the blue and yellow piece by taking the threshold values from the density plots and applying it to the respective color spaces in the same way we did in the second section. We don’t have to worry about the Intensity component when we are working in HSV, YCrCb and LAB color space. We just need to specify the thresholds for the color components. The values I’ve taken for generating the results are shown in the figures.  demo image Figure 17 : Demo Image 1 results for demo image 1 Figure 18 : Results for Yellow color detection on Demo Image 1 results on demo image 1 Figure 19 : Results for Blue color detection on Demo Image 1 Demo Image Figure 20 : Demo Image 2 results on demo image 2 Figure 21 : Results for Yellow color detection on Demo Image 2 results on demo image 2 Figure 22 : Results for Blue color detection on Demo Image 2 Demo Image Figure 23 : Demo Image 3 results for demo image 3 Figure 24 : Results for Yellow color detection on Demo Image 3 results on demo image 3 Figure 25 : Results for Blue color detection on Demo Image 3 In the above results I have taken the values directly from the density plot. We can also chose to take the values which belong to to most dense region in the density plot which will help in getting tighter control of the color range. That will leave some holes and stray pixels which can be cleaned using Erosion and Dilation followed by Filtering.  Other Useful Applications of Color spaces Histogram equalization is generally done on grayscale images. However, you can perform equalization of color images by converting the RGB image to YCbCr and doing histogram equalization of only the Y channel. Color Transfer between two images by converting the images to Lab color space. Many filters in smartphone camera applications like Google camera or Instagram make use of these Color space transforms to create those cool effects! P.S : If you’re interested in solving a Rubik’s cube, you can refer to this step-by-step guide.  Subscribe & Download Code If you liked this article and would like to download code (C++ and Python) and example images used in this post, please click here. Alternately, sign up to receive a free Computer Vision Resource Guide. In our newsletter, we share OpenCV tutorials and examples written in C++/Python, and Computer Vision and Machine Learning algorithms and news.  Download Example Code   Prev Previous Annotating Images Using OpenCV Next Image Filtering Using Convolution in OpenCV Next We use cookies to ensure that we give you the best experience on our website. If you continue to use this site we will assume that you are happy with it. Privacy policyAccept Subscribe Now Your Name Your e-mail  SUBMIT Disclaimer All views expressed on this site are my own and do not represent the opinions of OpenCV.org or any entity whatsoever with which I have been, am now, or will be affiliated.        Getting Started Installation PyTorch Getting Started with OpenCV Keras & Tensorflow Resource Guide Course Opencv Courses CV4Faces (Old) Information Privacy Policy Terms and Conditions About LearnOpenCV In 2007, right after finishing my Ph.D., I co-founded TAAZ Inc. with my advisor Dr. David Kriegman and Kevin Barnes. The scalability, and robustness of our computer vision and machine learning algorithms have been put to rigorous test by more than 100M users who have tried our products.  Read More Copyright © 2022 – BIG VISION LLC

---
## [123456Marina/instagram-dio](https://github.com/123456Marina/instagram-dio)@[8c5bc8cd43...](https://github.com/123456Marina/instagram-dio/commit/8c5bc8cd438b1b600eb71c7a63154619cae06174)
#### Saturday 2022-01-08 22:46:54 by MY NAME IS MARINA MONTEIRO DA ROCHA DE SOUZA

Update README.md

MY PURPOSE IN RE-CREATING THE INSTAGRAM PAGE IS TO BRING IMAGES AND MESSAGES OF FAITH AND HOPE TO PEOPLE WHO HAVE LOST A LOVED ONE DURING THE CORONAVIRUS PANDEMIC. I WANT YOU TO KNOW THAT IN FAITH IN GOD THERE WILL ALWAYS BE A NEW TOMORROW...

---
## [123456Marina/instagram-dio](https://github.com/123456Marina/instagram-dio)@[dbb2ee9340...](https://github.com/123456Marina/instagram-dio/commit/dbb2ee9340aad63d0c8790e2b051bd50ae87f9aa)
#### Saturday 2022-01-08 23:31:08 by MY NAME IS MARINA MONTEIRO DA ROCHA DE SOUZA

Update README.md

MY PURPOSE IN RE-CREATING THE INSTAGRAM PAGE IS TO BRING IMAGES AND MESSAGES OF FAITH AND HOPE TO PEOPLE WHO HAVE LOST A LOVED ONE DURING THE CORONAVIRUS PANDEMIC. LIKE ME THAT I LOST MY FATHER AT THE AGE OF 70, ON THE 27TH OF MAY 2021... I WANT YOU TO KNOW THAT IN FAITH IN GOD THERE WILL ALWAYS BE A NEW TOMORROW...

---

# [<](2022-01-07.md) 2022-01-08 [>](2022-01-09.md)

